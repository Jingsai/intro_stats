[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Statistics: an integrated textbook and workbook using R",
    "section": "",
    "text": "Introduction\nWelcome to statistics!\nIf you want, you can also download this book as a PDF or EPUB file. Be aware that the print versions are missing some of the richer formatting of the online version. Besides, the recommended way to work through this material is to download the R notebook file (.Rmd) at the top of each chapter and work through it in RStudio."
  },
  {
    "objectID": "index.html#history-and-goals",
    "href": "index.html#history-and-goals",
    "title": "Introduction to Statistics: an integrated textbook and workbook using R",
    "section": "History and goals",
    "text": "History and goals\nIn 2015, a group of interdisciplinary faculty at Westminster University (then called Westminster College) in Salt Lake City, Utah, started a process that led to the creation of a new Data Science program. Preparatory to creating a more rigorous introductory statistics course using the statistical software R, I wrote a series of 22 modules that filled a gap in the R training literature. Most R training at the time was focused either on learning to program using R as a computer language, or using R to do sophisticated statistical analysis. We needed our students to use R as a tool for elementary statistical methods and we needed the learning curve to be as gentle as possible. I decided early on that to make the modules more useful, they needed to be structured more like an interactive textbook rather than just a series of lab exercises, and so I spent the summer of 2016 writing a free, open-source, self-contained, and nearly fully-featured introductory statistics textbook. The first sections of the newly-created DATA 220 were offered in Fall, 2016, using the materials I created.\nSince then, I have been revising and updating the modules a little every semester. At some point, however, it became clear that some big changes needed to happen:\n\nThe modules were more or less aligned with the OpenIntro book Introduction to Statistics with Randomization and Simulation (ISRS) by David Diez, Christopher Barr, and Mine Çetinkaya-Rundel. That book has now been supplanted by Introduction to Modern Statistics (IMS) by Mine Çetinkaya-Rundel and Johanna Hardin, also published through the OpenIntro project.\nThe initial materials were written mostly using a mix of base R tools, some tidyverse tools, and the amazing resources of the mosaic package. I wanted to convert everything to be more aligned with tidyverse packages now that they are mature, well-supported, and becoming a de facto standard for doing data analysis in R.\nThe initial choice of data sets that served as examples and exercises for students was guided by convenience. As I had only a short amount of time to write an entire textbook from scratch, I tended to grab the first data sets I could find that met the conditions needed for the statistical principles I was trying to illustrate. It has become clear in the last few years that the material will be more engaging with more interesting data sets. Ideally, we should use at least some data sets that speak to issues of social justice.\nMaking statistics more inclusive requires us to confront some ugly chapters in the development of the subject. Statistical principles are often named after people. (These are supposedly the people who “discovered” the principle, but keep in mind Stigler’s Law of Eponymy which states that no scientific discovery is truly named after its original discoverer. In a neat bit of self-referential irony, Stephen Stigler was not the first person to make this observation.) The beliefs of some of these people were problematic. For example, Francis Galton (famous for the concept of “regression to the mean”), Karl Pearson (of the Pearson correlation coefficient), and Ronald Fisher (famous for many things, including the P-value) were all deeply involved in the eugenics movement of the late 19th and early 20th century. The previous modules almost never referenced this important historical background and context. Additionally, it’s important to discuss ethics, whether that be issues of data provenance, data manipulation, choice of analytic techniques, framing conclusions, and many other topics.\n\nThe efforts of my revisions are here online. I’ve tried to address all the concerns mentioned above:\n\nThe chapter are arranged to align somewhat with IMS. There isn’t quite a one-to-one correspondence, but teachers who want to use the chapters of my book to supplement instruction from IMS, or vice versa, should be able to do so pretty easily. In the Appendix, I’ve included a concordance that shows how the books’ chapters match up, along with some notes that explain when one book does more or less than the other.\nThe book is now completely aligned with the tidyverse and other packages that are designed to integrate into the tidyverse. All plotting is done with ggplot2 and all data manipulation is done with dplyr, tidyr, and forcats. Tables are created using tabyl from the janitor package. Inference is taught using the cool tools in the infer package.\nI have made an effort to find more interesting data sets. It’s tremendously difficult to find data that is both fascinating on its merits and also meets the pedagogical requirements of an introductory statistics course. I would like to use even more data that addresses social justice issues. There’s some in the book now, and I plan to incorporate even more in the future as I come across data sets that are suitable.\nWhen statistical tools are introduced, I have tried to give a little historical context about their development if I can. I’ve also tried to frame every step of the inferential process as a decision-making process that requires not only analytical expertise, but also solid ethical grounding. Again, there’s a lot more I could do here, and my goal is to continue to develop more such discussion as I can in future revisions.\n\nNow, instead of a bunch of separate module files, all the material is gathered in one place as chapters of a book. In each chapter (starting with Chapter 2), students can download the chapter as an R notebook file, open it in RStudio, and work through the material."
  },
  {
    "objectID": "index.html#philosophy-and-pedagogy",
    "href": "index.html#philosophy-and-pedagogy",
    "title": "Introduction to Statistics: an integrated textbook and workbook using R",
    "section": "Philosophy and pedagogy",
    "text": "Philosophy and pedagogy\nTo understand my statistics teaching philosophy, it’s worth telling you a little about my background in statistics.\nAt the risk of undermining my own credibility, I’d like to tell you about the first statistics class I took. In the mid-2000s, I was working on my Ph.D. at the University of California, San Diego, studying geometric topology. To make a little extra money and get some teaching experience under my belt, I started teaching night and summer classes at Miramar College, a local community college in the San Diego Community College District. I had been there for several semesters, mostly teaching pre-calculus, calculus, and other lower-division math classes. One day, I got a call from my department chair with my assignment for the upcoming semester. I was scheduled to teach intro stats. I was about to respond, “Oh, I’ve never taken a stats class before.” But remembering this was the way I earned money to be able to live in expensive San Diego County, I said, “Sounds great. By the way, do you happen to have an extra copy of the textbook we’ll be using?”\nYes, the first statistics class I took was the one I taught. Not ideal, I know.\nI was lucky to start teaching with Intro Stats by De Veaux, Velleman, and Bock, a book that was incredibly well-written and included a lot of resources for teachers like me. (I learned quickly that I wasn’t the only math professor in the world who got thrown into teaching statistics classes with little to no training.) I got my full-time appointment at Westminster in 2008 and continued to teach intro stats classes for many years to follow. As I mentioned earlier, we started the Data Science program at Westminster in 2016 and moved everything from our earlier hodgepodge of calculators, spreadsheets, and SPSS, over to R.\nEventually, I got interested in Bayesian statistics and read everything I could get my hands on. I became convinced that Bayesian statistics is the “right” way to do statistical analysis. I started teaching special topics courses in Bayesian Data Analysis and working with students on research projects that involved Bayesian methods. If it were up to me, every introductory statistics class in the world would be taught using Bayesian methods. I know that sounds like a strong statement. (And I put it in boldface, so it looks even stronger.) But I truly believe that in an alternate universe where Fisher and his disciples didn’t “win” the stats wars of the 20th century (and perhaps one in which computing power got a little more advanced a little earlier in the development of statistics), we would all be Bayesians. Bayesian thinking is far more intuitive and more closely aligned with our intuitions about probabilities and uncertainty.\nUnfortunately, our current universe timeline didn’t play out that way. So we are left with frequentism. It’s not that I necessarily object to frequentist tools. All tools are just tools, after all. However, the standard form of frequentist inference, with its null hypothesis significance testing, P-values, and confidence intervals, can be confusing. It’s bad enough that professional researchers struggle with them. We teach undergraduate students in introductory classes.\nOkay, so we are stuck not in the world we want, but the world we’ve got. At my institution and most others, intro stats is a service course that trains far more people who are outside the fields of mathematics and statistics. In that world, students will go on to careers where they interact with research that reports p-values and confidence intervals.\nSo what’s the best we can do for our students, given that limitation? We need to be laser-focused on teaching the frequentist logic of inference the best we can. I want student to see P-values in papers and know how to interpret those P-values correctly. I want students to understand what a confidence intervals tells them—and even more importantly, what it does not tell them. I want students to respect the severe limitations inherent in tests of significance. If we’re going to train frequentists, the least we can do is help them become good frequentists.\nOne source of inspiration for good statistical pedagogy comes from the Guidelines for Assessment and Instruction in Statistics Education (GAISE), a set of recommendations made by experienced stats educators and endorsed by the American Statistical Association. Their college guidelines are as follows:\n\nTeach statistical thinking.\n\n\nTeach statistics as an investigative process of problem-solving and decision-making.\nGive students experience with multivariable thinking.\n\n\nFocus on conceptual understanding.\nIntegrate real data with a context and purpose.\nFoster active learning.\nUse technology to explore concepts and analyze data.\nUse assessments to improve and evaluate student learning.\n\nIn every element of this book, I’ve tried to follow these guidelines:\n\nThe first part of the book is an extensive guide for exploratory data analysis. The rest of the book is about inference in the context of specific research questions that are answered using statistical tools. While multivariable thinking is a little harder to do in an intro stats class, I take the opportunity whenever possible to use graphs to explore more variables than we can handle with intro stats inferential techniques. I point out the the simple analyses taught in this class are only the first step in more comprehensive analyses that incorporate more information and control for confounders. I emphasize that students can continue their statistical growth by enrolling in more advanced stats classes.\nI often tell students that if they forget everything else from their stats class, the one think I want them to be able to do is interpret a P-value correctly. It’s not intuitive, so it takes an entire semester to set up the idea of a sampling distribution and explain over and over again how the P-value relates to it. In this book, I try to reinforce the logic of inference until the students know it almost instinctively. A huge pedagogical advantage is derived by using randomization and simulation to keep students from getting lost in the clouds of theoretical probability distributions. But they also need to know about the latter too. Every hypothesis test is presented both ways, a task made easy when using the infer package.\nThis is the thing I struggle with the most. Finding good data is hard. Over the years, I’ve found a few data sets I really like, but my goal is to continue to revise the book to incorporate more interesting data, especially data that serves to highlight issues of social justice.\nBack when I wrote the first set of modules that eventually became this book, the goal was to create assignments that merged content with activities so that students would be engaged in active learning. When these chapters are used in the classroom, students can collaborate with each other and with their professor. They learn by doing.\nUnlike most books out there, this book does not try to be agnostic about technology. This book is about doing statistics in R.\nThis one I’ll leave in the capable hands of the professors who use these materials. The chapter assignments should be completed and submitted, and that is one form of assessment. But I also believe in augmenting this material with other forms of assessment that may include supplemental assignments, open-ended data exploration, quizzes and tests, projects, etc."
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "Introduction to Statistics: an integrated textbook and workbook using R",
    "section": "Course structure",
    "text": "Course structure\nAs explained above, this book is meant to be a workbook that students complete as they’re reading.\nAt Westminster University, we host Posit Workbench on a server that is connected to our single sign-on (SSO) systems so that students can access RStudio through a browser using their campus online usernames and passwords. If you have the ability to convince your IT folks to get such a server up and running, it’s highly worth it. Rather than spending the first day of class troubleshooting while students try to install software on their machines, you can just have them log in and get started right away. Campus admins install packages and tweak settings to make sure all students have a standardized interface and consistent experience.\nIf you don’t have that luxury, you will need to have students download and install both R and RStudio. The installation processes for both pieces of software are very easy and straightforward for the majority of students. The book chapters here assume that the necessary packages are installed already, so if your students are running R on their own machines, they will need to use install.packages at the beginning of some of the chapters for any new packages that are introduced. (They are mentioned at the beginning of each chapter with instructions for installing them.)\nChapter 1 is fully online and introduces R and RStudio very gently using only commands at the Console. By the end of Chapter 1, they will have created a project called intro_stats in RStudio that should be used all semester to organize their work. There is a reminder at the beginning of all subsequent chapter to make sure they are in that project before starting to do any work. (Generally, there is no reason they will exit the project, but some students get curious and click on stuff.)\nIn Chapter 2, students are taught to click a link to download an R Notebook file (.Rmd). I have found that students struggle initially to get this file to the right place. If students are using RStudio Workbench online, they will need to use the “Upload” button in the Files tab in RStudio to get the file from their Downloads folder (or wherever they tell their machine to put downloaded files from the internet) into RStudio. If students are using R on their own machines, they will need to move the file from their Downloads folder into their project directory. There are some students who have never had to move files around on their computers, so this is a task that might require some guidance from classmates, TAs, or the professor. The location of the project directory and the downloaded files can vary from one machine to the next. They will have to use something like File Explorer for Windows or the Finder for MacOS, so there isn’t a single set of instructions that will get all students’ files successfully in the right place. Once the file is in the correct location, students can just click on it to open it in RStudio and start reading. Chapter 2 is all about using R Notebooks: markdown syntax, R code chunks, and inline code.\nBy Chapter 3, a rhythm is established that students will start to get used to:\n\nOpen the book online and open RStudio.\nInstall any packages in RStudio that are new to that chapter. (Not necessary for those using RStudio Workbench in a browser.)\nCheck to make sure they’re are in the intro_stats project.\nClick the link online to download the R Notebook file.\nMove the R Notebook file from the Downloads folder to the project directory.\nOpen up the R Notebook file.\nRestart R and Run All Chunks.\nStart reading and working.\n\nChapters 3 and 4 focus on exploratory data analysis for categorical and numerical data, respectively.\nChapter 5 is a primer on data manipulation using dplyr.\nChapters 6 and 7 cover correlation and regression. This “early regression” approach mirrors the IMS text. (IMS eventually circles back to hypothesis testing for regression, but this book does not. That’s a topic that is covered extensively in most second-semester stats classes.)\nChapters 8–11 are crucial for building the logical foundations for inference. The idea of a sampling distribution under the assumption of a null hypothesis is built up slowly and intuitively through randomization and simulation. By the end of Chapter 11, students will be fully introduced to the structure of a hypothesis test, and hopefully will have experienced the first sparks of intuition about why it “works.” All inference in this book is conducted using a “rubric” approach—basically, the steps are broken down into bite-sized pieces and students are expected to work through each step of the rubric every time they run a test. (The rubric steps are shown in the Appendix.)\nChapter 12 introduces a few more steps to the rubric for confidence intervals. As we are still using randomization to motivate inference, confidence intervals are calculated using the bootstrap approach for now.\nOnce students have developed a conceptual intuition for sampling distributions using simulation, we can introduce probability models as well. Chapter 13 introduces normal models and Chapter 14 explains why they are often appropriate for modeling sampling distributions.\nThe final chapters of the book (Chapters 15–22) are simply applications of inference in specific data settings: inference for one (Ch. 15) and two (Ch. 16) proportions, Chi-square tests for goodness-of-fit (Ch. 17) and independence (Ch. 18), inference for one mean (Ch. 19), paired data (Ch. 20), and two independent means (Ch. 21), and finally ANOVA (Ch. 22). Along the way, students learn about the chi-square, Student t, and F distributions. Although the last part of the book follows a fairly traditional parametric approach, every chapter still includes randomization and simulation to some degree so that students don’t lose track of the intuition behind sampling distributions under the assumption of a null hypothesis."
  },
  {
    "objectID": "index.html#onward-and-upward",
    "href": "index.html#onward-and-upward",
    "title": "Introduction to Statistics: an integrated textbook and workbook using R",
    "section": "Onward and upward",
    "text": "Onward and upward\nI hope you enjoy the textbook. You can provide feedback two ways:\n\nThe preferred method is to file an issue on the Github page: https://github.com/VectorPosse/intro_stats/issues\nAlternatively, send me an email: sraleigh@westminsteru.edu"
  },
  {
    "objectID": "01-intro_to_r.html#introduction",
    "href": "01-intro_to_r.html#introduction",
    "title": "1  Introduction to R",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nWelcome to R! This chapter will walk you through everything you need to know to get started using R.\nAs you go through this chapter (and all future chapters), please read slowly and carefully, and pay attention to detail. Many steps depend on the correct execution of all previous steps, so reading quickly and casually might come back to bite you later."
  },
  {
    "objectID": "01-intro_to_r.html#what-is-r",
    "href": "01-intro_to_r.html#what-is-r",
    "title": "1  Introduction to R",
    "section": "1.2 What is R?",
    "text": "1.2 What is R?\nR is a programming language specifically designed for doing statistics. Don’t be intimidated by the word “programming” though. The goal of this course is not to make you a computer programmer. To use R to do statistics, you don’t need know anything about programming at all. Every chapter throughout the whole course will give you examples of the commands you need to use. All you have to do is use those example commands as templates and make the necessary changes to adapt them to the data you’re trying to analyze.\nThe greatest thing about R is that it is free and open source. This means that you can download it and use it for free, and also that you can inspect and modify the source code for all R functions. This kind of transparency does not exist in commercial software. The net result is a robust, secure, widely-used language with literally tens of thousands of contributions from R users all over the world.\nR has also become a standard tool for statistical analysis, from academia to industry to government. Although some commercial packages are still widely used, many practitioners are switching to R due to its cost (free!) and relative ease of use. After this course, you will be able to list some R experience on your résumé and your future employer will value this. It might even help get you a job!"
  },
  {
    "objectID": "01-intro_to_r.html#rstudio",
    "href": "01-intro_to_r.html#rstudio",
    "title": "1  Introduction to R",
    "section": "1.3 RStudio",
    "text": "1.3 RStudio\nRStudio is an “Integrated Development Environment,” or IDE for short. An IDE is a tool for working with a programming language that is fancier than just a simple text editor. Most IDEs give you shortcuts, menus, debugging facilities, syntax highlighting, and other things to make your life as easy as possible.\nOpen RStudio so we can explore some of the areas you’ll be using in the future.\nOn the left side of your screen, you should see a big pane called the “Console”. There will be some startup text there, and below that, you should see a “command prompt”: the symbol “&gt;” followed by a blinking cursor. (If the cursor is not blinking, that means that the focus is in another pane. Click anywhere in the Console and the cursor should start blinking again.)\nA command prompt can be one of the more intimidating things about starting to use R. It’s just sitting there waiting for you to do something. Unlike other programs where you run commands from menus, R requires you to know what you need to type to make it work.\nWe’ll return to the Console in a moment.\nNext, look at the upper-right corner of the screen. There are at least three tabs in this pane starting with “Environment”, “History”, and “Connections”. The “Environment” (also called the “Global Environment”) keeps track of things you define while working with R. There’s nothing to see there yet because we haven’t defined anything! The “History” tab will likewise be empty; again, we haven’t done anything yet. We won’t use the “Connections” tab in this course. (Depending on the version of RStudio you are using and its configuration, you may see additional tabs, but we won’t need them for this course.)\nNow look at the lower-right corner of the screen. There are likely five tabs here: “Files”, “Plots”, “Packages”, “Help”, and “Viewer”. The “Files” tab will eventually contain the files you upload or create. “Plots” will show you the result of commands that produce graphs and charts. “Packages” will be explained later. “Help” is precisely what it sounds like; this will be a very useful place for you to get to know. We will never use the “Viewer” tab, so don’t worry about it."
  },
  {
    "objectID": "01-intro_to_r.html#try-something",
    "href": "01-intro_to_r.html#try-something",
    "title": "1  Introduction to R",
    "section": "1.4 Try something!",
    "text": "1.4 Try something!\nSo let’s do something in R! Go back to the Console and at the command prompt (the “&gt;” symbol with the blinking cursor), type\n\n1+1\n\nand hit Enter.\nCongratulations! You just ran your first command in R. It’s all downhill from here. R really is nothing more than a glorified calculator.\nOkay, let’s do something slightly more sophisticated. It’s important to note that R is case-sensitive, which means that lowercase letters and uppercase letters are treated differently. Type the following, making sure you use a lowercase c, and hit Enter:\n\nx &lt;- c(1, 3, 4, 7, 9)\n\nYou have just created a “vector”. When we use the letter c and enclose a list of things in parentheses, we tell R to “combine” those elements. So, a vector is just a collection of data. The little arrow &lt;- says to take what’s on the right and assign it to the symbol on the left. The vector x is now saved in memory. As long as you don’t terminate your current R session, this vector is available to you.\nCheck out the “Environment” pane now. You should see the vector x that you just created, along with some information about it. Next to x, it says num, which means your vector has numerical data. Then it says [1:5] which indicates that there are five elements in the vector x.\nAt the command prompt in the Console, type\n\nx\n\nand hit Enter. Yup, x is there. R knows what it is. You may be wondering about the [1] that appears at the beginning of the line. To see what that means, try typing this (and hit Enter—at some point here I’m going to stop reminding you to hit Enter after everything you type):\n\ny &lt;- letters\n\nR is clever, so the alphabet is built in under the name letters.\nType\n\ny\n\nNow can you see what the [1] meant above? Assuming the letters spilled onto more than one line of the Console, you should see a number in brackets at the beginning of each line telling you the numerical position of the first entry in each new line.\nSince we’ve done a few things, check out the “Global Environment” in the upper-right corner. You should see the two objects we’ve defined thus far, x and y. Now click on the “History” tab. Here you have all the commands you have run so far. This can be handy if you need to go back and re-run an earlier command, or if you want to modify an earlier command and it’s easier to edit it slightly than type it all over again. To get an older command back into the Console, either double-click on it, or select it and click the “To Console” button at the top of the pane.\nWhen we want to re-use an old command, it has usually not been that long since we last used it. In this case, there is an even more handy trick. Click in the Console so that the cursor is blinking at the blank command prompt. Now hit the up arrow on your keyboard. Do it again. Now hit the down arrow once or twice. This is a great way to access the most recently used commands from your command history.\nLet’s do something with x. Type\n\nsum(x)\n\nI bet you figured out what just happened.\nNow try\n\nmean(x)\n\nWhat if we wanted to save the mean of those five numbers for use later? We can assign the result to another variable! Type the following and observe the effect in the Environment.\n\nm &lt;- mean(x)\n\nIt makes no difference what letter or combination of letters we use to name our variables. For example,\n\nmean_x &lt;- mean(x)\n\njust saves the mean to a differently named variable. In general, variable names can be any combination of characters that are letters, numbers, underscore symbols (_), and dots (.). (In this course, we will prefer underscores over dots.) You cannot use spaces or any other special character in the names of variables.1 You should avoid variable names that are the same words as predefined R functions; for example, we should not type mean &lt;- mean(x)."
  },
  {
    "objectID": "01-intro_to_r.html#load-packages",
    "href": "01-intro_to_r.html#load-packages",
    "title": "1  Introduction to R",
    "section": "1.5 Load packages",
    "text": "1.5 Load packages\nPackages are collections of commands, functions, and sometimes data that people all over the world write and maintain. These packages extend the capabilities of R and add useful tools. For example, we would like to use the palmerpenguins package because it includes an interesting data set on penguins.\nIf you have installed R and RStudio on your own machine instead of accessing RStudio Workbench through a browser, you’ll need to type install.packages(\"palmerpenguins\") if you’ve never used the palmerpenguins package before. If you are using RStudio Workbench through a browser, you may not be able to install packages because you may not have admin privileges. If you need a package that is not installed, contact the person who administers your server.\nThe data set is called penguins. Let’s see what happens when we try to access this data set without loading the package that contains it. Try typing this:\n\npenguins\n\nYou should have received an error. That makes sense because R doesn’t know anything about a data set called penguins.\nNow—assuming you have the palmerpenguins package installed—type this at the command prompt:\n\nlibrary(palmerpenguins)\n\nIt didn’t look like anything happened. However, in the background, all the stuff in the palmerpenguins package became available to use.\nLet’s test that claim. Hit the up arrow twice and get back to where you see this at the Console (or you can manually re-type it, but that’s no fun!):\n\npenguins\n\nNow R knows about the penguins data, so the last command printed some of it to the Console.\nGo look at the “Packages” tab in the pane in the lower-right corner of the screen. Scroll down a little until you get to the “P”s. You should be able to find the palmerpenguins package. You’ll also notice a check mark by it, indicating that this package is loaded into your current R session.\nYou must use the library command in every new R session in which you want to use a package.2 If you terminate your R session, R forgets about the package. If you are ever in a situation where you are trying to use a command and you know you’re typing it correctly, but you’re still getting an error, check to see if the package containing that command has been loaded with library. (Many R commands are “base R” commands, meaning they come with R and no special package is required to access them. The set of letters you used above is one such example.)"
  },
  {
    "objectID": "01-intro_to_r.html#getting-help",
    "href": "01-intro_to_r.html#getting-help",
    "title": "1  Introduction to R",
    "section": "1.6 Getting help",
    "text": "1.6 Getting help\nThere are four important ways to get help with R. The first is the obvious “Help” tab in the lower-right pane on your screen. Click on that tab now. In the search bar at the right, type penguins and hit Enter. Take a few minutes to read the help file.\nHelp files are only as good as their authors. Fortunately, most package developers are conscientious enough to write decent help files. But don’t be surprised if the help file doesn’t quite tell you what you want to know. And for highly technical R functions, sometimes the help files are downright inscrutable. Try looking at the help file for the grep function. Can you honestly say you have any idea what this command does or how you might use it? Over time, as you become more knowledgeable about how R works, these help files get less mysterious.\nThe second way of getting help is from the Console. Go to the Console and type\n\n?letters\n\nThe question mark tells R you need help with the R command letters. This will bring up the help file in the same Help pane you were looking at before.\nSometimes, you don’t know exactly what the name of the command is. For example, suppose we misremembered the name and thought it was letter instead of letters. Try typing this:\n\n?letter\n\nYou should have received an error because there is no command called letter. Try this instead:\n\n??letter\n\nand scroll down a bit in the Help pane. Two question marks tell R not to be too picky about the spelling. This will bring up a whole bunch of possibilities in the Help pane, representing R’s best guess as to what you might be searching for. (In this case, it’s not easy to find. You’d have to know that the help file for letters appeared on a help page called base::Constants.)\nThe fourth way to get help—and often the most useful way—is to use your best friend Google. You don’t want to just search for “R”. (That’s the downside of using a single letter of the alphabet for the name of a programming language.) However, if you type “R __________” where you fill in the blank with the topic of interest, Google usually does a pretty good job sending you to relevant pages. Within the first few hits, in fact, you’ll often see an online copy of the same help file you see in R. Frequently, the next few hits lead to StackOverflow where very knowledgeable people post very helpful responses to common questions.\nUse Google to find out how to take the square root of a number in R. Test out your newly-discovered function on a few numbers to make sure it works."
  },
  {
    "objectID": "01-intro_to_r.html#understanding-the-data",
    "href": "01-intro_to_r.html#understanding-the-data",
    "title": "1  Introduction to R",
    "section": "1.7 Understanding the data",
    "text": "1.7 Understanding the data\nLet’s go back to the penguins data contained in the penguins data set from the palmerpenguins package.\nThe first thing we do to understand a data set is to read the help file on it. (We’ve already done this for the penguins data.) Of course, this only works for data files that come with R or with a package that can be loaded into R. If you are using R to analyze your own data, presumably you don’t need a help file. And if you’re analyzing data from another source, you’ll have to go to that source to find out about the data.\nWhen you read the help file for penguins, you may have noticed that it described the “Format” as being “A tibble with 344 rows and 8 variables.” What is a “tibble”?\nThe word “tibble” is an R-specific term that describes data organized in a specific way. A more common term is “data frame” (or sometimes “data table”). The idea is that in a data frame, the rows and the columns have very specific interpretations.\nEach row of a data frame represents a single object or observation. So in the penguins data, each row represents a penguin. If you have survey data, each row will usually represent a single person. But an “object” can be anything about which we collect data. State-level data might have 50 rows and each row represents an entire state.\nEach column of a data frame represents a variable, which is a property, attribute, or measurement made about the objects in the data. For example, the help file mentions that various pieces of information are recorded about each penguin, like species, bill length, flipper length, boy mass, sex, and so on. These are examples of variables. In a survey, for example, the variables will likely be the responses to individual questions.\nWe will use the terms tibble and data frame interchangeably in this course. They are not quite synonyms: tibbles are R-specific implementations of data frames, the latter being a more general term that applies in all statistical contexts. Nevertheless, there are no situations (at least not encountered in this course) where it makes any difference if a data set is called a tibble or a data frame.\nWe can also look at the data frame in “spreadsheet” form. Type\n\nView(penguins)\n\n(Be sure you’re using an upper-case “V” in View.) A new pane should open up in the upper-left corner of the screen. In that pane, the penguins data appears in a grid format, like a spreadsheet. The observations (individual penguins) are the rows and the variables (attributes and measurements about the penguins) are the columns. This will also let you sort each column by clicking on the arrows next to the variable name across the top.\nSometimes, we just need a little peek at the data. Try this to print just a few rows of data to the Console:\n\nhead(penguins)\n\nWe can customize this by specifying the number of rows to print. (Don’t forget about the up arrow trick!)\n\nhead(penguins, n = 10)\n\nThe tail command does something similar.\n\ntail(penguins)\n\nWhen we’re working with HTML documents like this one, it’s usually not necessary to use View, head, or tail because the HTML format will print the data frame a lot more neatly than it did in the Console. You do not need to type the following code; just look below it for the table that appears.\n\n\nWarning: package 'palmerpenguins' was built under R version 4.3.1\n\n\n\npenguins\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nYou can scroll through the rows by using the numbers at the bottom or the “Next” button. You can scroll through the variables by clicked the little black arrow pointed to the right in the upper-right corner. The only thing you can’t do here that you can do with View is sort the columns.\nWe want to understand the “structure” of our data. For this, we use the str command. Try it:\n\nstr(penguins)\n\nThis tells us several important things. First it says that we are looking at a tibble with 344 observations of 8 variables. We can isolate those pieces of information separately as well, if needed:\n\nNROW(penguins)\n\n\nNCOL(penguins)\n\nThese give you the number of rows and columns, respectively.\nThe str command also tells us about each of the variables in our data set. We’ll talk about these later.\nWe need to be able to summarize variables in the data set. The summary command is one way to do it:\n\nsummary(penguins)\n\nYou may not recognize terms like “Median” or “1st Qu.” or “3rd Qu.” yet. Nevertheless, you can see why this summary could come in handy."
  },
  {
    "objectID": "01-intro_to_r.html#understanding-the-variables",
    "href": "01-intro_to_r.html#understanding-the-variables",
    "title": "1  Introduction to R",
    "section": "1.8 Understanding the variables",
    "text": "1.8 Understanding the variables\nWhen we want to look at only one variable at a time, we use the dollar sign to grab it. Try this:\n\npenguins$body_mass_g\n\nThis will list the entire body_mass_g column, in other words, the body masses (in grams) of all the penguins in this particular study. If we only want to see the first few, we can use head like before.\n\nhead(penguins$body_mass_g)\n\nIf we want the structure of the variable body_mass_g, we do this:\n\nstr(penguins$body_mass_g)\n\nNotice the letters int at the beginning of the line. That stands for “integer” which is another word for whole number. In other words, the penguins’ body masses all appear in this data set as whole numbers. There are other data types you’ll see in the future:\n\nnum: This is for general numerical data (which can be integers as well as having decimal parts).\nchr: This means “character”, used for character strings, which can be any sequence of letters or numbers. For example, if the researcher recorded some notes for each penguin, these notes would be recorded in a character variable.\nfactor: This is for categorical data, which is data that groups observations together into categories. For example, species is categorical. These are generally recorded like character strings, but factor variables have more structure because they take on a limited number of possible values corresponding to a generally small number of categories. We’ll learn a lot more about factor variables in future chapters.\n\nThere are other data types, but the ones above are by far the most common that you’ll encounter on a regular basis.\nIf we want to summarize only the variable body_mass_g, we can do this:\n\nsummary(penguins$body_mass_g)\n\nWhile executing the commands above, you may have noticed entries listed as NA. These are “missing” values. It is worth paying attention to missing values and thinking carefully about why they might be missing. For now, just make a mental note that NA is the code R uses for data that is missing. (This would be the same as a blank cell in a spreadsheet.)"
  },
  {
    "objectID": "01-intro_to_r.html#projects",
    "href": "01-intro_to_r.html#projects",
    "title": "1  Introduction to R",
    "section": "1.9 Projects",
    "text": "1.9 Projects\nUsing files in R requires you to be organized. R uses what’s called a “working directory” to find the files it needs. Therefore, you can’t just put files any old place and expect R to be able to find them.\nOne way of ensuring that files are all located where R can find them is to organize your work into projects. Look in the far upper-right corner of the RStudio screen. You should see some text that says Project: (None). This means we are not currently in a project. We’re going to create a new project in preparation for the next chapter on using R Markdown.\nOpen the drop-down menu here and select New Project. When the dialog box opens, select New Directory, then New Project.\nYou’ll need to give your project a name. In general, this should be a descriptive name—one that could still remind you in several years what the project was about. The only thing to remember is that project names and file names should not have any spaces in them. In fact, you should avoid other kinds of special characters as well, like commas, number signs, etc. Stick to letters and numerals and you should be just fine. If you want a multi-word project name or file name, I recommend using underscores. R will allow you to name projects with spaces and modern operating systems are set up to handle file names with spaces, but there are certain things that either don’t work at all or require awkward workarounds when file names have spaces. In this case, let’s type intro_stats for the “Directory name”. Leave everything else alone and click Create Project.\nYou will see the screen refresh and R will restart.\nYou will see a new file called intro_stats.Rproj in the Files pane, but you should never touch that file. It’s just for RStudio to keep track of your project details.\nIf everything works the way it should, creating a new project will create a new folder, put you in that folder, and automatically make it your working directory.\nAny additional files you need for your project should be placed in this directory. In all future chapters, the first thing you will do is download the chapter file from the book website and place it here in your project folder. If you have installed R and RStudio on your own machine, you’ll need to navigate your system to find the downloaded file and move or copy it to your project working directory. (This is done most easily using File Explorer in Windows and the Finder in MacOS.) If you are using RStudio Workbench through a web browser, you’ll need to upload it to your project folder using the “Upload” button in the Files tab."
  },
  {
    "objectID": "01-intro_to_r.html#conclusion",
    "href": "01-intro_to_r.html#conclusion",
    "title": "1  Introduction to R",
    "section": "1.10 Conclusion",
    "text": "1.10 Conclusion\nIt is often said that there is a steep learning curve when learning R. This is true to some extent. R is harder to use at first than other types of software. Nevertheless, in this course, we will work hard to ease you over that first hurdle and get you moving relatively quickly. Don’t get frustrated and don’t give up! Learning R is worth the effort you put in. Eventually, you’ll grow to appreciate the power and flexibility of R for accomplishing a huge variety of statistical tasks.\nOnward and upward!"
  },
  {
    "objectID": "01-intro_to_r.html#footnotes",
    "href": "01-intro_to_r.html#footnotes",
    "title": "1  Introduction to R",
    "section": "",
    "text": "The official spec says that a valid variable name “consists of letters, numbers and the dot or underline characters and starts with a letter or the dot not followed by a number.”↩︎\nIf you have installed R and RStudio on your own machine instead of accessing RStudio Workbench through a browser, you’ll want to know that install.packages only has to be run once, the first time you want to install a package. If you’re using RStudio Workbench, you don’t even need to type that because your server admin will have already done it for you.↩︎"
  },
  {
    "objectID": "02-using_r_markdown-web.html#rmark-intro",
    "href": "02-using_r_markdown-web.html#rmark-intro",
    "title": "2  Using R Markdown",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nThis chapter will teach you how to use R Markdown to create quality documents that incorporate text and R code seamlessly.\nFirst, though, let’s make sure you are set up in your project in RStudio.\n\n2.1.1 Are you in your project?\nIf you followed the directions at the end of the last chapter, you should have created a project called intro_stats. Let’s make sure you’re in that project.\nLook at the upper right corner of the RStudio screen. Does it say intro_stats? If so, congratulations! You are in your project.\nIf you’re not in the intro_stats project, click on whatever it does say in the upper right corner (probably Project: (None)). You can click “Open Project” but it’s likely that the intro_stats project appears in the drop-down menu in your list of recently accessed projects. So click on the project intro_stats.\n\n\n2.1.2 Install new packages\nIf you are using RStudio Workbench, you do not need to install any packages. (Any packages you need should already be installed by the server administrators.)\nIf you are using R and RStudio on your own machine instead of accessing RStudio Workbench through a browser, you’ll need to type the following commands at the Console:\ninstall.packages(\"rmarkdown\")\ninstall.packages(\"tidyverse\")\n\n\n2.1.3 Download the R notebook file\nYou need to download this chapter as an R Notebook (.Rmd) file. Please click the following link to do so:\nhttps://vectorposse.github.io/intro_stats/chapter_downloads/02-using_r_markdown.Rmd\nThe file is now likely sitting in a Downloads folder on your machine (or wherever you have set up for web files to download). If you have installed R and RStudio on your own machine, you will need to move the file from your Downloads folder into the intro_stats project directory you created at the end of the last chapter. (Again, if you haven’t created the intro_stats project, please go back to Chapter 1 and follow the directions for doing that.) Moving files around is most easily done using File Explorer in Windows or the Finder in MacOS. If you are logged into RStudio Workbench instead, go to the Files tab and click the “Upload” button. From there, leave the first box alone (“Target directory”). Click the “Choose File” button and navigate to the folder on your machine containing the file 02_using-r-markdown.Rmd. Select that file and click “OK” to upload the file. Then you will be able to open the file in RStudio simply by clicking on it.\nIf you are reading this text online in the browser, be aware that there are several instructions below that won’t make any sense because you’re not looking at the plain text file with all the code in it. Much of the material in this book can be read and enjoyed online, but the real learning comes from downloading the chapter files (starting with Chapter 2—this one) and working through them in RStudio."
  },
  {
    "objectID": "02-using_r_markdown-web.html#rmark-whatis",
    "href": "02-using_r_markdown-web.html#rmark-whatis",
    "title": "2  Using R Markdown",
    "section": "2.2 What is R Markdown?",
    "text": "2.2 What is R Markdown?\nThe first question should really be, “What is Markdown?”\nMarkdown is a way of using plain text with simple characters to indicate formatting choices in a document. For example, in a Markdown file, one can make headers by using number signs (or hashtags as the kids are calling them these days1). The notebook file itself is just a plain text file. To see the formatting, the file has to be converted to HTML, which is the format used for web pages. (This process is described below.)\nR Markdown is a special version of Markdown that also allows you to include R code alongside the text. Here’s an example of a “code chunk”:\n\n1 + 1\n\n[1] 2\n\n\nClick the little dark green, right-facing arrow in the upper-right corner of the code chunk. (The icon I’m referring to is next to a faint gear icon and a lighter green icon with a downward-facing arrow.) When you “run” the code chunk like this, R produces output it. We’ll say more about code chunks later in this document.\nThis document—with text and code chunks together—is called an R Notebook file."
  },
  {
    "objectID": "02-using_r_markdown-web.html#rmark-previewing",
    "href": "02-using_r_markdown-web.html#rmark-previewing",
    "title": "2  Using R Markdown",
    "section": "2.3 Previewing a document",
    "text": "2.3 Previewing a document\nThere is a button in the toolbar right above the text that says “Preview”. Go ahead and push it. See what happens.\nOnce the pretty output is generated, take a few moments to look back and forth between it and the original R Notebook text file (the plain text in RStudio). You can see some tricks that we won’t need much (embedding web links, making lists, etc.) and some tricks that we will use in every chapter (like R code chunks).\nAt first, you’ll want to work back and forth between the R Notebook file and the HTML file to get used to how the formatting in the plain text file get translated to output in the HTML file. After a while, you will look at the HTML file less often and work mostly in the R Notebook file, only previewing when you are finished and ready to produce your final draft."
  },
  {
    "objectID": "02-using_r_markdown-web.html#rmark-literate",
    "href": "02-using_r_markdown-web.html#rmark-literate",
    "title": "2  Using R Markdown",
    "section": "2.4 Literate programming",
    "text": "2.4 Literate programming\nR Markdown is one way to implement a “literate programming” paradigm. The concept of literate programming was famously described by Donald Knuth, an eminent computer scientist. The idea is that computer programs should not appear in a sterile file that’s full of hard-to-read, abstruse lines of computer code. Instead, functional computer code should appear interspersed with writing that explains the code."
  },
  {
    "objectID": "02-using_r_markdown-web.html#rmark-reproducible",
    "href": "02-using_r_markdown-web.html#rmark-reproducible",
    "title": "2  Using R Markdown",
    "section": "2.5 Reproducible research",
    "text": "2.5 Reproducible research\nOne huge benefit of organizing your work into R Notebooks is that it makes your work reproducible. This means that anyone with access to your data and your R Notebook file should be able to re-create the exact same analysis you did.\nThis is a far cry from what generally happens in research. For example, if I do all my work in Microsoft Excel, I make a series of choices in how I format and analyze my data and all those choices take the form of menu commands that I point and click with my mouse. There is no record of the exact sequence of clicks that took me from point A to B all the way to Z. All I have to show for my work is the “clean” spreadsheet and anything I’ve written down or communicated about my results. If there were any errors along the way, they would be very hard to track down.2\nReproducibility should be a minimum prerequisite for all statistical analysis. Sadly, that is not the case in most of the research world. We are training you to be better."
  },
  {
    "objectID": "02-using_r_markdown-web.html#rmark-structure",
    "href": "02-using_r_markdown-web.html#rmark-structure",
    "title": "2  Using R Markdown",
    "section": "2.6 Structure of an R Notebook",
    "text": "2.6 Structure of an R Notebook\nLet’s start from the top. Look at the very beginning of the plain R Notebook file. (If you’re in RStudio, you are looking at the R Notebook file. If you are looking at the pretty HTML file, you’ll need to go back to RStudio.) The section at the very top of the file that starts and ends with three hyphens is called the YAML header. (Google it if you really care why.) The title of the document appears already, but you’ll need to substitute your name and today’s date in the obvious places. Scroll up and do that now.\nYou’ve made changes to the document, so you’ll need to push the “Preview” button again. Once that’s done, look at the resulting HTML document. The YAML header has been converted into a nicely formatted document header with the new information you’ve provided.\nNext, there is some weird looking code with instructions not to touch it. I recommend heeding that advice. This code will allow you to answer questions and have your responses appear in pretty blue boxes. In the body of the chapter, such answer boxes will be marked with tags ::: {.answer} and :::. Let’s try it:\n\nReplace this text here with something else. Then preview the document and see how it appears in the HTML file.\n\nBe careful not to delete the two lines starting with the three colons (:::) that surround your text! If you mess this up, the rest of the document’s formatting will get screwed up.\nTo be clear, the colorful answer boxes are not part of the standard R Markdown tool set. That’s why we had to define them manually near the top of the file. Note that the weird code itself does not show up in the HTML file. It works in the background to define the blue boxes that show up in the HTML file.\nWe also have section headers throughout, which in the R Notebook file look like:"
  },
  {
    "objectID": "02-using_r_markdown-web.html#section-header",
    "href": "02-using_r_markdown-web.html#section-header",
    "title": "2  Using R Markdown",
    "section": "Section header",
    "text": "Section header\nThe hashtags are Markdown code for formatting headers. Additional hashtags will create subsections:\n\nNot quite as big\nWe could actually use a single number sign, but # makes a header as big as the title, which is too big. Therefore, we will prefer ## for section headers and ### for subsections.\nYou do need to make sure that there is a blank line before and after each section header. To see why, look at the HTML document at this spot: ## Is this a new section? Do you see the problem?\nPut a blank line before and after the line above that says “Is this a new section?” Preview one more time and make sure that the line now shows up as a proper section header."
  },
  {
    "objectID": "02-using_r_markdown-web.html#rmark-othertricks",
    "href": "02-using_r_markdown-web.html#rmark-othertricks",
    "title": "2  Using R Markdown",
    "section": "2.7 Other formatting tricks",
    "text": "2.7 Other formatting tricks\nYou can make text italic or bold by using asterisks. (Don’t forget to look at the HTML to see the result.)\nYou can make bullet-point lists. These can be made with hyphens, but you’ll need to start after a blank line, then put the hyphens at the beginning of each new line, followed by a space, as follows:\n\nFirst item\nSecond item\n\nIf you want sub-items, indent at least two spaces and use a minus sign followed by a space.\n\nItem\n\nSub-item\nSub-item\n\nItem\nItem\n\nOr you can make ordered lists. Just use numbers and R Markdown will do all the work for you. Sub-items work the same way as above. (Again, make sure you’re starting after a blank line and that there is a space after the periods and hyphens.)\n\nFirst Item\n\n\nSub-item\nSub-item\n\n\nSecond Item\nThird Item\n\nWe can make horizontal rules. There are lots of ways of doing this, but I prefer a bunch of asterisks in a row.\n\nThere are many more formatting tricks available. For a good resource on all R Markdown stuff, click on this link for a “cheat sheet”. And note in the previous sentence the syntax for including hyperlinks in your document.3"
  },
  {
    "objectID": "02-using_r_markdown-web.html#rmark-codechunks",
    "href": "02-using_r_markdown-web.html#rmark-codechunks",
    "title": "2  Using R Markdown",
    "section": "2.8 R code chunks",
    "text": "2.8 R code chunks\nThe most powerful feature of R Markdown is the ability to do data analysis right inside the document. This is accomplished by including R code chunks. An R code chunk doesn’t just show you the R code in your output file; it also runs that code and generates output that appears right below the code chunk.\nAn R code chunk starts with three “backticks” followed by the letter r enclosed in braces, and it ends with three more backticks. (The backtick is usually in the upper-left corner of your keyboard, next to the number 1 and sharing a key with the tilde ~.)\nIn RStudio, click the little dark green, right-facing arrow in the upper-right corner of the code chunk below, just as you did earlier.\n\n# Here's some sample R code\ntest &lt;- c(1, 2, 3, 4)\nsum(test)\n\n[1] 10\n\n\nAfter pushing the dark green arrow, you should notice that the output of the R code appeared like magic. If you preview the HTML output, you should see the same output appear. If you hover your mouse over the dark green arrow, you should see the words “Run Current Chunk”. We’ll call this the Run button for short.\nWe need to address something here that always confuses people new to R and R Markdown. A number sign (aka “hashtag”) in an R Notebook gives us headers for sections and subsections. In R, however, a number sign indicates a “comment” line. In the R code above, the line # Here's some sample R code is not executed as R code. But you can clearly see that the two lines following were executed as R code. So be careful! Number signs inside and outside R code chunks behave very differently.\nTypically, the first code chunk that appears in our document will load any packages we need. We will be using a package called tidyverse (which is really a collection of lots of different packages) throughout the course. We load it now. Click on the Run button (the dark green, right-facing arrow) in the code chunk below.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThe output here consists of a bunch of information generated when trying to load the package. These are not errors, even though one section is labeled “Conflicts”. Usually, errors appear with the word “Error”, so it’s typically clear when something just didn’t work. Also note that once you’ve loaded a package, you don’t need to load it again until you restart your R session. For example, if you go back and try to run the code chunk above one more time, the output will disappear. That’s because tidyverse is already loaded, so the second “run” doesn’t actually generate output anymore.\nOkay, let’s do something interesting now. We’ll revisit the penguins data set we introduced in the previous chapter. Remember, though, that this data set also lives in a package that needs to be loaded. Run the code chunk below to load the palmerpenguins package:\n\nlibrary(palmerpenguins)\n\nWarning: package 'palmerpenguins' was built under R version 4.3.1\n\n\nLet’s see what happens when we try to run multiple commands in one code chunk:\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\ntail(penguins)\n\n# A tibble: 6 × 8\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Chinstrap Dream            45.7          17                 195        3650\n2 Chinstrap Dream            55.8          19.8               207        4000\n3 Chinstrap Dream            43.5          18.1               202        3400\n4 Chinstrap Dream            49.6          18.2               193        3775\n5 Chinstrap Dream            50.8          19                 210        4100\n6 Chinstrap Dream            50.2          18.7               198        3775\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\nIf you’re looking at this in RStudio, it’s a bit of a mess. RStudio did its best to give you what you asked for, but there are three separate commands here. The first two (head and tail) print some of the data, so the first two boxes of output are tables showing you the head and the tail of the data. The next one (str) normally just prints some information to the Console. So RStudio gave you an R Console box with the output of this command.\nIf you look at the HTML file, you can see the situation isn’t as bad. Each command and its corresponding output appear nicely separated there.\nNevertheless, it will be good practice and a good habit to get into to put multiple output-generating commands in their own R code chunks. Run the following code chunks and compare the output to the mess you saw above:\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\ntail(penguins)\n\n# A tibble: 6 × 8\n  species   island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;     &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Chinstrap Dream            45.7          17                 195        3650\n2 Chinstrap Dream            55.8          19.8               207        4000\n3 Chinstrap Dream            43.5          18.1               202        3400\n4 Chinstrap Dream            49.6          18.2               193        3775\n5 Chinstrap Dream            50.8          19                 210        4100\n6 Chinstrap Dream            50.2          18.7               198        3775\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\nThis won’t look any different in the HTML file, but it sure looks a lot cleaner in RStudio.\nWhat about the two lines of the first code chunk we ran above?\n\ntest &lt;- c(1, 2, 3, 4)\nsum(test)\n\n[1] 10\n\n\nShould these two lines be separated into two code chunks? If you run it, you’ll see only one piece of output. That’s because the line test &lt;- c(1, 2, 3, 4) works invisibly in the background. The vector test gets assigned, but no output is produced. Try it and see (push the Run button):\n\ntest &lt;- c(1, 2, 3, 4)\n\nSo while there’s no harm in separating these lines and putting them in their own chunks, it’s not strictly necessary. You really only need to separate lines when they produce output. (And even then, if you forget, RStudio will kindly give you multiple boxes of output.)\nSuppose we define a new variable called test2 in a code chunk. FOR PURPOSES OF THIS EXERCISE, DO NOT HIT THE RUN BUTTON YET! But do go look at the HTML file.\n\ntest2 &lt;- c(\"a\", \"b\", \"c\")\ntest2\n\n[1] \"a\" \"b\" \"c\"\n\n\nThe first line defines test2 invisibly. The second line asks R to print the value of test2, but in the HTML file we see no output. That’s because we have not run the code chunk yet. DON’T HIT THE RUN BUTTON YET!\nOkay, now go to the Console in RStudio (in the lower left corner of the screen). Try typing test2. You should get an “Error: object ‘test2’ not found.”\nWhy does this happen? The Global Environment doesn’t know about it yet. Look in the upper right corner of the screen, under the “Environment” tab. You should see test, but not test2.\nOkay, NOW GO BACK AND CLICK THE RUN BUTTON IN THE LAST CHUNK ABOVE. The output appears in RStudio below the code chunk and the Global Environment has been updated.\nThe take home message is this:\nBe sure to run all your code chunks in RStudio!\nIn RStudio, look in the toolbar above this document, toward the right. You should see the word “Run” with a little drop-down menu next to it. Click on that drop-down menu and select “Run All”. Do you see what happened? All the code chunks ran again, and that means that anything in the Global Environment will now be updated to reflect the definitions made in the R Notebook.\nIt’s a good idea to “Run All” when you first open a new R Notebook. This will ensure that all your code chunks have their output below them (meaning you don’t have to go through and click the Run button manually for each chunk, one at a time) and the Global Environment will accurately reflect the variables you are using.\nYou can “Run All” from time to time, but it’s easier just to “Run All” once at the beginning, and then Run individual R code chunks manually as you create them.\nNow go back to the Environment tab and find the icon with the little broom on it. Click it. You will get a popup warning you that you about to “remove all objects from the environment”. Click “Yes”. Now the Global Environment is empty. Go back to the “Run” menu and select “Run All”. All the objects you defined in the R Notebook file are back.\nClearing out your environment can be useful from time to time. Maybe you’ve been working on a chapter for a while and you’ve tried a bunch of stuff that didn’t work, or you went back and changed a bunch of code. Eventually, all that junk accumulates in your Global Environment and it can mess up your R Notebook. For example, let’s define a variable called my_variable.\n\nmy_variable &lt;- 42\n\nThen, let’s do some calculation with my_variable.\n\nmy_variable * 2\n\n[1] 84\n\n\nPerhaps later you decide you don’t really need my_variable. Put a hashtag in front of the code my_variable &lt;- 42 to comment it out so that it will no longer run, but don’t touch the next code chunk where you multiply it by 2. Now try running the code chunk with my_variable * 2 again. Note that my_variable is still sitting in your Global Environment, so you don’t get any error messages. R can still see and access my_variable.\nNow go to the “Run” menu and select “Restart R and Run All Chunks”. This clears the Global Environment and runs all the R code starting from the top of the R Notebook. This time you will get an error message: object 'my_variable' not found. You’ve tried to calculate with a variable called my_variable that doesn’t exist anymore. (The line in which it was defined has been commented out.)\nIt’s best to make sure all your code chunks will run when loaded from a clean R session. The “Restart R and Run All Chunks” option is an easy way to both clear your environment and re-run all code chunks. You can do this as often as you want, but you will definitely want to do this one last time when you are done. At the end of the chapter, when you are ready to prepare the final draft, please select “Restart R and Run All Chunks”. Make sure everything still works!\nTo get rid of the error above, uncomment the line my_variable &lt;- 42 by removing the hashtag you added earlier."
  },
  {
    "objectID": "02-using_r_markdown-web.html#rmark-inline",
    "href": "02-using_r_markdown-web.html#rmark-inline",
    "title": "2  Using R Markdown",
    "section": "2.9 Inline R commands",
    "text": "2.9 Inline R commands\nYou don’t need a standalone R code chunk to do computations. One neat feature is the ability to use R to calculate things right in the middle of your text.\nHere’s an example. Suppose we wanted to compute the mean body mass (in grams) for the penguins in the penguins data set. We could do this:\n\nmean(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 4201.754\n\n\n(The na.rm = TRUE part is necessary because two of the penguins are missing body mass data. More on missing data in future chapters.)\nBut we can also do this inline by using backticks and putting the letter r inside the first backtick. Go to the HTML document to see how the following sentence appears:\nThe mean body mass for penguins in the penguins data set is 4201.754386 grams.\nYou can (and should) check to make sure your inline R code is working by checking the HTML output, but you don’t necessarily need to go to the HTML file to find out. In RStudio, click so that the cursor is somewhere in the middle of the inline code chunk in the paragraph above. Now type Ctrl-Enter or Cmd-Enter (PC or Mac respectively). A little box should pop up that shows you the answer!\nNotice that in addition to the inline R command that calculated the mean, I also enclosed penguins in backticks to make it stand out in the output. I’ll continue to do that for all computer commands and R functions. But to be clear, putting a word in backticks is just a formatting trick. If you want inline R code, you also need the letter r followed by a space inside the backticks."
  },
  {
    "objectID": "02-using_r_markdown-web.html#rmark-copypaste",
    "href": "02-using_r_markdown-web.html#rmark-copypaste",
    "title": "2  Using R Markdown",
    "section": "2.10 Copying and pasting",
    "text": "2.10 Copying and pasting\nIn future chapters, you will be shown how to run statistical analyses using R. Each chapter will give extensive explanations of the statistical concepts and demonstrations of the necessary R code. Afterwards, there will be one or more exercises that ask you to apply your new-found knowledge to run similar analyses on your own with different data.\nThe idea is that you should be able to copy and paste the R code from the previously worked examples. But you must be thoughtful about how you do this. The code cannot just be copied and pasted blindly. It must be modified so that it applies to the exercises with new data. This requires that you understand what the code is doing. You cannot effectively modify the code if you don’t know which parts to modify.\nThere will also be exercises in which you are asked to provide your own explanations and interpretations of your analyses. These should not be copied and pasted from any previous work. These exercises are designed to help you understand the statistical concepts, so they must be in your own words, using your own understanding.\nIn order to be successful in these chapters, you must do the following:\n\nRead every part of the chapter carefully!\n\n\nIt will be tempting to skim over the paragraphs quickly and just jump from code chunk to code chunk. This will be highly detrimental to your ability to gain the necessary understanding—not just to complete the chapter, but to succeed in statistics overall.\n\n\nCopy and paste thoughtfully!\n\n\nNot every piece of code from the early part of the chapter will necessarily apply to the later exercises. And the code that does apply will need to be modified (sometimes quite heavily) to be able to run new analyses. Your job is to understand how the code works so that you can make changes to it without breaking things. If you don’t understand a piece of code, don’t copy and paste it until you’ve read and re-read the earlier exposition that explains how the code works.\n\nOne final note about copying and pasting. Sometimes, people will try to copy and paste code from the HTML output file. This is a bad idea. The HTML document uses special characters to make the output look pretty, but these characters don’t actually work as plain text in an R Notebook. The same applies to things copied and pasted from a Word document or another website. If you need to copy and paste code, be sure to find the plain text R Notebook file (the one with the .Rmd extension here in RStudio) and copy and paste from that."
  },
  {
    "objectID": "02-using_r_markdown-web.html#rmark-conclusion",
    "href": "02-using_r_markdown-web.html#rmark-conclusion",
    "title": "2  Using R Markdown",
    "section": "2.11 Conclusion",
    "text": "2.11 Conclusion\nThat’s it! There wasn’t too much you were asked to do for this assignment that will actually show up in the HTML output. (Make sure you did do the three things that were asked of you however: one was adding your name and the date to the YAML header, one was typing something in the blue answer box, and the last was to make a section header appear properly.) As you gain confidence and as we move into more serious stats material, you will be asked to do a lot more.\n\n2.11.1 Preparing and submitting your assignment\nIf you look in your project folder, you should see three files:\nintro_stats.Rproj\n02-using_r_markdown.Rmd\n02-using_r_markdown.nb.html\nThe first file (with extension .Rproj) you were instructed never to touch.\nThe next file (with extension .Rmd) is your R Notebook file. It’s the file you’re looking at right now. It is really nothing more than a plain text file, although when you open it in RStudio, some magic allows you to see the output from the code chunks you run.\nFinally, you have a file with extension .nb.html. That is the pretty output file generated when you hit the “Preview” button. (If you happen to see other files in your project folder, you should ignore those and not mess with them.) This is the “final product” of your work.\nThere are several steps that you should follow at the end of each of every chapter.\n\nFrom the “Run” menu, select “Restart R and Run All Chunks”.\nDeal with any code errors that crop up. Repeat steps 1–-2 until there are no more code errors.\nSpell check your document by clicking the icon with “ABC” and a check mark.\nHit the “Preview” button one last time to generate the final draft of the .nb.html file.\nProofread the HTML file carefully. If there are errors, go back and fix them, then repeat steps 1–5 again.\n\nIf you have completed this chapter as part of a statistics course, follow the directions you receive from your professor to submit your assignment."
  },
  {
    "objectID": "02-using_r_markdown-web.html#footnotes",
    "href": "02-using_r_markdown-web.html#footnotes",
    "title": "2  Using R Markdown",
    "section": "",
    "text": "Also called “pound signs” or “octothorpes”. This is also an example of formatting a footnote!↩︎\nIf you think these errors are trivial, Google ``Reinhart and Rogoff Excel error’’ to read about the catastrophic consequences of seemingly trivial Excel mistakes.↩︎\nYou can also access cheat sheets through the main Help menu in RStudio.↩︎"
  },
  {
    "objectID": "03-categorical_data-web.html#categorical-intro",
    "href": "03-categorical_data-web.html#categorical-intro",
    "title": "3  Categorical data",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nIn this chapter, we’ll learn about categorical data and how to summarize it using tables and graphs.\n\n3.1.1 Install new packages\nIf you are using RStudio Workbench, you do not need to install any packages. (Any packages you need should already be installed by the server administrators.)\nIf you are using R and RStudio on your own machine instead of accessing RStudio Workbench through a browser, you’ll need to type the following command at the Console:\ninstall.packages(\"janitor\")\n\n\n3.1.2 Download the R notebook file\nCheck the upper-right corner in RStudio to make sure you’re in your intro_stats project. Then click on the following link to download this chapter as an R notebook file (.Rmd).\nhttps://vectorposse.github.io/intro_stats/chapter_downloads/03-categorical_data.Rmd\nOnce the file is downloaded, move it to your project folder in RStudio and open it there.\n\n\n3.1.3 Restart R and run all chunks\nIn RStudio, in the toolbar above this document, find the “Run” drop-down menu and select “Restart R and Run All Chunks.”\nThis does two important things:\n\nR will restart. This will clear out the Global Environment and provide a fresh session for this new assignment. None of the clutter from previous chapters will be there to mess up your work in this chapter.\nAll the code chunks in this document will run so that you can see the output as you scroll past it. This saves you some effort in having to click the little green “Run” button in each code chunk as you come across it. (Also, if you forget to run one, that could cause errors later on, so this way, all the variables you need will be in the Global Environment for when they’re needed later.) You will still need to click the green arrow for new code chunks that you create, of course.\n\nAt the end of the assignment, you will “Restart R and Run All Chunks” once again to make sure that everything works smoothly and there are no lingering errors.\n\n\n3.1.4 Load packages\nWe load the tidyverse package since it also loads the ggplot2 package that we’ll use throughout the course to make graphs. It also loads several other packages, for example, one called dplyr to give us a command called mutate, and another called forcats to give us as_factor. (These will all be explained later.) The janitor package gives us the tabyl command for creating nice tables. Finally, We load the palmerpenguins package to work with the penguin data.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(palmerpenguins)\n\nWarning: package 'palmerpenguins' was built under R version 4.3.1"
  },
  {
    "objectID": "03-categorical_data-web.html#categorical-data",
    "href": "03-categorical_data-web.html#categorical-data",
    "title": "3  Categorical data",
    "section": "3.2 Categorical data",
    "text": "3.2 Categorical data\nData comes in different types depending on what is being measured. When people think of “data”, they often imagine numerical data, consisting of numbers. But there are other kinds of data as well.\nIn this chapter, we focus on categorical data that groups observations into categories.\nFor example, if we record the species of a penguin, that is not a number. It’s a word that classifies that penguin into one of a finite number of types. Whenever you see words in a data set, there’s a good chance that you’re looking at categorical data.\nEven “numbers” can sometimes represent categorical data. For example, suppose in a survey there is a Yes/No question. Instead of seeing the words “Yes” or “No”, though, you might see a data set with ones and zeros, where 1 = Yes and 0 = No. The presence of numbers does not automatically make that data numerical. In fact, the data is categorical. Yes and No are categories that sort the survey respondents into two groups based on their responses to a certain question.\nWhat about ZIP codes? They are recorded as numbers, and unlike the Yes/No example above, those numbers aren’t just substitutes for words. Nevertheless, ZIP codes are categorical. They sort addresses into a finite number of groups based on geographic proximity.\nAnother way to think of it is this: can the numerical values of ZIP codes be treated as numbers in any meaningful way? Can you take a sum or an average of ZIP codes? Sure, technically a computer can add up or average a set of ZIP codes, but would the result be a meaningful number? Since the answer is “no” we cannot think of ZIP codes as numbers, even though they are recorded that way.\n\nExercise 1\nThink of another type of data that would be recorded using numbers but should be thought of as categorical data.\n\nPlease write up your answer here."
  },
  {
    "objectID": "03-categorical_data-web.html#categorical-factor",
    "href": "03-categorical_data-web.html#categorical-factor",
    "title": "3  Categorical data",
    "section": "3.3 Factor variables",
    "text": "3.3 Factor variables\nR uses the term “factor variable” to refer to a categorical variable. Look at the structure of the penguins data below.\n\nstr(penguins)\n\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\nThe categorical variables species, island, and sex are coded correctly as factor variables.\nThe tidyverse package offers a function called glimpse that effectively does the same thing as str. We’ll use glimpse throughout the rest of the course.\n\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\nExercise 2\nLook at the output of str versus glimpse above. Write down any advantages or disadvantages you see using one versus the other. (You may also want to check the help file for the two commands to see if they offer any clues as to why you might use one over the other.)\n\nPlease write up your answer here.\n\n\nYour data set may already come with its variables coded correctly as factor variables, but often they are not. As described above, numbers are often used to represent categories, so R may think that those variables represent numerical data. Later, we’ll see an example of this and learn how to handle categorical variables that are not coded as factor variables in R."
  },
  {
    "objectID": "03-categorical_data-web.html#categorical-summarizing-one",
    "href": "03-categorical_data-web.html#categorical-summarizing-one",
    "title": "3  Categorical data",
    "section": "3.4 Summarizing one categorical variable",
    "text": "3.4 Summarizing one categorical variable\nIf you need to summarize a single categorical variable, a frequency table usually suffices. This is simply a table that counts up all the instances of each category. The word “frequency” is synonymous here with the word “count”.\nWe can use the table command:\n\ntable(penguins$species)\n\n\n   Adelie Chinstrap    Gentoo \n      152        68       124 \n\n\nRecall that the dollar sign means to grab the variable species from the tibble penguins.\nYou can also generate a relative frequency table which is a table that uses proportions or percentages instead of counts.\nNOTE: For purposes of this course, we’re going to be very careful about the terms proportion and percentage. For us, a proportion will always be a number between 0 and 1 whereas a percentage will be between 0 and 100. Calculating a percentage is the same as multiplying a proportion by 100.\nThe table command stops being convenient if you want proportions instead of counts. Instead, we will use the tabyl command from the janitor package that was loaded near the top of the chapter. The syntax for this command is a little different. The tibble goes first, followed by a comma, followed by the variable you want to summarize:\n\ntabyl(penguins, species)\n\n   species   n   percent\n    Adelie 152 0.4418605\n Chinstrap  68 0.1976744\n    Gentoo 124 0.3604651\n\n\nNow you get both counts and proportions. Note that in the output above, it’s a little misleading to call the last column “percent”. These are actually proportions, and we would have to multiply by 100 to get percentages.\nIt’s usually nice to have the column totals. We can achieve that by using an adorn function to get them as follows:\n\ntabyl(penguins, species) %&gt;%\n  adorn_totals()\n\n   species   n   percent\n    Adelie 152 0.4418605\n Chinstrap  68 0.1976744\n    Gentoo 124 0.3604651\n     Total 344 1.0000000\n\n\nWe’ll always include the totals at the bottom.\nIf you really want percentages, we can use a different adorn function:\n\ntabyl(penguins, species) %&gt;%\n    adorn_pct_formatting()\n\n   species   n percent\n    Adelie 152   44.2%\n Chinstrap  68   19.8%\n    Gentoo 124   36.0%\n\n\nAgain, we’ll also include adorn_totals so that we get the column totals.\n\ntabyl(penguins, species) %&gt;%\n    adorn_totals() %&gt;%\n    adorn_pct_formatting()\n\n   species   n percent\n    Adelie 152   44.2%\n Chinstrap  68   19.8%\n    Gentoo 124   36.0%\n     Total 344  100.0%\n\n\nThe syntax above looks a little confusing with the unusual %&gt;% symbols everywhere. You will learn more about that weird set of symbols in a later chapter. For now, you can just copy and paste this code and make any necessary changes to the tibble and/or variables names as needed.\n\nExercise 3(a)\nUse the tabyl command as above to create a frequency table for the sex of the penguins. Include the column totals at the bottom. (You will also get a relative frequency table for free.)\n\n\n# Add code here to create a frequency table for sex\n\n\n\n\nExercise 3(b)\nIn the table for sex that you just created, what does the row labeled &lt;NA&gt; mean?\n\nPlease write up your answer here.\n\n\n\nExercise 3(c)\nNow create a relative frequency table for sex that reports percentages and not proportions (still including the column totals at the bottom).\n\n\n# Add code here that reports percentages instead of proportions\n\n\n\n\nExercise 3(d)\nIn the previous tables, what is the difference between percent and valid_percent? Why are there two different sets of percentages being computed?\n\nPlease write up your answer here."
  },
  {
    "objectID": "03-categorical_data-web.html#categorical-graphing-one",
    "href": "03-categorical_data-web.html#categorical-graphing-one",
    "title": "3  Categorical data",
    "section": "3.5 Graphing one categorical variable",
    "text": "3.5 Graphing one categorical variable\nWhen asked, “What type of graph should I use when graphing a single categorical variable?” the simple answer is “None.” If you do need to summarize a categorical variable, a frequency table usually suffices.\nIf you really, really want a graph, the standard type is a bar chart. But before we can create one, we need to start learning about the very important tool we will use throughout the course for graphing. It’s called ggplot and it’s part of a package called ggplot2.1\nWe don’t have to load the ggplot2 package explicitly because it got loaded alongside a number of other packages when we called library(tidyverse) early on in the chapter.\n\n3.5.1 ggplot\nThe ggplot command is an all-purpose graphing utility. It uses a graphing philosophy derived from a book called The Grammar of Graphics by Leland Wilkinson. The basic idea is that each variable you want to plot should correspond to some element or “aesthetic” component of the graph. The obvious places for data to go are along the y-axis or x-axis, but other aesthetics are important too; graphs often use color, shape, or size to illustrate different aspects of data. Once these aesthetics have been defined, we will add “layers” to the graph. These are objects like dots, boxes, lines, or bars that dictate the type of graph we want to see.\nIn an introductory course, we won’t get too fancy with these graphs. But be aware that there’s a whole field of data visualization that studies clear and interesting ways to understand data graphically.\nIt will be easier to explain the ggplot syntax in the context of specific graph types, so let’s create a bar chart for species.\n\nggplot(penguins, aes(x = species)) +\n    geom_bar()\n\n\n\n\nWe’ll walk through this syntax step by step.\n\nThe first argument of the ggplot command is the name of the tibble, in this case, penguins.\nNext we define the aesthetics using aes and parentheses. Inside the parentheses, we assign any variables we want to plot to aesthetics of the graph. For this analysis, we are only interested in the variable species and for a bar chart, the categorical variable typically goes on the x-axis. That’s why it says x = species inside the aes argument.\nFinally, ggplot needs to know what kind of graph we want. Graph types are called “geoms” in the ggplot world, and geom_bar() tells ggplot to add a “bar chart layer”. Adding a layer is accomplished by literally typing a plus sign.\n\nThis can be modified somewhat to give proportions (relative frequencies) on the y-axis instead of counts. Unfortunately, the ggplot syntax is not very transparent here. My recommendation is to copy and paste the code below if you need to make a relative frequency bar chart in the future, making the necessary changes to the tibble and variable names, of course.\n\nggplot(penguins, aes(x = species, y = ..prop.., group = 1)) +\n    geom_bar()\n\nWarning: The dot-dot notation (`..prop..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(prop)` instead.\n\n\n\n\n\nThese bar charts are the graphical analogues of a frequency table and a relative frequency table, respectively.\n\nExercise 4\nIn a sentence or two at most, describe the distribution of species in this data set.\n\nPlease write up your answer here.\n\n\nWhat about pie charts? Just. Don’t.\nSeriously. Pie charts suck.2"
  },
  {
    "objectID": "03-categorical_data-web.html#categorical-summarizing-two",
    "href": "03-categorical_data-web.html#categorical-summarizing-two",
    "title": "3  Categorical data",
    "section": "3.6 Summarizing two categorical variables",
    "text": "3.6 Summarizing two categorical variables\nA table summarizing two categorical variables is called a contingency table (or pivot table, or cross-tabulation, or probably several other terms as well).\nFor example, we might pose the following question: is the distribution of sex among penguins in our data more or less balanced across the three species?\nWhen we work with two variables, typically we think of one variable as response and the other as predictor. The response variable is usually the variable of main interest. A predictor variable is another attribute that might predict or explain more about the response variable.\nFor example, our question is concerned with the sex distribution of penguins. We could create a relative frequency table of sex alone to see if male and female penguins are balanced in the data. In fact, you did that very thing above and saw that, indeed, there were roughly equal numbers of male and female penguins. But is that still true when we divide up the data into the three groups representing the separate species?\nTwo variables are called associated when there is a relationship between them. For example, if sex and species were associated, then the distribution of sex would change depending on the species. Maybe one species of penguin had more females and another had fewer females. Our prediction of the sex distribution would change based on the value of the predictor variable species.\nOn the other hand, two variables that are not associated are called independent. Independent variables are not related. If the sex distribution were the same across all species, then knowledge of the species would not change our predictions about the sex of a penguin. It wouldn’t matter because there was no relationship between sex and species.\nMost research questions that involve two or more variables are fundamentally questions of whether a response variable is associated with one or more predictor variables, or whether they are independent.\nLet’s check the contingency table. The tabyl command will place the first variable listed across the rows and the second one listed down the columns. Since we always include column totals, we want the predictor variable to be the column variable so we can see how the predictor groups are distributed in the data. Always list the response variable first.\n\ntabyl(penguins, sex, species) %&gt;%\n  adorn_totals()\n\n    sex Adelie Chinstrap Gentoo\n female     73        34     58\n   male     73        34     61\n   &lt;NA&gt;      6         0      5\n  Total    152        68    124\n\n\nEach column is a group, and our question is whether the distribution of sexes in each column is similar.\nThe last row of totals is called the marginal distribution (because it sits in the “margin” of the contingency table). It is equivalent to a frequency table for species.\n\n3.6.0.0.1 Exercise 5\nCounts can be misleading. For example, there are 73 female Adelie penguins, but only 34 female Chinstrap penguins. Does that mean that Adelie penguins are more likely to be female than Chinstrap penguins? Why or why not?\n\nPlease write up your answer here.\n\n\nA more fair way to compare across columns is to create relative frequencies. We can do this with a slightly different adorn command. The following code says that we want to compute column proportions (yes, I know the command is called adorn_percentages, but these are proportions):\n\ntabyl(penguins, sex, species) %&gt;%\n    adorn_totals() %&gt;%\n    adorn_percentages(\"col\")\n\n    sex     Adelie Chinstrap     Gentoo\n female 0.48026316       0.5 0.46774194\n   male 0.48026316       0.5 0.49193548\n   &lt;NA&gt; 0.03947368       0.0 0.04032258\n  Total 1.00000000       1.0 1.00000000\n\n\nIf we actually want percentages, we need one more line of code. This command—adorn_pct_formatting—is the same as we used before with frequency tables.\n\ntabyl(penguins, sex, species) %&gt;%\n    adorn_totals() %&gt;%\n    adorn_percentages(\"col\") %&gt;%\n    adorn_pct_formatting()\n\n    sex Adelie Chinstrap Gentoo\n female  48.0%     50.0%  46.8%\n   male  48.0%     50.0%  49.2%\n   &lt;NA&gt;   3.9%      0.0%   4.0%\n  Total 100.0%    100.0% 100.0%\n\n\nNow we can see that each column adds up to 100%. In other words, each species is now on equal footing, and only the distribution of sexes within each group matters.\n\n\n3.6.0.0.2 Exercise 6(a)\nWhat percentage of Adelie penguins are male? What percentage of Chinstrap penguins are male? What percentage of Gentoo penguins are male?\n\nPlease write up your answer here.\n\n\n\n3.6.0.0.3 Exercise 6(b)\nDoes sex appear to be associated with species for the penguins in this data set? Or are these variables independent?\n\nPlease write up your answer here.\n\n\nThe islands of Antarctica on which the penguins were observed and measured are recorded in the variable called island. Is the distribution of the three species of penguin the same (or similar) on the three islands?\n\n\n3.6.0.0.4 Exercise 7(a)\nChoosing which variables play the roles of response and predictor can be tricky. For the question above, with species and island, which is response and which is predictor?\nOne way to think about this is to ask the following two questions and see which one is closer to the question asked:\n\nGiven information about the species, are you interested in which island the penguin lives on? If so, species is a predictor and island is response. (You are using species to predict island.)\nGiven information about the island, are you interested in the species of the penguin? If so, island is a predictor and species is response. (You are using island to predict species.)\n\n\nPlease write up your answer here.\n\n\n\n3.6.0.0.5 Exercise 7(b)\nCreate a contingency table with percentages. List species first, followed by island. (Hey, that’s hint in case you need to go back and change your answer to part (a).)\n\n\n# Add code here to create a contingency table with percentages.\n\n\n\n\n3.6.0.0.6 Exercise 7(c)\nFinally, comment on the association or independence of the two variables.\n\nPlease write up your answer here."
  },
  {
    "objectID": "03-categorical_data-web.html#categorical-graphing-two",
    "href": "03-categorical_data-web.html#categorical-graphing-two",
    "title": "3  Categorical data",
    "section": "3.7 Graphing two categorical variables",
    "text": "3.7 Graphing two categorical variables\nA somewhat effective way to display two categorical variables is with a side-by-side bar chart. Here is the ggplot code for the relationship between sex and species.\n\nggplot(penguins, aes(fill = sex, x = species)) +\n    geom_bar(position = \"dodge\")\n\n\n\n\nThis is somewhat different from the first ggplot example you saw above, so let’s take a moment to go through it.\n\nThe first argument is the data frame penguins; no mystery there.\nThe second aesthetic x = species also makes a lot of sense. As species is our predictor variable—we’re using species to group the penguins, and then within each species, we’re interested in the sex distribution—species goes on the x-axis.\nHowever, sex does not go on the y-axis! (This is a very common mistake for novices.) The y-axis of a bar chart is always a count or a proportion/percentage, so no variable should ever go on the y-axis of a bar chart. In that case, how does sex enter the picture? Through the use of color! The aesthetic fill = sex says to use the sex variable to shade or “fill” the bars with different colors. You’ll also notice that ggplot makes a legend automatically with the colors so you can see which color corresponds to which value (in this case, “female”, “male”, or “NA” for the missing data).\n\nAnother unusual feature is the argument position = \"dodge\" in the geom_bar layer. Let’s see what happens if we remove it.\n\nggplot(penguins, aes(fill = sex, x = species)) + \n    geom_bar()\n\n\n\n\nWe get a stacked bar chart! This is another popular way of displaying two categorical variables, but we don’t tend to prefer it. Notice how difficult it is to compare the number of females across species; since there is no common baseline for the red segments of each bar, it is harder to determine which ones are bigger or smaller. (In this case, it’s fairly clear, but there are plenty of data sets for which the counts might be a lot closer.)\nSo let’s agree to use side-by-side bar charts. There is still one aspect of the side-by-side bar chart that is misleading, though. For example, the red bar for Adelie penguins is bigger than the red bar for Gentoo penguins. Does this mean Adelie penguins are more likely to be female?\nThis is the same issue we identified in an exercise above. To fix this problem, a better option here would be to use relative frequencies (i.e., proportions/percentages within each group) instead of counts on the y-axis. This is analogous to using proportions/percentages in a contingency table. Unfortunately, it is rather difficult to do this with ggplot. A compromise is available: by using position = fill, you can create a stacked bar chart that scales every group to 100%. Making comparisons across groups can still be hard, as explained above for any kind of stacked bar chart, but it works okay if there are only two categories in the response variable (as is almost the case with sex here, although the missing data distorts things a little at the bottom).\n\nggplot(penguins, aes(fill = sex, x = species)) +\n    geom_bar(position = \"fill\")\n\n\n\n\nThis graph does correctly show that the sexes are pretty much equally balances across all three species.\n\nExercise 8(a)\nUsing species and island, create a side-by-side bar chart. Be careful, though, to change the sample code above to make sure species is now the response variable (using the fill aesthetic) and that island is the explanatory variable (using x). (Hey, that’s another hint to go back and look at the previous exercise and make sure you got part (a) right!)\n\n\n# Add code here to make a side-by-side bar chart.\n\n\n\n\nExercise 8(b)\nComment on the association or independence of the two variables.\n\nPlease write up your answer here."
  },
  {
    "objectID": "03-categorical_data-web.html#categorical-recoding",
    "href": "03-categorical_data-web.html#categorical-recoding",
    "title": "3  Categorical data",
    "section": "3.8 Recoding factor variables",
    "text": "3.8 Recoding factor variables\nAs mentioned earlier, there are situations where a categorical variable is not recorded in R as a factor variable. Let’s look at the year variable:\n\nglimpse(penguins$year)\n\n int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n\n\nThese appear as integers. Yes, years are whole numbers, but why might this variable be treated as categorical data and not numerical data?\n\nExercise 9(a)\nUse the tabyl command to create a frequency table for year.\n\n\n# Add code here to make a frequency table for year.\n\n\n\n\nExercise 9(b)\nWhy is year better thought of as categorical data and not numerical data (at least for this data set—we’re not claiming years should always be treated as categorical)?\n\nPlease write up your answer here.\n\n\nWhile the tabyl command seemed to work just fine with the year data in integer format, there are other commands that will not work so well. For example, ggplot often fails to do the right thing when a categorical variable is coded as a number. Therefore, we need a way to change numerically coded variables to factors.\nThe code below uses a command called mutate that takes an old variable and creates a new variable. (You’ll learn more about this command in a later chapter. For now, you can just copy and paste this code if you need it again.) The name of the new variable can be anything we want; we’ll just call it year_fct. Then the real work is being done by the as_factor command that concerts the numeric year variable into a factor variable.\nObserve the effect below:\n\npenguins &lt;- penguins %&gt;%\n    mutate(year_fct = as_factor(year))\nglimpse(penguins)\n\nRows: 344\nColumns: 9\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n$ year_fct          &lt;fct&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…\n\n\n\n\nExercise 10(a)\nMake a contingency table of the species measured in each year using counts. Use the species variable first, followed by the new factor variable year_fct. (Think about why that order makes sense. We will always list the response variable first so that the categories of interest will be the rows and the groups will be the columns.)\n\n\n# Add code here to make a contingency table for species and year with counts.\n\n\n\n\nExercise 10(b)\nMake a contingency table of the species measured in each year using column percentages (not proportions). (Again, be sure to use the new factor variable year_fct, not the old variable year.)\n\n\n# Add code here to make a contingency table for species and year with percentages.\n\n\n\n\nExercise 10(c)\nHow similar or dissimilar are the distributions of species across the three years of the study?\n\nPlease write up your answer here."
  },
  {
    "objectID": "03-categorical_data-web.html#categorical-pub",
    "href": "03-categorical_data-web.html#categorical-pub",
    "title": "3  Categorical data",
    "section": "3.9 Publication-ready graphics",
    "text": "3.9 Publication-ready graphics\nLet’s go back to the first relative frequency bar chart from this chapter.\n\nggplot(penguins, aes(x = species, y = ..prop.., group = 1)) +\n    geom_bar()\n\n\n\n\nThe variable name species is already informative, but the y-axis is labeled with “prop”. Also note that this graph could use a title. We can do all this with labs (for labels). Observe:\n\nggplot(penguins, aes(x = species, y = ..prop.., group = 1)) +\n    geom_bar() +\n    labs(title = \"Distribution of species\",\n         y = \"Proportion\",\n         x = \"Species\")\n\n\n\n\n\nExercise 11\nModify the following side-by-side bar chart by adding a title and labels for both the fill variable and the x-axis variable. (Hint: you can use fill = sex inside the labs command just like you used title, y, and x.)\n\n\n# Modify the following side-by-side bar chart by adding a title and \n# labels for both the x-axis and the fill variable.\nggplot(penguins, aes(fill = sex, x = species)) +\n    geom_bar(position = \"dodge\")"
  },
  {
    "objectID": "03-categorical_data-web.html#categorical-summary",
    "href": "03-categorical_data-web.html#categorical-summary",
    "title": "3  Categorical data",
    "section": "3.10 Plotting summary data",
    "text": "3.10 Plotting summary data\nEverything we did above was summarizing raw data; that is, the data consisted of all the observations for each individual penguin. Often, though, when you find data out in the wild, that data will be summarized into a table already and you may not have access to the raw data.\nFor example, let’s suppose that you found some data online, but it looked like this:\n\n\n\nspecies\ncount\n\n\n\n\nAdelie\n152\n\n\nChinstrap\n68\n\n\nGentoo\n124\n\n\n\nThis raises two questions:\n\nHow would you get this data into R?\nHow would you plot the data?\n\nTo answer the first question, we show you how to create your own tibble. Here is the syntax:\n\npenguin_species_table &lt;- tibble(\n    species = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"),\n    count = c(152, 68, 124)\n)\npenguin_species_table\n\n# A tibble: 3 × 2\n  species   count\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nBasically, the tibble command creates a new tibble. Then each column of data must be entered manually as a “vector” using the c to group all the data values together for each column. Be careful about the placement of quotation marks, commas, and parentheses.\nOnce we have our summary data, we want to make a bar chart. But this won’t work:\n\nggplot(penguin_species_table, aes(x = species)) +\n    geom_bar()\n\n\n\n\n\nExercise 12\nExplain what went wrong with the previous command? Why does ggplot think that each species has count 1?\n\nPlease write up your answer here.\n\n\nInstead, we need to use geom_col. This works a lot like geom_bar except that it also requires a y value in its aesthetics to force the command to look for the counts in some other variable in the data.\n\nggplot(penguin_species_table, aes(x = species, y = count)) +\n    geom_col()\n\n\n\n\n\n\nExercise 13(a)\nUse the tabyl command to create a frequency table for island.\n\n\n# Add code here to create a frequency table for island\n\n\n\n\nExercise 13(b)\nUse the tibble command to create a new tibble manually that contains the frequency data for the island variable. It should have two columns, one called island and the other called count. Name it penguin_island_table.\n\n\n# Add code here to create a tibble with frequency data for island\n\n\n\n\nExercise 13(c)\nUse ggplot with geom_col to create a bar chart for island.\n\n\n# Add code here to create a bar chart for island"
  },
  {
    "objectID": "03-categorical_data-web.html#bonus-section-recovering-raw-data-from-tables",
    "href": "03-categorical_data-web.html#bonus-section-recovering-raw-data-from-tables",
    "title": "3  Categorical data",
    "section": "3.11 Bonus section: Recovering raw data from tables",
    "text": "3.11 Bonus section: Recovering raw data from tables\nSometimes we come across summary data instead of raw data. We’ve learned how to manually create tibbles with that summary data and use geom_col instead of geom_bar to graph it, but sometimes it is also useful to recover what the raw data would have been. Fortunately there are R tools to do exactly that.\nWe’ll continue with our example penguin_species_table, which we’ll reprint here for reference:\n\npenguin_species_table\n\n# A tibble: 3 × 2\n  species   count\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\nFrom this table, we know what the raw data for this variable should look like: there should be 152 rows that say “Adelie,” 68 rows that say “Chinstrap,” and 124 rows that say “Gentoo.” It would be very annoying, though, to make that whole tibble by hand. Fortunately, there are R tools that will create it for us.\nThe first thing we will need to do is turn our tibble into a tabyl. (I would like to apologize for how ridiculous that sentence sounds.)\n\npenguin_species_tabyl &lt;- as_tabyl(penguin_species_table)\npenguin_species_tabyl\n\n   species count\n    Adelie   152\n Chinstrap    68\n    Gentoo   124\n\n\nThe hero of the day is the function uncount from the tidyr package:\n\npenguin_species_raw &lt;- penguin_species_tabyl %&gt;%\n  uncount(count)\npenguin_species_raw\n\n# A tibble: 344 × 1\n   species\n   &lt;chr&gt;  \n 1 Adelie \n 2 Adelie \n 3 Adelie \n 4 Adelie \n 5 Adelie \n 6 Adelie \n 7 Adelie \n 8 Adelie \n 9 Adelie \n10 Adelie \n# ℹ 334 more rows\n\n\nClick through the rows of this table and you’ll see that it’s exactly what we wanted: “Adelie” is repeated 152 times, “Chinstrap” is repeated 68 times, and “Gentoo” is repeated 124 times. Neat!\n\n3.11.1 Recovering raw data from a contingency table\nThis strategy also works, with some modifications, for recovering the raw data presented in a contingency table. Previously, we saw the following contingency table showing the counts of each species broken down by sex:\n\n\n\nsex\nAdelie\nChinstrap\nGentoo\n\n\n\n\nfemale\n73\n34\n58\n\n\nmale\n73\n34\n61\n\n\n\n(Note: I’ve removed the unruly penguins who did not allow their sex to be determined.)\nAgain, we can imagine what the raw data would look like: there would be 73 rows where the species variable would say “Adelie” and the sex variable would say “female,” then 34 rows where the species variable would say “Chinstrap” and the sex variable would say “female,” and so on.\nWe can start by building a tibble with this information in the same way we built the tibble of penguin species counts. Note that the species labels now become the column headers.\n\npenguin_species_sex_table &lt;- tibble(\n  sex = c(\"female\", \"male\"),\n  Adelie = c(73, 73),\n  Chinstrap = c(34, 34),\n  Gentoo = c(58, 61)\n)\npenguin_species_sex_table\n\n# A tibble: 2 × 4\n  sex    Adelie Chinstrap Gentoo\n  &lt;chr&gt;   &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 female     73        34     58\n2 male       73        34     61\n\n\nOnce again, we’ll want to turn this tibble into a tabyl:\n\npenguin_species_sex_tabyl &lt;- as_tabyl(penguin_species_sex_table)\npenguin_species_sex_tabyl\n\n    sex Adelie Chinstrap Gentoo\n female     73        34     58\n   male     73        34     61\n\n\nIn order for the uncount function to work correctly, we need to have all the counts in a single column, but since this is a contingency table, our counts are spread out across several columns. To solve this problem, we’ll need to “pivot” the columns, turning them into rows. The command is called pivot_longer. (There is also a pivot_wider command that turns rows into columns, but we won’t need that one.)\n\npenguin_species_sex_tabyl %&gt;%\n  pivot_longer(cols = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"))\n\n# A tibble: 6 × 3\n  sex    name      value\n  &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;\n1 female Adelie       73\n2 female Chinstrap    34\n3 female Gentoo       58\n4 male   Adelie       73\n5 male   Chinstrap    34\n6 male   Gentoo       61\n\n\nIf we want a little more control over the names of the newly created columnds, we can add those as follows:\n\npenguin_species_sex_tabyl %&gt;%\n  pivot_longer(cols = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"),\n               names_to = \"species\",\n               values_to = \"count\")\n\n# A tibble: 6 × 3\n  sex    species   count\n  &lt;chr&gt;  &lt;chr&gt;     &lt;dbl&gt;\n1 female Adelie       73\n2 female Chinstrap    34\n3 female Gentoo       58\n4 male   Adelie       73\n5 male   Chinstrap    34\n6 male   Gentoo       61\n\n\nNow our data is in the form that uncount knows how to deal with. And indeed, we can assemble all these steps together into a pipeline. First, we should build the tibble. Then, we should turn the tibble into a tabyl (sorry), then pivot the tabyl, and finally uncount to get back to the raw data. Finally, we should store the result as a new tibble. Here are all the steps put together:\n\npenguin_species_sex_table &lt;- tibble(\n  sex = c(\"female\", \"male\"),\n  Adelie = c(73, 73),\n  Chinstrap = c(34, 34),\n  Gentoo = c(58, 61)\n) \npenguin_species_sex_table %&gt;%\n  as_tabyl() %&gt;%\n  pivot_longer(cols = c(\"Adelie\", \"Chinstrap\", \"Gentoo\"),\n               names_to = \"species\",\n               values_to = \"count\") %&gt;%\n  uncount(count) -&gt; penguin_species_sex_raw\n\npenguin_species_sex_raw\n\n# A tibble: 333 × 2\n   sex    species\n   &lt;chr&gt;  &lt;chr&gt;  \n 1 female Adelie \n 2 female Adelie \n 3 female Adelie \n 4 female Adelie \n 5 female Adelie \n 6 female Adelie \n 7 female Adelie \n 8 female Adelie \n 9 female Adelie \n10 female Adelie \n# ℹ 323 more rows\n\n\nIndeed, this new tibble looks just like how we wanted it to look."
  },
  {
    "objectID": "03-categorical_data-web.html#categorical-conclusion",
    "href": "03-categorical_data-web.html#categorical-conclusion",
    "title": "3  Categorical data",
    "section": "3.12 Conclusion",
    "text": "3.12 Conclusion\nYou can summarize a single categorical variable using a frequency table. For only one categorical variable, a graph is usually overkill, but if you really want a graph, the bar chart is the best option. Both raw counts and proportions/percentages can be useful.\nWe use contingency tables to summarize two categorical variables. Unless groups are of equal size, raw counts can be incredibly misleading here. You should include proportions/percentages to be able to compare the distributions across groups. If the proportions/percentages are roughly the same, the variables are more likely to be independent, whereas if the proportions/percentages are different, there may be an association between the variables. For graphing, the best choice is usually a side-by-side bar chart. A stacked bar chart will also work, especially if using relative frequencies on the y-axis, but it can be hard to compare across groups when the response variable has three or more categories.\nSometimes we come across categorical data that is recorded using numbers. Many R commands will not work properly if they expect factors and receive numbers, so we use the mutate command to create a new variable along with as_factor to convert the numbers to categories.\nSometimes we come across summary data instead of raw data. We can then manually create tibbles with that summary data and use geom_col instead of geom_bar to graph it.\n\n3.12.1 Preparing and submitting your assignment\n\nFrom the “Run” menu, select “Restart R and Run All Chunks”.\nDeal with any code errors that crop up. Repeat steps 1–-2 until there are no more code errors.\nSpell check your document by clicking the icon with “ABC” and a check mark.\nHit the “Preview” button one last time to generate the final draft of the .nb.html file.\nProofread the HTML file carefully. If there are errors, go back and fix them, then repeat steps 1–5 again.\n\nIf you have completed this chapter as part of a statistics course, follow the directions you receive from your professor to submit your assignment."
  },
  {
    "objectID": "03-categorical_data-web.html#footnotes",
    "href": "03-categorical_data-web.html#footnotes",
    "title": "3  Categorical data",
    "section": "",
    "text": "Why the “2”? It’s a long story. Google it if you’re interested in the history of the development of the ggplot2 package.↩︎\nhttps://medium.com/the-mission/to-pie-charts-3b1f57bcb34a↩︎"
  },
  {
    "objectID": "04-numerical_data-web.html#numerical-intro",
    "href": "04-numerical_data-web.html#numerical-intro",
    "title": "4  Numerical data",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nIn this chapter, we’ll learn about numerical data and how to summarize it through summary statistics and graphs.\n\n4.1.1 Install new packages\nThere are no new packages used in this chapter.\n\n\n4.1.2 Download the R notebook file\nCheck the upper-right corner in RStudio to make sure you’re in your intro_stats project. Then click on the following link to download this chapter as an R notebook file (.Rmd).\nhttps://vectorposse.github.io/intro_stats/chapter_downloads/04-numerical_data.Rmd\nOnce the file is downloaded, move it to your project folder in RStudio and open it there.\n\n\n4.1.3 Restart R and run all chunks\nIn RStudio, select “Restart R and Run All Chunks” from the “Run” menu.\n\n\n4.1.4 Load packages\nWe load the tidyverse package to get ggplot2 and the palmerpenguins package to work with the penguin data.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(palmerpenguins)\n\nWarning: package 'palmerpenguins' was built under R version 4.3.1"
  },
  {
    "objectID": "04-numerical_data-web.html#numerical-notation",
    "href": "04-numerical_data-web.html#numerical-notation",
    "title": "4  Numerical data",
    "section": "4.2 A note about mathematical notation",
    "text": "4.2 A note about mathematical notation\nFrom time to time, we will use mathematical notation that can’t be typed directly on the keyboard. For example, let’s suppose we want to typeset the quadratic formula, which involves a complicated fraction as well as a square root symbol.\nWhen such notation appears, it will be surrounded by double dollar signs as follows:\n\\[\nx = \\frac{-b \\pm \\sqrt{b^{2} - 4ac}}{2a}\n\\]\nThe R Notebook will interpret this special mathematical notation and render it on the screen as well as in the HTML document.1 If the nicely formatted formula does not appear on your screen, place your cursor anywhere inside the math formula and hit Ctrl-Enter or Cmd-Enter (PC or Mac respectively).\nSometimes, we want such math to appear inline. We can do this with single dollar signs. For example, the distance formula is \\(d = \\sqrt{(x_{2} - x_{1})^{2} + (y_{2} - y_{1})^{2}}\\), a fact you may have learned a long time ago.\nThis will not render visually in the R Notebook, but it will show up in the HTML file. If you want to check that it worked properly without having to preview the HTML, you can either hover your cursor over the math formula and wait a second, or you can place your cursor anywhere inside the math formula and hit Ctrl-Enter or Cmd-Enter (PC or Mac respectively) to see a pop-up window previewing the mathematical content properly formatted.\nYou will be shown examples of any mathematical notation you need to use in any given chapter, so feel free to copy/paste/modify any math notation you need."
  },
  {
    "objectID": "04-numerical_data-web.html#numerical-statistics",
    "href": "04-numerical_data-web.html#numerical-statistics",
    "title": "4  Numerical data",
    "section": "4.3 Statistics",
    "text": "4.3 Statistics\nThe word “statistics” has several meanings. On one hand, it’s an entire field of study, as in the subject of this course. More specifically, though, a “statistic” is any kind of numerical summary of data. While there are many ways to summarize data, they mostly fall into two main flavors: measures of center and measures of spread. Measures of center try to estimate some kind of average, middle, or common value in data. Measures of spread try to estimate something like the width, range, variability, or uncertainty of data.\nThere are two pairs of measurements that we will learn about in this chapter: the mean/standard deviation, and the median/IQR.\n\n4.3.1 Mean and standard deviation\nThe first pair of the summary statistics we’ll discuss consists of the mean and the standard deviation.\nThe mean of a variable \\(y\\)—denoted \\(\\bar{y}\\) and pronounced “y bar”—is calculated by summing all the values of the variable, and dividing by the total number of observations. In formula form, this is\n\\[\n\\bar{y} = \\frac{\\sum y}{n}.\n\\]\nThis is a measure of center since it estimates the “middle” of a set of numbers. It is calculated in R using the mean command.\nThroughout this chapter, we will be using the penguins data set. (If you need a reminder, look at the help file for penguins using one of the methods discussed in Chapter 2.)\nIf we want to calculate the mean body mass of our penguins (in grams), we type the following:\n\nmean(penguins$body_mass_g)\n\n[1] NA\n\n\nUnfortunately, this didn’t give us an answer. As you may recall from previous chapters, this is because we are missing several values of body mass in this data. We need an extra piece of code to tell R to ignore that missing data and give us the mean of the valid data.\n\nmean(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 4201.754\n\n\n(The term na.rm stands for “NA remove”.)\nWe never leave such numbers without interpretation. In a full, contextually meaningful sentence, we might say, “The mean body mass of this group of penguins is approximately 4200 grams.”\nNotice that we mentioned the penguins, placing this number in context, and we mentioned the units of measurement, grams. (Otherwise, what would this number mean? 4200 pounds? Okay, probably not, but you should always mention the units of measurement.) Also notice that we rounded the final value. A gram is a very small unit of measurement, so there is no need to report this value to many decimal places.\nIf we use inline code, we can say, “The mean body mass of this group of penguins is 4201.754386 grams.” There are ways of rounding this number as well, but it’s a bit of a hassle to do so in inline code.\nThe corresponding measure of spread is the standard deviation. Usually this is called \\(s\\) and is calculated using a much more complicated formula:\n\\[\ns = \\sqrt{\\frac{\\sum (y - \\bar{y})^2}{n - 1}}.\n\\]\nThis is a measure of spread because the \\((y - \\bar{y})\\) term measures the how far away each data point is from the mean.\nIn R, this is calculated with the sd command. Again, we’ll need to add na.rm = TRUE.\n\nsd(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 801.9545\n\n\n“The standard deviation of this group of penguins is about 801 grams.”\nOr using inline code:\n“The standard deviation of this group of penguins is 801.9545357 grams.”\nThe mean and the standard deviation should always be reported together. One without the other is incomplete and potentially misleading.\nAnother related measurement is the variance, but this is nothing more than the standard deviation squared:\n\\[\ns^2 = \\frac{\\sum (y - \\bar{y})^2}{n - 1}.\n\\]\n(Compare this formula to the one for the standard deviation. Nothing has changed except for the removal of the square root.) We rarely use the variance in an introductory stats class because it’s not as interpretable as the standard deviation. The main reason for this is units. If the data units are grams, then both the mean and the standard deviation are also reported in grams. The variance has units of “grams squared”, but what does that even mean? If you need to calculate the variance in R, the command is var.\n\nvar(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 643131.1\n\n\nYou can check and see that the number above really is just 801.9545357 squared. Regarding the inline code in the previous sentence, remember, in the R Notebook, you can click inside the inline code and hit Ctrl-Enter or Cmd-Enter. In the HTML document, the number will be calculated and will magically appear.\n\n\n4.3.2 Median and IQR\nAnother choice for measuring the center and spread of a data set is the median and the IQR.\nThe median is just the middle value if the list of values is ordered. In R, it is calculated using the median command.\n\nmedian(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 4050\n\n\nThe median body mass of these penguins is 4050 grams.\nThe median value depends on whether there are an even or odd number of data points. If there are an odd number, there is a middle value in the list. Convince yourself this is true; for example, look at the numbers 1 through 7.\n\n1:7\n\n[1] 1 2 3 4 5 6 7\n\n\nThe number 4 is in the middle of the list, with three numbers to either side.\nHowever, if there are an even number of data points, there is no number right in the middle:\n\n1:8\n\n[1] 1 2 3 4 5 6 7 8\n\n\nThe “midpoint” of this list would lie between 4 and 5. If this is the case, we calculate the median by taking the mean of the two numbers straddling the middle. In the case of 1 though 8 above, the median would be 4.5.\nLet’s print out the entire body_mass_g variable, all 342 valid values (not including the missing values, of course). If we’re clever about it, we can see them in order using the sort command.\n\nsort(penguins$body_mass_g)\n\n  [1] 2700 2850 2850 2900 2900 2900 2900 2925 2975 3000 3000 3050 3050 3050 3050\n [16] 3075 3100 3150 3150 3150 3150 3175 3175 3200 3200 3200 3200 3200 3250 3250\n [31] 3250 3250 3250 3275 3300 3300 3300 3300 3300 3300 3325 3325 3325 3325 3325\n [46] 3350 3350 3350 3350 3350 3400 3400 3400 3400 3400 3400 3400 3400 3425 3425\n [61] 3450 3450 3450 3450 3450 3450 3450 3450 3475 3475 3475 3500 3500 3500 3500\n [76] 3500 3500 3500 3525 3525 3550 3550 3550 3550 3550 3550 3550 3550 3550 3575\n [91] 3600 3600 3600 3600 3600 3600 3600 3625 3650 3650 3650 3650 3650 3650 3675\n[106] 3675 3700 3700 3700 3700 3700 3700 3700 3700 3700 3700 3700 3725 3725 3725\n[121] 3750 3750 3750 3750 3750 3775 3775 3775 3775 3800 3800 3800 3800 3800 3800\n[136] 3800 3800 3800 3800 3800 3800 3825 3850 3875 3900 3900 3900 3900 3900 3900\n[151] 3900 3900 3900 3900 3950 3950 3950 3950 3950 3950 3950 3950 3950 3950 3975\n[166] 4000 4000 4000 4000 4000 4050 4050 4050 4050 4050 4050 4075 4100 4100 4100\n[181] 4100 4100 4150 4150 4150 4150 4150 4150 4200 4200 4200 4200 4200 4250 4250\n[196] 4250 4250 4250 4275 4300 4300 4300 4300 4300 4300 4300 4300 4350 4350 4375\n[211] 4400 4400 4400 4400 4400 4400 4400 4400 4450 4450 4450 4450 4450 4475 4500\n[226] 4500 4500 4550 4550 4575 4600 4600 4600 4600 4600 4625 4625 4650 4650 4650\n[241] 4650 4650 4675 4700 4700 4700 4700 4700 4700 4725 4725 4725 4750 4750 4750\n[256] 4750 4750 4775 4800 4800 4800 4850 4850 4850 4850 4875 4875 4875 4900 4900\n[271] 4925 4925 4950 4950 4975 5000 5000 5000 5000 5000 5000 5050 5050 5050 5100\n[286] 5100 5100 5150 5150 5200 5200 5200 5200 5250 5250 5250 5300 5300 5300 5300\n[301] 5350 5350 5350 5400 5400 5400 5400 5400 5450 5500 5500 5500 5500 5500 5550\n[316] 5550 5550 5550 5550 5550 5600 5600 5650 5650 5650 5700 5700 5700 5700 5700\n[331] 5750 5800 5800 5850 5850 5850 5950 5950 6000 6000 6050 6300\n\n\n\nExercise 1\nIf there are 342 penguins in this data set with body mass data, between which two values in the list above would the median lie? In other words, between what two positions in the list will be median be found? Verify that the median you find from this list is the same as the one we calculated with the median command above.\n\nPlease write up your answer here.\n\n\nCalculating the interquartile range—or IQR—requires first the calculation of the first and third quartiles, denoted Q1 and Q3. If the median is the 50% mark in the sorted data, the first and third quartiles are the 25% and the 75% marks, respectively. One way to compute these by hand is to calculate the median of the lower and upper halves of the data separately. Then again, it’s hard to know how to split the data set into halves if there are an odd number of observations. There are many different methods for computing percentiles in general, but you don’t need to worry too much about the particular implementation in R. One you have Q1 and Q3, the IQR is just\n\\[\nIQR = Q3 - Q1\n\\]\nIn R, you can get the IQR by using—are you ready for this?—the IQR command.\n\nIQR(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 1200\n\n\nThe IQR for this group of penguins is 1200 grams.\nThe IQR is a measure of spread because the distance between Q1 and Q3 measures the span of the “middle 50%” of the data.\nA general function for computing any percentile in R is the quantile function. For example, since Q1 is the 25th percentile, you can compute it as follows:\n\nQ1 &lt;- quantile(penguins$body_mass_g, 0.25, na.rm = TRUE)\nQ1\n\n 25% \n3550 \n\n\nThe 25% label is cute, but somewhat unnecessary, and it will mess up a later command, so let’s get rid of it:\n\nQ1 &lt;- unname(Q1)\nQ1\n\n[1] 3550\n\n\n\n\nExercise 2(a)\nNow you compute Q3.\n\n\n# Add code here to compute, store, and print out Q3\n\n\n\n\nExercise 2(b)\nReassign Q3 using the unname command as we did above to strip the unnecessary label.\n\n\n# Add code here that uses the unname command \n\n\n\n\nExercise 2(c)\nFinally, check that the IQR calculated above matches the value you get from subtracting Q3 minus Q1.\n\n\n# Add code here to compute Q3 - Q1.\n\n\n\nThe median and the IQR should always be reported together.\nAlso, don’t mix and match. For example, it doesn’t really make sense to report the mean and the IQR. Nor should you report the median and the standard deviation. They go together in pairs: either the mean and the standard deviation together, or the median and the IQR together.\n\n\n\n4.3.3 Robust statistics\nSome statistics are more sensitive than others to features of the data. For example, outliers are data points that are far away from the bulk of the data. The mean and especially the standard deviation can change a lot when outliers are present. Also, skewness in the data frequently pulls the mean too far in the direction of the skew while simultaneously inflating the standard deviation. (We’ll learn more about skewed data later in this chapter.)\nOn the other hand, the median and IQR are “robust”, meaning that they do not change much (or at all) in the presence of outliers and they tend to be good summaries even for skewed data.\n\nExercise 3\nExplain why the median and IQR are robust. In other words, why does an outlier have little or no influence on the median and IQR?\n\nPlease write up your answer here.\n\n\n\n\n\n4.3.4 Five-number summary\nA five-number summary is the minimum, Q1, median, Q3, and maximum of a set of numbers.\nThe summary command in R gives you the five-number summary, and throws in the mean for good measure. (Note that it does not require na.rm = TRUE!)\n\nsummary(penguins$body_mass_g)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n   2700    3550    4050    4202    4750    6300       2 \n\n\nYou can, of course, isolate the various pieces of this. You already know most of the commands below. (These individual commands all do require na.rm = TRUE.)\n\nmin(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 2700\n\n\n\nmedian(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 4050\n\n\n\nmax(penguins$body_mass_g, na.rm = TRUE)\n\n[1] 6300\n\n\nRemember the quantile function from earlier, where we computed Q1? We’re going to use it in a new way. Instead of what we did earlier,\nquantile(penguins$body_mass_g, 0.25, na.rm = TRUE),\nwhat about this instead?\n\nquantile(penguins$body_mass_g, na.rm = TRUE)\n\n  0%  25%  50%  75% 100% \n2700 3550 4050 4750 6300 \n\n\n\nExercise 4\nWhat is the difference between the way quantile was used in a previous exercise versus the way it was used here? How did that change the output?\n\nPlease write up your answer here.\n\n\nAlso, don’t forget about the trick for using R commands inline. If you need to mention a statistic in the middle of a sentence, there is no need to break the sentence and display a code chunk. Be sure you’re looking at the R notebook file (not the HTML file) to note that the numbers in the next sentence are not manually entered, but are calculated on the fly:\nThere are 344 penguins in this data set and their median body mass is 4050 grams.\n\n\nExercise 5\nType a full, contextually meaningful sentence using inline R code (as above, but changing the commands) reporting the minimum and maximum body mass (in grams) in our data set.\n\nPlease write up your answer here."
  },
  {
    "objectID": "04-numerical_data-web.html#numerical-graphing-one",
    "href": "04-numerical_data-web.html#numerical-graphing-one",
    "title": "4  Numerical data",
    "section": "4.4 Graphing one numerical variable",
    "text": "4.4 Graphing one numerical variable\nFrom the penguins data, let’s consider again the body mass in grams. This is clearly a numerical variable.\nThe single most useful display of a single numerical variable is a histogram. Here is the ggplot command to do that:\n\nggplot(penguins, aes(x = body_mass_g)) +\n    geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\n4.4.1 The shape of data\nThe way histograms work is to create “bins”, which are ranges of numbers along the x-axis. R goes through the data and counts how many observations fall into each bin. In that way, a histogram is somewhat like a bar chart. However, a bar chart uses bars to represent distinct, discrete categories, whereas a histogram uses bars that are all next to each other to represent values along a continuous numerical range. Histograms are meant to give you–at a quick glance–a sense of the “shape” of the data.\nWhat do we mean by “shape”? Generally, we look for three things:\n\nModes\n\n\nModes are peaks in the data. These are places where data tends to cluster, representing common values of the numerical variable. In the penguin data, there appears to be a big mode between about 3500 and 4000 grams. When data has one clear mode, we call the data unimodal. But data can also be bimodal, or more generally, multimodal. This often happens when the data contains multiple groups that are different from each other. In this case, we know there are three species of penguin in the data, so if those species are drastically different in their body mass, we might be looking at multimodal data. We’ll explore this question more later in the chapter. For now, it’s hard to say what’s going on because the above histogram has a lot of spiky bars popping up all over. It’s not completely obvious how many modes there might be.\n\n\nSymmetry\n\n\nIf there is one mode, we can also ask if the data is spread evenly to the left and right of that mode. If so, we call the data symmetric. No data is perfectly symmetric, but we are looking for overall balance between the areas to the left and right of the mode. When data is not symmetric, we call is skewed. Assuming that there is one big mode around 3500 or 4000, the body mass data above is skewed. There is clearly more data above the mode than below the mode. The right side of the histogram stretches out further to the right of the mode than to the left. Therefore, the body mass data is right-skewed. There is a longer “tail” to the right. If it were the opposite, it would be left-skewed. It is common for beginning students to confuse these two terms. Be aware that we are not concerned about where the mode is. We want to know which side has more data spread into a longer tail. That is the direction of the skewness.\n\n\nOutliers.\n\n\nOutliers are data points that are far from the bulk of the data. The body mass data above appears to have no outliers. We are looking for a large gap between the main “mass” of data and any lingering data points far away from that mass. There is no such large gap in the histogram above.\n\nWhenever you are asked about the “shape” of a numerical variable, be sure to comment on (1) modes, (2) symmetry, and (3) outliers.\nGenerally, the default binning for ggplot histograms is not great. This is by design. The creator of the gglot2 package, Hadley Wickham, said the following:\n\n“In ggplot2, a very simple heuristic is used for the default number of bins: it uses 30, regardless of the data. This is perverse, and ignores all of the research on selecting good bin sizes automatically, but sends a clear message to the user that he or she needs to think about, and experiment with, the bin width. This message is reinforced with a warning that reminds the user to manually adjust the bin width.”\n\nIndeed, if you look at the output from the graphing command above, you can see that ggplot informs you that you should pick a better value for the binwidth. You can also see that the bins aren’t ideal. They are too narrow, which means that arbitrary differences between bins show up as “random” spikes all over the graph. These spikes can confuse the issue of how many modes appear in the data.\nInstead, we should aim to use bins that show the overall shape of the data and smooth it out a bit. Look back at the scale of the x-axis to assess how wide each bar should be. There’s no one correct answer. In this case, the bins ought to be a little wider. Since our x-axis goes from about 2500 to 6500, maybe we should try a binwidth of 250. And if 250 doesn’t look good, nothing prevents us from trying a different number.\nIt’s also easier to interpret the histogram when the bins’ edges line up with numbers that are easy to see in the plot. Use boundary to determine where you want the bin boundaries to fall. For example, if we set the boundary to 3500, that means that one bar will start with its left edge at 3500. This is convenient because there is a tick mark labeled there on the x-axis. The boundary number is pretty arbitrary; once one boundary is set, it determines where all the other bins will line up. With a binwidth of 250, we’d get the same graph if the boundary were set to 3000 or 3250 or 5750, or even 0. Any other multiple of 250 would give the same graph.\nWe use binwidth and boundary inside the parentheses of the geom_histogram to modify these parameters.\n\nggplot(penguins, aes(x = body_mass_g)) +\n    geom_histogram(binwidth = 250, boundary = 3500)\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nEven with the smoother look, it appears that there are multiple modes, maybe three? Do these correspond to the three species of penguin? Stay tuned.\n\nExercise 6(a)\nHere is a histogram of the penguin bill lengths (measured in millimeters):\n\nggplot(penguins, aes(x = bill_length_mm)) +\n    geom_histogram(binwidth = 6, boundary = 30)\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nWrite a short paragraph describing the shape of the distribution of penguin bill lengths, focusing on the three key shape features (modes, symmetry, and outliers).\n\nPlease write up your answer here.\n\n\n\nExercise 6(b)\nThe last question was a trick question!\nChange the binwidth (no need to change the boundary) to something smaller to see more clearly the bimodal nature of the distribution.\n\n\n# Add code here that changes the binwidth of the last histogram to see\n# the bimodal nature of the distribution.\n\n\n\n\nExercise 7(a)\nMake a histogram of the variable flipper_length_mm. Start with a histogram where you don’t modify the binwidth or boundary.\n\n\n# Add code here to create a histogram of flipper length\n\n\n\n\nExercise 7(b)\nBy examining the scale on the x-axis above, repeat the command, but this time change the binwidth and the boundary until you are satisfied that the bins are neither too wide nor too narrow.\n\n\n# Add code here to modify the histogram of flipper length,\n# adding binwidth and boundary\n\n\n\n\nExercise 7(c)\nWrite a short paragraph describing the shape of the distribution of penguin flipper lengths, focusing on the three key shape features (modes, symmetry, and outliers).\n\nPlease write up your answer here.\n\n\n\n\n4.4.2 Less useful plot types\nThere are several other graph types that one might see for a single numerical variable: e.g., dotplots, stem-and-leaf plots, boxplots, etc. I’m not a big fan of dotplots or stem-and-leaf plots as they are just messier versions of histograms. I do like boxplots, but they are typically less informative than histograms. Boxplots are much better for comparing groups, and we’ll see them later in the chapter."
  },
  {
    "objectID": "04-numerical_data-web.html#numerical-graphing-two",
    "href": "04-numerical_data-web.html#numerical-graphing-two",
    "title": "4  Numerical data",
    "section": "4.5 Graphing two numerical variables",
    "text": "4.5 Graphing two numerical variables\nThe proper graph for two numerical variables is a scatterplot. We graph the response variable on the y-axis and the predictor variable on the x-axis.\nLet’s consider a possible association between bill length and body mass. For this question, there is not really a strong preference for which variable serves as response and which variable servers as predictor. We’ll consider bill length as the response variable and body mass as the predictor.\nSince we are plotting two variables, we have two aesthetics, one on the y-axis (the response variable) and one on the x-axis (the predictor variable). Since scatterplots use points to plot each data value, the correct layer to add is geom_point().\n\nggplot(penguins, aes(y = bill_length_mm, x = body_mass_g)) +\n    geom_point()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nWe are looking for evidence of a relationship between the two variables. This will manifest as a pattern in the data. We are interested in answering the following questions:\n\nLinearity\n\n\nIs the association linear? In other words, do the data points lie roughly in a straight line pattern? The scatterplot above is a bit “cloudy” but generally moves from lower left to upper right in a straight (not curved pattern). It’s not a completely random scatter of dots.\n\n\nDirection\n\n\nIf the pattern is linear, it is a positive relationship or a negative one? Positive means that the line moves from lower left to upper right. Negative means it moves from upper left to lower right. If you recall the direction of slopes from high school algebra class, a positive association corresponds to a line with a positive slope, and similarly for a negative association. In the data above, lower values of body mass correspond to lower bill lengths, and higher values of body mass correspond to higher bill lengths. So this is a positive association.\n\n\nStrength\n\n\nIf there is a pattern, how tight is the pattern? Do the data points stay close to a straight line, or are they pretty spread out and only generally moving in one direction. A strong relationship is one that is tightly packed around a line or curve. The relationship above is not strong. We might use terms like “weak”, “moderately weak”, or “moderate”, but definitely not strong.\n\n\nOutliers\n\n\nAre there outliers? These will be points that are isolated and relatively far from the bulk of the data. There are a few points above that are borderline, but none is a particularly strong outlier, especially give how spread out the rest of the data is.\n\n\nExercise 8\nHere is a scatterplot of\n\nggplot(penguins, aes(y = flipper_length_mm, x = body_mass_g)) +\n    geom_point()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nWrite a short paragraph describing the association of penguin flipper lengths and body mass, focusing on the four key features (linearity, direction, strength, and outliers).\n\nPlease write up your answer here."
  },
  {
    "objectID": "04-numerical_data-web.html#numerical-graphing-grouped",
    "href": "04-numerical_data-web.html#numerical-graphing-grouped",
    "title": "4  Numerical data",
    "section": "4.6 Graphing grouped numerical data",
    "text": "4.6 Graphing grouped numerical data\nSuppose you want to analyze one numerical variable and one categorical variable. Usually, the idea here is that the categorical variable divides up the data into groups and you are interested in understanding the numerical variable for each group separately. Another way to say this is that your numerical variable is response and your categorical variable is predictor. (It is also possible for a categorical variable to be response and a numerical variable to be predictor. This is common in so-called “classification” problems. We will not cover this possibility in this course, but it is covered in more advanced courses.)\nThis turns out to be exactly what we need in the penguins data. Throughout the above exercises, there was a concern that the penguin measurements are fundamentally different among three different species of penguin.\nGraphically, there are two good options here. The first is a side-by-side boxplot.\n\nggplot(penguins, aes(y = body_mass_g, x = species)) +\n    geom_boxplot()\n\nWarning: Removed 2 rows containing non-finite values (`stat_boxplot()`).\n\n\n\n\n\nNotice the placement of the variables. The y-axis is body_mass_g, the numerical variable. The x-axis variable is species; the groups are placed along the x-axis. This is consistent with other graph types that place the response variable on the y-axis and the predictor variable on the x-axis.\nThe other possible graph is a stacked histogram. This uses a feature called “faceting” that creates a different plot for each group. The syntax is a little unusual.\n\nggplot(penguins, aes(x = body_mass_g)) +\n    geom_histogram() +\n    facet_grid(species ~ .)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nThe argument species ~ . in the facet_grid function means, “Put each species on a different row.” We’ll explore this notation a little later.\nAs always, the default bins suck, so let’s change them.\n\nggplot(penguins, aes(x = body_mass_g)) +\n    geom_histogram(binwidth = 250, boundary = 3500) +\n    facet_grid(species ~ .)\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nConsider the following subtle change in notation:\n\nggplot(penguins, aes(x = body_mass_g)) +\n    geom_histogram(binwidth = 250, boundary = 3500) +\n    facet_grid(. ~ species)\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\n\nExercise 9(a)\nExplain why that last graph (which might be called a side-by-side histogram) is less effective than the earlier stacked histogram. (Hint: what stays lined up when the histograms are stacked vertically rather than horizontally?)\n\nPlease write up your answer here.\n\n\n\nExercise 9(b)\nCan you figure out what’s going on with the weird syntax of species ~ . vs . ~ species? Explain it in your own words.\n\nPlease write up your answer here.\n\n\nThe other thing that kind of sucks is the fact that the y-axis is showing counts. That makes it harder to see the distribution of body mass among Chinstrap penguins, for example, as there are fewer of them in the data set. It would be nice to scale these using percentages.\n\nggplot(penguins, aes(x = body_mass_g)) +\n    geom_histogram(aes(y = ..density..),\n                   binwidth = 250, boundary = 3500) +\n    facet_grid(species ~ .)\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nDue to some technical issues in ggplot2, these are not strictly proportions. (If you were to add up the heights of all the bars, they would not add up to 100%.) Nevertheless, the graph is still useful because it does scale the groups to put them on equal footing. In other words, it treats each group as if they all had the same sample size.\n\n\nExercise 10\nChoose a numerical variable that’s not body mass and a categorical variable that’s not species from the penguins data set. Make both a side-by-side boxplot and a stacked histogram. Discuss the resulting graphs. Comment on the association (or independence) of the two variables. If there is an association, be sure to focus on the four key features (linearity, direction, strength, and outliers).\n\n\n# Add code here to create a side-by-side boxplot.\n\n\n# Add code here to create a stacked histogram.\n\nPlease write up your answer here."
  },
  {
    "objectID": "04-numerical_data-web.html#numerical-pub",
    "href": "04-numerical_data-web.html#numerical-pub",
    "title": "4  Numerical data",
    "section": "4.7 Publication-ready graphics",
    "text": "4.7 Publication-ready graphics\nThe great thing about ggplot2 graphics is that they are already quite pretty. To take them from exploratory data analysis to the next level, there are a few things we can do to tidy them up.\nLet’s go back to the first histogram from this chapter.\n\nggplot(penguins, aes(x = body_mass_g)) +\n    geom_histogram(binwidth = 250, boundary = 3500)\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nThe variable names of this data set are already pretty informative, but we can do a little better with labs (for labels). Observe:\n\nggplot(penguins, aes(x = body_mass_g)) +\n    geom_histogram(binwidth = 250, boundary = 3500) +\n    labs(title = \"Distribution of body mass for adult foraging penguins near\n         Palmer Station, Antarctica\",\n         x = \"Body mass (in grams)\",\n         y = \"Count\")\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nYou can also see that we took the opportunity to mention the units of measurement (grams) for our variable in the x-axis label. This is good practice.\nA quick note about formatting in R code chunks. Notice that I put different parts of the last ggplot command on their own separate lines. The command would still work if I did this:\n\nggplot(penguins, aes(x = body_mass_g)) + geom_histogram(binwidth = 250, boundary = 3500) + labs(title = \"Distribution of body mass for adult foraging penguins near Palmer Station, Antarctica\", x = \"Body mass (in grams)\", y = \"Count\")\n\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nBut it’s much harder to read. If you find that your code is “wrapping” to the next line, find some spots like commas or plus signs to break the code. Be sure to break the line after the comma or plus sign.\n\nExercise 11\nModify the following scatterplot by adding a title and labels for both the y-axis and x-axis.\n\n\n# Modify the following scatterplot by adding a title and \n# labels for both the y-axis and x-axis.\nggplot(penguins, aes(y = bill_length_mm, x = bill_depth_mm)) +\n    geom_point()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\n\n\n\nExercise 12\nThe previous scatterplot looked a little funny due to some odd groupings that we suspect (as usual) might be due to multiple species being measures. Add a new aesthetic (so, inside the parentheses following aes) to the following code to assign color = species. Comment on what you see.\n\n\n# Modify the code below to add color = species\nggplot(penguins, aes(y = bill_length_mm, x = bill_depth_mm)) +\n    geom_point()\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nPlease write up your answer here.\n\n\nEvery part of the graph can be customized, from the color scheme to the tick marks on the axes, to the major and minor grid lines that appear on the background. We won’t go into all that, but you can look at the ggplot2 documentation online and search Google for examples if you want to dig in and figure out how to do some of that stuff. However, the default options are often (but not always) the best, so be careful that your messing around doesn’t inadvertently make the graph less clear or less appealing."
  },
  {
    "objectID": "04-numerical_data-web.html#numerical-conclusion",
    "href": "04-numerical_data-web.html#numerical-conclusion",
    "title": "4  Numerical data",
    "section": "4.8 Conclusion",
    "text": "4.8 Conclusion\nSummary statistics are simple numbers that describe and summarize data sets. Measures of center tell us where the “middle” of our numerical data lies, and measures of spread tell us how spread out our numerical data is. These measures should always be reported in pairs, for example the mean/standard deviation, or the median/IQR.\nThe ggplot2 package with its ggplot command is a very versatile tool for creating nice graphs relatively easily. For a single numerical variable, the standard graph type is a histogram. For two numerical variables, use a scatterplot. For a numerical response with a categorical predictor, use either a side-by-side boxplot or a stacked histogram.\n\n4.8.1 Preparing and submitting your assignment\n\nFrom the “Run” menu, select “Restart R and Run All Chunks”.\nDeal with any code errors that crop up. Repeat steps 1–-2 until there are no more code errors.\nSpell check your document by clicking the icon with “ABC” and a check mark.\nHit the “Preview” button one last time to generate the final draft of the .nb.html file.\nProofread the HTML file carefully. If there are errors, go back and fix them, then repeat steps 1–5 again.\n\nIf you have completed this chapter as part of a statistics course, follow the directions you receive from your professor to submit your assignment."
  },
  {
    "objectID": "04-numerical_data-web.html#footnotes",
    "href": "04-numerical_data-web.html#footnotes",
    "title": "4  Numerical data",
    "section": "",
    "text": "This notation is part of a mathematical document preparation system called LaTeX, pronounced “Lay-tek” (not like the rubbery substance).↩︎"
  },
  {
    "objectID": "05-manipulating_data-web.html#manipulating-intro",
    "href": "05-manipulating_data-web.html#manipulating-intro",
    "title": "5  Manipulating data",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nThis tutorial will import some data from the web and then explore it using the amazing dplyr package, a package which is quickly becoming the de facto standard among R users for manipulating data. It’s part of the tidyverse that we’ve already used in several chapters.\n\n5.1.1 Install new packages\nThere are no new packages used in this chapter.\n\n\n5.1.2 Download the R notebook file\nCheck the upper-right corner in RStudio to make sure you’re in your intro_stats project. Then click on the following link to download this chapter as an R notebook file (.Rmd).\nhttps://vectorposse.github.io/intro_stats/chapter_downloads/05-manipulating_data.Rmd\nOnce the file is downloaded, move it to your project folder in RStudio and open it there.\n\n\n5.1.3 Restart R and run all chunks\nIn RStudio, select “Restart R and Run All Chunks” from the “Run” menu.\n\n\n5.1.4 Load packages\nWe load the tidyverse package as usual, but this time it is to give us access to the dplyr package, which is loaded alongside our other tidyverse packages like ggplot2. The tidyverse also has a package called readr that will allow us to import data from an external source (in this case, a web site).\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "05-manipulating_data-web.html#manipulating-csv",
    "href": "05-manipulating_data-web.html#manipulating-csv",
    "title": "5  Manipulating data",
    "section": "5.2 Importing CSV data",
    "text": "5.2 Importing CSV data\nFor most of the chapters, we use data sets that are either included in base R or included in a package that can be loaded into R. But it is useful to see how to get a data set from outside the R ecosystem. This depends a lot on the format of the data file, but a common format is a “comma-separated values” file, or CSV file. If you have a data set that is not formatted as a CSV file, it is usually pretty easy to open it in something like Google Spreadsheets or Microsoft Excel and then re-save it as a CSV file.\nThe file we’ll import is a random sample from all the commercial domestic flights that departed from Houston, Texas, in 2011.\nWe use the read_csv command to import a CSV file. In this case, we’re grabbing the file from a web page where the file is hosted. If you have a file on your computer, you can also put the file into your project directory and import it from there. Put the URL (for a web page) or the filename (for a file in your project directory) in quotes inside the read_csvcommand. We also need to assign the output to a tibble, so we’ve called it hf for “Houston flights”.\n\nhf &lt;- read_csv(\"https://vectorposse.github.io/intro_stats/data/hf.csv\")\n\nRows: 22758 Columns: 21\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): UniqueCarrier, TailNum, Origin, Dest, CancellationCode\ndbl (16): Year, Month, DayofMonth, DayOfWeek, DepTime, ArrTime, FlightNum, A...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nhf\n\n# A tibble: 22,758 × 21\n    Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier FlightNum\n   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n 1  2011     1         12         3    1419    1515 AA                  428\n 2  2011     1         17         1    1530    1634 AA                  428\n 3  2011     1         24         1    1356    1513 AA                  428\n 4  2011     1          9         7     714     829 AA                  460\n 5  2011     1         18         2     721     827 AA                  460\n 6  2011     1         22         6     717     829 AA                  460\n 7  2011     1         11         2    1953    2051 AA                  533\n 8  2011     1         14         5    2119    2229 AA                  533\n 9  2011     1         26         3    2009    2103 AA                  533\n10  2011     1         14         5    1629    1734 AA                 1121\n# ℹ 22,748 more rows\n# ℹ 13 more variables: TailNum &lt;chr&gt;, ActualElapsedTime &lt;dbl&gt;, AirTime &lt;dbl&gt;,\n#   ArrDelay &lt;dbl&gt;, DepDelay &lt;dbl&gt;, Origin &lt;chr&gt;, Dest &lt;chr&gt;, Distance &lt;dbl&gt;,\n#   TaxiIn &lt;dbl&gt;, TaxiOut &lt;dbl&gt;, Cancelled &lt;dbl&gt;, CancellationCode &lt;chr&gt;,\n#   Diverted &lt;dbl&gt;\n\n\n\nglimpse(hf)\n\nRows: 22,758\nColumns: 21\n$ Year              &lt;dbl&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011…\n$ Month             &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ DayofMonth        &lt;dbl&gt; 12, 17, 24, 9, 18, 22, 11, 14, 26, 14, 18, 20, 3, 12…\n$ DayOfWeek         &lt;dbl&gt; 3, 1, 1, 7, 2, 6, 2, 5, 3, 5, 2, 4, 1, 3, 6, 4, 1, 3…\n$ DepTime           &lt;dbl&gt; 1419, 1530, 1356, 714, 721, 717, 1953, 2119, 2009, 1…\n$ ArrTime           &lt;dbl&gt; 1515, 1634, 1513, 829, 827, 829, 2051, 2229, 2103, 1…\n$ UniqueCarrier     &lt;chr&gt; \"AA\", \"AA\", \"AA\", \"AA\", \"AA\", \"AA\", \"AA\", \"AA\", \"AA\"…\n$ FlightNum         &lt;dbl&gt; 428, 428, 428, 460, 460, 460, 533, 533, 533, 1121, 1…\n$ TailNum           &lt;chr&gt; \"N577AA\", \"N518AA\", \"N531AA\", \"N586AA\", \"N558AA\", \"N…\n$ ActualElapsedTime &lt;dbl&gt; 56, 64, 77, 75, 66, 72, 58, 70, 54, 65, 135, 144, 64…\n$ AirTime           &lt;dbl&gt; 41, 48, 43, 51, 46, 47, 44, 45, 39, 47, 114, 111, 46…\n$ ArrDelay          &lt;dbl&gt; 5, 84, 3, -6, -8, -6, -29, 69, -17, -11, 39, -1, -2,…\n$ DepDelay          &lt;dbl&gt; 19, 90, -4, -6, 1, -3, -12, 74, 4, -1, 44, -5, -1, 1…\n$ Origin            &lt;chr&gt; \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IAH\", \"IA…\n$ Dest              &lt;chr&gt; \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DFW\", \"DF…\n$ Distance          &lt;dbl&gt; 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 96…\n$ TaxiIn            &lt;dbl&gt; 4, 8, 6, 11, 7, 18, 3, 5, 9, 8, 7, 20, 5, 8, 8, 7, 4…\n$ TaxiOut           &lt;dbl&gt; 11, 8, 28, 13, 13, 7, 11, 20, 6, 10, 14, 13, 13, 10,…\n$ Cancelled         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ CancellationCode  &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ Diverted          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\nThe one disadvantage of a file imported from the internet or your computer is that it does not come with a help file. (Only packages in R have help files.) Hopefully you have access to some kind of information about the data you’re importing. In this case, we get lucky because the full Houston flights data set happens to be available in a package called hflights.\n\nExercise 1\nGo to the help tab in RStudio and search for hflights. Of the several options that appear, click the one from the hflights package (listed as hflights::hflights). Review the help file so you know what all the variables mean. Report below how many cases are in the original hflights data. What fraction of the original data has been sampled in the CSV file we imported above?\n\nPlease write up your answer here."
  },
  {
    "objectID": "05-manipulating_data-web.html#manipulating-dplyr",
    "href": "05-manipulating_data-web.html#manipulating-dplyr",
    "title": "5  Manipulating data",
    "section": "5.3 Introduction to dplyr",
    "text": "5.3 Introduction to dplyr\nThe dplyr package (pronounced “dee-ply-er”) contains tools for manipulating the rows and columns of tibbles. The key to using dplyr is to familiarize yourself with the “key verbs”:\n\nselect (and rename)\nfilter (and slice)\narrange\nmutate (and transmute)\nsummarise (with group_by)\n\nWe’ll consider these one by one. We won’t have time to cover every aspect of these functions. More information appears in the help files, as well as this very helpful “cheat sheet”: https://raw.githubusercontent.com/rstudio/cheatsheets/main/data-transformation.pdf"
  },
  {
    "objectID": "05-manipulating_data-web.html#manipulating-select",
    "href": "05-manipulating_data-web.html#manipulating-select",
    "title": "5  Manipulating data",
    "section": "5.4 select",
    "text": "5.4 select\nThe select verb is very easy. It just selects some subset of variables (the columns of your data set).\nThe select command from the dplyr package illustrates one of the common issues R users face. Because the word “select” is pretty common, and selecting things is a common task, there are multiple packages that have a function called select. Depending on the order in which packages were loaded, R might get confused as to which version of select you want and try to apply the wrong one. One way to get the correct version is to specify the package in the syntax. Instead of typing select, we can type dplyr::select to ensure we get the version from the dplyr package. We’ll do this in all future uses of the select function. (The other functions in this chapter don’t cause us trouble because we don’t use any other packages whose functions conflict like this.)\nSuppose all we wanted to see was the carrier, origin, and destination. We would type\n\nhf_select &lt;- dplyr::select(hf, UniqueCarrier, Origin, Dest)\nhf_select\n\n# A tibble: 22,758 × 3\n   UniqueCarrier Origin Dest \n   &lt;chr&gt;         &lt;chr&gt;  &lt;chr&gt;\n 1 AA            IAH    DFW  \n 2 AA            IAH    DFW  \n 3 AA            IAH    DFW  \n 4 AA            IAH    DFW  \n 5 AA            IAH    DFW  \n 6 AA            IAH    DFW  \n 7 AA            IAH    DFW  \n 8 AA            IAH    DFW  \n 9 AA            IAH    DFW  \n10 AA            IAH    DFW  \n# ℹ 22,748 more rows\n\n\nA brief but important aside here: there is nothing special about the variable name hf_select. I could have typed\nbeef_gravy &lt;- dplyr::select(hf, UniqueCarrier, Origin, Dest)\nand it would work just as well. Generally speaking, though, you want to give variables a name that reflects the intent of your analysis.\nAlso, it is important to assign the result to a new variable. If I had typed\nhf &lt;- dplyr::select(hf, UniqueCarrier, Origin, Dest)\nthis would have overwritten the original tibble hf with this new version with only three variables. I want to preserve hf because I want to do other things with the entire data set later. The take-home message here is this: Major modifications to your data should generally be given a new variable name. There are caveats here, though. Every time you create a new variable, you also fill up more memory with your creation. If you check your Global Environment, you’ll see that both hf and hf_select are sitting in there. We’ll have more to say about this in a moment.\nOkay, back to the select function. The first argument of select is the tibble. After that, just list all the names of the variables you want to select.\nIf you don’t like the names of the variables, you can change them as part of the select process.\n\nhf_select &lt;- dplyr::select(hf,\n                           carrier = UniqueCarrier,\n                           origin = Origin,\n                           dest = Dest)\nhf_select\n\n# A tibble: 22,758 × 3\n   carrier origin dest \n   &lt;chr&gt;   &lt;chr&gt;  &lt;chr&gt;\n 1 AA      IAH    DFW  \n 2 AA      IAH    DFW  \n 3 AA      IAH    DFW  \n 4 AA      IAH    DFW  \n 5 AA      IAH    DFW  \n 6 AA      IAH    DFW  \n 7 AA      IAH    DFW  \n 8 AA      IAH    DFW  \n 9 AA      IAH    DFW  \n10 AA      IAH    DFW  \n# ℹ 22,748 more rows\n\n\n(Note here that I am overwriting hf_select which had been defined slightly differently before. However, these two versions of hf_select are basically the same object, so no need to keep two copies here.)\nThere are a few notational shortcuts. For example, see what the following do.\n\nhf_select2 &lt;- dplyr::select(hf, DayOfWeek:UniqueCarrier)\nhf_select2\n\n# A tibble: 22,758 × 4\n   DayOfWeek DepTime ArrTime UniqueCarrier\n       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;        \n 1         3    1419    1515 AA           \n 2         1    1530    1634 AA           \n 3         1    1356    1513 AA           \n 4         7     714     829 AA           \n 5         2     721     827 AA           \n 6         6     717     829 AA           \n 7         2    1953    2051 AA           \n 8         5    2119    2229 AA           \n 9         3    2009    2103 AA           \n10         5    1629    1734 AA           \n# ℹ 22,748 more rows\n\n\n\nhf_select3 &lt;- dplyr::select(hf, starts_with(\"Taxi\"))\nhf_select3\n\n# A tibble: 22,758 × 2\n   TaxiIn TaxiOut\n    &lt;dbl&gt;   &lt;dbl&gt;\n 1      4      11\n 2      8       8\n 3      6      28\n 4     11      13\n 5      7      13\n 6     18       7\n 7      3      11\n 8      5      20\n 9      9       6\n10      8      10\n# ℹ 22,748 more rows\n\n\n\nExercise 2\nWhat is contained in the new tibbles hf_select2 and hf_select3? In other words, what does the colon (:) appear to do and what does starts_with appear to do in the select function?\n\nPlease write up your answer here.\n\n\nThe cheat sheet shows a lot more of these “helper functions” if you’re interested.\nThe other command that’s related to select is rename. The only difference is that select will throw away any columns you don’t select (which is what you want and expect, typically), whereas rename will keep all the columns, but rename those you designate.\n\n\nExercise 3\nPutting a minus sign in front of a variable name in the select command will remove the variable. Create a tibble called hf_select4 that removes Year, DayofMonth, DayOfWeek, FlightNum, and Diverted. (Be careful with the unusual—and inconsistent!—capitalization in those variable names.) In the second part of the code chunk below, type hf_select4 so that the tibble prints to the screen (just like in all the above examples).\n\n\n# Add code here to define hf_select4.\n# Add code here to print hf_select4."
  },
  {
    "objectID": "05-manipulating_data-web.html#manipulating-rm",
    "href": "05-manipulating_data-web.html#manipulating-rm",
    "title": "5  Manipulating data",
    "section": "5.5 The rm command",
    "text": "5.5 The rm command\nRecall that earlier we mentioned the pros and cons of creating a new tibble every time we make a change. On one hand, making a new tibble instead of overwriting the original one will keep the original one available so that we can run different commands on it. On the other hand, making a new tibble does eat up a lot of memory.\nOne way to get rid of an object once we are done with it is the rm command, where rm is short for “remove”. When you run the code chunk below, you’ll see that all the tibbles we created with select will disappear from your Global Environment.\n\nrm(hf_select, hf_select2, hf_select3)\n\nIf you need one these tibbles back later, you can always go back and re-run the code chunk that defined it.\nWe’ll use rm at the end of some of the following sections so that we don’t use up too much memory.\n\nExercise 4\nRemove hf_select4 (that you created in Exercise 3) from the Global Environment.\n\n\n# Add code here to remove hf_select4."
  },
  {
    "objectID": "05-manipulating_data-web.html#manipulating-filter",
    "href": "05-manipulating_data-web.html#manipulating-filter",
    "title": "5  Manipulating data",
    "section": "5.6 filter",
    "text": "5.6 filter\nThe filter verb works a lot like select, but for rows instead of columns.\nFor example, let’s say we only want to see Delta flights. We use filter:\n\nhf_filter &lt;- filter(hf, UniqueCarrier == \"DL\")\nhf_filter\n\n# A tibble: 265 × 21\n    Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier FlightNum\n   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n 1  2011     1          4         2    1834    2134 DL                   54\n 2  2011     1          5         3    1606    1903 DL                    8\n 3  2011     1          5         3     543     834 DL                 1248\n 4  2011     1          7         5    1603    1902 DL                    8\n 5  2011     1          7         5    1245    1539 DL                 1204\n 6  2011     1          7         5     933    1225 DL                 1590\n 7  2011     1          8         6     921    1210 DL                 1590\n 8  2011     1         12         3      NA      NA DL                 1590\n 9  2011     1         13         4     928    1224 DL                 1590\n10  2011     1         13         4     656     947 DL                 1900\n# ℹ 255 more rows\n# ℹ 13 more variables: TailNum &lt;chr&gt;, ActualElapsedTime &lt;dbl&gt;, AirTime &lt;dbl&gt;,\n#   ArrDelay &lt;dbl&gt;, DepDelay &lt;dbl&gt;, Origin &lt;chr&gt;, Dest &lt;chr&gt;, Distance &lt;dbl&gt;,\n#   TaxiIn &lt;dbl&gt;, TaxiOut &lt;dbl&gt;, Cancelled &lt;dbl&gt;, CancellationCode &lt;chr&gt;,\n#   Diverted &lt;dbl&gt;\n\n\nIn the printout of the tibble above, if you can’t see the UniqueCarrier column, click the black arrow on the right to scroll through the columns until you can see it. You can click “Next” at the bottom to scroll through the rows.\n\nExercise 5\nHow many rows did we get in the hf_filter tibble? What do you notice about the UniqueCarrier of all those rows?\n\nPlease write up your answer here.\n\n\nJust like select, the first argument of filter is the name of the tibble. Following that, you must specify some condition. Only rows meeting that condition will be included in the output.\nOne thing that is unusual here is the double equal sign (UniqueCarrier == \"DL\"). This won’t be a mystery to people with programming experience, but it tends to be a sticking point for the rest of us. A single equals sign represents assignment. If I type x = 3, what I mean is, “Take the letter x and assign it the value 3.” In R, we would also write x &lt;- 3 to mean the same thing. The first line of the code chunk below assigns x to be 3. Therefore, the following line that just says x creates the output “3”.\n\nx = 3\nx\n\n[1] 3\n\n\nOn the other hand, x == 3 means something completely different. This is a logical statement that is either true or false. Either x is 3, in which case we get TRUE or x is not 3, and we get FALSE.\n\nx == 3\n\n[1] TRUE\n\n\n(It’s true because we just assigned x to be 3 in the previous code chunk!)\nIn the above filter command, we are saying, “Give me the rows where the value of UniqueCarrier is \"DL\", or, in other words, where the statement UniqueCarrier == \"DL\" is true.\nAs another example, suppose we wanted to find out all flights that leave before 6:00 a.m.\n\nhf_filter2 &lt;- filter(hf, DepTime &lt; 600)\nhf_filter2\n\n# A tibble: 230 × 21\n    Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier FlightNum\n   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n 1  2011     1         20         4     556     912 AA                 1994\n 2  2011     1         21         5     555     822 CO                  446\n 3  2011     1         18         2     555     831 CO                  446\n 4  2011     1         16         7     556     722 CO                  199\n 5  2011     1          5         3     558    1009 CO                   89\n 6  2011     1          1         6     558    1006 CO                   89\n 7  2011     1          5         3     543     834 DL                 1248\n 8  2011     1          3         1     555     749 US                  270\n 9  2011     1          6         4     556     801 US                  270\n10  2011     1         13         4     552     713 US                  270\n# ℹ 220 more rows\n# ℹ 13 more variables: TailNum &lt;chr&gt;, ActualElapsedTime &lt;dbl&gt;, AirTime &lt;dbl&gt;,\n#   ArrDelay &lt;dbl&gt;, DepDelay &lt;dbl&gt;, Origin &lt;chr&gt;, Dest &lt;chr&gt;, Distance &lt;dbl&gt;,\n#   TaxiIn &lt;dbl&gt;, TaxiOut &lt;dbl&gt;, Cancelled &lt;dbl&gt;, CancellationCode &lt;chr&gt;,\n#   Diverted &lt;dbl&gt;\n\n\n\n\nExercise 6\nLook at the help file for hflights again. Why do we have to use the number 600 in the command above? (Read the description of the DepTime variable.)\n\nPlease write up your answer here.\n\n\nIf we need two or more conditions, we use & for “and” and | for “or”. The following will give us only the Delta flights that departed before 6:00 a.m.\n\nhf_filter3 &lt;- filter(hf, UniqueCarrier == \"DL\" & DepTime &lt; 600)\nhf_filter3\n\n# A tibble: 30 × 21\n    Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier FlightNum\n   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n 1  2011     1          5         3     543     834 DL                 1248\n 2  2011     1         16         7     542     834 DL                 1248\n 3  2011     1         19         3     538     844 DL                 1248\n 4  2011     1         22         6     540     850 DL                 1248\n 5  2011     1         26         3     540     851 DL                 1248\n 6  2011     2         12         6     538     823 DL                 1248\n 7  2011     2         15         2     539     840 DL                 1248\n 8  2011     2         16         3     540     829 DL                 1248\n 9  2011     2         21         1     552     856 DL                 1248\n10  2011     3          2         3     557     902 DL                 2375\n# ℹ 20 more rows\n# ℹ 13 more variables: TailNum &lt;chr&gt;, ActualElapsedTime &lt;dbl&gt;, AirTime &lt;dbl&gt;,\n#   ArrDelay &lt;dbl&gt;, DepDelay &lt;dbl&gt;, Origin &lt;chr&gt;, Dest &lt;chr&gt;, Distance &lt;dbl&gt;,\n#   TaxiIn &lt;dbl&gt;, TaxiOut &lt;dbl&gt;, Cancelled &lt;dbl&gt;, CancellationCode &lt;chr&gt;,\n#   Diverted &lt;dbl&gt;\n\n\nAgain, check the cheat sheet for more complicated condition-checking if needed.\n\n\nExercise 7(a)\nThe symbol != means “not equal to” in R. Use the filter command to create a tibble called hf_filter4 that finds all flights except those flying into Salt Lake City (“SLC”). As before, print the output to the screen.\n\n\n# Add code here to define hf_filter4.\n# Add code here to print hf_filter4.\n\n\n\n\nExercise 7(b)\nBased on the output of the previous part, how many flights were there flying into SLC? (In other words, how many rows were removed from the original hf tibble to produce hf_filter4?)\n\nPlease write up your answer here.\n\n\n\nExercise 8\nUse the rm command to remove all the extra tibbles you created in this section with filter.\n\n\n# Add code here to remove all filtered tibbles.\n\n\n\nThe slice command is related, but fairly useless in practice. It will allow you to extract rows by position. So slice(hf, 1:10) will give you the first 10 rows. As a general rule, the information available in a tibble should never depend on the order in which the rows appear. Therefore, no function you run should make any assumptions about the ordering of your data. The only reason one might want to think about the order of data is for convenience in presenting that data visually for someone to inspect. And that brings us to…"
  },
  {
    "objectID": "05-manipulating_data-web.html#manipulating-arrange",
    "href": "05-manipulating_data-web.html#manipulating-arrange",
    "title": "5  Manipulating data",
    "section": "5.7 arrange",
    "text": "5.7 arrange\nThis just re-orders the rows, sorting on the values of one or more specified columns. As I mentioned before, in most data analyses you work with summaries of the data that do not depend on the order of the rows, so this is not quite as interesting as some of the other verbs. In fact, since the re-ordering is usually for the visual benefit of the reader, there is often no need to store the output in a new variable. We’ll just print the output to the screen.\n\narrange(hf, ActualElapsedTime)\n\n# A tibble: 22,758 × 21\n    Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier FlightNum\n   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n 1  2011    10          5         3    1656    1731 WN                 2493\n 2  2011     4         13         3    1207    1243 WN                 2025\n 3  2011     7         19         2    1043    1119 CO                 1583\n 4  2011     2         22         2    1426    1503 WN                 1773\n 5  2011     3         19         6    1629    1706 WN                 3805\n 6  2011     5         31         2    1937    2014 WN                  819\n 7  2011     7         16         6    1632    1709 WN                  912\n 8  2011     8         22         1    1708    1745 WN                 1754\n 9  2011     9         30         5    1955    2032 WN                 1959\n10  2011     9          1         4    1735    1812 WN                 1754\n# ℹ 22,748 more rows\n# ℹ 13 more variables: TailNum &lt;chr&gt;, ActualElapsedTime &lt;dbl&gt;, AirTime &lt;dbl&gt;,\n#   ArrDelay &lt;dbl&gt;, DepDelay &lt;dbl&gt;, Origin &lt;chr&gt;, Dest &lt;chr&gt;, Distance &lt;dbl&gt;,\n#   TaxiIn &lt;dbl&gt;, TaxiOut &lt;dbl&gt;, Cancelled &lt;dbl&gt;, CancellationCode &lt;chr&gt;,\n#   Diverted &lt;dbl&gt;\n\n\nScroll over to the ActualElapsedTime variable in the output above (using the black right arrow) to see that these are now sorted in ascending order.\n\nExercise 9\nHow long is the shortest actual elapsed time? Why is this flight so short? (Hint: look at the destination.) Which airline flies that route? You may have to use your best friend Google to look up airport and airline codes.\n\nPlease write up your answer here.\n\n\nIf you want descending order, do this:\n\narrange(hf, desc(ActualElapsedTime))\n\n# A tibble: 22,758 × 21\n    Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier FlightNum\n   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n 1  2011     2          4         5     941    1428 CO                    1\n 2  2011    11          8         2     937    1417 CO                    1\n 3  2011    11         11         5     930    1408 CO                    1\n 4  2011    12         30         5     936    1413 CO                    1\n 5  2011    12          8         4     935    1410 CO                    1\n 6  2011    10         17         1     938    1311 CO                    1\n 7  2011     6         27         1     936    1308 CO                    1\n 8  2011     3         24         4     926    1256 CO                    1\n 9  2011    12         27         2     935    1405 CO                    1\n10  2011     3          9         3     933    1402 CO                    1\n# ℹ 22,748 more rows\n# ℹ 13 more variables: TailNum &lt;chr&gt;, ActualElapsedTime &lt;dbl&gt;, AirTime &lt;dbl&gt;,\n#   ArrDelay &lt;dbl&gt;, DepDelay &lt;dbl&gt;, Origin &lt;chr&gt;, Dest &lt;chr&gt;, Distance &lt;dbl&gt;,\n#   TaxiIn &lt;dbl&gt;, TaxiOut &lt;dbl&gt;, Cancelled &lt;dbl&gt;, CancellationCode &lt;chr&gt;,\n#   Diverted &lt;dbl&gt;\n\n\n\n\nExercise 10\nHow long is the longest actual elapsed time? Why is this flight so long? Which airline flies that route? Again, you may have to use your best friend Google to look up airport and airline codes.\n\nPlease write up your answer here.\n\n\n\nExercise 11(a)\nYou can sort by multiple columns. The first column listed will be the first in the sort order, and then within each level of that first variable, the next column will be sorted, etc. Print a tibble that sorts first by destination (Dest) and then by arrival time (ArrTime), both in the default ascending order.\n\n\n# Add code here to sort hf first by Dest and then by ArrTime.\n\n\n\n\nExercise 11(b)\nBased on the output of the previous part, what is the first airport code alphabetically and to what city does it correspond? (Use Google if you need to link the airport code to a city name.) At what time did the earliest flight to that city arrive?\n\nPlease write up your answer here."
  },
  {
    "objectID": "05-manipulating_data-web.html#manipulating-mutate",
    "href": "05-manipulating_data-web.html#manipulating-mutate",
    "title": "5  Manipulating data",
    "section": "5.8 mutate",
    "text": "5.8 mutate\nFrequently, we want to create new variables that combine information from one or more existing variables. We use mutate for this. For example, suppose we wanted to find the total time of the flight. We might do this by adding up the minutes from several variables: TaxiOut, AirTime, and TaxiIn, and assigning that sum to a new variable called total. Scroll all the way to the right in the output below (using the black right arrow) to see the new total variable.\n\nhf_mutate &lt;- mutate(hf, total = TaxiOut + AirTime + TaxiIn)\nhf_mutate\n\n# A tibble: 22,758 × 22\n    Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier FlightNum\n   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n 1  2011     1         12         3    1419    1515 AA                  428\n 2  2011     1         17         1    1530    1634 AA                  428\n 3  2011     1         24         1    1356    1513 AA                  428\n 4  2011     1          9         7     714     829 AA                  460\n 5  2011     1         18         2     721     827 AA                  460\n 6  2011     1         22         6     717     829 AA                  460\n 7  2011     1         11         2    1953    2051 AA                  533\n 8  2011     1         14         5    2119    2229 AA                  533\n 9  2011     1         26         3    2009    2103 AA                  533\n10  2011     1         14         5    1629    1734 AA                 1121\n# ℹ 22,748 more rows\n# ℹ 14 more variables: TailNum &lt;chr&gt;, ActualElapsedTime &lt;dbl&gt;, AirTime &lt;dbl&gt;,\n#   ArrDelay &lt;dbl&gt;, DepDelay &lt;dbl&gt;, Origin &lt;chr&gt;, Dest &lt;chr&gt;, Distance &lt;dbl&gt;,\n#   TaxiIn &lt;dbl&gt;, TaxiOut &lt;dbl&gt;, Cancelled &lt;dbl&gt;, CancellationCode &lt;chr&gt;,\n#   Diverted &lt;dbl&gt;, total &lt;dbl&gt;\n\n\nAs it turns out, that was wasted effort because that variable already exists in ActualElapsedTime. The all.equal command below checks that both specified columns contain the exact same values.\n\nall.equal(hf_mutate$total, hf$ActualElapsedTime)\n\n[1] TRUE\n\n\nPerhaps we want a variable that just classifies a flight as arriving late or not. Scroll all the way to the right in the output below to see the new late variable.\n\nhf_mutate2 &lt;- mutate(hf, late = (ArrDelay &gt; 0))\nhf_mutate2\n\n# A tibble: 22,758 × 22\n    Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier FlightNum\n   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n 1  2011     1         12         3    1419    1515 AA                  428\n 2  2011     1         17         1    1530    1634 AA                  428\n 3  2011     1         24         1    1356    1513 AA                  428\n 4  2011     1          9         7     714     829 AA                  460\n 5  2011     1         18         2     721     827 AA                  460\n 6  2011     1         22         6     717     829 AA                  460\n 7  2011     1         11         2    1953    2051 AA                  533\n 8  2011     1         14         5    2119    2229 AA                  533\n 9  2011     1         26         3    2009    2103 AA                  533\n10  2011     1         14         5    1629    1734 AA                 1121\n# ℹ 22,748 more rows\n# ℹ 14 more variables: TailNum &lt;chr&gt;, ActualElapsedTime &lt;dbl&gt;, AirTime &lt;dbl&gt;,\n#   ArrDelay &lt;dbl&gt;, DepDelay &lt;dbl&gt;, Origin &lt;chr&gt;, Dest &lt;chr&gt;, Distance &lt;dbl&gt;,\n#   TaxiIn &lt;dbl&gt;, TaxiOut &lt;dbl&gt;, Cancelled &lt;dbl&gt;, CancellationCode &lt;chr&gt;,\n#   Diverted &lt;dbl&gt;, late &lt;lgl&gt;\n\n\nThis one is a little tricky. Keep in mind that ArrDelay &gt; 0 is a logical condition that is either true or false, so that truth value is what is recorded in the late variable. If the arrival delay is a positive number of minutes, the flight is considered “late”, and if the arrival delay is zero or negative, it’s not late.\nIf we would rather see more descriptive words than TRUE or FALSE, we have to do something even more tricky. Look at the late variable in the output below.\n\nhf_mutate3 &lt;- mutate(hf,\n                     late = as_factor(ifelse(ArrDelay &gt; 0, \n                                             \"Late\", \"On time\")))\nhf_mutate3\n\n# A tibble: 22,758 × 22\n    Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier FlightNum\n   &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n 1  2011     1         12         3    1419    1515 AA                  428\n 2  2011     1         17         1    1530    1634 AA                  428\n 3  2011     1         24         1    1356    1513 AA                  428\n 4  2011     1          9         7     714     829 AA                  460\n 5  2011     1         18         2     721     827 AA                  460\n 6  2011     1         22         6     717     829 AA                  460\n 7  2011     1         11         2    1953    2051 AA                  533\n 8  2011     1         14         5    2119    2229 AA                  533\n 9  2011     1         26         3    2009    2103 AA                  533\n10  2011     1         14         5    1629    1734 AA                 1121\n# ℹ 22,748 more rows\n# ℹ 14 more variables: TailNum &lt;chr&gt;, ActualElapsedTime &lt;dbl&gt;, AirTime &lt;dbl&gt;,\n#   ArrDelay &lt;dbl&gt;, DepDelay &lt;dbl&gt;, Origin &lt;chr&gt;, Dest &lt;chr&gt;, Distance &lt;dbl&gt;,\n#   TaxiIn &lt;dbl&gt;, TaxiOut &lt;dbl&gt;, Cancelled &lt;dbl&gt;, CancellationCode &lt;chr&gt;,\n#   Diverted &lt;dbl&gt;, late &lt;fct&gt;\n\n\nThe as_factor command tells R that late should be a categorical variable. Without it, the variable would be a “character” variable, meaning a list of character strings. It won’t matter for us here, but in any future analysis, you want categorical data to be treated as such by R.\nThe main focus here is on the ifelse construction. The ifelse function takes a condition as its first argument. If the condition is true, it returns the value in the second slot, and if it’s false (the “else” part of if/else), it returns the value in the third slot. In other words, if ArrDelay &gt; 0, this means the flight is late, so the new late variable should say “Late”; whereas, if ArrDelay is not greater than zero (so either zero or possibly negative if the flight arrived early), then the new variable should say “On Time”.\nHaving said that, I would generally recommend that you leave these kinds of variables as logical types. It’s much easier to summarize such variables in R, namely because R treats TRUE as 1 and FALSE as 0, allowing us to do things like this:\n\nmean(hf_mutate2$late, na.rm = TRUE)\n\n[1] 0.4761522\n\n\nThis gives us the proportion of late flights.\nNote that we needed na.rm as you’ve seen in previous chapter. For example, look at the 93rd row of the tibble:\n\nslice(hf_mutate2, 93)\n\n# A tibble: 1 × 22\n   Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier FlightNum\n  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;             &lt;dbl&gt;\n1  2011     1         27         4      NA      NA CO                  258\n# ℹ 14 more variables: TailNum &lt;chr&gt;, ActualElapsedTime &lt;dbl&gt;, AirTime &lt;dbl&gt;,\n#   ArrDelay &lt;dbl&gt;, DepDelay &lt;dbl&gt;, Origin &lt;chr&gt;, Dest &lt;chr&gt;, Distance &lt;dbl&gt;,\n#   TaxiIn &lt;dbl&gt;, TaxiOut &lt;dbl&gt;, Cancelled &lt;dbl&gt;, CancellationCode &lt;chr&gt;,\n#   Diverted &lt;dbl&gt;, late &lt;lgl&gt;\n\n\nNotice that all the times are missing. There are a bunch of rows like this. Since there is not always an arrival delay listed, the ArrDelay variable doesn’t always have a value, and if ArrDelay is NA, the late variable will be too. So if we try to calculate the mean with just the mean command, this happens:\n\nmean(hf_mutate2$late)\n\n[1] NA\n\n\n\nExercise 12\nWhy does taking the mean of a bunch of zeros and ones give us the proportion of ones? (Think about the formula for the mean. What happens when we take the sum of all the zeros and ones, and what happens when we divide by the total?)\n\nPlease write up your answer here.\n\n\n\nExercise 13\nCreate a new tibble called hf_mutate4 that uses the mutate command to create a new variable called dist_k which measures the flight distance in kilometers instead of miles. (Hint: to get from miles to kilometers, multiply the distance by 1.60934.) Print the output to the screen.\n\n\n# Add code here to define hf_mutate4.\n# Add code here to print hf_mutate4.\n\n\n\nA related verb is transmute. The only difference between mutate and transmute is that mutate creates the new column(s) and keeps all the old ones too, whereas transmute will throw away all the columns except the newly created ones. This is not something that you generally want to do, but there are exceptions. For example, if I was preparing a report and I needed only to talk about flights being late or not, it would do no harm (and would save some memory) to throw away everything except the late variable.\nBefore moving on to the next section, we’ll clean up the extra tibbles lying around. You’ll need to manually click the run button in the next code chunk since you have defined hf_mutate4.\n\nrm(hf_mutate, hf_mutate2, hf_mutate3, hf_mutate4)\n\nWarning in rm(hf_mutate, hf_mutate2, hf_mutate3, hf_mutate4): object\n'hf_mutate4' not found"
  },
  {
    "objectID": "05-manipulating_data-web.html#manipulating-summ-group",
    "href": "05-manipulating_data-web.html#manipulating-summ-group",
    "title": "5  Manipulating data",
    "section": "5.9 summarise (with group_by)",
    "text": "5.9 summarise (with group_by)\nFirst, before you mention that summarise is spelled wrong…well, the author of the dplyr package is named Hadley Wickham (same author as the ggplot2 package) and he is from New Zealand. So that’s the way he spells it. He was nice enough to include the summarize function as an alias if you need to use it ’cause this is ’Murica!\nThe summarise function, by itself, is kind of boring, and doesn’t do anything that couldn’t be done more easily with base R functions.\n\nsummarise(hf, mean(Distance))\n\n# A tibble: 1 × 1\n  `mean(Distance)`\n             &lt;dbl&gt;\n1             791.\n\n\n\nmean(hf$Distance)\n\n[1] 790.5861\n\n\nWhere summarise shines is in combination with group_by. For example, let’s suppose that we want to see average flight distances, but broken down by airline. We can do the following:\n\nhf_summ_grouped &lt;- group_by(hf, UniqueCarrier)\nhf_summ &lt;- summarise(hf_summ_grouped, mean(Distance))\nhf_summ\n\n# A tibble: 15 × 2\n   UniqueCarrier `mean(Distance)`\n   &lt;chr&gt;                    &lt;dbl&gt;\n 1 AA                        470.\n 2 AS                       1874 \n 3 B6                       1428 \n 4 CO                       1097.\n 5 DL                        723.\n 6 EV                        788.\n 7 F9                        883 \n 8 FL                        686.\n 9 MQ                        701.\n10 OO                        823.\n11 UA                       1204.\n12 US                        982.\n13 WN                        613.\n14 XE                        590.\n15 YV                        982.\n\n\n\n5.9.1 Piping\nThis is a good spot to introduce a time-saving and helpful device called “piping”, denoted by the symbol %&gt;%. We’ve seen this weird combination of symbols in past chapters, but we haven’t really explained what they do.\nPiping always looks more complicated than it really is. The technical definition is that\nx %&gt;% f(y)\nis equivalent to\nf(x, y).\nAs a simple example, we could add two numbers like this:\n\nsum(2, 3)\n\n[1] 5\n\n\nOr using the pipe, we could do it like this:\n\n2 %&gt;% sum(3)\n\n[1] 5\n\n\nAll this is really saying is that the pipe takes the thing on its left, and plugs it into the first slot of the function on its right. So why do we care?\nLet’s revisit the combination group_by/summarise example above. There are two ways to do this without pipes, and both are a little ugly. One way is above, where you have to keep reassigning the output to new variables (in the case above, to hf_summ_grouped and then hf_summ). The other way is to nest the functions:\n\nsummarise(group_by(hf, UniqueCarrier), mean(Distance))\n\n# A tibble: 15 × 2\n   UniqueCarrier `mean(Distance)`\n   &lt;chr&gt;                    &lt;dbl&gt;\n 1 AA                        470.\n 2 AS                       1874 \n 3 B6                       1428 \n 4 CO                       1097.\n 5 DL                        723.\n 6 EV                        788.\n 7 F9                        883 \n 8 FL                        686.\n 9 MQ                        701.\n10 OO                        823.\n11 UA                       1204.\n12 US                        982.\n13 WN                        613.\n14 XE                        590.\n15 YV                        982.\n\n\nThis requires a lot of brain power to parse. In part, this is because the function is inside-out: first you group hf by UniqueCarrier, and then the result of that is summarized. Here’s how the pipe fixes it:\n\nhf %&gt;%\n    group_by(UniqueCarrier) %&gt;%\n    summarise(mean(Distance))\n\n# A tibble: 15 × 2\n   UniqueCarrier `mean(Distance)`\n   &lt;chr&gt;                    &lt;dbl&gt;\n 1 AA                        470.\n 2 AS                       1874 \n 3 B6                       1428 \n 4 CO                       1097.\n 5 DL                        723.\n 6 EV                        788.\n 7 F9                        883 \n 8 FL                        686.\n 9 MQ                        701.\n10 OO                        823.\n11 UA                       1204.\n12 US                        982.\n13 WN                        613.\n14 XE                        590.\n15 YV                        982.\n\n\nLook at the group_by line. The group_by function should take two arguments, the tibble, and then the grouping variable. It appears to have only one argument. But look at the previous line. The pipe says to insert whatever is on its left (hf) into the first slot of the function on its right (group_by). So the net effect is still to evaluate the function group_by(hf, UniqueCarrier).\nNow look at the summarise line. Again, summarise is a function of two inputs, but all we see is the part that finds the mean. The pipe at the end of the previous line tells the summarise function to insert the stuff already computed (the grouped tibble returned by group_by(hf, UniqueCarrier)) into the first slot of the summarise function.\nPiping takes a little getting used to, but once you’re good at it, you’ll never go back. It’s just makes more sense semantically. When I read the above set of commands, I see a set of instructions in chronological order:\n\nStart with the tibble hf.\nNext, group by the carrier.\nNext, summarize each group using the mean distance.\n\nNow we can assign the result of all that to the new variable hf_summ:\n\nhf_summ &lt;- hf %&gt;%\n    group_by(UniqueCarrier) %&gt;%\n    summarise(mean(Distance))\nhf_summ\n\n# A tibble: 15 × 2\n   UniqueCarrier `mean(Distance)`\n   &lt;chr&gt;                    &lt;dbl&gt;\n 1 AA                        470.\n 2 AS                       1874 \n 3 B6                       1428 \n 4 CO                       1097.\n 5 DL                        723.\n 6 EV                        788.\n 7 F9                        883 \n 8 FL                        686.\n 9 MQ                        701.\n10 OO                        823.\n11 UA                       1204.\n12 US                        982.\n13 WN                        613.\n14 XE                        590.\n15 YV                        982.\n\n\nSome people even take this one step further. The result of all the above is assigned to a new variable hf_summ that currently appears as the first command (hf_summ &lt;- ...) But you could write this as\n\nhf %&gt;%\n    group_by(UniqueCarrier) %&gt;%\n    summarise(mean(Distance)) -&gt; hf_summ\n\nNow it says the following:\n\nStart with the tibble hf.\nNext, group by the carrier.\nNext, summarize each group using the mean distance.\nFinally, assign the result to a new variable called hf_summ.\n\nIn other words, the arrow operator for assignment works both directions!\nLet’s try some counting. This one is common enough that dplyr doesn’t even make us use group_by and summarise. We can just use the command count. What if we wanted to know how many flights correspond to each carrier?\n\nhf_summ2 &lt;- hf %&gt;%\n    count(UniqueCarrier)\nhf_summ2\n\n# A tibble: 15 × 2\n   UniqueCarrier     n\n   &lt;chr&gt;         &lt;int&gt;\n 1 AA              325\n 2 AS               37\n 3 B6               70\n 4 CO             7004\n 5 DL              265\n 6 EV              221\n 7 F9               84\n 8 FL              214\n 9 MQ              465\n10 OO             1607\n11 UA              208\n12 US              409\n13 WN             4535\n14 XE             7306\n15 YV                8\n\n\nAlso note that we can give summary columns a new name if we wish. In hf_summ, we didn’t give the new column an explicit name, so it showed up in our tibble as a column called mean(Distance). If we want to change it, we can do this:\n\nhf_summ &lt;- hf %&gt;%\n    group_by(UniqueCarrier) %&gt;%\n    summarise(mean_dist = mean(Distance))\nhf_summ\n\n# A tibble: 15 × 2\n   UniqueCarrier mean_dist\n   &lt;chr&gt;             &lt;dbl&gt;\n 1 AA                 470.\n 2 AS                1874 \n 3 B6                1428 \n 4 CO                1097.\n 5 DL                 723.\n 6 EV                 788.\n 7 F9                 883 \n 8 FL                 686.\n 9 MQ                 701.\n10 OO                 823.\n11 UA                1204.\n12 US                 982.\n13 WN                 613.\n14 XE                 590.\n15 YV                 982.\n\n\nLook at the earlier version of hf_summ and compare it to the one above. Make sure you see that the name of the second column changed.\nThe new count column of hf_summ2 is just called n. That’s okay, but if we insist on giving it a more user-friendly name, we can do so as follows:\n\nhf_summ2 &lt;- hf %&gt;%\n    count(UniqueCarrier, name = \"total_count\")\nhf_summ2\n\n# A tibble: 15 × 2\n   UniqueCarrier total_count\n   &lt;chr&gt;               &lt;int&gt;\n 1 AA                    325\n 2 AS                     37\n 3 B6                     70\n 4 CO                   7004\n 5 DL                    265\n 6 EV                    221\n 7 F9                     84\n 8 FL                    214\n 9 MQ                    465\n10 OO                   1607\n11 UA                    208\n12 US                    409\n13 WN                   4535\n14 XE                   7306\n15 YV                      8\n\n\nThis is a little different because it requires us to use a name argument and put the new name in quotes.\n\nExercise 14(a)\nCreate a tibble called hf_summ3 that lists the total count of flights for each day of the week. Be sure to use the pipe as above. Print the output to the screen. (You don’t need to give the count column a new name.)\n\n\n# Add code here to define hf_summ3.\n# Add code here to print hf_summ3.\n\n\n\n\nExercise 14(b)\nAccording to the output in the previous part, what day of the week had the fewest flights? (Assume 1 = Monday.)\n\nPlease write up your answer here.\n\n\nThe tibbles created in this section are all just a few rows each. They don’t take up much memory, so we don’t really need to remove them. You can if you want, but it’s not necessary."
  },
  {
    "objectID": "05-manipulating_data-web.html#manipulating-all-together",
    "href": "05-manipulating_data-web.html#manipulating-all-together",
    "title": "5  Manipulating data",
    "section": "5.10 Putting it all together",
    "text": "5.10 Putting it all together\nOften we need more than one of these verbs. In many data analyses, we need to do a sequence of operations to get at the answer we seek. This is most easily accomplished using a more complicated sequence of pipes.\nHere’s a example of multi-step piping. Let’s say that we only care about Delta flights, and even then, we only want to know about the month of the flight and the departure delay. From there, we wish to group by month so we can find the maximum departure delay by month. Here is a solution, piping hot and ready to go. [groan]\n\nhf_grand_finale &lt;- hf %&gt;%\n    filter(UniqueCarrier == \"DL\") %&gt;%\n    dplyr::select(Month, DepDelay) %&gt;%\n    group_by(Month) %&gt;%\n    summarise(max_delay = max(DepDelay, na.rm = TRUE))\nhf_grand_finale\n\n# A tibble: 12 × 2\n   Month max_delay\n   &lt;dbl&gt;     &lt;dbl&gt;\n 1     1        26\n 2     2       460\n 3     3       202\n 4     4        23\n 5     5       127\n 6     6       184\n 7     7       360\n 8     8        48\n 9     9       292\n10    10        90\n11    11        10\n12    12        14\n\n\nGo through each line of code carefully and translate it into English:\n\nWe define a variable called hf_grand_finale that starts with the original hf data.\nWe filter this data so that only Delta flights will be analyzed.\nWe select the variables Month and DepDelay, throwing away all other variables that are not of interest to us. (Don’t forget to use the dplyr::select syntax to make sure we get the right function!)\nWe group_by month so that the results will be displayed by month.\nWe summarise each month by listing the maximum value of DepDelay that appears within each month.\nWe print the result to the screen.\n\nNotice in the summarise line, we again took advantage of dplyr’s ability to rename any variable along the way, assigning our computation to the new variable max_delay. Also note the need for na.rm = TRUE so that the max command ignores any missing values.\nA minor simplification results from the realization that summarise must throw away any variables it doesn’t need. (Think about why for a second: what would summarise do with, say, ArrTime if we’ve only asked it to calculate the maximum value of DepDelay for each month?) So you could write this instead, removing the select clause:\n\nhf_grand_finale &lt;- hf %&gt;%\n    filter(UniqueCarrier == \"DL\") %&gt;%\n    group_by(Month) %&gt;%\n    summarise(max_delay = max(DepDelay, na.rm = TRUE))\nhf_grand_finale\n\n# A tibble: 12 × 2\n   Month max_delay\n   &lt;dbl&gt;     &lt;dbl&gt;\n 1     1        26\n 2     2       460\n 3     3       202\n 4     4        23\n 5     5       127\n 6     6       184\n 7     7       360\n 8     8        48\n 9     9       292\n10    10        90\n11    11        10\n12    12        14\n\n\nCheck that you get the same result. With massive data sets, it’s possible that the selection and sequence of these verbs matter, but you don’t see an appreciable difference here, even with 22758 rows. (There are ways of benchmarking performance in R, but that is a more advanced topic.)\n\nExercise 15\nSummarize in your own words what information is contained in the hf_grand_finale tibble. In other words, what are the numbers in the max_delay column telling us? Be specific!\n\nPlease write up your answer here.\n\nThe remaining exercises are probably the most challenging you’ve seen so far in the course. Take each slowly. Read the instructions carefully. Go back through the chapter and identify which “verb” needs to be used for each part of the task. Examine the sample code in those sections carefully to make sure you get the syntax right. Create a sequence of “pipes” to do each task, one-by-one. Copy and paste pieces of code from earlier and make minor changes to adapt the code to the given task.\n\n\nExercise 16\nCreate a tibble that counts the flights to LAX grouped by day of the week. (Hint: you need to filter to get flights to LAX. Then you’ll need to count using DayOfWeek as a grouping variable. Because you’re using count, you don’t need group_by or summarise.) Print the output to the screen.\n\n\n# Add code here to count the flights to LAX\n# grouped by day of the week.\n# Print the output to the screen.\n\n\n\n\nExercise 17\nCreate a tibble that finds the median distance flight for each airline. Sort the resulting tibble from highest distance to lowest. (Hint: You’ll need to group_by carrier and summarise using the median function. Finally, you’ll need to arrange the result according to the median distance variable that you just created.) Print the output to the screen.\n\n\n# Add code here to find the median distance by airline.\n# Print the output to the screen."
  },
  {
    "objectID": "05-manipulating_data-web.html#manipulating-conclusion",
    "href": "05-manipulating_data-web.html#manipulating-conclusion",
    "title": "5  Manipulating data",
    "section": "5.11 Conclusion",
    "text": "5.11 Conclusion\nRaw data often doesn’t come in the right form for us to run our analyses. The dplyr verbs are powerful tools for manipulating tibbles until they are in the right form.\n\n5.11.1 Preparing and submitting your assignment\n\nFrom the “Run” menu, select “Restart R and Run All Chunks”.\nDeal with any code errors that crop up. Repeat steps 1–-2 until there are no more code errors.\nSpell check your document by clicking the icon with “ABC” and a check mark.\nHit the “Preview” button one last time to generate the final draft of the .nb.html file.\nProofread the HTML file carefully. If there are errors, go back and fix them, then repeat steps 1–5 again.\n\nIf you have completed this chapter as part of a statistics course, follow the directions you receive from your professor to submit your assignment."
  },
  {
    "objectID": "06-correlation-web.html#correlation-intro",
    "href": "06-correlation-web.html#correlation-intro",
    "title": "6  Correlation",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nIn this chapter, we will learn about the concept of correlation, which is a way of measuring a linear relationship between two numerical variables.\n\n6.1.1 Install new packages\nIf you are using RStudio Workbench, you do not need to install any packages. (Any packages you need should already be installed by the server administrators.)\nIf you are using R and RStudio on your own machine instead of accessing RStudio Workbench through a browser, you’ll need to type the following command at the Console:\ninstall.packages(\"faraway\")\n\n\n6.1.2 Download the R notebook file\nCheck the upper-right corner in RStudio to make sure you’re in your intro_stats project. Then click on the following link to download this chapter as an R notebook file (.Rmd).\nhttps://vectorposse.github.io/intro_stats/chapter_downloads/06-correlation.Rmd\nOnce the file is downloaded, move it to your project folder in RStudio and open it there.\n\n\n6.1.3 Restart R and run all chunks\nIn RStudio, select “Restart R and Run All Chunks” from the “Run” menu.\n\n\n6.1.4 Load packages\nWe load the now-standard tidyverse package. We also include the faraway package to access data about Chicago in the 1970s.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(faraway)\n\nWarning: package 'faraway' was built under R version 4.3.1"
  },
  {
    "objectID": "06-correlation-web.html#correlation-redlining",
    "href": "06-correlation-web.html#correlation-redlining",
    "title": "6  Correlation",
    "section": "6.2 Redlining in Chicago",
    "text": "6.2 Redlining in Chicago\nThe data set we will use throughout this chapter is from Chicago in the 1970s studying the practice of “redlining”.\n\nExercise 1\nDo an internet search for “redlining”.\nConsult at least two or three sources. Then, in your own words (not copied and pasted from any of the websites you consulted), explain what “redlining” means.\n\nPlease write up your answer here.\n\n\nThe chredlin data set appears in the faraway package accompanying a book by Julian Faraway (Practical Regression and Anova using R, 2002.) Faraway explains:\n\n“In a study of insurance availability in Chicago, the U.S. Commission on Civil Rights attempted to examine charges by several community organizations that insurance companies were redlining their neighborhoods, i.e. canceling policies or refusing to insure or renew. First the Illinois Department of Insurance provided the number of cancellations, non-renewals, new policies, and renewals of homeowners and residential fire insurance policies by ZIP code for the months of December 1977 through February 1978. The companies that provided this information account for more than 70% of the homeowners insurance policies written in the City of Chicago. The department also supplied the number of FAIR plan policies written an renewed in Chicago by zip code for the months of December 1977 through May 1978. Since most FAIR plan policyholders secure such coverage only after they have been rejected by the voluntary market, rather than as a result of a preference for that type of insurance, the distribution of FAIR plan policies is another measure of insurance availability in the voluntary market.”\n\nIn other words, the degree to which residents obtained FAIR policies can be seen as an indirect measure of redlining. This participation in an “involuntary” market is thought to be largely driven by rejection of coverage under more traditional insurance plans.\n\n\n6.2.1 Exploratory data analysis\nBefore we learn about correlation, let’s get to know our data a little better.\nType ?chredlin at the Console to read the help file. While it’s not very informative about how the data was collected, it does have crucial information about the way the data is structured.\nHere is the data set:\n\nchredlin\n\n      race fire theft  age involact income side\n60626 10.0  6.2    29 60.4      0.0 11.744    n\n60640 22.2  9.5    44 76.5      0.1  9.323    n\n60613 19.6 10.5    36 73.5      1.2  9.948    n\n60657 17.3  7.7    37 66.9      0.5 10.656    n\n60614 24.5  8.6    53 81.4      0.7  9.730    n\n60610 54.0 34.1    68 52.6      0.3  8.231    n\n60611  4.9 11.0    75 42.6      0.0 21.480    n\n60625  7.1  6.9    18 78.5      0.0 11.104    n\n60618  5.3  7.3    31 90.1      0.4 10.694    n\n60647 21.5 15.1    25 89.8      1.1  9.631    n\n60622 43.1 29.1    34 82.7      1.9  7.995    n\n60631  1.1  2.2    14 40.2      0.0 13.722    n\n60646  1.0  5.7    11 27.9      0.0 16.250    n\n60656  1.7  2.0    11  7.7      0.0 13.686    n\n60630  1.6  2.5    22 63.8      0.0 12.405    n\n60634  1.5  3.0    17 51.2      0.0 12.198    n\n60641  1.8  5.4    27 85.1      0.0 11.600    n\n60635  1.0  2.2     9 44.4      0.0 12.765    n\n60639  2.5  7.2    29 84.2      0.2 11.084    n\n60651 13.4 15.1    30 89.8      0.8 10.510    n\n60644 59.8 16.5    40 72.7      0.8  9.784    n\n60624 94.4 18.4    32 72.9      1.8  7.342    n\n60612 86.2 36.2    41 63.1      1.8  6.565    n\n60607 50.2 39.7   147 83.0      0.9  7.459    n\n60623 74.2 18.5    22 78.3      1.9  8.014    s\n60608 55.5 23.3    29 79.0      1.5  8.177    s\n60616 62.3 12.2    46 48.0      0.6  8.212    s\n60632  4.4  5.6    23 71.5      0.3 11.230    s\n60609 46.2 21.8     4 73.1      1.3  8.330    s\n60653 99.7 21.6    31 65.0      0.9  5.583    s\n60615 73.5  9.0    39 75.4      0.4  8.564    s\n60638 10.7  3.6    15 20.8      0.0 12.102    s\n60629  1.5  5.0    32 61.8      0.0 11.876    s\n60636 48.8 28.6    27 78.1      1.4  9.742    s\n60621 98.9 17.4    32 68.6      2.2  7.520    s\n60637 90.6 11.3    34 73.4      0.8  7.388    s\n60652  1.4  3.4    17  2.0      0.0 13.842    s\n60620 71.2 11.9    46 57.0      0.9 11.040    s\n60619 94.1 10.5    42 55.9      0.9 10.332    s\n60649 66.1 10.7    43 67.5      0.4 10.908    s\n60617 36.4 10.8    34 58.0      0.9 11.156    s\n60655  1.0  4.8    19 15.2      0.0 13.323    s\n60643 42.5 10.4    25 40.8      0.5 12.960    s\n60628 35.1 15.6    28 57.8      1.0 11.260    s\n60627 47.4  7.0     3 11.4      0.2 10.080    s\n60633 34.0  7.1    23 49.2      0.3 11.428    s\n60645  3.1  4.9    27 46.6      0.0 13.731    n\n\n\n\nExercise 2\nWhat do each of the rows of this data set represent? You’ll need to refer to the help file. (They are not individual people.)\n\nPlease write up your answer here.\n\n\n\nExercise 3\nThe race variable is numeric. Why? What do these numbers represent? (Again, refer to the help file.)\n\nPlease write up your answer here.\n\n\nThe glimpse command gives a concise overview of all the variables present.\n\nglimpse(chredlin)\n\nRows: 47\nColumns: 7\n$ race     &lt;dbl&gt; 10.0, 22.2, 19.6, 17.3, 24.5, 54.0, 4.9, 7.1, 5.3, 21.5, 43.1…\n$ fire     &lt;dbl&gt; 6.2, 9.5, 10.5, 7.7, 8.6, 34.1, 11.0, 6.9, 7.3, 15.1, 29.1, 2…\n$ theft    &lt;dbl&gt; 29, 44, 36, 37, 53, 68, 75, 18, 31, 25, 34, 14, 11, 11, 22, 1…\n$ age      &lt;dbl&gt; 60.4, 76.5, 73.5, 66.9, 81.4, 52.6, 42.6, 78.5, 90.1, 89.8, 8…\n$ involact &lt;dbl&gt; 0.0, 0.1, 1.2, 0.5, 0.7, 0.3, 0.0, 0.0, 0.4, 1.1, 1.9, 0.0, 0…\n$ income   &lt;dbl&gt; 11.744, 9.323, 9.948, 10.656, 9.730, 8.231, 21.480, 11.104, 1…\n$ side     &lt;fct&gt; n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n, n…\n\n\n\n\nExercise 4(a)\nWhich variable listed above represents participation in the FAIR plan? How is it measured? (Again, refer to the help file.)\n\nPlease write up your answer here.\n\n\n\nExercise 4(b)\nWhy is it important to analyze the number of plans per 100 housing units as opposed to the total number of plans across each ZIP code? (Hint: what happens if some ZIP codes are larger than others?)\n\nPlease write up your answer here.\n\n\nWe are interested in the association between race and involact. If redlining plays a role in driving people toward FAIR plan policies, we would expect there to be a relationship between the racial composition of a ZIP code and the number of FAIR plan policies obtained in that ZIP code.\n\n\nExercise 5(a)\nSince race is a numerical variable, what type of graph or chart is appropriate for visualizing it? (You may need to refer back to the “Numerical data” chapter.)\n\nPlease write up your answer here.\n\n\n\nExercise 5(b)\nUsing ggplot code, create the type of graph you identified above. (Again, refer back to the “Numerical data” chapter for sample code if you’ve forgotten.) After creating the initial plot, be sure to go back and set the binwidth and boundary to sensible values.\n\n\n# Add code here to create a plot of race\n\n\n\n\nExercise 5(c)\nDescribe the shape of the race variable using the three key shape descriptors (modes, symmetry, and outliers).\n\nPlease write up your answer here.\n\n\n\nExercise 5(d)\nCreate the same kind of graph as above, but for involact. (Again, go back and set the binwidth and boundary to sensible values.)\n\n\n# Add code here to create a plot of race\n\n\n\n\nExercise 5(e)\nDescribe the shape of the involact variable using the three key shape descriptors (modes, symmetry, and outliers).\n\nPlease write up your answer here.\n\n\n\nExercise 5(f)\nSince both race and involact are numerical variables, what type of graph or chart is appropriate for visualizing the relationship between them?\n\nPlease write up your answer here.\n\n\n\nExercise 5(g)\nFor our research question, is race functioning as a predictor variable or as the response variable? What about involact? Why? Explain why it makes more sense to think of one of them as the predictor and the other as the response.\n\nPlease write up your answer here.\n\n\n\nExercise 5(h)\nUsing ggplot code, create the type of graph you identified above. Be sure to put involact on the y-axis and race` on the x-axis.\n\n\n# Add code here to create a plot of involact against race"
  },
  {
    "objectID": "06-correlation-web.html#correlation-correlation",
    "href": "06-correlation-web.html#correlation-correlation",
    "title": "6  Correlation",
    "section": "6.3 Correlation",
    "text": "6.3 Correlation\nThe word correlation describes a linear relationship between two numerical variables. As long as certain conditions are met, we can calculate a statistic called the correlation coefficient, often denoted with a lowercase r.\nThere are several different ways to compute a statistic that measures correlation. The most common way, and the way we will learn in this chapter, is often attributed to an English mathematician named Karl Pearson. According to his Wikipedia page,\n\n“Pearson was also a proponent of social Darwinism, eugenics and scientific racism.”\n\n\nExercise 6\nDo an internet search for each of the following terms:\n\nSocial Darwinism\nEugenics\nScientific racism\n\nConsult at least two or three sources for each term. Then, in your own words (not copied and pasted from any of the websites you consulted), explain what these terms mean.\n\nPlease write up your answer here.\n\n\nWhile Pearson is often credited with its discovery, the so-called “Pearson correlation coefficient” was first developed by a French scientist, Auguste Bravais. Due to the misattribution of discovery, along with the desire to disassociate the useful tool of correlation from its problematic applications to racism and eugenics, we will just refer to it as the correlation coefficient (without a name attached).\nThe correlation coefficient, r, has some important properties.\n\nThe correlation coefficient is a number between -1 and 1.\nA value close to 0 indicates little or no correlation.\nA value close to 1 indicates strong positive correlation.\nA value close to -1 indicates strong negative correlation.\n\nIn between 0 and 1 (or -1), we often use words like weak, moderately weak, moderate, and moderately strong. There are no exact cutoffs for when such words apply. You must learn from experience how to judge scatterplots and r values to make such determinations.\nA correlation is positive when low values of one variable are associated with low values of the other value. Similarly, high values of one variable are associated with high values of the other. For example, exercise is positively correlated with burning calories. Low exercise levels will burn a few calories; high exercise levels burn more calories, on average.\nA correlation is negative when low values of one variable are associated with high values of the other value, and vice versa. For example, tooth brushing is negatively correlated with cavities. Less tooth brushing may result in more cavities; more tooth brushing is associated with fewer calories, on average."
  },
  {
    "objectID": "06-correlation-web.html#correlation-conditions",
    "href": "06-correlation-web.html#correlation-conditions",
    "title": "6  Correlation",
    "section": "6.4 Conditions for correlation",
    "text": "6.4 Conditions for correlation\nTwo variables are considered “associated” any time there is any type of relationship between them (i.e., they are not independent). However, in statistics, we reserve the word “correlation” for situations meeting more stringent conditions:\n\nThe two variables must be numerical.1\nThere is a somewhat linear relationship between the variables, as shown in a scatterplot.\nThere are no serious outliers.\n\nFor condition (2) above, keep in mind that real data in scatterplots very rarely lines up in a perfect straight line. Instead, you will see a “cloud” of dots. All we want to know is whether that cloud of dots mostly moves from one corner of the scatterplot to the other. Violations of this condition will usually be for one of two reasons:\n\nThe dots are scattered completely randomly with no discernible pattern.\nThe dots have a pattern or shape to them, but that shape is curved and not linear.\n\n\nExercise 7\nCheck the three conditions for the relationship between involact and race. For conditions (2) and (3), you’ll need to check the scatterplot you created above. (You did create a scatterplot for one of the exercises above, right?)\n\nPlease write up your answer here."
  },
  {
    "objectID": "06-correlation-web.html#correlation-calculating",
    "href": "06-correlation-web.html#correlation-calculating",
    "title": "6  Correlation",
    "section": "6.5 Calculating correlation",
    "text": "6.5 Calculating correlation\nSince the conditions are met, We calculate the correlation coefficient using the cor command.\n\ncor(chredlin$race, chredlin$involact)\n\n[1] 0.713754\n\n\nThe order of the variables doesn’t matter; correlation is symmetric, so the r value is the same independent of the choice of response and predictor variables.\nSince the correlation between involact and race is a positive number and slightly closer to 1 than 0, we might call this a “moderate” positive correlation. You can tell from the scatterplot above that the relationship is not a strong relationship. The words you choose should match the graphs you create and the statistics you calculate.\n\nExercise 8(a)\nCreate a scatterplot of income against race. (Put income on the y-axis and race on the x-axis.)\n\n\n# Add code here to create a scatterplot of income against race\n\n\n\n\nExercise 8(b)\nCheck the three conditions for the relationship between income and race. Which condition is pretty seriously violated here?\n\nPlease write up your answer here.\n\n\n\n\n\n\n\n\nExercise 9(a)\nCreate a scatterplot of theft against fire. (Put theft on the y-axis and fire on the x-axis.)\n\n\n# Add code here to create a scatterplot of theft against fire\n\n\n\n\nExercise 9(b)\nCheck the three conditions for the relationship between theft and fire. Which condition is pretty seriously violated here?\n\n\n\n\n\n\nPlease write up your answer here.\n\n\n\nExercise 9(c)\nEven though the conditions are not met, what if you calculated the correlation coefficient anyway? Try it.\n\n\n# Add code here to calculate the correlation coefficient between theft and fire\n\n\n\n\nExercise 9(d)\nSuppose you hadn’t looked at the scatterplot and you only saw the correlation coefficient you calculated in the previous part. What would your conclusion be about the relationship between theft and fire. Why would that conclusion be misleading?\n\nPlease write up your answer here.\n\nThe lesson learned here is that you should never try to interpret a correlation coefficient without looking at a plot of the data to assure that the conditions are met and that the result is a sensible thing to interpret."
  },
  {
    "objectID": "06-correlation-web.html#correlation-causation",
    "href": "06-correlation-web.html#correlation-causation",
    "title": "6  Correlation",
    "section": "6.6 Correlation is not causation",
    "text": "6.6 Correlation is not causation\nWhen two variables are correlated—indeed, associated in any way, not just in a linear relationship—that means that there is a relationship between them. However, that does not mean that one variable causes the other variable.\nFor example, we discovered above that there was a moderate correlation between the racial composition of a ZIP code and the new FAIR policies created in those ZIP codes. However, being part of a racial minority does not cause someone to seek out alternative forms of insurance, at least not directly. In this case, the racial composition of certain neighborhoods, though racist policies, affected the availability of certain forms of insurance for residents in those neighborhoods. And that, in turn, caused residents to seek other forms of insurance.\nIn the Chicago example, there is still likely a causal connection between one variable (race) and the other (involact), but it was indirect. In other cases, there is no causal connection at all. Here are a few of my favorite examples.\n\nExercise 10\nIce cream sales are positively correlated with drowning deaths. Does eating ice cream cause you to drown? (Perhaps the myth about swimming within one hour of eating is really true!) Does drowning deaths cause ice cream sales to rise? (Perhaps people are so sad about all the drownings that they have to go out for ice cream to cheer themselves up?)\nSee if you can figure out the real reason why ice cream sales are positively correlated with drowning deaths.\n\nPlease write up your answer here.\n\n\nIn the Chicago example, the causal effect was indirect. In the example from the exercise above, there is no causation whatsoever between the two variables. Instead, the causal effect was generated by a third factor that caused both ice cream sales to go up, and also happened to cause drowning deaths to go up. (Or, equivalently stated, it caused ice cream sales to be low during certain times of the year and also caused the drowning deaths to be low as well.) Such a factor is called a lurking variable. When a correlation between two variables exists due solely to the intervention of a lurking variable, that correlation is called a spurious correlation. The correlation is real; a scatterplot of ice cream sales and drowning deaths would show a positive relationship. But the reasons for that correlation to exist have nothing to do with any kind of direct causal link between the two.\nHere’s another one:\n\n\nExercise 11\nMost studies involving children create a number of weird correlations. For example, the height of children is very strongly correlated to pretty much everything you can measure about scholastic aptitude. For example, vocabulary count (the number of words children can use fluently in a sentence) is strongly correlated to height. Are tall people just smarter than short people?\nThe answer is, of course, no. The correlation is spurious. So what’s the lurking variable?\n\nPlease write up your answer here."
  },
  {
    "objectID": "06-correlation-web.html#correlation-obs-exp",
    "href": "06-correlation-web.html#correlation-obs-exp",
    "title": "6  Correlation",
    "section": "6.7 Observational studies versus experiments",
    "text": "6.7 Observational studies versus experiments\nSo when is a statistical finding (like correlation, for example) evidence of a causal relationship? Before we can answer that question, we need a few more definitions.\nA lot of data comes from “observational studies” where we simply observe or measure things as they are “in the wild,” so to speak. We don’t interfere in any way. We just write down what we see. Polls are usually observational in that we ask people questions and record their responses. We do not try to manipulate their responses in any way. We just ask the questions and observe the answers. Field studies are often observational. We go out in nature and write stuff down as we observe it.\nAnother way to gather data is an experiment. In an experiment, we introduce a manipulation or treatment to try to ascertain its effect. For example. if we’re testing a new drug, we will likely give the drug to one group of patients and a placebo to the other.\n\nExercise 12\nHere’s another internet rabbit hole for you. First, look up the definition of placebo. You do not need to write up your own version of that definition here; just familiarize yourself with the term if you’re not already familiar with it. Next, find some websites about the placebo effect and read those.\nGiven what you have learned about the placebo effect, why is it important to have a placebo group in a drug trial? Why not just give one set of patients the drug and compare them to another group that takes no pill at all?\n\nPlease write up your answer here.\n\n\nThe goal of the experiment is to learn whether the treatment (in this example, the drug) is effective when compared to the control (in this example, the placebo).\nNote that the word “effective” implies a causal claim. We want to know if the drug causes patients to get better.\nUnlike an observational study, in which the relationship between variables can be caused by a lurking variable, in an experiment, we purposefully manipulate one of the variables and try to control all others. For example, we manipulate the drug variable (we purposefully give some people the drug and others the placebo). But we control the amount of the drug given and the schedule on which patients are required to take the pills.\nThere are lots of things we cannot control. For example, it would be very difficult to control the diet of every person in the experiment. Could diet play a role in whether a patient gets better? Sure, so how do we know diet is not a lurking variable? In the context of an experiment, lurking variables are often called “confounders” or “confounding variables”. (The two terms are basically synonymous.)\nOne way to mitigate the effect of confounders that we cannot directly control is to randomize the patients into the treatment and control groups. With random selection, there will likely be people who have relatively healthy diets in both the control and treatment groups. If the drugs work, in theory they should still work better for the treatment group than for those taking the placebo. And likewise, patients with less healthy diets will generally be mixed up in both groups, and the drug should also work better for them.\nThe mantra of experimental design is, “Control as much as you can. Randomize to take care of the rest.”\nThere are lots of aspects of experimental design that we will not go into here (for example, blinding and blocking). But we will continue to mention the differences between observational studies and experiments in future chapters as we exercise caution in making causal claims."
  },
  {
    "objectID": "06-correlation-web.html#correlation-pred-exp",
    "href": "06-correlation-web.html#correlation-pred-exp",
    "title": "6  Correlation",
    "section": "6.8 Prediction versus explanation",
    "text": "6.8 Prediction versus explanation\nEven when claims are not causal, we can use associations (and correlations more specifically) for purposes of prediction.\n\nExercise 13\nIf I tell you that ice cream sales are high right now, can you make a reasonable prediction about the relative number of drowning deaths this month (high or low)? Why or why not?\n\nPlease write up your answer here.\n\n\nSo even when there is no direct causal link between two variables, if they are positively correlated, then large values of one variable are associated with large values of the other variable. So if I tell you one value is large, it is reasonable to predict that the other value will be large as well.\nWe use the language “predictor” variable and “response” variable to reinforce this idea.\nIn a properly designed and controlled experiment, we can use different language. In this case, we can explain the outcome using the treatment variable. If we’ve controlled for everything else, the only possible explanation for a difference between the treatment and control groups must be the treatment variable. If the patients get better on the drug (more so than those on the placebo) and we’ve controlled for every other possible confounding variable, the only possible explanation is that the drug works. The drug “explains” the difference in the response variable.\nBe careful, as sometimes statisticians use the term “explanatory variable” to mean any kind of variable that predicts or explains. In this course, we will try to use the term “predictor variable” exclusively."
  },
  {
    "objectID": "06-correlation-web.html#correlation-conclusion",
    "href": "06-correlation-web.html#correlation-conclusion",
    "title": "6  Correlation",
    "section": "6.9 Conclusion",
    "text": "6.9 Conclusion\nIf we have two numerical variables that have a linear association between them (also assuming there are no serious outliers), we can compute the correlation coefficient that measures the strength and direction of that linear association.\nKeep in mind that in an observational study, this correlation is a measure of association, but it does not signify that one variable causes the other. It’s possible that one variable causes the other, but it’s also possible that a third “lurking” variable is responsible for the association. Either way, the fact that a relationship exists means it is possible to use values of one variable to make reasonable predictions about the values of the other variable.\nIn a properly designed experiment, the manipulation of one variable while controlling for others (and randomizing to take care of other confounders) ensures that there is a causal link between the treatment variable and the response of interest. In this case, the treatment can “explain” the response, not just predict it.\n\n6.9.1 Preparing and submitting your assignment\n\nFrom the “Run” menu, select “Restart R and Run All Chunks”.\nDeal with any code errors that crop up. Repeat steps 1–-2 until there are no more code errors.\nSpell check your document by clicking the icon with “ABC” and a check mark.\nHit the “Preview” button one last time to generate the final draft of the .nb.html file.\nProofread the HTML file carefully. If there are errors, go back and fix them, then repeat steps 1–5 again.\n\nIf you have completed this chapter as part of a statistics course, follow the directions you receive from your professor to submit your assignment."
  },
  {
    "objectID": "06-correlation-web.html#footnotes",
    "href": "06-correlation-web.html#footnotes",
    "title": "6  Correlation",
    "section": "",
    "text": "There are other ways of measuring association for variables that are not numerical, but these aren’t covered in this course.↩︎"
  },
  {
    "objectID": "07-regression-web.html#regression-intro",
    "href": "07-regression-web.html#regression-intro",
    "title": "7  Regression",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nIn this chapter we will learn how to run a regression analysis. Regression provides a model for the linear relationship between two numerical variables.\n\n7.1.1 Install new packages\nIf you are using RStudio Workbench, you do not need to install any packages. (Any packages you need should already be installed by the server administrators.)\nIf you are using R and RStudio on your own machine instead of accessing RStudio Workbench through a browser, you’ll need to type the following command at the Console:\ninstall.packages(\"broom\")\n\n\n7.1.2 Download the R notebook file\nCheck the upper-right corner in RStudio to make sure you’re in your intro_stats project. Then click on the following link to download this chapter as an R notebook file (.Rmd).\nhttps://vectorposse.github.io/intro_stats/chapter_downloads/07-regression.Rmd\nOnce the file is downloaded, move it to your project folder in RStudio and open it there.\n\n\n7.1.3 Restart R and run all chunks\nIn RStudio, select “Restart R and Run All Chunks” from the “Run” menu.\n\n\n7.1.4 Load packages\nWe load the tidyverse package. The faraway package will give access to the Chicago redlining data introduced in the previous chapter and the palmerpenguins package gives us the penguins data. Finally, the broom package will provide tools for cleaning up the output of the regression analysis we perform.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(faraway)\n\nWarning: package 'faraway' was built under R version 4.3.1\n\nlibrary(palmerpenguins)\n\nWarning: package 'palmerpenguins' was built under R version 4.3.1\n\nlibrary(broom)"
  },
  {
    "objectID": "07-regression-web.html#regression-regression",
    "href": "07-regression-web.html#regression-regression",
    "title": "7  Regression",
    "section": "7.2 Regression",
    "text": "7.2 Regression\nWhen we have a linear relationship between two numerical variables, we learned in the last chapter that we can compute the correlation coefficient. One serious limitation of the correlation coefficient is that it is only a single number, and therefore, it doesn’t provide a whole lot of information about the nature of the linear relationship itself. It only gives clues as to the strength and direction of the association.\nIt will be helpful to model this linear relationship with an actual straight line. Such a line is called a regression line. It is also known as a best-fit line or least-squares line for reasons that we will get to later in the chapter.\nThe mathematics involved in figuring out what this line should be is more complicated than we cover in this book. Fortunately, R will do all the complicated calculations for us and we’ll focus on understanding what they mean.\nRecall the chredlin data set from the last chapter investigating the practice of redlining in Chicago in the 1970s. Let’s review the scatterplot of involact, the number of FAIR policies per 100 housing units, against race, the racial composition of each ZIP code as a percentage of minority residents. (Recall that each row of the data represents an entire ZIP code.)\n\nggplot(chredlin, aes(y = involact, x = race)) +\n    geom_point()\n\n\n\n\n\nExercise 1\nDoes the Chicago redlining data come from an observational study or an experiment? How do you know?\n\nPlease write up your answer here.\n\n\nIf certain conditions are met, we can graph a regression line; just add a geom_smooth layer to the scatterplot:\n\nggplot(chredlin, aes(y = involact, x = race)) +\n    geom_point() +\n    geom_smooth(method = lm, se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe method = lm argument is telling ggplot to use a “linear model”. The se = FALSE argument tells ggplot to draw just the line and nothing else. (What else might it try to draw? You are encouraged to go back to the code above and take out se = FALSE to see for yourself. However, we are not yet in a position to be able to explain the gray band that appears. We will return to this mystery in a future chapter.)\nOf all possible lines, the blue line comes the closest to each point in the scatterplot. If we wiggled the line a little bit, it might get closer to a few points, but the net effect would be to make it further from other points. This is the mathematically optimal line of best fit."
  },
  {
    "objectID": "07-regression-web.html#regression-models",
    "href": "07-regression-web.html#regression-models",
    "title": "7  Regression",
    "section": "7.3 Models",
    "text": "7.3 Models\nWe used the word “model” when referring to the regression line above. What does that word mean in this context?\nA model is something that represents something else, often on a smaller scale or in simplified form. A model is often an idealized form of something that may be quite messy or complex in reality. In statistics, a model is a representation of the way data is generated. For example, we may believe that as minority representation increases in a neighborhood, that neighborhood is more likely to be subject to racially discriminatory practices. We may even posit that the relationship is linear; i.e., for every percentage point increase in racial minorities, we expect some kind of proportional increase in racial discrimination, as measured in this case by FAIR policies. We say that this is our hypothesis about the data-generating process: we suspect that the data we see results from a sociological process that uses the minority representation of a neighborhood to generate data about FAIR policies.\nThe assumption of a linear relationship between these two quantities is just that—an assumption. It is not necessarily “true”, whatever “true” might mean in this kind of question. It is a convenient device that makes a simplifying assumption in order to allow us to do something meaningful in a statistical analysis. If such a model—despite its simplifying caricature—helps us make meaningful predictions to study something important like racial discrimination, then the model is useful.\nThe first thing we acknowledge when working with a model is that the model does not generate the data in a rigid, deterministic way. If you look at the scatterplot above, even assuming the blue line represents a “correct” data-generating process, the data points don’t fall on the blue line. The blue line gives us only a sense of where the data might be, but there is additional space between the line and the points. These spaces are often referred to as errors. In statistics, the word “error” does not mean the same thing as “mistake”. Error is just the difference between an idealized model prediction and the real location of data. In the context of linear regression, we will use the term residual instead. After the model is done making a prediction, the residuals are “left over” to account for the different between the model and the actual data.\nThe most important thing to remember about models is that they aren’t real. They are idealizations and simplifications. The degree to which we can trust models, then, comes down to certain assumptions we make about the data-generating process. These assumptions cannot be completely verified—after all, we will never know the exact data-generating process. But there are certain conditions we can check to know if the assumptions we make are reasonable.\n\nExercise 2\nDo an internet search for the phrase “statistical model” and/or “statistical modeling”. Read at least two or three sources. List below one important aspect of statistical modeling you find in your search that wasn’t mentioned in the paragraphs above. (Some of the sources you find may be a little technical. You should, for now, skip over the technical explanations. Try to find several sources that address the issue in non-technical ways. The additional information you mention below should be something non-technical that you understand.)\n\nPlease write up your answer here."
  },
  {
    "objectID": "07-regression-web.html#regression-conditions",
    "href": "07-regression-web.html#regression-conditions",
    "title": "7  Regression",
    "section": "7.4 Checking conditions",
    "text": "7.4 Checking conditions\nWe need to be careful here. Although we graphed the blue regression line above, we have not checked any conditions. Therefore, it is inappropriate to fit a regression line at this point. Once the line is seen, it cannot easily be “unseen”, and it’s crucial that you don’t trick your eyes into believing there is a linear relationship before checking the conditions that justify that belief.\nThe regression line we saw above makes no sense unless we know that regression is appropriate. The conditions for running a regression analysis include all the conditions you checked for a correlation analysis in the last chapter:\n\nThe two variables must be numerical.\nThere is a somewhat linear relationship between the variables, as shown in a scatterplot.\nThere are no serious outliers.\n\n\nExercise 3\nCheck these three conditions for the regression between involact and race (using the scatterplot above for conditions (2) and (3).)\n\n\n\n\n\n\n\n\nHowever, there is an additional condition to check to ensure that our regression model is appropriate. It concerns the residuals, but as we haven’t computed anything yet, we have nothing to analyze. We’ll return to this condition later."
  },
  {
    "objectID": "07-regression-web.html#regression-calculating",
    "href": "07-regression-web.html#regression-calculating",
    "title": "7  Regression",
    "section": "7.5 Calculating the regression line",
    "text": "7.5 Calculating the regression line\nWhat is the equation of the regression line? In your algebra class you learned that a line takes the form \\(y = mx + b\\) where \\(m\\) is the slope and \\(b\\) is the y-intercept. Statisticians write the equation in a slightly different form:\n\\[\n\\hat{y} = b_{0} + b_{1} x\n\\]\nThe intercept is \\(b_{0}\\) and the slope is \\(b_{1}\\). We use \\(\\hat{y}\\) (pronounced “y hat”) instead of \\(y\\) because when we plug in values of \\(x\\), we do not get back the exact values of \\(y\\) from the data. The line, after all, does not actually pass through most (if any) actual data points. Instead, this equation gives us “predicted” values of \\(y\\) that lie on the regression line. These predicted \\(y\\) values are called \\(\\hat{y}\\).\nTo run a regression analysis and calculate the values of the intercept and slope, we use the lm command in R. (Again, lm stands for “linear model”.) This command requires us to specify a “formula” that tells R the relationship we want to model. It uses special syntax in a very specific order:\n\nThe response variable,\na “tilde” ~ (this key is usually in the upper-left corner of your keyboard, above the backtick),\nthe predictor variable.\n\nAfter a comma, we then specify the data set in which those variables live using data =. Here’s the whole command:\n\nlm(involact ~ race, data = chredlin)\n\n\nCall:\nlm(formula = involact ~ race, data = chredlin)\n\nCoefficients:\n(Intercept)         race  \n    0.12922      0.01388  \n\n\nThe response variable always goes before the tilde and the predictor variable always goes after.\nLet’s store that result for future use. The convention we’ll use in this book is to name things using the variables involved. For example,\n\ninvolact_race_lm &lt;- lm(involact ~ race, data = chredlin)\ninvolact_race_lm\n\n\nCall:\nlm(formula = involact ~ race, data = chredlin)\n\nCoefficients:\n(Intercept)         race  \n    0.12922      0.01388  \n\n\nThe variable involact_race_lm now contains all the information we need about the linear regression model."
  },
  {
    "objectID": "07-regression-web.html#regression-interpreting",
    "href": "07-regression-web.html#regression-interpreting",
    "title": "7  Regression",
    "section": "7.6 Interpreting the coefficients",
    "text": "7.6 Interpreting the coefficients\nLook at the output of the lm command above.\nThe intercept is 0.12922 and the slope is 0.01388. The number 0.12922 is labeled with (Intercept), so that’s pretty obvious. But how do we know the number 0.01388 corresponds to the slope? Process of elimination, I suppose. But there’s another good reason too. The equation of the regression line can be written\n\\[\n\\hat{y} = 0.12922 + 0.01388 x\n\\]\nWhen we report the equation of the regression line, we typically use words instead of \\(\\hat{y}\\) and \\(x\\) to make the equation more interpretable in the context of the problem. For example, for this data, we would write the equation as\n\\[\n\\widehat{involact} = 0.12922 + 0.01388 race\n\\]\nThe slope is the coefficient of race, or the number attached to race. (The intercept is not attached to anything; it’s just a constant term out front there.)\nThe slope \\(b_{1}\\) is always interpretable. This model predicts that one unit of increase in the x-direction corresponds to a change of 0.01388 units in the y-direction. Let’s phrase it this way:\n\nThe model predicts that an increase of one percentage point in the composition of racial minorities corresponds to an increase of 0.01388 new FAIR policies per 100 housing units.\n\nThe intercept \\(b_{0}\\) is a different story. There is always a literal interpretation:\n\nThe model predicts that a ZIP code with 0% racial minorites will generate 0.12922 new FAIR policies.\n\nIn some cases (rarely), that interpretation might make sense. In most cases, though, it is physically impossible for the predictor variable to take a value of 0, or the value 0 is way outside the range of the data. Whenever we use a model to make a prediction outside of reasonable values, we call that extrapolation.\nFor the Chicago data, we likely don’t have a case of extrapolation. While it is not literally true that any ZIP code has 0% racial minorities, we can see in the scatterplot that there are values very close to zero.\n\nExercise 4\nUse the arrange command from dplyr to sort the chredlin data frame by race (using the default ascending order). What is the value of race for the three ZIP codes with the smallest percentage of minority residents?\n\n\n# Add code here to sort by race\n\nPlease write up your answer here.\n\n\nAgain, even though there are no ZIP codes with 0% racial minorities, there are a bunch that are close to zero, so the literal interpretation of the intercept is also likely a sensible one in this case.\n\n\nExercise 5\nLet’s think through something else the intercept might be telling us in this case. The presumption is that FAIR policies are obtained mostly by folks who can’t get insurance policies in other ways. Some of that is driven by racial discrimination, but maybe not all of it. What does the intercept have to say about the number of FAIR policies that are obtained not due to denial of coverage from racial discrimination?\n\nPlease write up your answer here."
  },
  {
    "objectID": "07-regression-web.html#regression-rescaling",
    "href": "07-regression-web.html#regression-rescaling",
    "title": "7  Regression",
    "section": "7.7 Rescaling to make interpretations more meaningful",
    "text": "7.7 Rescaling to make interpretations more meaningful\nLet’s revisit the interpretation of the slope:\n\nThe model predicts that an increase of one percentage point in the composition of racial minorities corresponds to an increase of 0.01388 new FAIR policies per 100 housing units.\n\nThis is a perfectly correct statement, but one percentage point change is not very much. It’s hard to think about comparing two neighborhoods that differ by only one percent. This scale also makes the predicted change in the response variable hard to interpret. How many policies is 0.01388 per 100 housing units?\nOne way to make these kinds of statements more interpretable is to change the scale. What if we increase 10 percentage points instead of only 1 percentage point? In other words, what if we move 10 times as far along the x-axis. The response variable will also have to move 10 times as far. This is the new statement:\n\nThe model predicts that an increase of 10 percentage points in the composition of racial minorities corresponds to an increase of 0.1388 new FAIR policies per 100 housing units.\n\nIn this case, the decimal 0.1388 is maybe still not completely clear, but at least an increase of 10 percentage points is a meaningful difference between neighborhoods.\n\nExercise 6\nSince the last number is a per capita type measure, we can also rescale it. If the model predicts an increase in 0.1388 new FAIR policies per 100 households (corresponding to 10 percentage points increase in racial minorities), how many FAIR policies would that be in 1000 households?\n\nPlease write up your answer here."
  },
  {
    "objectID": "07-regression-web.html#regression-tidy",
    "href": "07-regression-web.html#regression-tidy",
    "title": "7  Regression",
    "section": "7.8 The tidy command",
    "text": "7.8 The tidy command\nRecall the output of the lm command:\n\ninvolact_race_lm\n\n\nCall:\nlm(formula = involact ~ race, data = chredlin)\n\nCoefficients:\n(Intercept)         race  \n    0.12922      0.01388  \n\n\n(We did not have to run lm again. We had this output stored in the variable involact_race_lm.)\nThat summary is fine, but what if we needed to reference the slope and intercept using inline code? Or what if we wanted to grab those numbers and use them in further calculations?\nThe problem is that the results of lm just print the output in an unstructured way. If we want structured input, we can use the tidy command from the broom package. This will take the results of lm and organize the output into a tibble.\n\ntidy(involact_race_lm)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic      p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept)   0.129    0.0966       1.34 0.188       \n2 race          0.0139   0.00203      6.84 0.0000000178\n\n\nLet’s store that tibble so we can refer to it in the future.\n\ninvolact_race_tidy &lt;- tidy(involact_race_lm)\ninvolact_race_tidy\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic      p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept)   0.129    0.0966       1.34 0.188       \n2 race          0.0139   0.00203      6.84 0.0000000178\n\n\nThe intercept is stored in the estimate column, in the first row. The slope is stored in the same column, but in the second row. (There is a lot more information here to the right of the estimate column, but we will not know what these numbers mean until later in the course.)\nWe can grab the estimate column with the dollar sign as we’ve seen before:\n\ninvolact_race_tidy$estimate\n\n[1] 0.12921803 0.01388235\n\n\nThis is a “vector” of two values, the intercept and the slope, respectively.\nWhat if we want only one value at a time? We can grab individual elements of a vector using square brackets as follows:\n\ninvolact_race_tidy$estimate[1]\n\n[1] 0.129218\n\n\n\ninvolact_race_tidy$estimate[2]\n\n[1] 0.01388235\n\n\nHere is the interpretation of the slope again, but this time, we’ll use inline code:\n\nThe model predicts that an increase of 1 percentage points in the composition of racial minorities corresponds to an increase of 0.0138824 new FAIR policies per 100 housing units.\n\nClick somewhere inside the backticks on the line above and hit Ctrl-Enter or Cmd-Enter (PC or Mac respectively). You should see the number 0.01388235 pop up. If you Preview the HTML version of the document, you will also see the number there (not the code).\nWhat if we want to apply re-scaling to make this number more interpretable? The stuff inside the inline code chunk is just R code, so we can do any kind of calculation with it we want.\n\nThe model predicts that an increase of 10 percentage points in the composition of racial minorities corresponds to an increase of 0.1388235 new FAIR policies per 100 housing units.\n\nNow the number will be 0.1388235, ten times as large.\n\nExercise 7\nCopy and paste the interpretation of the intercept from earlier, but replace the number 0.12922 with an inline code chunk that grabs that number from the estimate column of the involact_race_tidy tibble. (Remember that the intercept is the first element of that vector, not the second element like the slope.)\n\nPlease write up your answer here."
  },
  {
    "objectID": "07-regression-web.html#regression-residuals",
    "href": "07-regression-web.html#regression-residuals",
    "title": "7  Regression",
    "section": "7.9 Residuals",
    "text": "7.9 Residuals\nEarlier, we promised to revisit the topic of residuals. Residuals are measured as the vertical distances from each data point to the regression line. We can see that visually below. (Don’t worry about the complexity of the ggplot code used to create this picture. You will not need to create a plot like this on your own, so just focus on the graph that is created below.)\n\nggplot(chredlin, aes(y = involact, x = race)) +\n    geom_segment(x = 35.1, xend  = 35.1,\n                 y = 0.6164886, yend = 0.6164886 + 0.38351139,\n                 color = \"red\", size = 2) +\n    geom_segment(x = 66.1, xend = 66.1,\n                 y = 1.0468415, yend = 1.0468415 - 0.64684154,\n                 color = \"red\", size = 2) +\n    geom_point() +\n    geom_smooth(method = lm, se = FALSE)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nThe graph above shows the regression line and two of the residuals as red line segments. (There is a residual for all 47 ZIP codes; only two are shown in this graph.) The one on the left corresponds to ZIP code with 35% racial minority. The regression line predicts that, if the model were true, such a ZIP code would have a value of involact of about 0.6. But the actual data for that ZIP code has an involact value of 1. The residual is the difference, about 0.4. In other words, the true data point is 0.4 units higher than the model prediction. This represents a positive residual; the actual data is 0.4 units above the line. Data points that lie below the regression line have negative residuals.\n\nExercise 8\nLook at the residual on the right. This corresponds to a ZIP code with about 66% racial minorities. First, estimate the value of involact that the model predicts for this ZIP code. (This is the y-value of the point on the regression line.) Next, report the actual involact value for this ZIP code. Finally, subtract these two numbers to get an approximate value for the residual. Should this residual be a positive number or a negative number?\nYou can just estimate with your eyeballs for now. You don’t need to be super precise.\n\nPlease write up your answer here.\n\n\nMore formally, let’s call the residual \\(e\\). This is standard notation, as “e” stands for “error”. Again, though, it’s not an error in the sense of a mistake. It’s an error in the sense that the model is not perfectly accurate, so it doesn’t predict the data points exactly. The degree to which the prediction misses is the “error” or “residual”. It is given by the following formula:\n\\[\ne = y - \\hat{y}\n\\]\n\n\nExercise 9\nThere are two symbols on the right-hand side of the equation above, \\(y\\) and \\(\\hat{y}\\). Which one is the actual data value and which one is the predicted value (the one on the line)?\n\nPlease write up your answer here.\n\n\nThe residuals are used to determine the regression line. The correct regression line will be the one that results in the smallest residuals overall. How do we measure the overall set of residuals? We can’t just calculate the average residual. Because the regression line should go through the middle of the data, the positive residuals will cancel out the negative residuals and the mean residual will just be zero. That’s not very useful.\nInstead, what we do is square the residuals. That makes all of them positive. Then we add together all the squared residuals and that sum is the thing we try to minimize. Well, we don’t do that manually because it’s hard, so we let the computer do that for us. Because the regression line minimizes the sum of the squared residuals, the regression line is often called the least-squares line.\nRecall earlier when we mentioned that there was one additional condition to check in order for linear regression to make sense. This condition is that there should not be any kind of pattern in the residuals.\nWe know that some of the points are going to lie above the line (positive residuals) and some of the points will lie below the line (negative residuals). What we need is for the spread of the residuals to be pretty balanced across the length of the regression line and for the residuals not to form any kind of curved pattern.\nTo check this condition, we’ll need to calculate the residuals first. To do so, we introduce a new function from the broom package. Whereas tidy serves up information about the intercept and the slope of the regression line, augment gives us extra information for each data point.\n\ninvolact_race_aug &lt;- augment(involact_race_lm)\ninvolact_race_aug\n\n# A tibble: 47 × 9\n   .rownames involact  race .fitted .resid   .hat .sigma .cooksd .std.resid\n   &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;\n 1 60626          0    10     0.268 -0.268 0.0341  0.452 0.00651     -0.608\n 2 60640          0.1  22.2   0.437 -0.337 0.0246  0.451 0.00731     -0.761\n 3 60613          1.2  19.6   0.401  0.799 0.0261  0.437 0.0436       1.80 \n 4 60657          0.5  17.3   0.369  0.131 0.0277  0.453 0.00124      0.295\n 5 60614          0.7  24.5   0.469  0.231 0.0235  0.453 0.00326      0.520\n 6 60610          0.3  54     0.879 -0.579 0.0287  0.445 0.0253      -1.31 \n 7 60611          0     4.9   0.197 -0.197 0.0398  0.453 0.00417     -0.448\n 8 60625          0     7.1   0.228 -0.228 0.0372  0.453 0.00517     -0.517\n 9 60618          0.4   5.3   0.203  0.197 0.0393  0.453 0.00411      0.448\n10 60647          1.1  21.5   0.428  0.672 0.0250  0.442 0.0295       1.52 \n# ℹ 37 more rows\n\n\nThe first three columns consist of the row names (the ZIP codes) followed by the actual data values we started with for involact and race. But now we’ve “augmented” the original data with some new stuff too. (We won’t learn about anything past the fifth column in this course, though.)\nThe fourth column—called .fitted—is \\(\\hat{y}\\), or the point on the line that corresponds to the given \\(x\\) value. Let’s check and make sure this is working as advertised.\nThe regression equation from above is\n\\[\n\\widehat{involact} = 0.12922 + 0.01388 race\n\\]\nTake, for example, the first row in the tibble above, the one corresponding to ZIP code 60626. The value of race is 10.0. Plug that value into the equation above:\n\\[\n\\widehat{involact} = 0.12922 + 0.01388(10.0) = 0.268\n\\]\nThe model predicts that a ZIP code with 10% racial minorities will have about 0.268 new FAIR policies per 100 housing units. The corresponding number in the .fitted column is 0.2680416, so that’s correct.\nNow skip over to the fifth column of the augment output, the one that says .resid. If this is the residual \\(e\\), then it should be \\(y - \\hat{y}\\). Since \\(y\\) is the actual value of involact and \\(\\hat{y}\\) is the value predicted by the model, we should get for the first row of output\n\\[\ne = y - \\hat{y} = 0.0 - 0.268 = -0.268\n\\]\nYup, it works!\nTo check for patterns in the residuals, we’ll create a residual plot. A residual plot graphs the residuals above each value along the x-axis. (In the command below, we also add a blue horizontal reference line so that it is clear which points have positive or negative residuals.)\n\nggplot(involact_race_aug, aes(y = .resid, x = race)) +\n    geom_point() +\n    geom_hline(yintercept = 0, color = \"blue\")\n\n\n\n\nPay close attention to the ggplot code. Notice that the tibble in the first slot is not chredlin as it was before. The residuals we need to plot are not stored in the raw chredlin data. We had to calculate the residuals using the augment command, and those residuals are then stored in a different place that we named involact_race_aug. In the latter tibble, the residuals themselves are stored in a variable called .resid. (Don’t forget the dot in .resid.)\nWe are looking for systematic patterns in the residuals. A good residual plot should look like the most boring plot you’ve ever seen.\nFor the most part, the residual plot above looks pretty good. The one exception is the clustering near the left edge of the graph.\n\n\nExercise 10\nRefer back and forth between the original scatterplot created earlier (with the regression line) and the residual plot above. Can you explain why there is a line of data points with negative residuals along the left edge of the residual plot?\n\nPlease write up your answer here.\n\n\nResidual patterns that are problematic often involve curved data (where the dots follow a curve around the horizontal reference line instead of spreading evenly around it) and heteroscedasticity, which is a fanning out pattern from left to right.\nOther than the weird cluster of points at the left, the rest of the residual plot looks pretty good. Ignoring those ZIP codes with 0 FAIR policies, the rest of the residuals stretch, on average, about the same height above and below the line across the whole width of the plot. There is only one slightly large residual at about the 40% mark, but it’s not extreme, and it doesn’t look like a severe outlier in the original scatterplot.\nWhat does a bad residual plot look like? The code below will run an ill-advised regression analysis on fire, the number of fires (per 100 housing units), against age, the percent of housing units built before 1939. The residual plot appears below.\n\nfire_age_lm &lt;- lm(fire ~ age, data = chredlin)\nfire_age_aug &lt;- augment(fire_age_lm)\nggplot(fire_age_aug, aes(y = .resid, x = age)) +\n    geom_point() +\n    geom_hline(yintercept = 0, color = \"blue\")\n\n\n\n\n\n\nExercise 11\nUsing the vocabulary established above, explain why the residual plot above is bad.\n\nPlease write up your answer here.\n\n\nOf course, we should never even get as far as running a regression analysis and making a residual plot if we perform exploratory data analysis as we’re supposed to.\n\n\nExercise 12(a)\nIf you were truly interested in investigating an association between the fire risk and the age of buildings in a ZIP code, the first thing you would do is create a scatterplot. Go ahead and do that below. Use fire as the response variable and age as the predictor.\n\n\n# Add code here to create a scatterplot of fire against age\n\n\n\n\nExercise 12(b)\nFrom the scatterplot above, explain why you wouldn’t even get as far as running a regression analysis. (Think of the conditions.)\n\nPlease write up your answer here.\n\n\nTo review, the conditions for a regression analysis are as follows (including the newest fourth condition):\n\nThe two variables must be numerical.\nThere is a somewhat linear relationship between the variables, as shown in a scatterplot.\nThere are no serious outliers.\nThere is no pattern in the residuals."
  },
  {
    "objectID": "07-regression-web.html#regression-r2",
    "href": "07-regression-web.html#regression-r2",
    "title": "7  Regression",
    "section": "7.10 \\(R^2\\)",
    "text": "7.10 \\(R^2\\)\nWe’ve seen that the correlation coefficient r is of limited utility. In addition to being only a single statistic to summarize a linear association, the number doesn’t have any kind of intrinsic meaning. It can only be judged by how close it is to 0 or 1 (or -1) in conjunction with a scatterplot to give you a sense of the strength of the correlation. In particular, some people try to interpret r as some kind of percentage, but it’s not.\nOn the other hand, when we square the correlation coefficient, we do get an interpretable number. For some reason, instead of writing \\(r^2\\), statisticians write \\(R^2\\), with a capital R. (I can’t find the historical reason why this is so.) In any event, \\(R^2\\) can be interpreted as a percentage! It represents the percent of variation in the y variable that can be explained by variation in the x variable.\nHere we introduce the last of the broom functions: glance. Whereas tidy reports the intercept and slope, and augment reports values associated to each data point separately, the glance function gathers up summaries for the entire model. (Do not confuse glance with glimpse. The latter is a nicer version of str that just summarizes the variables in a tibble.)\n\ninvolact_race_glance &lt;- glance(involact_race_lm)\ninvolact_race_glance\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic      p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.509         0.499 0.449      46.7 0.0000000178     1  -28.0  62.0  67.6\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nA more advanced statistics course might discuss the other model summaries present in the glance output. The \\(R^{2}\\) value is stored in the r.squared (inexplicably, now written with a lowercase r). Its value is 0.51. We will word it this way:\n\n51% of the variability in FAIR policies can be accounted for by variability in racial composition.\n\nAnother way to think about this is to imagine all the factors that might go into the number of FAIR policies obtained in a ZIP code. That number varies across ZIP codes, with some ZIP codes having essentially 0 FAIR policies per 100 housing units, and others having quite a bit more, up to 2 or more per 100 housing units. What accounts for this discrepancy among ZIP codes? Is it the varying racial composition of those neighborhoods? To some degree, yes. We have seen that more racially diverse neighborhoods, on average, require more FAIR policies. But is race the only factor? Probably not. Income, for example, might play a role. People in low income neighborhoods may not be able to acquire traditional insurance due to its cost or their poor credit, etc. That also accounts for some of the variability among ZIP codes. Are there likely even more factors? Most assuredly. In fact, if 51% of the variability in FAIR policies can be accounted for by variability in racial composition. then 49% must be accounted for by other variables. These other variables may or may not be collected in our data, and we will never be able to determine all the factors that go into varying FAIR policy numbers.\n\\(R^2\\) is a measure of the fit of the model. High values of \\(R^2\\) mean that the line predicts the data values closely, whereas lower values of \\(R^2\\) mean that there is still a lot of variability left in the residuals (again, due to other factors that are not measured in the model).\n\nExercise 13\nCalculate the correlation coefficient r between involact and race using the cor command. (You might have to look back at the last chapter to remember the syntax.) Store that value as r.\nIn a separate code chunk, square that value using the command r^2. Verify that the square of the correlation coefficient is the same as the \\(R^2\\) value reported in the glance output above.\n\n\n# Add code here to calculate the correlation coefficient\n\n\n# Add code here to square the correlation coefficient"
  },
  {
    "objectID": "07-regression-web.html#regression-multiple",
    "href": "07-regression-web.html#regression-multiple",
    "title": "7  Regression",
    "section": "7.11 Multiple predictors",
    "text": "7.11 Multiple predictors\nThe discussion of \\(R^2\\) above highlights the fact that a single predictor will rarely account for all or even most of the variability in a response variable. Is there a way to take other predictors into account?\nThe answer is yes, and the statistical technique involved is called multiple regression. Multiple regression is a deep subject, worthy of entire courses. Suffice it to say here that more advanced stats courses go into the ways in which multiple predictors can be included in a regression.\nOne easy thing we can do is incorporate a categorical variable into a graph and see if that categorical variable might play a role in the regression analysis. For example, there is a variance called side in chredlin that indicates whether the ZIP code is on the north side (n) or south side(s) of Chicago. As described in an earlier chapter, we can use color to distinguish between the ZIP codes.\n\nggplot(chredlin, aes(y = involact, x = race, color = side)) +\n    geom_point()\n\n\n\n\n\nExercise 14\nDo neighborhoods with higher percent racial minorities tend to be on the north or south side of Chicago?\n\nPlease write up your answer here.\n\n\nDoes this affect the regression? We haven’t checked the conditions carefully for this new question, so we will exercise caution in coming to any definitive conclusions. But visually, there does appear to be a difference in the models generated for ZIP codes on the north versus south sides:\n\nggplot(chredlin, aes(y = involact, x = race, color = side)) +\n    geom_point() +\n    geom_smooth(method = lm, se = FALSE)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\nExercise 15\nAlthough the slopes appear to be different, this is quite misleading. Focus on just the red dots. Which regression condition appears to be violated if we only consider the north side regression? How does that violation appear to affect the slope of the regression line?\n\nPlease write up your answer here."
  },
  {
    "objectID": "07-regression-web.html#regression-your-turn",
    "href": "07-regression-web.html#regression-your-turn",
    "title": "7  Regression",
    "section": "7.12 Your turn",
    "text": "7.12 Your turn\nLet’s revisit the penguins data. Imagine that it was much easier to measure body mass than it was to measure flipper length. (I’m not a penguin expert, so I don’t know if that’s true, but it seems plausible. Weighing a penguin can be done without human contact, for example.) Can we accurately predict flipper length from body mass? (This means that flipper_length_mm should be the response variable on the y-axis and body_mass_g should be the predictor variable on the x-axis.)\n\nExercise 16(a)\nCreate a scatterplot of the data. Do not include a regression line yet. (In other words, there should be no geom_smooth in this plot.)\n\n\n# Add code here to create a scatterplot of the data\n\n\n\n\nExercise 16(b)\nUse the scatterplot above to check the first three conditions of regression.\n\n\n\n\n\n\n\n\n\nExercise 16(c)\nAs we’re reasonably satisfied that the first three conditions are met and regression is worth pursuing, run the lm command to perform the regression analysis. Assign the output to the name fl_bm_lm. Be sure to type the variable name fl_bm_lm on its own line so that the output is printed in this file.\nThen use tidy, augment, and glance respectively on the output. Assign the output to the names fl_bm_tidy, fl_bm_aug, and fl_bm_glance. Again, in each code chunk, type the output variable name on its own line to ensure that it prints in this file.\n\n\n# Add code here to generate and print regression output with lm\n\n\n# Add code here to \"tidy\" and print the output from lm\n\n\n# Add code here to \"augment\" and print the output from lm\n\n\n# Add code here to \"glance\" at and print the output from lm\n\n\n\n\nExercise 16(d)\nUse the augment output from above to create a residual plot with a blue horizontal reference line.\n\n\n# Add code here to create a residual plot\n\n\n\n\nExercise 16(e)\nUse the residual plot to check the fourth regression condition.\n\nPlease write up your answer here.\n\n\n\nExercise 16(f)\nWith all the conditions met, plot the regression line on top of the scatterplot of the data. (Use geom_smooth with method = lm and se = FALSE as in the examples earlier.)\n\n\n# Add code here to plot the regression line on the scatterplot\n\n\n\n\nExercise 16(g)\nUsing the values of the intercept and slope from the tidy output, write the regression equation mathematically (enclosing your answer in double dollar signs as above), using contextually meaningful variable names.\n\n\\[\nwrite-math-here\n\\]\n\n\n\nExercise 16(h)\nInterpret the slope in a full, contextually meaningful sentence.\n\nPlease write up your answer here.\n\n\n\nExercise 16(i)\nGive a literal interpretation of the intercept. Then comment on the appropriateness of that interpretation. (In other words, does the intercept make sense, or is it a case of extrapolation?)\n\nPlease write up your answer here.\n\n\n\nExercise 16(j)\nUse the equation of the regression line to predict the flipper length of a penguin with body mass 4200 grams. Show your work. Then put that prediction into a full, contextually meaningful sentence.\n\nPlease write up your answer here.\n\n\n\nExercise 16(k)\nUsing the value of \\(R^2\\) from the glance output for the model of flipper length by body mass, write a full, contextually meaningful sentence interpreting that value.\n\nPlease write up your answer here.\n\n\n\nExercise 16(l)\nAdd color = species to the aes portion of the ggplot command to look at the regression lines for the three different species separately. Comment on the slopes of those three regression lines.\n\n\n# Add code here to plot regressions by species\n\nPlease write up your answer here."
  },
  {
    "objectID": "07-regression-web.html#regression-conclusion",
    "href": "07-regression-web.html#regression-conclusion",
    "title": "7  Regression",
    "section": "7.13 Conclusion",
    "text": "7.13 Conclusion\nGoing beyond mere correlation, a regression analysis allows us to specify a linear model in the form of an equation. Assuming the conditions are met, this allows us to say more about the association. For example, the slope predicts how the response changes when comparing two values of the predictor. In fact, we can use the regression line to make a prediction for any reasonable value of the predictor (being careful not to extrapolate). Because regression is only a model, these predictions will not be exactly correct. Real data comes with residuals, meaning deviations from the idealized predictions of the model. But if those residuals are relatively small then the \\(R^2\\) value will be large and the model does a good job making reasonably accurate predictions.\n\n7.13.1 Preparing and submitting your assignment\n\nFrom the “Run” menu, select “Restart R and Run All Chunks”.\nDeal with any code errors that crop up. Repeat steps 1–-2 until there are no more code errors.\nSpell check your document by clicking the icon with “ABC” and a check mark.\nHit the “Preview” button one last time to generate the final draft of the .nb.html file.\nProofread the HTML file carefully. If there are errors, go back and fix them, then repeat steps 1–5 again.\n\nIf you have completed this chapter as part of a statistics course, follow the directions you receive from your professor to submit your assignment."
  },
  {
    "objectID": "08-intro_to_randomization_1-web.html#randomization1-intro",
    "href": "08-intro_to_randomization_1-web.html#randomization1-intro",
    "title": "8  Introduction to randomization, Part 1",
    "section": "8.1 Introduction",
    "text": "8.1 Introduction\nIn this module, we’ll learn about randomization and simulation. When we want to understand how sampling works, it’s helpful to simulate the process of drawing samples repeatedly from a population. In the days before computing, this was very difficult to do. Now, a few simple lines of computer code can generate thousands (even millions) of random samples, often in a matter of seconds or less.\n\n8.1.1 Install new packages\nIf you are using RStudio Workbench, you do not need to install any packages. (Any packages you need should already be installed by the server administrators.)\nIf you are using R and RStudio on your own machine instead of accessing RStudio Workbench through a browser, you’ll need to type the following command at the Console:\ninstall.packages(\"mosaic\")\n\n\n8.1.2 Download the R notebook file\nCheck the upper-right corner in RStudio to make sure you’re in your intro_stats project. Then click on the following link to download this chapter as an R notebook file (.Rmd).\nhttps://vectorposse.github.io/intro_stats/chapter_downloads/08-intro_to_randomization_1.Rmd\nOnce the file is downloaded, move it to your project folder in RStudio and open it there.\n\n\n8.1.3 Restart R and run all chunks\nIn RStudio, select “Restart R and Run All Chunks” from the “Run” menu.\n\n\n8.1.4 Load packages\nWe load the tidyverse package. The mosaic package contains some tools for making it easier to learn about randomization and simulation.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nWarning: package 'mosaic' was built under R version 4.3.1\n\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum"
  },
  {
    "objectID": "08-intro_to_randomization_1-web.html#randomization1-sample-pop",
    "href": "08-intro_to_randomization_1-web.html#randomization1-sample-pop",
    "title": "8  Introduction to randomization, Part 1",
    "section": "8.2 Sample and population",
    "text": "8.2 Sample and population\nThe goal of the next few chapters is to help you think about the process of sampling from a population. What do these terms mean?\nA population is a group of objects we would like to study. If that sounds vague, that’s because it is. A population can be a group of any size and of any type of thing in which we’re interested. Often, populations refer to groups of people. For example, in an election, the population of interest is all voters. But if you’re a biologist, you might study populations of other kinds of organisms. If you’re an engineer, you might study populations of bolts on bridges. If you’re in finance, you might study populations of loans.\nPopulations are usually inaccessible in their entirety. It is impossible to survey every voter in any reasonably sized election, for example. Therefore, to study them, we have to collect a sample. A sample is a subset of the population. We might conduct a poll of 2000 voters to try to learn about voting intentions for the entire population. Of course, for that to work, the sample has to be representative of its population. We’ll have more to say about that in the future."
  },
  {
    "objectID": "08-intro_to_randomization_1-web.html#randomization1-coin",
    "href": "08-intro_to_randomization_1-web.html#randomization1-coin",
    "title": "8  Introduction to randomization, Part 1",
    "section": "8.3 Flipping a coin",
    "text": "8.3 Flipping a coin\nBefore we talk about how samples are obtained from populations in the real world, we’re going to perform some simulations.\nOne of the simplest acts to simulate is flipping a coin. We could get an actual coin and physically flip it over and over again, but that is time-consuming and annoying. It is much easier to flip a “virtual” coin inside the computer. One way to accomplish this in R is to use the rflip command from the mosaic package. To make sure we’re flipping a fair coin, we’ll say that we want a 50% chance of heads by including the parameter prob = 0.5.\nOne more bit of technical detail. Since there will be some randomness involved here, we will need to include an R command to ensure that we all get the same results every time this code runs. This is called “setting the seed”. Don’t worry too much about what this is doing under the hood. The basic idea is that two people who start with the same seed will generate the same sequence of “random” numbers.\nThe seed 1234 in the chunk below is totally arbitrary. It could have been any number at all. (And, in fact, we’ll use different numbers just for fun.) If you change the seed, you will get different output, so we all need to use the same seed. But the actual common value we all use for the seed is irrelevant.\nHere is one coin flip with a 50% chance of coming up heads:\n\nset.seed(1234)\nrflip(1, prob = 0.5)\n\n\nFlipping 1 coin [ Prob(Heads) = 0.5 ] ...\n\nT\n\nNumber of Heads: 0 [Proportion Heads: 0]\n\n\nHere are ten coin flips, each with a 50% chance of coming up heads:\n\nset.seed(1234)\nrflip(10, prob = 0.5)\n\n\nFlipping 10 coins [ Prob(Heads) = 0.5 ] ...\n\nT H H H H H T T H H\n\nNumber of Heads: 7 [Proportion Heads: 0.7]\n\n\nJust to confirm that this is a random process, let’s flip ten coins again (but without setting the seed again):\n\nrflip(10, prob = 0.5)\n\n\nFlipping 10 coins [ Prob(Heads) = 0.5 ] ...\n\nH H T H T H T T T T\n\nNumber of Heads: 4 [Proportion Heads: 0.4]\n\n\nIf we return to the previous seed of 1234, we should obtain the same ten coin flips we did at first:\n\nset.seed(1234)\nrflip(10, prob = 0.5)\n\n\nFlipping 10 coins [ Prob(Heads) = 0.5 ] ...\n\nT H H H H H T T H H\n\nNumber of Heads: 7 [Proportion Heads: 0.7]\n\n\nAnd just to see the effect of setting a different seed:\n\nset.seed(9999)\nrflip(10, prob = 0.5)\n\n\nFlipping 10 coins [ Prob(Heads) = 0.5 ] ...\n\nH H H T H H T H H H\n\nNumber of Heads: 8 [Proportion Heads: 0.8]\n\n\n\nExercise 1\nIn ten coin flips, how many would you generally expect to come up heads? Is that the actual number of heads you saw in the simulations above? Why aren’t the simulations coming up with the expected number of heads each time?\n\nPlease write up your answer here."
  },
  {
    "objectID": "08-intro_to_randomization_1-web.html#randomization1-multiple",
    "href": "08-intro_to_randomization_1-web.html#randomization1-multiple",
    "title": "8  Introduction to randomization, Part 1",
    "section": "8.4 Multiple simulations",
    "text": "8.4 Multiple simulations\nSuppose now that you are not the only person flipping coins. Suppose a bunch of people in a room are all flipping coins. We’ll start with ten coin flips per person, a task that could be reasonably done even without a computer.\nYou might observe three heads in ten flips. Fine, but what about everyone else in the room? What numbers of heads will they see?\nThe do command from mosaic is a way of doing something multiple times. Imagine there are twenty people in the room, each flipping a coin ten times, each time with a 50% probability of coming up heads. Observe:\n\nset.seed(12345)\ndo(20) * rflip(10, prob = 0.5)\n\n    n heads tails prop\n1  10     2     8  0.2\n2  10     5     5  0.5\n3  10     5     5  0.5\n4  10     4     6  0.4\n5  10     4     6  0.4\n6  10     7     3  0.7\n7  10     6     4  0.6\n8  10     5     5  0.5\n9  10     7     3  0.7\n10 10     7     3  0.7\n11 10     6     4  0.6\n12 10     7     3  0.7\n13 10     7     3  0.7\n14 10     6     4  0.6\n15 10     7     3  0.7\n16 10     6     4  0.6\n17 10     7     3  0.7\n18 10     3     7  0.3\n19 10     4     6  0.4\n20 10     7     3  0.7\n\n\nThe syntax could not be any simpler: do(20) * means, literally, “do twenty times.” In other words, this command is telling R to repeat an action twenty times, where the action is flipping a single coin ten times.\nYou’ll notice that in place of a list of outcomes (H or T) of all the individual flips, we have instead a summary of the number of heads and tails each person sees. Each row represents a person, and the columns give information about each person’s flips. (There are n = 10 flips for each person, but then the number of heads/tails—and the corresponding “proportion” of heads—changes from person to person.)\nLooking at the above rows and columns, we see that the output of our little coin-flipping experiment is actually stored in a data frame! Let’s give it a name and work with it.\n\nset.seed(12345)\ncoin_flips_20_10 &lt;- do(20) * rflip(10, prob = 0.5)\ncoin_flips_20_10\n\n    n heads tails prop\n1  10     2     8  0.2\n2  10     5     5  0.5\n3  10     5     5  0.5\n4  10     4     6  0.4\n5  10     4     6  0.4\n6  10     7     3  0.7\n7  10     6     4  0.6\n8  10     5     5  0.5\n9  10     7     3  0.7\n10 10     7     3  0.7\n11 10     6     4  0.6\n12 10     7     3  0.7\n13 10     7     3  0.7\n14 10     6     4  0.6\n15 10     7     3  0.7\n16 10     6     4  0.6\n17 10     7     3  0.7\n18 10     3     7  0.3\n19 10     4     6  0.4\n20 10     7     3  0.7\n\n\nIt is significant that we can store our outcomes this way. Because we have a data frame, we can apply all our data analysis tools (graphs, charts, tables, summary statistics, etc.) to the “data” generated from our set of simulations.\nFor example, what is the mean number of heads these twenty people observed?\n\nmean(coin_flips_20_10$heads)\n\n[1] 5.6\n\n\n\nExercise 2\nThe data frame coin_flips_20_10 contains four variables: n, heads, tails, and prop. In the code chunk above, we calculated mean(coin_flips_20_10$heads) which gave us the mean count of heads for all people flipping coins. Instead of calculating the mean count of heads, change the variable from heads to prop to calculate the mean proportion of heads. Then explain why your answer makes sense in light of the mean count of heads calculated above.\n\n\n# Add code here to calculate the mean proportion of heads.\n\nPlease write up your answer here.\n\n\nLet’s look at a histogram of the number of heads we see in the simulated flips. (The fancy stuff in scale_x_continuous is just making sure that the x-axis goes from 0 to 10 and that the tick marks appear on each whole number.)\n\nggplot(coin_flips_20_10, aes(x = heads)) +\n    geom_histogram(binwidth = 0.5) +\n    scale_x_continuous(limits = c(-1, 11), breaks = seq(0, 10, 1))\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\nLet’s do the same thing, but now let’s consider the proportion of heads.\n\nggplot(coin_flips_20_10, aes(x = prop)) +\n    geom_histogram(binwidth = 0.05) +\n    scale_x_continuous(limits = c(-0.1, 1.1), breaks = seq(0, 1, 0.1))\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`)."
  },
  {
    "objectID": "08-intro_to_randomization_1-web.html#randomization1-bigger",
    "href": "08-intro_to_randomization_1-web.html#randomization1-bigger",
    "title": "8  Introduction to randomization, Part 1",
    "section": "8.5 Bigger and better!",
    "text": "8.5 Bigger and better!\nWith only twenty people, it was possible that, for example, nobody would get all heads or all tails. Indeed, in coin_flips_20_10 there were no people who got all heads or all tails. Also, there were more people with six and seven heads than with five heads, even though we “expected” the average to be five heads. There is nothing particularly significant about that; it happened by pure chance alone. Another run through the above commands would generate a somewhat different outcome. That’s what happens when things are random.\nInstead, let’s imagine that we recruited way more people to flip coins with us. Let’s try it again with 2000 people:\n\nset.seed(1234)\ncoin_flips_2000_10 &lt;- do(2000) * rflip(10, prob = 0.5)\ncoin_flips_2000_10\n\n      n heads tails prop\n1    10     4     6  0.4\n2    10     4     6  0.4\n3    10     4     6  0.4\n4    10     6     4  0.6\n5    10     5     5  0.5\n6    10     4     6  0.4\n7    10     4     6  0.4\n8    10     4     6  0.4\n9    10     3     7  0.3\n10   10     1     9  0.1\n11   10     5     5  0.5\n12   10     5     5  0.5\n13   10     7     3  0.7\n14   10     7     3  0.7\n15   10     5     5  0.5\n16   10     3     7  0.3\n17   10     5     5  0.5\n18   10     5     5  0.5\n19   10     9     1  0.9\n20   10     6     4  0.6\n21   10     7     3  0.7\n22   10     2     8  0.2\n23   10     6     4  0.6\n24   10     6     4  0.6\n25   10     5     5  0.5\n26   10     4     6  0.4\n27   10     5     5  0.5\n28   10     5     5  0.5\n29   10     6     4  0.6\n30   10     6     4  0.6\n31   10     3     7  0.3\n32   10     3     7  0.3\n33   10     4     6  0.4\n34   10     5     5  0.5\n35   10     7     3  0.7\n36   10     6     4  0.6\n37   10     4     6  0.4\n38   10     3     7  0.3\n39   10     7     3  0.7\n40   10     6     4  0.6\n41   10     6     4  0.6\n42   10     3     7  0.3\n43   10     7     3  0.7\n44   10     9     1  0.9\n45   10     7     3  0.7\n46   10     5     5  0.5\n47   10     4     6  0.4\n48   10     6     4  0.6\n49   10     7     3  0.7\n50   10     8     2  0.8\n51   10     6     4  0.6\n52   10     5     5  0.5\n53   10     7     3  0.7\n54   10     7     3  0.7\n55   10     5     5  0.5\n56   10     6     4  0.6\n57   10     5     5  0.5\n58   10     5     5  0.5\n59   10     7     3  0.7\n60   10     3     7  0.3\n61   10     4     6  0.4\n62   10     6     4  0.6\n63   10     6     4  0.6\n64   10     6     4  0.6\n65   10     5     5  0.5\n66   10     6     4  0.6\n67   10     5     5  0.5\n68   10     4     6  0.4\n69   10     4     6  0.4\n70   10     4     6  0.4\n71   10     4     6  0.4\n72   10     4     6  0.4\n73   10     7     3  0.7\n74   10     3     7  0.3\n75   10     7     3  0.7\n76   10     6     4  0.6\n77   10     6     4  0.6\n78   10     4     6  0.4\n79   10     7     3  0.7\n80   10     4     6  0.4\n81   10     4     6  0.4\n82   10     1     9  0.1\n83   10     7     3  0.7\n84   10     7     3  0.7\n85   10     7     3  0.7\n86   10     3     7  0.3\n87   10     6     4  0.6\n88   10     4     6  0.4\n89   10     7     3  0.7\n90   10     4     6  0.4\n91   10     3     7  0.3\n92   10     4     6  0.4\n93   10     5     5  0.5\n94   10     6     4  0.6\n95   10     6     4  0.6\n96   10     4     6  0.4\n97   10     7     3  0.7\n98   10     5     5  0.5\n99   10     5     5  0.5\n100  10     4     6  0.4\n101  10     6     4  0.6\n102  10     3     7  0.3\n103  10     5     5  0.5\n104  10     6     4  0.6\n105  10     5     5  0.5\n106  10     6     4  0.6\n107  10     2     8  0.2\n108  10     4     6  0.4\n109  10     4     6  0.4\n110  10     2     8  0.2\n111  10     5     5  0.5\n112  10     4     6  0.4\n113  10     5     5  0.5\n114  10     4     6  0.4\n115  10     1     9  0.1\n116  10     5     5  0.5\n117  10     2     8  0.2\n118  10     8     2  0.8\n119  10     4     6  0.4\n120  10     7     3  0.7\n121  10     5     5  0.5\n122  10     7     3  0.7\n123  10     5     5  0.5\n124  10     6     4  0.6\n125  10     4     6  0.4\n126  10     6     4  0.6\n127  10     8     2  0.8\n128  10     2     8  0.2\n129  10     6     4  0.6\n130  10     4     6  0.4\n131  10     6     4  0.6\n132  10     3     7  0.3\n133  10     3     7  0.3\n134  10     5     5  0.5\n135  10     6     4  0.6\n136  10     3     7  0.3\n137  10     7     3  0.7\n138  10     6     4  0.6\n139  10     5     5  0.5\n140  10     5     5  0.5\n141  10     4     6  0.4\n142  10     7     3  0.7\n143  10     3     7  0.3\n144  10     4     6  0.4\n145  10     4     6  0.4\n146  10     6     4  0.6\n147  10     6     4  0.6\n148  10     6     4  0.6\n149  10     7     3  0.7\n150  10     8     2  0.8\n151  10     3     7  0.3\n152  10     3     7  0.3\n153  10     4     6  0.4\n154  10     4     6  0.4\n155  10     3     7  0.3\n156  10     2     8  0.2\n157  10     3     7  0.3\n158  10     7     3  0.7\n159  10     5     5  0.5\n160  10     3     7  0.3\n161  10     4     6  0.4\n162  10     6     4  0.6\n163  10     4     6  0.4\n164  10     5     5  0.5\n165  10     4     6  0.4\n166  10     4     6  0.4\n167  10     3     7  0.3\n168  10     4     6  0.4\n169  10     4     6  0.4\n170  10     4     6  0.4\n171  10     4     6  0.4\n172  10     4     6  0.4\n173  10     7     3  0.7\n174  10     3     7  0.3\n175  10     8     2  0.8\n176  10     5     5  0.5\n177  10     8     2  0.8\n178  10     4     6  0.4\n179  10     5     5  0.5\n180  10     3     7  0.3\n181  10     7     3  0.7\n182  10     5     5  0.5\n183  10     4     6  0.4\n184  10     3     7  0.3\n185  10     6     4  0.6\n186  10     6     4  0.6\n187  10     7     3  0.7\n188  10     3     7  0.3\n189  10     5     5  0.5\n190  10     7     3  0.7\n191  10     4     6  0.4\n192  10     6     4  0.6\n193  10     4     6  0.4\n194  10     5     5  0.5\n195  10     5     5  0.5\n196  10     8     2  0.8\n197  10     9     1  0.9\n198  10     5     5  0.5\n199  10     7     3  0.7\n200  10     5     5  0.5\n201  10     4     6  0.4\n202  10     5     5  0.5\n203  10     3     7  0.3\n204  10     5     5  0.5\n205  10     6     4  0.6\n206  10     3     7  0.3\n207  10     4     6  0.4\n208  10     3     7  0.3\n209  10     4     6  0.4\n210  10     9     1  0.9\n211  10     4     6  0.4\n212  10     5     5  0.5\n213  10     6     4  0.6\n214  10     3     7  0.3\n215  10     5     5  0.5\n216  10     7     3  0.7\n217  10     4     6  0.4\n218  10     6     4  0.6\n219  10     4     6  0.4\n220  10     4     6  0.4\n221  10     4     6  0.4\n222  10     4     6  0.4\n223  10    10     0  1.0\n224  10     4     6  0.4\n225  10     3     7  0.3\n226  10     8     2  0.8\n227  10     7     3  0.7\n228  10     6     4  0.6\n229  10     6     4  0.6\n230  10     4     6  0.4\n231  10     6     4  0.6\n232  10     4     6  0.4\n233  10     6     4  0.6\n234  10     3     7  0.3\n235  10     4     6  0.4\n236  10     4     6  0.4\n237  10     5     5  0.5\n238  10     3     7  0.3\n239  10     4     6  0.4\n240  10     7     3  0.7\n241  10     8     2  0.8\n242  10     6     4  0.6\n243  10     6     4  0.6\n244  10     7     3  0.7\n245  10     6     4  0.6\n246  10     6     4  0.6\n247  10     8     2  0.8\n248  10     4     6  0.4\n249  10     4     6  0.4\n250  10     4     6  0.4\n251  10     4     6  0.4\n252  10     5     5  0.5\n253  10     5     5  0.5\n254  10     3     7  0.3\n255  10     4     6  0.4\n256  10     5     5  0.5\n257  10     6     4  0.6\n258  10     6     4  0.6\n259  10     6     4  0.6\n260  10     8     2  0.8\n261  10     5     5  0.5\n262  10     5     5  0.5\n263  10     1     9  0.1\n264  10     6     4  0.6\n265  10     3     7  0.3\n266  10     4     6  0.4\n267  10     6     4  0.6\n268  10     7     3  0.7\n269  10     7     3  0.7\n270  10     5     5  0.5\n271  10     5     5  0.5\n272  10     5     5  0.5\n273  10     5     5  0.5\n274  10     6     4  0.6\n275  10     5     5  0.5\n276  10     6     4  0.6\n277  10     6     4  0.6\n278  10     5     5  0.5\n279  10     5     5  0.5\n280  10     5     5  0.5\n281  10    10     0  1.0\n282  10     5     5  0.5\n283  10     7     3  0.7\n284  10     4     6  0.4\n285  10     5     5  0.5\n286  10     6     4  0.6\n287  10     6     4  0.6\n288  10     3     7  0.3\n289  10     6     4  0.6\n290  10     5     5  0.5\n291  10     7     3  0.7\n292  10     4     6  0.4\n293  10     4     6  0.4\n294  10     3     7  0.3\n295  10     8     2  0.8\n296  10     2     8  0.2\n297  10     5     5  0.5\n298  10     4     6  0.4\n299  10     7     3  0.7\n300  10     3     7  0.3\n301  10     3     7  0.3\n302  10     6     4  0.6\n303  10     6     4  0.6\n304  10     6     4  0.6\n305  10     4     6  0.4\n306  10     5     5  0.5\n307  10     4     6  0.4\n308  10     5     5  0.5\n309  10     3     7  0.3\n310  10     6     4  0.6\n311  10     6     4  0.6\n312  10     5     5  0.5\n313  10     4     6  0.4\n314  10     3     7  0.3\n315  10     5     5  0.5\n316  10     3     7  0.3\n317  10     4     6  0.4\n318  10     6     4  0.6\n319  10     4     6  0.4\n320  10     2     8  0.2\n321  10     5     5  0.5\n322  10     6     4  0.6\n323  10     4     6  0.4\n324  10     6     4  0.6\n325  10     4     6  0.4\n326  10     4     6  0.4\n327  10     6     4  0.6\n328  10     5     5  0.5\n329  10     7     3  0.7\n330  10     4     6  0.4\n331  10     3     7  0.3\n332  10     4     6  0.4\n333  10     5     5  0.5\n334  10     5     5  0.5\n335  10     6     4  0.6\n336  10     4     6  0.4\n337  10     3     7  0.3\n338  10     6     4  0.6\n339  10     4     6  0.4\n340  10     2     8  0.2\n341  10     7     3  0.7\n342  10     3     7  0.3\n343  10     6     4  0.6\n344  10     4     6  0.4\n345  10     0    10  0.0\n346  10     3     7  0.3\n347  10     6     4  0.6\n348  10     5     5  0.5\n349  10     7     3  0.7\n350  10     3     7  0.3\n351  10     6     4  0.6\n352  10     7     3  0.7\n353  10     6     4  0.6\n354  10     8     2  0.8\n355  10     6     4  0.6\n356  10     4     6  0.4\n357  10     8     2  0.8\n358  10     2     8  0.2\n359  10     4     6  0.4\n360  10     6     4  0.6\n361  10     2     8  0.2\n362  10     4     6  0.4\n363  10     5     5  0.5\n364  10     4     6  0.4\n365  10     7     3  0.7\n366  10     6     4  0.6\n367  10     6     4  0.6\n368  10     2     8  0.2\n369  10     4     6  0.4\n370  10     6     4  0.6\n371  10     2     8  0.2\n372  10     4     6  0.4\n373  10     2     8  0.2\n374  10     4     6  0.4\n375  10     8     2  0.8\n376  10     6     4  0.6\n377  10     6     4  0.6\n378  10     6     4  0.6\n379  10     6     4  0.6\n380  10     6     4  0.6\n381  10     6     4  0.6\n382  10     8     2  0.8\n383  10     4     6  0.4\n384  10     6     4  0.6\n385  10     4     6  0.4\n386  10     3     7  0.3\n387  10     6     4  0.6\n388  10     4     6  0.4\n389  10     6     4  0.6\n390  10     5     5  0.5\n391  10     4     6  0.4\n392  10     6     4  0.6\n393  10     6     4  0.6\n394  10     5     5  0.5\n395  10     4     6  0.4\n396  10     6     4  0.6\n397  10     4     6  0.4\n398  10     7     3  0.7\n399  10     4     6  0.4\n400  10     6     4  0.6\n401  10     3     7  0.3\n402  10     6     4  0.6\n403  10     7     3  0.7\n404  10     4     6  0.4\n405  10     6     4  0.6\n406  10     3     7  0.3\n407  10     7     3  0.7\n408  10     8     2  0.8\n409  10     4     6  0.4\n410  10     6     4  0.6\n411  10     4     6  0.4\n412  10     3     7  0.3\n413  10     4     6  0.4\n414  10     7     3  0.7\n415  10     3     7  0.3\n416  10     5     5  0.5\n417  10     5     5  0.5\n418  10     7     3  0.7\n419  10     6     4  0.6\n420  10     5     5  0.5\n421  10     6     4  0.6\n422  10     3     7  0.3\n423  10     5     5  0.5\n424  10     4     6  0.4\n425  10     5     5  0.5\n426  10     5     5  0.5\n427  10     3     7  0.3\n428  10     6     4  0.6\n429  10     4     6  0.4\n430  10     6     4  0.6\n431  10     7     3  0.7\n432  10     7     3  0.7\n433  10     5     5  0.5\n434  10     4     6  0.4\n435  10     4     6  0.4\n436  10     3     7  0.3\n437  10     4     6  0.4\n438  10     5     5  0.5\n439  10     7     3  0.7\n440  10     5     5  0.5\n441  10     5     5  0.5\n442  10     7     3  0.7\n443  10     8     2  0.8\n444  10     6     4  0.6\n445  10     5     5  0.5\n446  10     4     6  0.4\n447  10     3     7  0.3\n448  10     5     5  0.5\n449  10     6     4  0.6\n450  10     7     3  0.7\n451  10     9     1  0.9\n452  10     5     5  0.5\n453  10     5     5  0.5\n454  10     3     7  0.3\n455  10     5     5  0.5\n456  10     5     5  0.5\n457  10     5     5  0.5\n458  10     3     7  0.3\n459  10     3     7  0.3\n460  10     5     5  0.5\n461  10     4     6  0.4\n462  10     7     3  0.7\n463  10     7     3  0.7\n464  10     3     7  0.3\n465  10     4     6  0.4\n466  10     5     5  0.5\n467  10     5     5  0.5\n468  10     3     7  0.3\n469  10     8     2  0.8\n470  10     5     5  0.5\n471  10     6     4  0.6\n472  10     5     5  0.5\n473  10     7     3  0.7\n474  10     4     6  0.4\n475  10     4     6  0.4\n476  10     5     5  0.5\n477  10     2     8  0.2\n478  10     6     4  0.6\n479  10     6     4  0.6\n480  10     2     8  0.2\n481  10     6     4  0.6\n482  10     5     5  0.5\n483  10     5     5  0.5\n484  10     6     4  0.6\n485  10     4     6  0.4\n486  10     5     5  0.5\n487  10     6     4  0.6\n488  10     3     7  0.3\n489  10     3     7  0.3\n490  10     6     4  0.6\n491  10     4     6  0.4\n492  10     7     3  0.7\n493  10     4     6  0.4\n494  10     6     4  0.6\n495  10     4     6  0.4\n496  10     8     2  0.8\n497  10     5     5  0.5\n498  10     6     4  0.6\n499  10     6     4  0.6\n500  10     4     6  0.4\n501  10     4     6  0.4\n502  10     5     5  0.5\n503  10     3     7  0.3\n504  10     3     7  0.3\n505  10     6     4  0.6\n506  10     5     5  0.5\n507  10     6     4  0.6\n508  10     5     5  0.5\n509  10     5     5  0.5\n510  10     6     4  0.6\n511  10     5     5  0.5\n512  10     4     6  0.4\n513  10     6     4  0.6\n514  10     5     5  0.5\n515  10     5     5  0.5\n516  10     9     1  0.9\n517  10     4     6  0.4\n518  10     2     8  0.2\n519  10     3     7  0.3\n520  10     4     6  0.4\n521  10     2     8  0.2\n522  10     6     4  0.6\n523  10     6     4  0.6\n524  10     7     3  0.7\n525  10     5     5  0.5\n526  10     7     3  0.7\n527  10     7     3  0.7\n528  10     2     8  0.2\n529  10     4     6  0.4\n530  10     8     2  0.8\n531  10     5     5  0.5\n532  10     6     4  0.6\n533  10     8     2  0.8\n534  10     3     7  0.3\n535  10     4     6  0.4\n536  10     6     4  0.6\n537  10     8     2  0.8\n538  10     4     6  0.4\n539  10     4     6  0.4\n540  10     6     4  0.6\n541  10     5     5  0.5\n542  10     4     6  0.4\n543  10     5     5  0.5\n544  10     5     5  0.5\n545  10     3     7  0.3\n546  10     4     6  0.4\n547  10     6     4  0.6\n548  10     4     6  0.4\n549  10     6     4  0.6\n550  10     4     6  0.4\n551  10     6     4  0.6\n552  10     3     7  0.3\n553  10     5     5  0.5\n554  10     6     4  0.6\n555  10     5     5  0.5\n556  10     8     2  0.8\n557  10     2     8  0.2\n558  10     5     5  0.5\n559  10     4     6  0.4\n560  10     5     5  0.5\n561  10     4     6  0.4\n562  10     6     4  0.6\n563  10     6     4  0.6\n564  10     4     6  0.4\n565  10     2     8  0.2\n566  10     3     7  0.3\n567  10     6     4  0.6\n568  10     3     7  0.3\n569  10     5     5  0.5\n570  10     7     3  0.7\n571  10     8     2  0.8\n572  10     6     4  0.6\n573  10     4     6  0.4\n574  10     6     4  0.6\n575  10     3     7  0.3\n576  10     4     6  0.4\n577  10     5     5  0.5\n578  10     7     3  0.7\n579  10     4     6  0.4\n580  10     4     6  0.4\n581  10     2     8  0.2\n582  10     6     4  0.6\n583  10     5     5  0.5\n584  10     5     5  0.5\n585  10     5     5  0.5\n586  10     6     4  0.6\n587  10     6     4  0.6\n588  10     8     2  0.8\n589  10     5     5  0.5\n590  10     8     2  0.8\n591  10     5     5  0.5\n592  10     6     4  0.6\n593  10     7     3  0.7\n594  10     3     7  0.3\n595  10     4     6  0.4\n596  10     2     8  0.2\n597  10     5     5  0.5\n598  10     6     4  0.6\n599  10     6     4  0.6\n600  10     7     3  0.7\n601  10     4     6  0.4\n602  10     6     4  0.6\n603  10     6     4  0.6\n604  10     5     5  0.5\n605  10     5     5  0.5\n606  10     7     3  0.7\n607  10     7     3  0.7\n608  10     6     4  0.6\n609  10     3     7  0.3\n610  10     4     6  0.4\n611  10     9     1  0.9\n612  10     6     4  0.6\n613  10     5     5  0.5\n614  10     4     6  0.4\n615  10     6     4  0.6\n616  10     4     6  0.4\n617  10     7     3  0.7\n618  10     3     7  0.3\n619  10     6     4  0.6\n620  10     5     5  0.5\n621  10     7     3  0.7\n622  10     5     5  0.5\n623  10     5     5  0.5\n624  10     5     5  0.5\n625  10     6     4  0.6\n626  10     3     7  0.3\n627  10     4     6  0.4\n628  10     8     2  0.8\n629  10     6     4  0.6\n630  10     6     4  0.6\n631  10     5     5  0.5\n632  10     3     7  0.3\n633  10     5     5  0.5\n634  10     4     6  0.4\n635  10     6     4  0.6\n636  10     7     3  0.7\n637  10     5     5  0.5\n638  10     4     6  0.4\n639  10     4     6  0.4\n640  10     5     5  0.5\n641  10     3     7  0.3\n642  10     4     6  0.4\n643  10     5     5  0.5\n644  10     7     3  0.7\n645  10     5     5  0.5\n646  10     5     5  0.5\n647  10     5     5  0.5\n648  10     4     6  0.4\n649  10     5     5  0.5\n650  10     7     3  0.7\n651  10     3     7  0.3\n652  10     6     4  0.6\n653  10     6     4  0.6\n654  10     8     2  0.8\n655  10     7     3  0.7\n656  10     4     6  0.4\n657  10     7     3  0.7\n658  10     5     5  0.5\n659  10     7     3  0.7\n660  10     6     4  0.6\n661  10     2     8  0.2\n662  10     8     2  0.8\n663  10     2     8  0.2\n664  10     6     4  0.6\n665  10     4     6  0.4\n666  10     3     7  0.3\n667  10     5     5  0.5\n668  10     6     4  0.6\n669  10     6     4  0.6\n670  10     4     6  0.4\n671  10     7     3  0.7\n672  10     2     8  0.2\n673  10     2     8  0.2\n674  10     6     4  0.6\n675  10     5     5  0.5\n676  10     8     2  0.8\n677  10     5     5  0.5\n678  10     5     5  0.5\n679  10     5     5  0.5\n680  10     5     5  0.5\n681  10     6     4  0.6\n682  10     4     6  0.4\n683  10     2     8  0.2\n684  10     6     4  0.6\n685  10     4     6  0.4\n686  10     5     5  0.5\n687  10     5     5  0.5\n688  10     6     4  0.6\n689  10     6     4  0.6\n690  10     4     6  0.4\n691  10     4     6  0.4\n692  10     4     6  0.4\n693  10     5     5  0.5\n694  10     5     5  0.5\n695  10     5     5  0.5\n696  10     5     5  0.5\n697  10     6     4  0.6\n698  10     6     4  0.6\n699  10     5     5  0.5\n700  10     7     3  0.7\n701  10     2     8  0.2\n702  10     7     3  0.7\n703  10     7     3  0.7\n704  10     1     9  0.1\n705  10     5     5  0.5\n706  10     5     5  0.5\n707  10     4     6  0.4\n708  10     4     6  0.4\n709  10     6     4  0.6\n710  10     3     7  0.3\n711  10     4     6  0.4\n712  10     5     5  0.5\n713  10     8     2  0.8\n714  10     3     7  0.3\n715  10     6     4  0.6\n716  10     5     5  0.5\n717  10     4     6  0.4\n718  10     2     8  0.2\n719  10     3     7  0.3\n720  10     1     9  0.1\n721  10     3     7  0.3\n722  10     6     4  0.6\n723  10     3     7  0.3\n724  10     5     5  0.5\n725  10     5     5  0.5\n726  10     7     3  0.7\n727  10     7     3  0.7\n728  10     3     7  0.3\n729  10     4     6  0.4\n730  10     5     5  0.5\n731  10     7     3  0.7\n732  10     6     4  0.6\n733  10     7     3  0.7\n734  10     8     2  0.8\n735  10     6     4  0.6\n736  10     2     8  0.2\n737  10     6     4  0.6\n738  10     6     4  0.6\n739  10     5     5  0.5\n740  10     4     6  0.4\n741  10     6     4  0.6\n742  10     5     5  0.5\n743  10     5     5  0.5\n744  10     4     6  0.4\n745  10     5     5  0.5\n746  10     4     6  0.4\n747  10     3     7  0.3\n748  10     5     5  0.5\n749  10     6     4  0.6\n750  10     6     4  0.6\n751  10     7     3  0.7\n752  10     4     6  0.4\n753  10     4     6  0.4\n754  10     5     5  0.5\n755  10     6     4  0.6\n756  10     6     4  0.6\n757  10     3     7  0.3\n758  10     5     5  0.5\n759  10     4     6  0.4\n760  10     5     5  0.5\n761  10     5     5  0.5\n762  10     5     5  0.5\n763  10     5     5  0.5\n764  10     4     6  0.4\n765  10     5     5  0.5\n766  10     5     5  0.5\n767  10     5     5  0.5\n768  10     5     5  0.5\n769  10     7     3  0.7\n770  10     3     7  0.3\n771  10     2     8  0.2\n772  10     6     4  0.6\n773  10     8     2  0.8\n774  10     5     5  0.5\n775  10     7     3  0.7\n776  10     6     4  0.6\n777  10     5     5  0.5\n778  10     7     3  0.7\n779  10     3     7  0.3\n780  10     5     5  0.5\n781  10     6     4  0.6\n782  10     3     7  0.3\n783  10     4     6  0.4\n784  10     5     5  0.5\n785  10     5     5  0.5\n786  10     7     3  0.7\n787  10     5     5  0.5\n788  10     5     5  0.5\n789  10     2     8  0.2\n790  10     6     4  0.6\n791  10     5     5  0.5\n792  10     8     2  0.8\n793  10     5     5  0.5\n794  10     4     6  0.4\n795  10     6     4  0.6\n796  10     5     5  0.5\n797  10     7     3  0.7\n798  10     6     4  0.6\n799  10     5     5  0.5\n800  10     5     5  0.5\n801  10     3     7  0.3\n802  10     4     6  0.4\n803  10     3     7  0.3\n804  10     3     7  0.3\n805  10     3     7  0.3\n806  10     5     5  0.5\n807  10     5     5  0.5\n808  10     7     3  0.7\n809  10     4     6  0.4\n810  10     7     3  0.7\n811  10     5     5  0.5\n812  10     5     5  0.5\n813  10     5     5  0.5\n814  10     5     5  0.5\n815  10     5     5  0.5\n816  10     4     6  0.4\n817  10     7     3  0.7\n818  10     4     6  0.4\n819  10     4     6  0.4\n820  10     3     7  0.3\n821  10     6     4  0.6\n822  10     6     4  0.6\n823  10     6     4  0.6\n824  10     8     2  0.8\n825  10     3     7  0.3\n826  10     3     7  0.3\n827  10     6     4  0.6\n828  10     7     3  0.7\n829  10     5     5  0.5\n830  10     3     7  0.3\n831  10     6     4  0.6\n832  10     6     4  0.6\n833  10     5     5  0.5\n834  10     6     4  0.6\n835  10     5     5  0.5\n836  10     8     2  0.8\n837  10     5     5  0.5\n838  10     5     5  0.5\n839  10     3     7  0.3\n840  10     2     8  0.2\n841  10     4     6  0.4\n842  10     6     4  0.6\n843  10     7     3  0.7\n844  10     7     3  0.7\n845  10     3     7  0.3\n846  10     3     7  0.3\n847  10     3     7  0.3\n848  10     4     6  0.4\n849  10     5     5  0.5\n850  10     6     4  0.6\n851  10     4     6  0.4\n852  10     3     7  0.3\n853  10     4     6  0.4\n854  10     5     5  0.5\n855  10     4     6  0.4\n856  10     6     4  0.6\n857  10     6     4  0.6\n858  10     7     3  0.7\n859  10     5     5  0.5\n860  10     5     5  0.5\n861  10     4     6  0.4\n862  10     6     4  0.6\n863  10     4     6  0.4\n864  10     6     4  0.6\n865  10     6     4  0.6\n866  10     6     4  0.6\n867  10     2     8  0.2\n868  10     4     6  0.4\n869  10     3     7  0.3\n870  10     5     5  0.5\n871  10     7     3  0.7\n872  10     5     5  0.5\n873  10     5     5  0.5\n874  10     4     6  0.4\n875  10     6     4  0.6\n876  10     7     3  0.7\n877  10     4     6  0.4\n878  10     3     7  0.3\n879  10     5     5  0.5\n880  10     7     3  0.7\n881  10     6     4  0.6\n882  10     7     3  0.7\n883  10     8     2  0.8\n884  10     6     4  0.6\n885  10     3     7  0.3\n886  10     6     4  0.6\n887  10     4     6  0.4\n888  10     4     6  0.4\n889  10     5     5  0.5\n890  10     5     5  0.5\n891  10     7     3  0.7\n892  10     5     5  0.5\n893  10     7     3  0.7\n894  10     5     5  0.5\n895  10     6     4  0.6\n896  10     3     7  0.3\n897  10     6     4  0.6\n898  10     4     6  0.4\n899  10     4     6  0.4\n900  10     2     8  0.2\n901  10     7     3  0.7\n902  10     7     3  0.7\n903  10     6     4  0.6\n904  10     7     3  0.7\n905  10     4     6  0.4\n906  10     3     7  0.3\n907  10     3     7  0.3\n908  10     3     7  0.3\n909  10     6     4  0.6\n910  10     5     5  0.5\n911  10     5     5  0.5\n912  10     8     2  0.8\n913  10     7     3  0.7\n914  10     5     5  0.5\n915  10     3     7  0.3\n916  10     6     4  0.6\n917  10     3     7  0.3\n918  10     6     4  0.6\n919  10     4     6  0.4\n920  10     8     2  0.8\n921  10     5     5  0.5\n922  10     6     4  0.6\n923  10     2     8  0.2\n924  10     6     4  0.6\n925  10     3     7  0.3\n926  10     5     5  0.5\n927  10     4     6  0.4\n928  10     3     7  0.3\n929  10     6     4  0.6\n930  10     5     5  0.5\n931  10     5     5  0.5\n932  10     4     6  0.4\n933  10     4     6  0.4\n934  10     4     6  0.4\n935  10     7     3  0.7\n936  10     3     7  0.3\n937  10     2     8  0.2\n938  10     5     5  0.5\n939  10     3     7  0.3\n940  10     6     4  0.6\n941  10     5     5  0.5\n942  10     6     4  0.6\n943  10     5     5  0.5\n944  10     4     6  0.4\n945  10     4     6  0.4\n946  10     3     7  0.3\n947  10     3     7  0.3\n948  10     4     6  0.4\n949  10     4     6  0.4\n950  10     5     5  0.5\n951  10     9     1  0.9\n952  10     3     7  0.3\n953  10     7     3  0.7\n954  10     8     2  0.8\n955  10     7     3  0.7\n956  10     6     4  0.6\n957  10     5     5  0.5\n958  10     5     5  0.5\n959  10     7     3  0.7\n960  10     5     5  0.5\n961  10     4     6  0.4\n962  10     5     5  0.5\n963  10     7     3  0.7\n964  10     5     5  0.5\n965  10     4     6  0.4\n966  10     5     5  0.5\n967  10     8     2  0.8\n968  10     5     5  0.5\n969  10     4     6  0.4\n970  10     6     4  0.6\n971  10     6     4  0.6\n972  10     3     7  0.3\n973  10     5     5  0.5\n974  10     4     6  0.4\n975  10     6     4  0.6\n976  10     4     6  0.4\n977  10     4     6  0.4\n978  10     5     5  0.5\n979  10     8     2  0.8\n980  10     5     5  0.5\n981  10     6     4  0.6\n982  10     5     5  0.5\n983  10     4     6  0.4\n984  10     3     7  0.3\n985  10     7     3  0.7\n986  10     6     4  0.6\n987  10     4     6  0.4\n988  10     4     6  0.4\n989  10     4     6  0.4\n990  10     5     5  0.5\n991  10     7     3  0.7\n992  10     2     8  0.2\n993  10     4     6  0.4\n994  10     5     5  0.5\n995  10     5     5  0.5\n996  10     4     6  0.4\n997  10     7     3  0.7\n998  10     4     6  0.4\n999  10     4     6  0.4\n1000 10     2     8  0.2\n1001 10     8     2  0.8\n1002 10     5     5  0.5\n1003 10     4     6  0.4\n1004 10     6     4  0.6\n1005 10     5     5  0.5\n1006 10     3     7  0.3\n1007 10     7     3  0.7\n1008 10     5     5  0.5\n1009 10     6     4  0.6\n1010 10     5     5  0.5\n1011 10     6     4  0.6\n1012 10     7     3  0.7\n1013 10     4     6  0.4\n1014 10     3     7  0.3\n1015 10     7     3  0.7\n1016 10     5     5  0.5\n1017 10     7     3  0.7\n1018 10     8     2  0.8\n1019 10     5     5  0.5\n1020 10     6     4  0.6\n1021 10     4     6  0.4\n1022 10     6     4  0.6\n1023 10     7     3  0.7\n1024 10     5     5  0.5\n1025 10     6     4  0.6\n1026 10     5     5  0.5\n1027 10     4     6  0.4\n1028 10     5     5  0.5\n1029 10     6     4  0.6\n1030 10     3     7  0.3\n1031 10     4     6  0.4\n1032 10     5     5  0.5\n1033 10     3     7  0.3\n1034 10     6     4  0.6\n1035 10     5     5  0.5\n1036 10     5     5  0.5\n1037 10     4     6  0.4\n1038 10     5     5  0.5\n1039 10     4     6  0.4\n1040 10     7     3  0.7\n1041 10     5     5  0.5\n1042 10     6     4  0.6\n1043 10     4     6  0.4\n1044 10     9     1  0.9\n1045 10     4     6  0.4\n1046 10     6     4  0.6\n1047 10     6     4  0.6\n1048 10     5     5  0.5\n1049 10     3     7  0.3\n1050 10     8     2  0.8\n1051 10     4     6  0.4\n1052 10     6     4  0.6\n1053 10     6     4  0.6\n1054 10     7     3  0.7\n1055 10     5     5  0.5\n1056 10     5     5  0.5\n1057 10     6     4  0.6\n1058 10     5     5  0.5\n1059 10     7     3  0.7\n1060 10     7     3  0.7\n1061 10     3     7  0.3\n1062 10     4     6  0.4\n1063 10     8     2  0.8\n1064 10     5     5  0.5\n1065 10     7     3  0.7\n1066 10     6     4  0.6\n1067 10     6     4  0.6\n1068 10     4     6  0.4\n1069 10     6     4  0.6\n1070 10     5     5  0.5\n1071 10     6     4  0.6\n1072 10     6     4  0.6\n1073 10     4     6  0.4\n1074 10     5     5  0.5\n1075 10     4     6  0.4\n1076 10     4     6  0.4\n1077 10     5     5  0.5\n1078 10     6     4  0.6\n1079 10     6     4  0.6\n1080 10     4     6  0.4\n1081 10     7     3  0.7\n1082 10     3     7  0.3\n1083 10     3     7  0.3\n1084 10     3     7  0.3\n1085 10     2     8  0.2\n1086 10     4     6  0.4\n1087 10     4     6  0.4\n1088 10     4     6  0.4\n1089 10     9     1  0.9\n1090 10     7     3  0.7\n1091 10     8     2  0.8\n1092 10     6     4  0.6\n1093 10     4     6  0.4\n1094 10     4     6  0.4\n1095 10     5     5  0.5\n1096 10     4     6  0.4\n1097 10     7     3  0.7\n1098 10     5     5  0.5\n1099 10     8     2  0.8\n1100 10     3     7  0.3\n1101 10     3     7  0.3\n1102 10     6     4  0.6\n1103 10     7     3  0.7\n1104 10     6     4  0.6\n1105 10     5     5  0.5\n1106 10     5     5  0.5\n1107 10     6     4  0.6\n1108 10     8     2  0.8\n1109 10     5     5  0.5\n1110 10     7     3  0.7\n1111 10     7     3  0.7\n1112 10     5     5  0.5\n1113 10     3     7  0.3\n1114 10     5     5  0.5\n1115 10     4     6  0.4\n1116 10     3     7  0.3\n1117 10     5     5  0.5\n1118 10     4     6  0.4\n1119 10     4     6  0.4\n1120 10     2     8  0.2\n1121 10     7     3  0.7\n1122 10     5     5  0.5\n1123 10     8     2  0.8\n1124 10     6     4  0.6\n1125 10     5     5  0.5\n1126 10     6     4  0.6\n1127 10     5     5  0.5\n1128 10     4     6  0.4\n1129 10     5     5  0.5\n1130 10     7     3  0.7\n1131 10     5     5  0.5\n1132 10     4     6  0.4\n1133 10     4     6  0.4\n1134 10     6     4  0.6\n1135 10     5     5  0.5\n1136 10     6     4  0.6\n1137 10     5     5  0.5\n1138 10     4     6  0.4\n1139 10     3     7  0.3\n1140 10     6     4  0.6\n1141 10     6     4  0.6\n1142 10     4     6  0.4\n1143 10     4     6  0.4\n1144 10     2     8  0.2\n1145 10     2     8  0.2\n1146 10     8     2  0.8\n1147 10     5     5  0.5\n1148 10     4     6  0.4\n1149 10     4     6  0.4\n1150 10     5     5  0.5\n1151 10     5     5  0.5\n1152 10     5     5  0.5\n1153 10     6     4  0.6\n1154 10     6     4  0.6\n1155 10     7     3  0.7\n1156 10     4     6  0.4\n1157 10     3     7  0.3\n1158 10     7     3  0.7\n1159 10     4     6  0.4\n1160 10     5     5  0.5\n1161 10     5     5  0.5\n1162 10     5     5  0.5\n1163 10     7     3  0.7\n1164 10     6     4  0.6\n1165 10     5     5  0.5\n1166 10     4     6  0.4\n1167 10     7     3  0.7\n1168 10     6     4  0.6\n1169 10     7     3  0.7\n1170 10     5     5  0.5\n1171 10     6     4  0.6\n1172 10     6     4  0.6\n1173 10     7     3  0.7\n1174 10     4     6  0.4\n1175 10     7     3  0.7\n1176 10     7     3  0.7\n1177 10     3     7  0.3\n1178 10     6     4  0.6\n1179 10     5     5  0.5\n1180 10     5     5  0.5\n1181 10     5     5  0.5\n1182 10     6     4  0.6\n1183 10     2     8  0.2\n1184 10     5     5  0.5\n1185 10     2     8  0.2\n1186 10     6     4  0.6\n1187 10     6     4  0.6\n1188 10     3     7  0.3\n1189 10     4     6  0.4\n1190 10     4     6  0.4\n1191 10     4     6  0.4\n1192 10     6     4  0.6\n1193 10     7     3  0.7\n1194 10     3     7  0.3\n1195 10     3     7  0.3\n1196 10     3     7  0.3\n1197 10     4     6  0.4\n1198 10     3     7  0.3\n1199 10     1     9  0.1\n1200 10     6     4  0.6\n1201 10     7     3  0.7\n1202 10     2     8  0.2\n1203 10     4     6  0.4\n1204 10     5     5  0.5\n1205 10     6     4  0.6\n1206 10     4     6  0.4\n1207 10     4     6  0.4\n1208 10     5     5  0.5\n1209 10     6     4  0.6\n1210 10     3     7  0.3\n1211 10     2     8  0.2\n1212 10     3     7  0.3\n1213 10     3     7  0.3\n1214 10     4     6  0.4\n1215 10     5     5  0.5\n1216 10     5     5  0.5\n1217 10     6     4  0.6\n1218 10     6     4  0.6\n1219 10     4     6  0.4\n1220 10     3     7  0.3\n1221 10     5     5  0.5\n1222 10     5     5  0.5\n1223 10     4     6  0.4\n1224 10     7     3  0.7\n1225 10     5     5  0.5\n1226 10     4     6  0.4\n1227 10     5     5  0.5\n1228 10     5     5  0.5\n1229 10     3     7  0.3\n1230 10     6     4  0.6\n1231 10     5     5  0.5\n1232 10     5     5  0.5\n1233 10     5     5  0.5\n1234 10     6     4  0.6\n1235 10     4     6  0.4\n1236 10     5     5  0.5\n1237 10     4     6  0.4\n1238 10     6     4  0.6\n1239 10     6     4  0.6\n1240 10     7     3  0.7\n1241 10     8     2  0.8\n1242 10     6     4  0.6\n1243 10     6     4  0.6\n1244 10     5     5  0.5\n1245 10     4     6  0.4\n1246 10     6     4  0.6\n1247 10     4     6  0.4\n1248 10     8     2  0.8\n1249 10     2     8  0.2\n1250 10     5     5  0.5\n1251 10     4     6  0.4\n1252 10     6     4  0.6\n1253 10     6     4  0.6\n1254 10     4     6  0.4\n1255 10     2     8  0.2\n1256 10     7     3  0.7\n1257 10     5     5  0.5\n1258 10     7     3  0.7\n1259 10     5     5  0.5\n1260 10     6     4  0.6\n1261 10     6     4  0.6\n1262 10     5     5  0.5\n1263 10     6     4  0.6\n1264 10     4     6  0.4\n1265 10     7     3  0.7\n1266 10     4     6  0.4\n1267 10     3     7  0.3\n1268 10     4     6  0.4\n1269 10     5     5  0.5\n1270 10     3     7  0.3\n1271 10     5     5  0.5\n1272 10     4     6  0.4\n1273 10     7     3  0.7\n1274 10     5     5  0.5\n1275 10     4     6  0.4\n1276 10     8     2  0.8\n1277 10     5     5  0.5\n1278 10     4     6  0.4\n1279 10     3     7  0.3\n1280 10     4     6  0.4\n1281 10     5     5  0.5\n1282 10     5     5  0.5\n1283 10     4     6  0.4\n1284 10     7     3  0.7\n1285 10     4     6  0.4\n1286 10     3     7  0.3\n1287 10     4     6  0.4\n1288 10     4     6  0.4\n1289 10     5     5  0.5\n1290 10     3     7  0.3\n1291 10     7     3  0.7\n1292 10     6     4  0.6\n1293 10     5     5  0.5\n1294 10     5     5  0.5\n1295 10     7     3  0.7\n1296 10     2     8  0.2\n1297 10     4     6  0.4\n1298 10     2     8  0.2\n1299 10     4     6  0.4\n1300 10     6     4  0.6\n1301 10     4     6  0.4\n1302 10     6     4  0.6\n1303 10     5     5  0.5\n1304 10     9     1  0.9\n1305 10     5     5  0.5\n1306 10     5     5  0.5\n1307 10     5     5  0.5\n1308 10     5     5  0.5\n1309 10     6     4  0.6\n1310 10     1     9  0.1\n1311 10     6     4  0.6\n1312 10     2     8  0.2\n1313 10     6     4  0.6\n1314 10     6     4  0.6\n1315 10     7     3  0.7\n1316 10     9     1  0.9\n1317 10     5     5  0.5\n1318 10     4     6  0.4\n1319 10     6     4  0.6\n1320 10     3     7  0.3\n1321 10     4     6  0.4\n1322 10     3     7  0.3\n1323 10     6     4  0.6\n1324 10     6     4  0.6\n1325 10     6     4  0.6\n1326 10     4     6  0.4\n1327 10     6     4  0.6\n1328 10     6     4  0.6\n1329 10     5     5  0.5\n1330 10     5     5  0.5\n1331 10     3     7  0.3\n1332 10     6     4  0.6\n1333 10     2     8  0.2\n1334 10     4     6  0.4\n1335 10     8     2  0.8\n1336 10     3     7  0.3\n1337 10     4     6  0.4\n1338 10     5     5  0.5\n1339 10     4     6  0.4\n1340 10     7     3  0.7\n1341 10     3     7  0.3\n1342 10     3     7  0.3\n1343 10     7     3  0.7\n1344 10     7     3  0.7\n1345 10     4     6  0.4\n1346 10     3     7  0.3\n1347 10     7     3  0.7\n1348 10     3     7  0.3\n1349 10     4     6  0.4\n1350 10     4     6  0.4\n1351 10     7     3  0.7\n1352 10     5     5  0.5\n1353 10     6     4  0.6\n1354 10     8     2  0.8\n1355 10     3     7  0.3\n1356 10     7     3  0.7\n1357 10     4     6  0.4\n1358 10     4     6  0.4\n1359 10     4     6  0.4\n1360 10     3     7  0.3\n1361 10     4     6  0.4\n1362 10     7     3  0.7\n1363 10     7     3  0.7\n1364 10     9     1  0.9\n1365 10     5     5  0.5\n1366 10     8     2  0.8\n1367 10     5     5  0.5\n1368 10     7     3  0.7\n1369 10     3     7  0.3\n1370 10     8     2  0.8\n1371 10     9     1  0.9\n1372 10     5     5  0.5\n1373 10     6     4  0.6\n1374 10     6     4  0.6\n1375 10     8     2  0.8\n1376 10     6     4  0.6\n1377 10     3     7  0.3\n1378 10     3     7  0.3\n1379 10     5     5  0.5\n1380 10     6     4  0.6\n1381 10     4     6  0.4\n1382 10     7     3  0.7\n1383 10     8     2  0.8\n1384 10     7     3  0.7\n1385 10     5     5  0.5\n1386 10     5     5  0.5\n1387 10     6     4  0.6\n1388 10     4     6  0.4\n1389 10     6     4  0.6\n1390 10     6     4  0.6\n1391 10     6     4  0.6\n1392 10     3     7  0.3\n1393 10     5     5  0.5\n1394 10     4     6  0.4\n1395 10     2     8  0.2\n1396 10     5     5  0.5\n1397 10     4     6  0.4\n1398 10     6     4  0.6\n1399 10     3     7  0.3\n1400 10     6     4  0.6\n1401 10     6     4  0.6\n1402 10     3     7  0.3\n1403 10     4     6  0.4\n1404 10     6     4  0.6\n1405 10     5     5  0.5\n1406 10     6     4  0.6\n1407 10     6     4  0.6\n1408 10     4     6  0.4\n1409 10     4     6  0.4\n1410 10     6     4  0.6\n1411 10     4     6  0.4\n1412 10     7     3  0.7\n1413 10     5     5  0.5\n1414 10     6     4  0.6\n1415 10     5     5  0.5\n1416 10     4     6  0.4\n1417 10     7     3  0.7\n1418 10     7     3  0.7\n1419 10     6     4  0.6\n1420 10     3     7  0.3\n1421 10     6     4  0.6\n1422 10     3     7  0.3\n1423 10     6     4  0.6\n1424 10     8     2  0.8\n1425 10     5     5  0.5\n1426 10     6     4  0.6\n1427 10     3     7  0.3\n1428 10     8     2  0.8\n1429 10     5     5  0.5\n1430 10     4     6  0.4\n1431 10     6     4  0.6\n1432 10     6     4  0.6\n1433 10     6     4  0.6\n1434 10     3     7  0.3\n1435 10     7     3  0.7\n1436 10     5     5  0.5\n1437 10     5     5  0.5\n1438 10     3     7  0.3\n1439 10     6     4  0.6\n1440 10     4     6  0.4\n1441 10     5     5  0.5\n1442 10     7     3  0.7\n1443 10     4     6  0.4\n1444 10     6     4  0.6\n1445 10     4     6  0.4\n1446 10     7     3  0.7\n1447 10     6     4  0.6\n1448 10     3     7  0.3\n1449 10     4     6  0.4\n1450 10     6     4  0.6\n1451 10     5     5  0.5\n1452 10     5     5  0.5\n1453 10     8     2  0.8\n1454 10     6     4  0.6\n1455 10     5     5  0.5\n1456 10     4     6  0.4\n1457 10     7     3  0.7\n1458 10     7     3  0.7\n1459 10     5     5  0.5\n1460 10     4     6  0.4\n1461 10     5     5  0.5\n1462 10     7     3  0.7\n1463 10     3     7  0.3\n1464 10     6     4  0.6\n1465 10     5     5  0.5\n1466 10     5     5  0.5\n1467 10     4     6  0.4\n1468 10     2     8  0.2\n1469 10     4     6  0.4\n1470 10     6     4  0.6\n1471 10     6     4  0.6\n1472 10     7     3  0.7\n1473 10     5     5  0.5\n1474 10     6     4  0.6\n1475 10     3     7  0.3\n1476 10     6     4  0.6\n1477 10     7     3  0.7\n1478 10     6     4  0.6\n1479 10     5     5  0.5\n1480 10     9     1  0.9\n1481 10     7     3  0.7\n1482 10     6     4  0.6\n1483 10     6     4  0.6\n1484 10     5     5  0.5\n1485 10     3     7  0.3\n1486 10     4     6  0.4\n1487 10     6     4  0.6\n1488 10     6     4  0.6\n1489 10     3     7  0.3\n1490 10     6     4  0.6\n1491 10     5     5  0.5\n1492 10     6     4  0.6\n1493 10     4     6  0.4\n1494 10     5     5  0.5\n1495 10     3     7  0.3\n1496 10     7     3  0.7\n1497 10     5     5  0.5\n1498 10     6     4  0.6\n1499 10     5     5  0.5\n1500 10     0    10  0.0\n1501 10     4     6  0.4\n1502 10     3     7  0.3\n1503 10     6     4  0.6\n1504 10     4     6  0.4\n1505 10     5     5  0.5\n1506 10     6     4  0.6\n1507 10     3     7  0.3\n1508 10     4     6  0.4\n1509 10     4     6  0.4\n1510 10     6     4  0.6\n1511 10     5     5  0.5\n1512 10     4     6  0.4\n1513 10     4     6  0.4\n1514 10     3     7  0.3\n1515 10     2     8  0.2\n1516 10     1     9  0.1\n1517 10     3     7  0.3\n1518 10     8     2  0.8\n1519 10     4     6  0.4\n1520 10     6     4  0.6\n1521 10     7     3  0.7\n1522 10     5     5  0.5\n1523 10     2     8  0.2\n1524 10     4     6  0.4\n1525 10     5     5  0.5\n1526 10     6     4  0.6\n1527 10     5     5  0.5\n1528 10     6     4  0.6\n1529 10     6     4  0.6\n1530 10     7     3  0.7\n1531 10     7     3  0.7\n1532 10     3     7  0.3\n1533 10     7     3  0.7\n1534 10     5     5  0.5\n1535 10     3     7  0.3\n1536 10     5     5  0.5\n1537 10     3     7  0.3\n1538 10     2     8  0.2\n1539 10     4     6  0.4\n1540 10     3     7  0.3\n1541 10     4     6  0.4\n1542 10     3     7  0.3\n1543 10     6     4  0.6\n1544 10     3     7  0.3\n1545 10     5     5  0.5\n1546 10     8     2  0.8\n1547 10     6     4  0.6\n1548 10     5     5  0.5\n1549 10     5     5  0.5\n1550 10     3     7  0.3\n1551 10     6     4  0.6\n1552 10     6     4  0.6\n1553 10     2     8  0.2\n1554 10     5     5  0.5\n1555 10     5     5  0.5\n1556 10     2     8  0.2\n1557 10     7     3  0.7\n1558 10     6     4  0.6\n1559 10     4     6  0.4\n1560 10     7     3  0.7\n1561 10     7     3  0.7\n1562 10     4     6  0.4\n1563 10     4     6  0.4\n1564 10     6     4  0.6\n1565 10     4     6  0.4\n1566 10     6     4  0.6\n1567 10     4     6  0.4\n1568 10     6     4  0.6\n1569 10     6     4  0.6\n1570 10     5     5  0.5\n1571 10     6     4  0.6\n1572 10     6     4  0.6\n1573 10     4     6  0.4\n1574 10     4     6  0.4\n1575 10     6     4  0.6\n1576 10     9     1  0.9\n1577 10     4     6  0.4\n1578 10     6     4  0.6\n1579 10     4     6  0.4\n1580 10     4     6  0.4\n1581 10     5     5  0.5\n1582 10     2     8  0.2\n1583 10     6     4  0.6\n1584 10     4     6  0.4\n1585 10     8     2  0.8\n1586 10     8     2  0.8\n1587 10     4     6  0.4\n1588 10     3     7  0.3\n1589 10     6     4  0.6\n1590 10     4     6  0.4\n1591 10     4     6  0.4\n1592 10     6     4  0.6\n1593 10     4     6  0.4\n1594 10     3     7  0.3\n1595 10     4     6  0.4\n1596 10     7     3  0.7\n1597 10     5     5  0.5\n1598 10     4     6  0.4\n1599 10     8     2  0.8\n1600 10     6     4  0.6\n1601 10     7     3  0.7\n1602 10     5     5  0.5\n1603 10     5     5  0.5\n1604 10     3     7  0.3\n1605 10     5     5  0.5\n1606 10     5     5  0.5\n1607 10     4     6  0.4\n1608 10     7     3  0.7\n1609 10     4     6  0.4\n1610 10     5     5  0.5\n1611 10     6     4  0.6\n1612 10     4     6  0.4\n1613 10     6     4  0.6\n1614 10     3     7  0.3\n1615 10     7     3  0.7\n1616 10     6     4  0.6\n1617 10     5     5  0.5\n1618 10     3     7  0.3\n1619 10     6     4  0.6\n1620 10     9     1  0.9\n1621 10     6     4  0.6\n1622 10     7     3  0.7\n1623 10     8     2  0.8\n1624 10     5     5  0.5\n1625 10     4     6  0.4\n1626 10     3     7  0.3\n1627 10     3     7  0.3\n1628 10     4     6  0.4\n1629 10     8     2  0.8\n1630 10     6     4  0.6\n1631 10     5     5  0.5\n1632 10     5     5  0.5\n1633 10     5     5  0.5\n1634 10     5     5  0.5\n1635 10     4     6  0.4\n1636 10     8     2  0.8\n1637 10     6     4  0.6\n1638 10     4     6  0.4\n1639 10     6     4  0.6\n1640 10     7     3  0.7\n1641 10     4     6  0.4\n1642 10     7     3  0.7\n1643 10     5     5  0.5\n1644 10     6     4  0.6\n1645 10     3     7  0.3\n1646 10     6     4  0.6\n1647 10     4     6  0.4\n1648 10     3     7  0.3\n1649 10     4     6  0.4\n1650 10     4     6  0.4\n1651 10     6     4  0.6\n1652 10     3     7  0.3\n1653 10     6     4  0.6\n1654 10     8     2  0.8\n1655 10     4     6  0.4\n1656 10     4     6  0.4\n1657 10     5     5  0.5\n1658 10     6     4  0.6\n1659 10     3     7  0.3\n1660 10     5     5  0.5\n1661 10     5     5  0.5\n1662 10     5     5  0.5\n1663 10     3     7  0.3\n1664 10     8     2  0.8\n1665 10     5     5  0.5\n1666 10     6     4  0.6\n1667 10     5     5  0.5\n1668 10     4     6  0.4\n1669 10     7     3  0.7\n1670 10     4     6  0.4\n1671 10     5     5  0.5\n1672 10     3     7  0.3\n1673 10     3     7  0.3\n1674 10     3     7  0.3\n1675 10     6     4  0.6\n1676 10     3     7  0.3\n1677 10     6     4  0.6\n1678 10     4     6  0.4\n1679 10     8     2  0.8\n1680 10     4     6  0.4\n1681 10     6     4  0.6\n1682 10     4     6  0.4\n1683 10     6     4  0.6\n1684 10     6     4  0.6\n1685 10     4     6  0.4\n1686 10     6     4  0.6\n1687 10     7     3  0.7\n1688 10     6     4  0.6\n1689 10     5     5  0.5\n1690 10     5     5  0.5\n1691 10     6     4  0.6\n1692 10     6     4  0.6\n1693 10     7     3  0.7\n1694 10     5     5  0.5\n1695 10     6     4  0.6\n1696 10     5     5  0.5\n1697 10     5     5  0.5\n1698 10     5     5  0.5\n1699 10     3     7  0.3\n1700 10     7     3  0.7\n1701 10     6     4  0.6\n1702 10     5     5  0.5\n1703 10     4     6  0.4\n1704 10     5     5  0.5\n1705 10     8     2  0.8\n1706 10     3     7  0.3\n1707 10     7     3  0.7\n1708 10     5     5  0.5\n1709 10     4     6  0.4\n1710 10     4     6  0.4\n1711 10     6     4  0.6\n1712 10     6     4  0.6\n1713 10     6     4  0.6\n1714 10     6     4  0.6\n1715 10     5     5  0.5\n1716 10     7     3  0.7\n1717 10     3     7  0.3\n1718 10     7     3  0.7\n1719 10     4     6  0.4\n1720 10     6     4  0.6\n1721 10     5     5  0.5\n1722 10     1     9  0.1\n1723 10     6     4  0.6\n1724 10     1     9  0.1\n1725 10     5     5  0.5\n1726 10     4     6  0.4\n1727 10     5     5  0.5\n1728 10     4     6  0.4\n1729 10     5     5  0.5\n1730 10     6     4  0.6\n1731 10     6     4  0.6\n1732 10     5     5  0.5\n1733 10     5     5  0.5\n1734 10     4     6  0.4\n1735 10     5     5  0.5\n1736 10     5     5  0.5\n1737 10     3     7  0.3\n1738 10     5     5  0.5\n1739 10     5     5  0.5\n1740 10     7     3  0.7\n1741 10     4     6  0.4\n1742 10     4     6  0.4\n1743 10     5     5  0.5\n1744 10     4     6  0.4\n1745 10     2     8  0.2\n1746 10     8     2  0.8\n1747 10     5     5  0.5\n1748 10     4     6  0.4\n1749 10     6     4  0.6\n1750 10     6     4  0.6\n1751 10     7     3  0.7\n1752 10     5     5  0.5\n1753 10     4     6  0.4\n1754 10     4     6  0.4\n1755 10     5     5  0.5\n1756 10     2     8  0.2\n1757 10     7     3  0.7\n1758 10     2     8  0.2\n1759 10     4     6  0.4\n1760 10     5     5  0.5\n1761 10     6     4  0.6\n1762 10     5     5  0.5\n1763 10     3     7  0.3\n1764 10     5     5  0.5\n1765 10     8     2  0.8\n1766 10     5     5  0.5\n1767 10     6     4  0.6\n1768 10     4     6  0.4\n1769 10     7     3  0.7\n1770 10     6     4  0.6\n1771 10     5     5  0.5\n1772 10     4     6  0.4\n1773 10     5     5  0.5\n1774 10     6     4  0.6\n1775 10     6     4  0.6\n1776 10     3     7  0.3\n1777 10     3     7  0.3\n1778 10     4     6  0.4\n1779 10     3     7  0.3\n1780 10     5     5  0.5\n1781 10     6     4  0.6\n1782 10     5     5  0.5\n1783 10     5     5  0.5\n1784 10     4     6  0.4\n1785 10     3     7  0.3\n1786 10     6     4  0.6\n1787 10     5     5  0.5\n1788 10     7     3  0.7\n1789 10     2     8  0.2\n1790 10     4     6  0.4\n1791 10     5     5  0.5\n1792 10     5     5  0.5\n1793 10     5     5  0.5\n1794 10     6     4  0.6\n1795 10     7     3  0.7\n1796 10     5     5  0.5\n1797 10     6     4  0.6\n1798 10     4     6  0.4\n1799 10     5     5  0.5\n1800 10     6     4  0.6\n1801 10     6     4  0.6\n1802 10     6     4  0.6\n1803 10     2     8  0.2\n1804 10     4     6  0.4\n1805 10     5     5  0.5\n1806 10     5     5  0.5\n1807 10     7     3  0.7\n1808 10     2     8  0.2\n1809 10     5     5  0.5\n1810 10     6     4  0.6\n1811 10     5     5  0.5\n1812 10     4     6  0.4\n1813 10     5     5  0.5\n1814 10     4     6  0.4\n1815 10     4     6  0.4\n1816 10     7     3  0.7\n1817 10     7     3  0.7\n1818 10     8     2  0.8\n1819 10     3     7  0.3\n1820 10     5     5  0.5\n1821 10     4     6  0.4\n1822 10     6     4  0.6\n1823 10     6     4  0.6\n1824 10     6     4  0.6\n1825 10     5     5  0.5\n1826 10     5     5  0.5\n1827 10     5     5  0.5\n1828 10     5     5  0.5\n1829 10     7     3  0.7\n1830 10     4     6  0.4\n1831 10     4     6  0.4\n1832 10     6     4  0.6\n1833 10     4     6  0.4\n1834 10     3     7  0.3\n1835 10     5     5  0.5\n1836 10     7     3  0.7\n1837 10     6     4  0.6\n1838 10     7     3  0.7\n1839 10     4     6  0.4\n1840 10     6     4  0.6\n1841 10     6     4  0.6\n1842 10     8     2  0.8\n1843 10     4     6  0.4\n1844 10     6     4  0.6\n1845 10     3     7  0.3\n1846 10     2     8  0.2\n1847 10     4     6  0.4\n1848 10     5     5  0.5\n1849 10     3     7  0.3\n1850 10     6     4  0.6\n1851 10     5     5  0.5\n1852 10     9     1  0.9\n1853 10     1     9  0.1\n1854 10     6     4  0.6\n1855 10     7     3  0.7\n1856 10     5     5  0.5\n1857 10     9     1  0.9\n1858 10     8     2  0.8\n1859 10     6     4  0.6\n1860 10     5     5  0.5\n1861 10     4     6  0.4\n1862 10     5     5  0.5\n1863 10     4     6  0.4\n1864 10     8     2  0.8\n1865 10     4     6  0.4\n1866 10     6     4  0.6\n1867 10     3     7  0.3\n1868 10     7     3  0.7\n1869 10     5     5  0.5\n1870 10     7     3  0.7\n1871 10     7     3  0.7\n1872 10     9     1  0.9\n1873 10     4     6  0.4\n1874 10     7     3  0.7\n1875 10     6     4  0.6\n1876 10     7     3  0.7\n1877 10     7     3  0.7\n1878 10     5     5  0.5\n1879 10     6     4  0.6\n1880 10     6     4  0.6\n1881 10     4     6  0.4\n1882 10     5     5  0.5\n1883 10     5     5  0.5\n1884 10     4     6  0.4\n1885 10     5     5  0.5\n1886 10     6     4  0.6\n1887 10     5     5  0.5\n1888 10     3     7  0.3\n1889 10     6     4  0.6\n1890 10     2     8  0.2\n1891 10     4     6  0.4\n1892 10     6     4  0.6\n1893 10     4     6  0.4\n1894 10     6     4  0.6\n1895 10     4     6  0.4\n1896 10     4     6  0.4\n1897 10     4     6  0.4\n1898 10     6     4  0.6\n1899 10     5     5  0.5\n1900 10     7     3  0.7\n1901 10     4     6  0.4\n1902 10     3     7  0.3\n1903 10     6     4  0.6\n1904 10     6     4  0.6\n1905 10     2     8  0.2\n1906 10     5     5  0.5\n1907 10     3     7  0.3\n1908 10     4     6  0.4\n1909 10     5     5  0.5\n1910 10     4     6  0.4\n1911 10     5     5  0.5\n1912 10     6     4  0.6\n1913 10     8     2  0.8\n1914 10     7     3  0.7\n1915 10     3     7  0.3\n1916 10     4     6  0.4\n1917 10     4     6  0.4\n1918 10     4     6  0.4\n1919 10     4     6  0.4\n1920 10     4     6  0.4\n1921 10     4     6  0.4\n1922 10     3     7  0.3\n1923 10     5     5  0.5\n1924 10     4     6  0.4\n1925 10     8     2  0.8\n1926 10     5     5  0.5\n1927 10     5     5  0.5\n1928 10     3     7  0.3\n1929 10     6     4  0.6\n1930 10     7     3  0.7\n1931 10     4     6  0.4\n1932 10     5     5  0.5\n1933 10     4     6  0.4\n1934 10     3     7  0.3\n1935 10     6     4  0.6\n1936 10     7     3  0.7\n1937 10     5     5  0.5\n1938 10     5     5  0.5\n1939 10     5     5  0.5\n1940 10     5     5  0.5\n1941 10     3     7  0.3\n1942 10     4     6  0.4\n1943 10     3     7  0.3\n1944 10     7     3  0.7\n1945 10     4     6  0.4\n1946 10     3     7  0.3\n1947 10     4     6  0.4\n1948 10     5     5  0.5\n1949 10     6     4  0.6\n1950 10     6     4  0.6\n1951 10     4     6  0.4\n1952 10     9     1  0.9\n1953 10     5     5  0.5\n1954 10     5     5  0.5\n1955 10     5     5  0.5\n1956 10     4     6  0.4\n1957 10     3     7  0.3\n1958 10     7     3  0.7\n1959 10     6     4  0.6\n1960 10     3     7  0.3\n1961 10     4     6  0.4\n1962 10     7     3  0.7\n1963 10     7     3  0.7\n1964 10     6     4  0.6\n1965 10     6     4  0.6\n1966 10     4     6  0.4\n1967 10     7     3  0.7\n1968 10     6     4  0.6\n1969 10     5     5  0.5\n1970 10     4     6  0.4\n1971 10     4     6  0.4\n1972 10     1     9  0.1\n1973 10     7     3  0.7\n1974 10     3     7  0.3\n1975 10     4     6  0.4\n1976 10     5     5  0.5\n1977 10     4     6  0.4\n1978 10     4     6  0.4\n1979 10     3     7  0.3\n1980 10     3     7  0.3\n1981 10     4     6  0.4\n1982 10     4     6  0.4\n1983 10     5     5  0.5\n1984 10     4     6  0.4\n1985 10     2     8  0.2\n1986 10     4     6  0.4\n1987 10     4     6  0.4\n1988 10     4     6  0.4\n1989 10     5     5  0.5\n1990 10     7     3  0.7\n1991 10     3     7  0.3\n1992 10     4     6  0.4\n1993 10     6     4  0.6\n1994 10     4     6  0.4\n1995 10     7     3  0.7\n1996 10     4     6  0.4\n1997 10     6     4  0.6\n1998 10     6     4  0.6\n1999 10     3     7  0.3\n2000 10     8     2  0.8\n\n\nThis is the same idea as before, but now there are 2000 rows in the data frame instead of 20.\n\nmean(coin_flips_2000_10$heads)\n\n[1] 5.0245\n\n\n\nggplot(coin_flips_2000_10, aes(x = heads)) +\n    geom_histogram(binwidth = 0.5) +\n    scale_x_continuous(limits = c(-1, 11), breaks = seq(0, 10, 1))\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\nThis is helpful. In contrast with the set of simulations with twenty people, the last histogram gives us something closer to what we expect. The mode is at five heads, and every possible number of heads is represented, with decreasing counts as one moves away from five. With 2000 people flipping coins, all possible outcomes—including rare ones—are better represented.\nHere is the the same histogram, but this time with the proportion of heads instead of the count of heads:\n\nggplot(coin_flips_2000_10, aes(x = prop)) +\n    geom_histogram(binwidth = 0.05) +\n    scale_x_continuous(limits = c(-0.1, 1.1), breaks = seq(0, 1, 0.1))\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\n\nExercise 3\nDo you think the shape of the distribution would be appreciably different if we used 20,000 or even 200,000 people? Why or why not? (Normally, I would encourage you to test your theory by trying it in R. However, it takes a long time to simulate that many flips and I don’t want you to tie up resources and memory. Think through this in your head.)\n\nPlease write up your answer here.\n\n\nFrom now on, we will insist on using at least a thousand simulations—if not more—to make sure that we represent the full range of possible outcomes.1"
  },
  {
    "objectID": "08-intro_to_randomization_1-web.html#randomization1-more",
    "href": "08-intro_to_randomization_1-web.html#randomization1-more",
    "title": "8  Introduction to randomization, Part 1",
    "section": "8.6 More flips",
    "text": "8.6 More flips\nNow let’s increase the number of coin flips each person performs. We’ll still use 2000 simulations (imagine 2000 people all flipping coins), but this time, each person will flip the coin 1000 times instead of only 10 times. The first code chunk below accounts for a substantial amount of the time it takes to run the code in this document.\n\nset.seed(1234)\ncoin_flips_2000_1000 &lt;- do(2000) * rflip(1000, prob = 0.5)\ncoin_flips_2000_1000\n\n        n heads tails  prop\n1    1000   485   515 0.485\n2    1000   515   485 0.515\n3    1000   481   519 0.481\n4    1000   508   492 0.508\n5    1000   499   501 0.499\n6    1000   516   484 0.516\n7    1000   497   503 0.497\n8    1000   497   503 0.497\n9    1000   494   506 0.494\n10   1000   528   472 0.528\n11   1000   495   505 0.495\n12   1000   483   517 0.483\n13   1000   520   480 0.520\n14   1000   528   472 0.528\n15   1000   478   522 0.478\n16   1000   516   484 0.516\n17   1000   493   507 0.493\n18   1000   524   476 0.524\n19   1000   473   527 0.473\n20   1000   516   484 0.516\n21   1000   529   471 0.529\n22   1000   516   484 0.516\n23   1000   535   465 0.535\n24   1000   491   509 0.491\n25   1000   500   500 0.500\n26   1000   497   503 0.497\n27   1000   507   493 0.507\n28   1000   515   485 0.515\n29   1000   493   507 0.493\n30   1000   482   518 0.482\n31   1000   485   515 0.485\n32   1000   493   507 0.493\n33   1000   498   502 0.498\n34   1000   490   510 0.490\n35   1000   485   515 0.485\n36   1000   495   505 0.495\n37   1000   488   512 0.488\n38   1000   496   504 0.496\n39   1000   491   509 0.491\n40   1000   488   512 0.488\n41   1000   488   512 0.488\n42   1000   524   476 0.524\n43   1000   500   500 0.500\n44   1000   516   484 0.516\n45   1000   514   486 0.514\n46   1000   479   521 0.479\n47   1000   488   512 0.488\n48   1000   469   531 0.469\n49   1000   515   485 0.515\n50   1000   520   480 0.520\n51   1000   486   514 0.486\n52   1000   507   493 0.507\n53   1000   509   491 0.509\n54   1000   467   533 0.467\n55   1000   467   533 0.467\n56   1000   504   496 0.504\n57   1000   483   517 0.483\n58   1000   513   487 0.513\n59   1000   518   482 0.518\n60   1000   493   507 0.493\n61   1000   516   484 0.516\n62   1000   507   493 0.507\n63   1000   509   491 0.509\n64   1000   508   492 0.508\n65   1000   511   489 0.511\n66   1000   491   509 0.491\n67   1000   524   476 0.524\n68   1000   515   485 0.515\n69   1000   524   476 0.524\n70   1000   510   490 0.510\n71   1000   482   518 0.482\n72   1000   498   502 0.498\n73   1000   507   493 0.507\n74   1000   490   510 0.490\n75   1000   501   499 0.501\n76   1000   502   498 0.502\n77   1000   520   480 0.520\n78   1000   528   472 0.528\n79   1000   504   496 0.504\n80   1000   501   499 0.501\n81   1000   507   493 0.507\n82   1000   486   514 0.486\n83   1000   500   500 0.500\n84   1000   505   495 0.505\n85   1000   494   506 0.494\n86   1000   505   495 0.505\n87   1000   512   488 0.512\n88   1000   521   479 0.521\n89   1000   497   503 0.497\n90   1000   501   499 0.501\n91   1000   489   511 0.489\n92   1000   497   503 0.497\n93   1000   500   500 0.500\n94   1000   470   530 0.470\n95   1000   511   489 0.511\n96   1000   504   496 0.504\n97   1000   460   540 0.460\n98   1000   493   507 0.493\n99   1000   477   523 0.477\n100  1000   489   511 0.489\n101  1000   511   489 0.511\n102  1000   519   481 0.519\n103  1000   491   509 0.491\n104  1000   464   536 0.464\n105  1000   493   507 0.493\n106  1000   497   503 0.497\n107  1000   515   485 0.515\n108  1000   491   509 0.491\n109  1000   472   528 0.472\n110  1000   505   495 0.505\n111  1000   503   497 0.503\n112  1000   489   511 0.489\n113  1000   530   470 0.530\n114  1000   510   490 0.510\n115  1000   521   479 0.521\n116  1000   488   512 0.488\n117  1000   453   547 0.453\n118  1000   489   511 0.489\n119  1000   486   514 0.486\n120  1000   481   519 0.481\n121  1000   495   505 0.495\n122  1000   484   516 0.484\n123  1000   534   466 0.534\n124  1000   500   500 0.500\n125  1000   497   503 0.497\n126  1000   524   476 0.524\n127  1000   494   506 0.494\n128  1000   505   495 0.505\n129  1000   479   521 0.479\n130  1000   493   507 0.493\n131  1000   488   512 0.488\n132  1000   482   518 0.482\n133  1000   519   481 0.519\n134  1000   497   503 0.497\n135  1000   531   469 0.531\n136  1000   481   519 0.481\n137  1000   510   490 0.510\n138  1000   500   500 0.500\n139  1000   476   524 0.476\n140  1000   493   507 0.493\n141  1000   490   510 0.490\n142  1000   469   531 0.469\n143  1000   484   516 0.484\n144  1000   534   466 0.534\n145  1000   491   509 0.491\n146  1000   510   490 0.510\n147  1000   507   493 0.507\n148  1000   495   505 0.495\n149  1000   526   474 0.526\n150  1000   497   503 0.497\n151  1000   510   490 0.510\n152  1000   496   504 0.496\n153  1000   470   530 0.470\n154  1000   502   498 0.502\n155  1000   485   515 0.485\n156  1000   516   484 0.516\n157  1000   513   487 0.513\n158  1000   510   490 0.510\n159  1000   484   516 0.484\n160  1000   517   483 0.517\n161  1000   512   488 0.512\n162  1000   492   508 0.492\n163  1000   513   487 0.513\n164  1000   478   522 0.478\n165  1000   503   497 0.503\n166  1000   485   515 0.485\n167  1000   489   511 0.489\n168  1000   477   523 0.477\n169  1000   508   492 0.508\n170  1000   530   470 0.530\n171  1000   476   524 0.476\n172  1000   510   490 0.510\n173  1000   475   525 0.475\n174  1000   479   521 0.479\n175  1000   497   503 0.497\n176  1000   505   495 0.505\n177  1000   506   494 0.506\n178  1000   514   486 0.514\n179  1000   511   489 0.511\n180  1000   536   464 0.536\n181  1000   487   513 0.487\n182  1000   489   511 0.489\n183  1000   487   513 0.487\n184  1000   503   497 0.503\n185  1000   493   507 0.493\n186  1000   530   470 0.530\n187  1000   496   504 0.496\n188  1000   495   505 0.495\n189  1000   481   519 0.481\n190  1000   503   497 0.503\n191  1000   482   518 0.482\n192  1000   504   496 0.504\n193  1000   513   487 0.513\n194  1000   523   477 0.523\n195  1000   512   488 0.512\n196  1000   512   488 0.512\n197  1000   508   492 0.508\n198  1000   528   472 0.528\n199  1000   498   502 0.498\n200  1000   529   471 0.529\n201  1000   516   484 0.516\n202  1000   490   510 0.490\n203  1000   498   502 0.498\n204  1000   499   501 0.499\n205  1000   502   498 0.502\n206  1000   498   502 0.498\n207  1000   503   497 0.503\n208  1000   521   479 0.521\n209  1000   509   491 0.509\n210  1000   509   491 0.509\n211  1000   492   508 0.492\n212  1000   496   504 0.496\n213  1000   516   484 0.516\n214  1000   494   506 0.494\n215  1000   487   513 0.487\n216  1000   509   491 0.509\n217  1000   487   513 0.487\n218  1000   490   510 0.490\n219  1000   520   480 0.520\n220  1000   495   505 0.495\n221  1000   500   500 0.500\n222  1000   491   509 0.491\n223  1000   511   489 0.511\n224  1000   475   525 0.475\n225  1000   515   485 0.515\n226  1000   477   523 0.477\n227  1000   501   499 0.501\n228  1000   509   491 0.509\n229  1000   490   510 0.490\n230  1000   498   502 0.498\n231  1000   494   506 0.494\n232  1000   521   479 0.521\n233  1000   477   523 0.477\n234  1000   510   490 0.510\n235  1000   517   483 0.517\n236  1000   506   494 0.506\n237  1000   477   523 0.477\n238  1000   490   510 0.490\n239  1000   524   476 0.524\n240  1000   503   497 0.503\n241  1000   514   486 0.514\n242  1000   506   494 0.506\n243  1000   482   518 0.482\n244  1000   507   493 0.507\n245  1000   504   496 0.504\n246  1000   501   499 0.501\n247  1000   482   518 0.482\n248  1000   480   520 0.480\n249  1000   511   489 0.511\n250  1000   497   503 0.497\n251  1000   471   529 0.471\n252  1000   510   490 0.510\n253  1000   523   477 0.523\n254  1000   485   515 0.485\n255  1000   505   495 0.505\n256  1000   507   493 0.507\n257  1000   473   527 0.473\n258  1000   495   505 0.495\n259  1000   465   535 0.465\n260  1000   501   499 0.501\n261  1000   460   540 0.460\n262  1000   499   501 0.499\n263  1000   524   476 0.524\n264  1000   514   486 0.514\n265  1000   503   497 0.503\n266  1000   469   531 0.469\n267  1000   496   504 0.496\n268  1000   489   511 0.489\n269  1000   507   493 0.507\n270  1000   466   534 0.466\n271  1000   482   518 0.482\n272  1000   520   480 0.520\n273  1000   513   487 0.513\n274  1000   492   508 0.492\n275  1000   486   514 0.486\n276  1000   498   502 0.498\n277  1000   507   493 0.507\n278  1000   494   506 0.494\n279  1000   499   501 0.499\n280  1000   498   502 0.498\n281  1000   459   541 0.459\n282  1000   495   505 0.495\n283  1000   498   502 0.498\n284  1000   495   505 0.495\n285  1000   488   512 0.488\n286  1000   518   482 0.518\n287  1000   502   498 0.502\n288  1000   503   497 0.503\n289  1000   476   524 0.476\n290  1000   495   505 0.495\n291  1000   495   505 0.495\n292  1000   503   497 0.503\n293  1000   482   518 0.482\n294  1000   518   482 0.518\n295  1000   514   486 0.514\n296  1000   520   480 0.520\n297  1000   498   502 0.498\n298  1000   523   477 0.523\n299  1000   516   484 0.516\n300  1000   483   517 0.483\n301  1000   504   496 0.504\n302  1000   505   495 0.505\n303  1000   502   498 0.502\n304  1000   486   514 0.486\n305  1000   540   460 0.540\n306  1000   510   490 0.510\n307  1000   507   493 0.507\n308  1000   482   518 0.482\n309  1000   509   491 0.509\n310  1000   486   514 0.486\n311  1000   474   526 0.474\n312  1000   511   489 0.511\n313  1000   484   516 0.484\n314  1000   499   501 0.499\n315  1000   496   504 0.496\n316  1000   505   495 0.505\n317  1000   487   513 0.487\n318  1000   520   480 0.520\n319  1000   483   517 0.483\n320  1000   515   485 0.515\n321  1000   513   487 0.513\n322  1000   509   491 0.509\n323  1000   520   480 0.520\n324  1000   509   491 0.509\n325  1000   480   520 0.480\n326  1000   524   476 0.524\n327  1000   507   493 0.507\n328  1000   509   491 0.509\n329  1000   493   507 0.493\n330  1000   464   536 0.464\n331  1000   526   474 0.526\n332  1000   513   487 0.513\n333  1000   505   495 0.505\n334  1000   509   491 0.509\n335  1000   500   500 0.500\n336  1000   499   501 0.499\n337  1000   520   480 0.520\n338  1000   491   509 0.491\n339  1000   488   512 0.488\n340  1000   483   517 0.483\n341  1000   508   492 0.508\n342  1000   474   526 0.474\n343  1000   482   518 0.482\n344  1000   485   515 0.485\n345  1000   516   484 0.516\n346  1000   511   489 0.511\n347  1000   490   510 0.490\n348  1000   519   481 0.519\n349  1000   493   507 0.493\n350  1000   508   492 0.508\n351  1000   492   508 0.492\n352  1000   500   500 0.500\n353  1000   503   497 0.503\n354  1000   478   522 0.478\n355  1000   511   489 0.511\n356  1000   495   505 0.495\n357  1000   472   528 0.472\n358  1000   468   532 0.468\n359  1000   504   496 0.504\n360  1000   478   522 0.478\n361  1000   485   515 0.485\n362  1000   503   497 0.503\n363  1000   487   513 0.487\n364  1000   482   518 0.482\n365  1000   485   515 0.485\n366  1000   507   493 0.507\n367  1000   477   523 0.477\n368  1000   504   496 0.504\n369  1000   502   498 0.502\n370  1000   492   508 0.492\n371  1000   485   515 0.485\n372  1000   491   509 0.491\n373  1000   502   498 0.502\n374  1000   483   517 0.483\n375  1000   510   490 0.510\n376  1000   508   492 0.508\n377  1000   500   500 0.500\n378  1000   501   499 0.501\n379  1000   518   482 0.518\n380  1000   528   472 0.528\n381  1000   500   500 0.500\n382  1000   486   514 0.486\n383  1000   487   513 0.487\n384  1000   511   489 0.511\n385  1000   483   517 0.483\n386  1000   485   515 0.485\n387  1000   485   515 0.485\n388  1000   520   480 0.520\n389  1000   486   514 0.486\n390  1000   492   508 0.492\n391  1000   519   481 0.519\n392  1000   478   522 0.478\n393  1000   509   491 0.509\n394  1000   494   506 0.494\n395  1000   482   518 0.482\n396  1000   490   510 0.490\n397  1000   488   512 0.488\n398  1000   538   462 0.538\n399  1000   483   517 0.483\n400  1000   515   485 0.515\n401  1000   489   511 0.489\n402  1000   511   489 0.511\n403  1000   486   514 0.486\n404  1000   501   499 0.501\n405  1000   497   503 0.497\n406  1000   515   485 0.515\n407  1000   514   486 0.514\n408  1000   504   496 0.504\n409  1000   526   474 0.526\n410  1000   481   519 0.481\n411  1000   505   495 0.505\n412  1000   504   496 0.504\n413  1000   511   489 0.511\n414  1000   510   490 0.510\n415  1000   494   506 0.494\n416  1000   515   485 0.515\n417  1000   510   490 0.510\n418  1000   488   512 0.488\n419  1000   490   510 0.490\n420  1000   506   494 0.506\n421  1000   489   511 0.489\n422  1000   514   486 0.514\n423  1000   524   476 0.524\n424  1000   492   508 0.492\n425  1000   502   498 0.502\n426  1000   519   481 0.519\n427  1000   500   500 0.500\n428  1000   516   484 0.516\n429  1000   515   485 0.515\n430  1000   496   504 0.496\n431  1000   479   521 0.479\n432  1000   481   519 0.481\n433  1000   521   479 0.521\n434  1000   485   515 0.485\n435  1000   492   508 0.492\n436  1000   507   493 0.507\n437  1000   507   493 0.507\n438  1000   497   503 0.497\n439  1000   516   484 0.516\n440  1000   491   509 0.491\n441  1000   518   482 0.518\n442  1000   490   510 0.490\n443  1000   502   498 0.502\n444  1000   521   479 0.521\n445  1000   504   496 0.504\n446  1000   495   505 0.495\n447  1000   500   500 0.500\n448  1000   513   487 0.513\n449  1000   497   503 0.497\n450  1000   488   512 0.488\n451  1000   497   503 0.497\n452  1000   532   468 0.532\n453  1000   519   481 0.519\n454  1000   487   513 0.487\n455  1000   500   500 0.500\n456  1000   509   491 0.509\n457  1000   506   494 0.506\n458  1000   508   492 0.508\n459  1000   524   476 0.524\n460  1000   520   480 0.520\n461  1000   509   491 0.509\n462  1000   551   449 0.551\n463  1000   512   488 0.512\n464  1000   497   503 0.497\n465  1000   500   500 0.500\n466  1000   493   507 0.493\n467  1000   508   492 0.508\n468  1000   514   486 0.514\n469  1000   524   476 0.524\n470  1000   508   492 0.508\n471  1000   493   507 0.493\n472  1000   513   487 0.513\n473  1000   515   485 0.515\n474  1000   494   506 0.494\n475  1000   487   513 0.487\n476  1000   464   536 0.464\n477  1000   511   489 0.511\n478  1000   484   516 0.484\n479  1000   527   473 0.527\n480  1000   485   515 0.485\n481  1000   495   505 0.495\n482  1000   515   485 0.515\n483  1000   484   516 0.484\n484  1000   464   536 0.464\n485  1000   541   459 0.541\n486  1000   512   488 0.512\n487  1000   506   494 0.506\n488  1000   500   500 0.500\n489  1000   522   478 0.522\n490  1000   507   493 0.507\n491  1000   521   479 0.521\n492  1000   511   489 0.511\n493  1000   486   514 0.486\n494  1000   501   499 0.501\n495  1000   515   485 0.515\n496  1000   473   527 0.473\n497  1000   499   501 0.499\n498  1000   515   485 0.515\n499  1000   519   481 0.519\n500  1000   488   512 0.488\n501  1000   508   492 0.508\n502  1000   484   516 0.484\n503  1000   484   516 0.484\n504  1000   502   498 0.502\n505  1000   489   511 0.489\n506  1000   495   505 0.495\n507  1000   519   481 0.519\n508  1000   521   479 0.521\n509  1000   506   494 0.506\n510  1000   515   485 0.515\n511  1000   499   501 0.499\n512  1000   514   486 0.514\n513  1000   527   473 0.527\n514  1000   504   496 0.504\n515  1000   469   531 0.469\n516  1000   489   511 0.489\n517  1000   503   497 0.503\n518  1000   531   469 0.531\n519  1000   497   503 0.497\n520  1000   499   501 0.499\n521  1000   483   517 0.483\n522  1000   501   499 0.501\n523  1000   481   519 0.481\n524  1000   516   484 0.516\n525  1000   491   509 0.491\n526  1000   486   514 0.486\n527  1000   492   508 0.492\n528  1000   498   502 0.498\n529  1000   522   478 0.522\n530  1000   487   513 0.487\n531  1000   477   523 0.477\n532  1000   501   499 0.501\n533  1000   490   510 0.490\n534  1000   487   513 0.487\n535  1000   490   510 0.490\n536  1000   484   516 0.484\n537  1000   489   511 0.489\n538  1000   502   498 0.502\n539  1000   490   510 0.490\n540  1000   493   507 0.493\n541  1000   509   491 0.509\n542  1000   523   477 0.523\n543  1000   501   499 0.501\n544  1000   482   518 0.482\n545  1000   498   502 0.498\n546  1000   481   519 0.481\n547  1000   502   498 0.502\n548  1000   499   501 0.499\n549  1000   504   496 0.504\n550  1000   487   513 0.487\n551  1000   481   519 0.481\n552  1000   483   517 0.483\n553  1000   488   512 0.488\n554  1000   491   509 0.491\n555  1000   532   468 0.532\n556  1000   509   491 0.509\n557  1000   495   505 0.495\n558  1000   493   507 0.493\n559  1000   519   481 0.519\n560  1000   475   525 0.475\n561  1000   523   477 0.523\n562  1000   474   526 0.474\n563  1000   461   539 0.461\n564  1000   479   521 0.479\n565  1000   528   472 0.528\n566  1000   502   498 0.502\n567  1000   503   497 0.503\n568  1000   501   499 0.501\n569  1000   487   513 0.487\n570  1000   504   496 0.504\n571  1000   504   496 0.504\n572  1000   509   491 0.509\n573  1000   493   507 0.493\n574  1000   498   502 0.498\n575  1000   488   512 0.488\n576  1000   514   486 0.514\n577  1000   482   518 0.482\n578  1000   483   517 0.483\n579  1000   500   500 0.500\n580  1000   485   515 0.485\n581  1000   503   497 0.503\n582  1000   476   524 0.476\n583  1000   518   482 0.518\n584  1000   502   498 0.502\n585  1000   496   504 0.496\n586  1000   501   499 0.501\n587  1000   501   499 0.501\n588  1000   520   480 0.520\n589  1000   489   511 0.489\n590  1000   499   501 0.499\n591  1000   484   516 0.484\n592  1000   504   496 0.504\n593  1000   510   490 0.510\n594  1000   499   501 0.499\n595  1000   490   510 0.490\n596  1000   503   497 0.503\n597  1000   486   514 0.486\n598  1000   489   511 0.489\n599  1000   505   495 0.505\n600  1000   493   507 0.493\n601  1000   490   510 0.490\n602  1000   482   518 0.482\n603  1000   522   478 0.522\n604  1000   525   475 0.525\n605  1000   503   497 0.503\n606  1000   471   529 0.471\n607  1000   501   499 0.501\n608  1000   504   496 0.504\n609  1000   495   505 0.495\n610  1000   504   496 0.504\n611  1000   494   506 0.494\n612  1000   530   470 0.530\n613  1000   484   516 0.484\n614  1000   489   511 0.489\n615  1000   500   500 0.500\n616  1000   508   492 0.508\n617  1000   492   508 0.492\n618  1000   478   522 0.478\n619  1000   534   466 0.534\n620  1000   489   511 0.489\n621  1000   503   497 0.503\n622  1000   504   496 0.504\n623  1000   484   516 0.484\n624  1000   494   506 0.494\n625  1000   483   517 0.483\n626  1000   509   491 0.509\n627  1000   520   480 0.520\n628  1000   489   511 0.489\n629  1000   501   499 0.501\n630  1000   500   500 0.500\n631  1000   483   517 0.483\n632  1000   514   486 0.514\n633  1000   513   487 0.513\n634  1000   499   501 0.499\n635  1000   492   508 0.492\n636  1000   464   536 0.464\n637  1000   508   492 0.508\n638  1000   506   494 0.506\n639  1000   499   501 0.499\n640  1000   500   500 0.500\n641  1000   512   488 0.512\n642  1000   491   509 0.491\n643  1000   510   490 0.510\n644  1000   487   513 0.487\n645  1000   484   516 0.484\n646  1000   475   525 0.475\n647  1000   501   499 0.501\n648  1000   478   522 0.478\n649  1000   490   510 0.490\n650  1000   493   507 0.493\n651  1000   510   490 0.510\n652  1000   493   507 0.493\n653  1000   519   481 0.519\n654  1000   542   458 0.542\n655  1000   495   505 0.495\n656  1000   527   473 0.527\n657  1000   537   463 0.537\n658  1000   509   491 0.509\n659  1000   461   539 0.461\n660  1000   502   498 0.502\n661  1000   508   492 0.508\n662  1000   496   504 0.496\n663  1000   487   513 0.487\n664  1000   510   490 0.510\n665  1000   488   512 0.488\n666  1000   517   483 0.517\n667  1000   503   497 0.503\n668  1000   456   544 0.456\n669  1000   470   530 0.470\n670  1000   475   525 0.475\n671  1000   510   490 0.510\n672  1000   492   508 0.492\n673  1000   492   508 0.492\n674  1000   506   494 0.506\n675  1000   492   508 0.492\n676  1000   485   515 0.485\n677  1000   500   500 0.500\n678  1000   499   501 0.499\n679  1000   512   488 0.512\n680  1000   490   510 0.490\n681  1000   502   498 0.502\n682  1000   489   511 0.489\n683  1000   499   501 0.499\n684  1000   493   507 0.493\n685  1000   494   506 0.494\n686  1000   515   485 0.515\n687  1000   488   512 0.488\n688  1000   487   513 0.487\n689  1000   504   496 0.504\n690  1000   504   496 0.504\n691  1000   481   519 0.481\n692  1000   487   513 0.487\n693  1000   512   488 0.512\n694  1000   512   488 0.512\n695  1000   474   526 0.474\n696  1000   498   502 0.498\n697  1000   504   496 0.504\n698  1000   510   490 0.510\n699  1000   501   499 0.501\n700  1000   517   483 0.517\n701  1000   507   493 0.507\n702  1000   478   522 0.478\n703  1000   536   464 0.536\n704  1000   484   516 0.484\n705  1000   482   518 0.482\n706  1000   485   515 0.485\n707  1000   510   490 0.510\n708  1000   487   513 0.487\n709  1000   484   516 0.484\n710  1000   504   496 0.504\n711  1000   499   501 0.499\n712  1000   507   493 0.507\n713  1000   490   510 0.490\n714  1000   511   489 0.511\n715  1000   521   479 0.521\n716  1000   507   493 0.507\n717  1000   504   496 0.504\n718  1000   489   511 0.489\n719  1000   487   513 0.487\n720  1000   502   498 0.502\n721  1000   502   498 0.502\n722  1000   491   509 0.491\n723  1000   484   516 0.484\n724  1000   500   500 0.500\n725  1000   512   488 0.512\n726  1000   491   509 0.491\n727  1000   496   504 0.496\n728  1000   485   515 0.485\n729  1000   523   477 0.523\n730  1000   515   485 0.515\n731  1000   503   497 0.503\n732  1000   509   491 0.509\n733  1000   487   513 0.487\n734  1000   508   492 0.508\n735  1000   480   520 0.480\n736  1000   499   501 0.499\n737  1000   495   505 0.495\n738  1000   502   498 0.502\n739  1000   516   484 0.516\n740  1000   493   507 0.493\n741  1000   484   516 0.484\n742  1000   475   525 0.475\n743  1000   483   517 0.483\n744  1000   508   492 0.508\n745  1000   523   477 0.523\n746  1000   502   498 0.502\n747  1000   503   497 0.503\n748  1000   519   481 0.519\n749  1000   483   517 0.483\n750  1000   484   516 0.484\n751  1000   501   499 0.501\n752  1000   494   506 0.494\n753  1000   511   489 0.511\n754  1000   507   493 0.507\n755  1000   493   507 0.493\n756  1000   501   499 0.501\n757  1000   507   493 0.507\n758  1000   507   493 0.507\n759  1000   522   478 0.522\n760  1000   475   525 0.475\n761  1000   501   499 0.501\n762  1000   478   522 0.478\n763  1000   504   496 0.504\n764  1000   506   494 0.506\n765  1000   499   501 0.499\n766  1000   492   508 0.492\n767  1000   503   497 0.503\n768  1000   501   499 0.501\n769  1000   512   488 0.512\n770  1000   491   509 0.491\n771  1000   503   497 0.503\n772  1000   484   516 0.484\n773  1000   525   475 0.525\n774  1000   527   473 0.527\n775  1000   514   486 0.514\n776  1000   507   493 0.507\n777  1000   485   515 0.485\n778  1000   482   518 0.482\n779  1000   502   498 0.502\n780  1000   492   508 0.492\n781  1000   494   506 0.494\n782  1000   501   499 0.501\n783  1000   492   508 0.492\n784  1000   502   498 0.502\n785  1000   516   484 0.516\n786  1000   505   495 0.505\n787  1000   497   503 0.497\n788  1000   492   508 0.492\n789  1000   497   503 0.497\n790  1000   511   489 0.511\n791  1000   499   501 0.499\n792  1000   507   493 0.507\n793  1000   493   507 0.493\n794  1000   491   509 0.491\n795  1000   480   520 0.480\n796  1000   512   488 0.512\n797  1000   520   480 0.520\n798  1000   482   518 0.482\n799  1000   511   489 0.511\n800  1000   517   483 0.517\n801  1000   497   503 0.497\n802  1000   513   487 0.513\n803  1000   502   498 0.502\n804  1000   521   479 0.521\n805  1000   505   495 0.505\n806  1000   479   521 0.479\n807  1000   508   492 0.508\n808  1000   516   484 0.516\n809  1000   500   500 0.500\n810  1000   517   483 0.517\n811  1000   479   521 0.479\n812  1000   493   507 0.493\n813  1000   507   493 0.507\n814  1000   519   481 0.519\n815  1000   496   504 0.496\n816  1000   497   503 0.497\n817  1000   498   502 0.498\n818  1000   500   500 0.500\n819  1000   507   493 0.507\n820  1000   527   473 0.527\n821  1000   463   537 0.463\n822  1000   506   494 0.506\n823  1000   511   489 0.511\n824  1000   523   477 0.523\n825  1000   515   485 0.515\n826  1000   527   473 0.527\n827  1000   519   481 0.519\n828  1000   490   510 0.490\n829  1000   505   495 0.505\n830  1000   511   489 0.511\n831  1000   469   531 0.469\n832  1000   492   508 0.492\n833  1000   497   503 0.497\n834  1000   523   477 0.523\n835  1000   480   520 0.480\n836  1000   493   507 0.493\n837  1000   529   471 0.529\n838  1000   523   477 0.523\n839  1000   499   501 0.499\n840  1000   523   477 0.523\n841  1000   501   499 0.501\n842  1000   505   495 0.505\n843  1000   523   477 0.523\n844  1000   504   496 0.504\n845  1000   492   508 0.492\n846  1000   470   530 0.470\n847  1000   493   507 0.493\n848  1000   511   489 0.511\n849  1000   485   515 0.485\n850  1000   510   490 0.510\n851  1000   498   502 0.498\n852  1000   506   494 0.506\n853  1000   501   499 0.501\n854  1000   519   481 0.519\n855  1000   514   486 0.514\n856  1000   489   511 0.489\n857  1000   513   487 0.513\n858  1000   533   467 0.533\n859  1000   485   515 0.485\n860  1000   499   501 0.499\n861  1000   490   510 0.490\n862  1000   508   492 0.508\n863  1000   482   518 0.482\n864  1000   496   504 0.496\n865  1000   496   504 0.496\n866  1000   525   475 0.525\n867  1000   500   500 0.500\n868  1000   480   520 0.480\n869  1000   493   507 0.493\n870  1000   500   500 0.500\n871  1000   489   511 0.489\n872  1000   503   497 0.503\n873  1000   479   521 0.479\n874  1000   500   500 0.500\n875  1000   499   501 0.499\n876  1000   502   498 0.502\n877  1000   485   515 0.485\n878  1000   515   485 0.515\n879  1000   512   488 0.512\n880  1000   509   491 0.509\n881  1000   499   501 0.499\n882  1000   477   523 0.477\n883  1000   515   485 0.515\n884  1000   490   510 0.490\n885  1000   505   495 0.505\n886  1000   499   501 0.499\n887  1000   495   505 0.495\n888  1000   527   473 0.527\n889  1000   514   486 0.514\n890  1000   513   487 0.513\n891  1000   505   495 0.505\n892  1000   504   496 0.504\n893  1000   482   518 0.482\n894  1000   499   501 0.499\n895  1000   491   509 0.491\n896  1000   474   526 0.474\n897  1000   513   487 0.513\n898  1000   492   508 0.492\n899  1000   504   496 0.504\n900  1000   511   489 0.511\n901  1000   488   512 0.488\n902  1000   534   466 0.534\n903  1000   485   515 0.485\n904  1000   471   529 0.471\n905  1000   511   489 0.511\n906  1000   502   498 0.502\n907  1000   517   483 0.517\n908  1000   520   480 0.520\n909  1000   525   475 0.525\n910  1000   517   483 0.517\n911  1000   495   505 0.495\n912  1000   497   503 0.497\n913  1000   493   507 0.493\n914  1000   496   504 0.496\n915  1000   472   528 0.472\n916  1000   503   497 0.503\n917  1000   512   488 0.512\n918  1000   488   512 0.488\n919  1000   482   518 0.482\n920  1000   496   504 0.496\n921  1000   474   526 0.474\n922  1000   502   498 0.502\n923  1000   490   510 0.490\n924  1000   516   484 0.516\n925  1000   488   512 0.488\n926  1000   489   511 0.489\n927  1000   477   523 0.477\n928  1000   511   489 0.511\n929  1000   486   514 0.486\n930  1000   482   518 0.482\n931  1000   486   514 0.486\n932  1000   506   494 0.506\n933  1000   492   508 0.492\n934  1000   482   518 0.482\n935  1000   509   491 0.509\n936  1000   511   489 0.511\n937  1000   477   523 0.477\n938  1000   507   493 0.507\n939  1000   506   494 0.506\n940  1000   497   503 0.497\n941  1000   506   494 0.506\n942  1000   495   505 0.495\n943  1000   513   487 0.513\n944  1000   511   489 0.511\n945  1000   486   514 0.486\n946  1000   486   514 0.486\n947  1000   511   489 0.511\n948  1000   492   508 0.492\n949  1000   475   525 0.475\n950  1000   490   510 0.490\n951  1000   488   512 0.488\n952  1000   493   507 0.493\n953  1000   485   515 0.485\n954  1000   509   491 0.509\n955  1000   486   514 0.486\n956  1000   504   496 0.504\n957  1000   477   523 0.477\n958  1000   512   488 0.512\n959  1000   501   499 0.501\n960  1000   487   513 0.487\n961  1000   493   507 0.493\n962  1000   492   508 0.492\n963  1000   512   488 0.512\n964  1000   505   495 0.505\n965  1000   494   506 0.494\n966  1000   494   506 0.494\n967  1000   493   507 0.493\n968  1000   502   498 0.502\n969  1000   498   502 0.498\n970  1000   498   502 0.498\n971  1000   517   483 0.517\n972  1000   525   475 0.525\n973  1000   530   470 0.530\n974  1000   503   497 0.503\n975  1000   486   514 0.486\n976  1000   525   475 0.525\n977  1000   503   497 0.503\n978  1000   493   507 0.493\n979  1000   485   515 0.485\n980  1000   485   515 0.485\n981  1000   529   471 0.529\n982  1000   508   492 0.508\n983  1000   495   505 0.495\n984  1000   488   512 0.488\n985  1000   519   481 0.519\n986  1000   515   485 0.515\n987  1000   464   536 0.464\n988  1000   524   476 0.524\n989  1000   522   478 0.522\n990  1000   520   480 0.520\n991  1000   508   492 0.508\n992  1000   512   488 0.512\n993  1000   504   496 0.504\n994  1000   481   519 0.481\n995  1000   450   550 0.450\n996  1000   500   500 0.500\n997  1000   499   501 0.499\n998  1000   487   513 0.487\n999  1000   481   519 0.481\n1000 1000   498   502 0.498\n1001 1000   520   480 0.520\n1002 1000   492   508 0.492\n1003 1000   532   468 0.532\n1004 1000   512   488 0.512\n1005 1000   503   497 0.503\n1006 1000   482   518 0.482\n1007 1000   486   514 0.486\n1008 1000   518   482 0.518\n1009 1000   469   531 0.469\n1010 1000   468   532 0.468\n1011 1000   471   529 0.471\n1012 1000   524   476 0.524\n1013 1000   500   500 0.500\n1014 1000   514   486 0.514\n1015 1000   510   490 0.510\n1016 1000   478   522 0.478\n1017 1000   518   482 0.518\n1018 1000   503   497 0.503\n1019 1000   512   488 0.512\n1020 1000   506   494 0.506\n1021 1000   492   508 0.492\n1022 1000   513   487 0.513\n1023 1000   499   501 0.499\n1024 1000   469   531 0.469\n1025 1000   497   503 0.497\n1026 1000   491   509 0.491\n1027 1000   508   492 0.508\n1028 1000   498   502 0.498\n1029 1000   500   500 0.500\n1030 1000   513   487 0.513\n1031 1000   502   498 0.502\n1032 1000   528   472 0.528\n1033 1000   482   518 0.482\n1034 1000   497   503 0.497\n1035 1000   510   490 0.510\n1036 1000   509   491 0.509\n1037 1000   490   510 0.490\n1038 1000   500   500 0.500\n1039 1000   470   530 0.470\n1040 1000   481   519 0.481\n1041 1000   510   490 0.510\n1042 1000   465   535 0.465\n1043 1000   501   499 0.501\n1044 1000   495   505 0.495\n1045 1000   490   510 0.490\n1046 1000   491   509 0.491\n1047 1000   497   503 0.497\n1048 1000   495   505 0.495\n1049 1000   532   468 0.532\n1050 1000   497   503 0.497\n1051 1000   510   490 0.510\n1052 1000   488   512 0.488\n1053 1000   480   520 0.480\n1054 1000   532   468 0.532\n1055 1000   484   516 0.484\n1056 1000   512   488 0.512\n1057 1000   491   509 0.491\n1058 1000   498   502 0.498\n1059 1000   495   505 0.495\n1060 1000   482   518 0.482\n1061 1000   495   505 0.495\n1062 1000   489   511 0.489\n1063 1000   486   514 0.486\n1064 1000   515   485 0.515\n1065 1000   500   500 0.500\n1066 1000   494   506 0.494\n1067 1000   520   480 0.520\n1068 1000   516   484 0.516\n1069 1000   497   503 0.497\n1070 1000   511   489 0.511\n1071 1000   499   501 0.499\n1072 1000   475   525 0.475\n1073 1000   480   520 0.480\n1074 1000   508   492 0.508\n1075 1000   487   513 0.487\n1076 1000   483   517 0.483\n1077 1000   500   500 0.500\n1078 1000   502   498 0.502\n1079 1000   471   529 0.471\n1080 1000   526   474 0.526\n1081 1000   494   506 0.494\n1082 1000   507   493 0.507\n1083 1000   508   492 0.508\n1084 1000   487   513 0.487\n1085 1000   493   507 0.493\n1086 1000   504   496 0.504\n1087 1000   514   486 0.514\n1088 1000   512   488 0.512\n1089 1000   499   501 0.499\n1090 1000   531   469 0.531\n1091 1000   485   515 0.485\n1092 1000   515   485 0.515\n1093 1000   475   525 0.475\n1094 1000   473   527 0.473\n1095 1000   487   513 0.487\n1096 1000   481   519 0.481\n1097 1000   486   514 0.486\n1098 1000   466   534 0.466\n1099 1000   475   525 0.475\n1100 1000   513   487 0.513\n1101 1000   497   503 0.497\n1102 1000   523   477 0.523\n1103 1000   491   509 0.491\n1104 1000   521   479 0.521\n1105 1000   489   511 0.489\n1106 1000   512   488 0.512\n1107 1000   496   504 0.496\n1108 1000   517   483 0.517\n1109 1000   533   467 0.533\n1110 1000   527   473 0.527\n1111 1000   533   467 0.533\n1112 1000   497   503 0.497\n1113 1000   490   510 0.490\n1114 1000   481   519 0.481\n1115 1000   491   509 0.491\n1116 1000   489   511 0.489\n1117 1000   472   528 0.472\n1118 1000   511   489 0.511\n1119 1000   494   506 0.494\n1120 1000   545   455 0.545\n1121 1000   498   502 0.498\n1122 1000   490   510 0.490\n1123 1000   516   484 0.516\n1124 1000   475   525 0.475\n1125 1000   494   506 0.494\n1126 1000   537   463 0.537\n1127 1000   481   519 0.481\n1128 1000   495   505 0.495\n1129 1000   488   512 0.488\n1130 1000   490   510 0.490\n1131 1000   486   514 0.486\n1132 1000   527   473 0.527\n1133 1000   501   499 0.501\n1134 1000   505   495 0.505\n1135 1000   502   498 0.502\n1136 1000   494   506 0.494\n1137 1000   495   505 0.495\n1138 1000   517   483 0.517\n1139 1000   480   520 0.480\n1140 1000   477   523 0.477\n1141 1000   505   495 0.505\n1142 1000   516   484 0.516\n1143 1000   526   474 0.526\n1144 1000   518   482 0.518\n1145 1000   495   505 0.495\n1146 1000   511   489 0.511\n1147 1000   493   507 0.493\n1148 1000   506   494 0.506\n1149 1000   498   502 0.498\n1150 1000   504   496 0.504\n1151 1000   509   491 0.509\n1152 1000   487   513 0.487\n1153 1000   504   496 0.504\n1154 1000   496   504 0.496\n1155 1000   512   488 0.512\n1156 1000   477   523 0.477\n1157 1000   514   486 0.514\n1158 1000   511   489 0.511\n1159 1000   475   525 0.475\n1160 1000   464   536 0.464\n1161 1000   448   552 0.448\n1162 1000   526   474 0.526\n1163 1000   538   462 0.538\n1164 1000   499   501 0.499\n1165 1000   487   513 0.487\n1166 1000   509   491 0.509\n1167 1000   501   499 0.501\n1168 1000   481   519 0.481\n1169 1000   509   491 0.509\n1170 1000   486   514 0.486\n1171 1000   487   513 0.487\n1172 1000   491   509 0.491\n1173 1000   489   511 0.489\n1174 1000   475   525 0.475\n1175 1000   474   526 0.474\n1176 1000   473   527 0.473\n1177 1000   513   487 0.513\n1178 1000   517   483 0.517\n1179 1000   497   503 0.497\n1180 1000   469   531 0.469\n1181 1000   520   480 0.520\n1182 1000   457   543 0.457\n1183 1000   532   468 0.532\n1184 1000   500   500 0.500\n1185 1000   514   486 0.514\n1186 1000   522   478 0.522\n1187 1000   517   483 0.517\n1188 1000   518   482 0.518\n1189 1000   503   497 0.503\n1190 1000   506   494 0.506\n1191 1000   504   496 0.504\n1192 1000   509   491 0.509\n1193 1000   506   494 0.506\n1194 1000   511   489 0.511\n1195 1000   496   504 0.496\n1196 1000   513   487 0.513\n1197 1000   505   495 0.505\n1198 1000   512   488 0.512\n1199 1000   495   505 0.495\n1200 1000   512   488 0.512\n1201 1000   495   505 0.495\n1202 1000   527   473 0.527\n1203 1000   495   505 0.495\n1204 1000   513   487 0.513\n1205 1000   515   485 0.515\n1206 1000   488   512 0.488\n1207 1000   495   505 0.495\n1208 1000   494   506 0.494\n1209 1000   505   495 0.505\n1210 1000   500   500 0.500\n1211 1000   483   517 0.483\n1212 1000   505   495 0.505\n1213 1000   523   477 0.523\n1214 1000   508   492 0.508\n1215 1000   498   502 0.498\n1216 1000   499   501 0.499\n1217 1000   489   511 0.489\n1218 1000   505   495 0.505\n1219 1000   509   491 0.509\n1220 1000   501   499 0.501\n1221 1000   496   504 0.496\n1222 1000   496   504 0.496\n1223 1000   504   496 0.504\n1224 1000   491   509 0.491\n1225 1000   500   500 0.500\n1226 1000   523   477 0.523\n1227 1000   499   501 0.499\n1228 1000   489   511 0.489\n1229 1000   486   514 0.486\n1230 1000   515   485 0.515\n1231 1000   494   506 0.494\n1232 1000   496   504 0.496\n1233 1000   496   504 0.496\n1234 1000   486   514 0.486\n1235 1000   533   467 0.533\n1236 1000   487   513 0.487\n1237 1000   485   515 0.485\n1238 1000   503   497 0.503\n1239 1000   508   492 0.508\n1240 1000   510   490 0.510\n1241 1000   496   504 0.496\n1242 1000   497   503 0.497\n1243 1000   504   496 0.504\n1244 1000   470   530 0.470\n1245 1000   512   488 0.512\n1246 1000   526   474 0.526\n1247 1000   487   513 0.487\n1248 1000   508   492 0.508\n1249 1000   505   495 0.505\n1250 1000   519   481 0.519\n1251 1000   490   510 0.490\n1252 1000   475   525 0.475\n1253 1000   479   521 0.479\n1254 1000   509   491 0.509\n1255 1000   500   500 0.500\n1256 1000   479   521 0.479\n1257 1000   529   471 0.529\n1258 1000   518   482 0.518\n1259 1000   510   490 0.510\n1260 1000   482   518 0.482\n1261 1000   498   502 0.498\n1262 1000   478   522 0.478\n1263 1000   498   502 0.498\n1264 1000   521   479 0.521\n1265 1000   501   499 0.501\n1266 1000   489   511 0.489\n1267 1000   502   498 0.502\n1268 1000   509   491 0.509\n1269 1000   502   498 0.502\n1270 1000   455   545 0.455\n1271 1000   486   514 0.486\n1272 1000   524   476 0.524\n1273 1000   510   490 0.510\n1274 1000   492   508 0.492\n1275 1000   484   516 0.484\n1276 1000   480   520 0.480\n1277 1000   520   480 0.520\n1278 1000   486   514 0.486\n1279 1000   506   494 0.506\n1280 1000   492   508 0.492\n1281 1000   512   488 0.512\n1282 1000   522   478 0.522\n1283 1000   525   475 0.525\n1284 1000   494   506 0.494\n1285 1000   500   500 0.500\n1286 1000   499   501 0.499\n1287 1000   522   478 0.522\n1288 1000   494   506 0.494\n1289 1000   525   475 0.525\n1290 1000   506   494 0.506\n1291 1000   496   504 0.496\n1292 1000   524   476 0.524\n1293 1000   475   525 0.475\n1294 1000   465   535 0.465\n1295 1000   495   505 0.495\n1296 1000   517   483 0.517\n1297 1000   502   498 0.502\n1298 1000   494   506 0.494\n1299 1000   518   482 0.518\n1300 1000   479   521 0.479\n1301 1000   513   487 0.513\n1302 1000   522   478 0.522\n1303 1000   494   506 0.494\n1304 1000   499   501 0.499\n1305 1000   493   507 0.493\n1306 1000   535   465 0.535\n1307 1000   495   505 0.495\n1308 1000   507   493 0.507\n1309 1000   509   491 0.509\n1310 1000   500   500 0.500\n1311 1000   480   520 0.480\n1312 1000   524   476 0.524\n1313 1000   489   511 0.489\n1314 1000   504   496 0.504\n1315 1000   516   484 0.516\n1316 1000   521   479 0.521\n1317 1000   532   468 0.532\n1318 1000   518   482 0.518\n1319 1000   500   500 0.500\n1320 1000   502   498 0.502\n1321 1000   491   509 0.491\n1322 1000   529   471 0.529\n1323 1000   513   487 0.513\n1324 1000   489   511 0.489\n1325 1000   496   504 0.496\n1326 1000   515   485 0.515\n1327 1000   498   502 0.498\n1328 1000   495   505 0.495\n1329 1000   459   541 0.459\n1330 1000   521   479 0.521\n1331 1000   515   485 0.515\n1332 1000   491   509 0.491\n1333 1000   496   504 0.496\n1334 1000   514   486 0.514\n1335 1000   497   503 0.497\n1336 1000   515   485 0.515\n1337 1000   483   517 0.483\n1338 1000   497   503 0.497\n1339 1000   496   504 0.496\n1340 1000   495   505 0.495\n1341 1000   497   503 0.497\n1342 1000   499   501 0.499\n1343 1000   515   485 0.515\n1344 1000   520   480 0.520\n1345 1000   520   480 0.520\n1346 1000   513   487 0.513\n1347 1000   504   496 0.504\n1348 1000   528   472 0.528\n1349 1000   489   511 0.489\n1350 1000   512   488 0.512\n1351 1000   527   473 0.527\n1352 1000   503   497 0.503\n1353 1000   471   529 0.471\n1354 1000   478   522 0.478\n1355 1000   501   499 0.501\n1356 1000   491   509 0.491\n1357 1000   504   496 0.504\n1358 1000   502   498 0.502\n1359 1000   471   529 0.471\n1360 1000   492   508 0.492\n1361 1000   488   512 0.488\n1362 1000   494   506 0.494\n1363 1000   531   469 0.531\n1364 1000   473   527 0.473\n1365 1000   487   513 0.487\n1366 1000   503   497 0.503\n1367 1000   494   506 0.494\n1368 1000   530   470 0.530\n1369 1000   496   504 0.496\n1370 1000   517   483 0.517\n1371 1000   526   474 0.526\n1372 1000   515   485 0.515\n1373 1000   488   512 0.488\n1374 1000   455   545 0.455\n1375 1000   503   497 0.503\n1376 1000   494   506 0.494\n1377 1000   527   473 0.527\n1378 1000   503   497 0.503\n1379 1000   472   528 0.472\n1380 1000   511   489 0.511\n1381 1000   488   512 0.488\n1382 1000   493   507 0.493\n1383 1000   520   480 0.520\n1384 1000   524   476 0.524\n1385 1000   508   492 0.508\n1386 1000   515   485 0.515\n1387 1000   519   481 0.519\n1388 1000   490   510 0.490\n1389 1000   477   523 0.477\n1390 1000   508   492 0.508\n1391 1000   515   485 0.515\n1392 1000   520   480 0.520\n1393 1000   489   511 0.489\n1394 1000   500   500 0.500\n1395 1000   519   481 0.519\n1396 1000   493   507 0.493\n1397 1000   509   491 0.509\n1398 1000   489   511 0.489\n1399 1000   494   506 0.494\n1400 1000   508   492 0.508\n1401 1000   513   487 0.513\n1402 1000   514   486 0.514\n1403 1000   516   484 0.516\n1404 1000   502   498 0.502\n1405 1000   496   504 0.496\n1406 1000   483   517 0.483\n1407 1000   516   484 0.516\n1408 1000   502   498 0.502\n1409 1000   510   490 0.510\n1410 1000   469   531 0.469\n1411 1000   487   513 0.487\n1412 1000   518   482 0.518\n1413 1000   499   501 0.499\n1414 1000   463   537 0.463\n1415 1000   521   479 0.521\n1416 1000   483   517 0.483\n1417 1000   469   531 0.469\n1418 1000   493   507 0.493\n1419 1000   496   504 0.496\n1420 1000   482   518 0.482\n1421 1000   477   523 0.477\n1422 1000   536   464 0.536\n1423 1000   507   493 0.507\n1424 1000   505   495 0.505\n1425 1000   511   489 0.511\n1426 1000   517   483 0.517\n1427 1000   510   490 0.510\n1428 1000   486   514 0.486\n1429 1000   520   480 0.520\n1430 1000   493   507 0.493\n1431 1000   497   503 0.497\n1432 1000   491   509 0.491\n1433 1000   520   480 0.520\n1434 1000   494   506 0.494\n1435 1000   514   486 0.514\n1436 1000   479   521 0.479\n1437 1000   506   494 0.506\n1438 1000   492   508 0.492\n1439 1000   474   526 0.474\n1440 1000   501   499 0.501\n1441 1000   504   496 0.504\n1442 1000   507   493 0.507\n1443 1000   482   518 0.482\n1444 1000   512   488 0.512\n1445 1000   506   494 0.506\n1446 1000   516   484 0.516\n1447 1000   504   496 0.504\n1448 1000   508   492 0.508\n1449 1000   504   496 0.504\n1450 1000   499   501 0.499\n1451 1000   520   480 0.520\n1452 1000   484   516 0.484\n1453 1000   504   496 0.504\n1454 1000   499   501 0.499\n1455 1000   499   501 0.499\n1456 1000   500   500 0.500\n1457 1000   503   497 0.503\n1458 1000   488   512 0.488\n1459 1000   474   526 0.474\n1460 1000   504   496 0.504\n1461 1000   510   490 0.510\n1462 1000   498   502 0.498\n1463 1000   510   490 0.510\n1464 1000   523   477 0.523\n1465 1000   525   475 0.525\n1466 1000   475   525 0.475\n1467 1000   496   504 0.496\n1468 1000   482   518 0.482\n1469 1000   506   494 0.506\n1470 1000   468   532 0.468\n1471 1000   500   500 0.500\n1472 1000   486   514 0.486\n1473 1000   508   492 0.508\n1474 1000   517   483 0.517\n1475 1000   507   493 0.507\n1476 1000   518   482 0.518\n1477 1000   508   492 0.508\n1478 1000   482   518 0.482\n1479 1000   504   496 0.504\n1480 1000   483   517 0.483\n1481 1000   521   479 0.521\n1482 1000   506   494 0.506\n1483 1000   510   490 0.510\n1484 1000   500   500 0.500\n1485 1000   473   527 0.473\n1486 1000   516   484 0.516\n1487 1000   505   495 0.505\n1488 1000   486   514 0.486\n1489 1000   467   533 0.467\n1490 1000   522   478 0.522\n1491 1000   515   485 0.515\n1492 1000   495   505 0.495\n1493 1000   476   524 0.476\n1494 1000   497   503 0.497\n1495 1000   514   486 0.514\n1496 1000   490   510 0.490\n1497 1000   518   482 0.518\n1498 1000   508   492 0.508\n1499 1000   480   520 0.480\n1500 1000   501   499 0.501\n1501 1000   490   510 0.490\n1502 1000   475   525 0.475\n1503 1000   493   507 0.493\n1504 1000   498   502 0.498\n1505 1000   541   459 0.541\n1506 1000   484   516 0.484\n1507 1000   508   492 0.508\n1508 1000   453   547 0.453\n1509 1000   530   470 0.530\n1510 1000   491   509 0.491\n1511 1000   496   504 0.496\n1512 1000   520   480 0.520\n1513 1000   508   492 0.508\n1514 1000   504   496 0.504\n1515 1000   524   476 0.524\n1516 1000   510   490 0.510\n1517 1000   500   500 0.500\n1518 1000   490   510 0.490\n1519 1000   505   495 0.505\n1520 1000   509   491 0.509\n1521 1000   525   475 0.525\n1522 1000   493   507 0.493\n1523 1000   511   489 0.511\n1524 1000   497   503 0.497\n1525 1000   479   521 0.479\n1526 1000   489   511 0.489\n1527 1000   528   472 0.528\n1528 1000   515   485 0.515\n1529 1000   492   508 0.492\n1530 1000   498   502 0.498\n1531 1000   518   482 0.518\n1532 1000   484   516 0.484\n1533 1000   485   515 0.485\n1534 1000   502   498 0.502\n1535 1000   515   485 0.515\n1536 1000   535   465 0.535\n1537 1000   529   471 0.529\n1538 1000   481   519 0.481\n1539 1000   505   495 0.505\n1540 1000   492   508 0.492\n1541 1000   478   522 0.478\n1542 1000   514   486 0.514\n1543 1000   491   509 0.491\n1544 1000   494   506 0.494\n1545 1000   498   502 0.498\n1546 1000   487   513 0.487\n1547 1000   494   506 0.494\n1548 1000   511   489 0.511\n1549 1000   510   490 0.510\n1550 1000   488   512 0.488\n1551 1000   491   509 0.491\n1552 1000   544   456 0.544\n1553 1000   514   486 0.514\n1554 1000   501   499 0.501\n1555 1000   506   494 0.506\n1556 1000   485   515 0.485\n1557 1000   505   495 0.505\n1558 1000   490   510 0.490\n1559 1000   502   498 0.502\n1560 1000   500   500 0.500\n1561 1000   485   515 0.485\n1562 1000   503   497 0.503\n1563 1000   483   517 0.483\n1564 1000   517   483 0.517\n1565 1000   509   491 0.509\n1566 1000   510   490 0.510\n1567 1000   488   512 0.488\n1568 1000   491   509 0.491\n1569 1000   526   474 0.526\n1570 1000   484   516 0.484\n1571 1000   494   506 0.494\n1572 1000   498   502 0.498\n1573 1000   481   519 0.481\n1574 1000   520   480 0.520\n1575 1000   504   496 0.504\n1576 1000   512   488 0.512\n1577 1000   510   490 0.510\n1578 1000   503   497 0.503\n1579 1000   501   499 0.501\n1580 1000   495   505 0.495\n1581 1000   497   503 0.497\n1582 1000   533   467 0.533\n1583 1000   521   479 0.521\n1584 1000   492   508 0.492\n1585 1000   496   504 0.496\n1586 1000   484   516 0.484\n1587 1000   487   513 0.487\n1588 1000   495   505 0.495\n1589 1000   476   524 0.476\n1590 1000   483   517 0.483\n1591 1000   520   480 0.520\n1592 1000   502   498 0.502\n1593 1000   497   503 0.497\n1594 1000   495   505 0.495\n1595 1000   510   490 0.510\n1596 1000   500   500 0.500\n1597 1000   517   483 0.517\n1598 1000   513   487 0.513\n1599 1000   491   509 0.491\n1600 1000   475   525 0.475\n1601 1000   498   502 0.498\n1602 1000   516   484 0.516\n1603 1000   493   507 0.493\n1604 1000   485   515 0.485\n1605 1000   504   496 0.504\n1606 1000   496   504 0.496\n1607 1000   480   520 0.480\n1608 1000   498   502 0.498\n1609 1000   530   470 0.530\n1610 1000   470   530 0.470\n1611 1000   516   484 0.516\n1612 1000   514   486 0.514\n1613 1000   500   500 0.500\n1614 1000   469   531 0.469\n1615 1000   495   505 0.495\n1616 1000   489   511 0.489\n1617 1000   503   497 0.503\n1618 1000   475   525 0.475\n1619 1000   492   508 0.492\n1620 1000   504   496 0.504\n1621 1000   488   512 0.488\n1622 1000   492   508 0.492\n1623 1000   516   484 0.516\n1624 1000   479   521 0.479\n1625 1000   502   498 0.502\n1626 1000   490   510 0.490\n1627 1000   493   507 0.493\n1628 1000   517   483 0.517\n1629 1000   509   491 0.509\n1630 1000   498   502 0.498\n1631 1000   517   483 0.517\n1632 1000   497   503 0.497\n1633 1000   519   481 0.519\n1634 1000   493   507 0.493\n1635 1000   500   500 0.500\n1636 1000   501   499 0.501\n1637 1000   486   514 0.486\n1638 1000   502   498 0.502\n1639 1000   500   500 0.500\n1640 1000   505   495 0.505\n1641 1000   464   536 0.464\n1642 1000   500   500 0.500\n1643 1000   502   498 0.502\n1644 1000   488   512 0.488\n1645 1000   480   520 0.480\n1646 1000   491   509 0.491\n1647 1000   529   471 0.529\n1648 1000   490   510 0.490\n1649 1000   487   513 0.487\n1650 1000   494   506 0.494\n1651 1000   527   473 0.527\n1652 1000   493   507 0.493\n1653 1000   512   488 0.512\n1654 1000   512   488 0.512\n1655 1000   481   519 0.481\n1656 1000   486   514 0.486\n1657 1000   459   541 0.459\n1658 1000   487   513 0.487\n1659 1000   481   519 0.481\n1660 1000   544   456 0.544\n1661 1000   479   521 0.479\n1662 1000   513   487 0.513\n1663 1000   501   499 0.501\n1664 1000   480   520 0.480\n1665 1000   489   511 0.489\n1666 1000   491   509 0.491\n1667 1000   503   497 0.503\n1668 1000   527   473 0.527\n1669 1000   506   494 0.506\n1670 1000   487   513 0.487\n1671 1000   506   494 0.506\n1672 1000   506   494 0.506\n1673 1000   485   515 0.485\n1674 1000   525   475 0.525\n1675 1000   520   480 0.520\n1676 1000   490   510 0.490\n1677 1000   508   492 0.508\n1678 1000   488   512 0.488\n1679 1000   505   495 0.505\n1680 1000   485   515 0.485\n1681 1000   508   492 0.508\n1682 1000   473   527 0.473\n1683 1000   503   497 0.503\n1684 1000   526   474 0.526\n1685 1000   496   504 0.496\n1686 1000   524   476 0.524\n1687 1000   498   502 0.498\n1688 1000   540   460 0.540\n1689 1000   486   514 0.486\n1690 1000   491   509 0.491\n1691 1000   499   501 0.499\n1692 1000   521   479 0.521\n1693 1000   496   504 0.496\n1694 1000   501   499 0.501\n1695 1000   485   515 0.485\n1696 1000   482   518 0.482\n1697 1000   510   490 0.510\n1698 1000   488   512 0.488\n1699 1000   499   501 0.499\n1700 1000   486   514 0.486\n1701 1000   496   504 0.496\n1702 1000   504   496 0.504\n1703 1000   499   501 0.499\n1704 1000   484   516 0.484\n1705 1000   489   511 0.489\n1706 1000   491   509 0.491\n1707 1000   515   485 0.515\n1708 1000   476   524 0.476\n1709 1000   508   492 0.508\n1710 1000   485   515 0.485\n1711 1000   483   517 0.483\n1712 1000   529   471 0.529\n1713 1000   552   448 0.552\n1714 1000   483   517 0.483\n1715 1000   511   489 0.511\n1716 1000   479   521 0.479\n1717 1000   496   504 0.496\n1718 1000   511   489 0.511\n1719 1000   530   470 0.530\n1720 1000   501   499 0.501\n1721 1000   505   495 0.505\n1722 1000   527   473 0.527\n1723 1000   495   505 0.495\n1724 1000   496   504 0.496\n1725 1000   494   506 0.494\n1726 1000   486   514 0.486\n1727 1000   495   505 0.495\n1728 1000   503   497 0.503\n1729 1000   493   507 0.493\n1730 1000   475   525 0.475\n1731 1000   493   507 0.493\n1732 1000   501   499 0.501\n1733 1000   511   489 0.511\n1734 1000   487   513 0.487\n1735 1000   480   520 0.480\n1736 1000   471   529 0.471\n1737 1000   482   518 0.482\n1738 1000   527   473 0.527\n1739 1000   494   506 0.494\n1740 1000   500   500 0.500\n1741 1000   527   473 0.527\n1742 1000   521   479 0.521\n1743 1000   498   502 0.498\n1744 1000   487   513 0.487\n1745 1000   488   512 0.488\n1746 1000   534   466 0.534\n1747 1000   492   508 0.492\n1748 1000   491   509 0.491\n1749 1000   516   484 0.516\n1750 1000   496   504 0.496\n1751 1000   496   504 0.496\n1752 1000   497   503 0.497\n1753 1000   508   492 0.508\n1754 1000   488   512 0.488\n1755 1000   526   474 0.526\n1756 1000   495   505 0.495\n1757 1000   510   490 0.510\n1758 1000   504   496 0.504\n1759 1000   496   504 0.496\n1760 1000   501   499 0.501\n1761 1000   562   438 0.562\n1762 1000   505   495 0.505\n1763 1000   493   507 0.493\n1764 1000   513   487 0.513\n1765 1000   506   494 0.506\n1766 1000   517   483 0.517\n1767 1000   499   501 0.499\n1768 1000   489   511 0.489\n1769 1000   488   512 0.488\n1770 1000   516   484 0.516\n1771 1000   479   521 0.479\n1772 1000   494   506 0.494\n1773 1000   506   494 0.506\n1774 1000   497   503 0.497\n1775 1000   485   515 0.485\n1776 1000   482   518 0.482\n1777 1000   518   482 0.518\n1778 1000   483   517 0.483\n1779 1000   496   504 0.496\n1780 1000   480   520 0.480\n1781 1000   487   513 0.487\n1782 1000   511   489 0.511\n1783 1000   507   493 0.507\n1784 1000   474   526 0.474\n1785 1000   506   494 0.506\n1786 1000   493   507 0.493\n1787 1000   497   503 0.497\n1788 1000   507   493 0.507\n1789 1000   535   465 0.535\n1790 1000   501   499 0.501\n1791 1000   514   486 0.514\n1792 1000   528   472 0.528\n1793 1000   486   514 0.486\n1794 1000   482   518 0.482\n1795 1000   484   516 0.484\n1796 1000   503   497 0.503\n1797 1000   528   472 0.528\n1798 1000   507   493 0.507\n1799 1000   478   522 0.478\n1800 1000   536   464 0.536\n1801 1000   500   500 0.500\n1802 1000   489   511 0.489\n1803 1000   527   473 0.527\n1804 1000   487   513 0.487\n1805 1000   515   485 0.515\n1806 1000   481   519 0.481\n1807 1000   496   504 0.496\n1808 1000   489   511 0.489\n1809 1000   524   476 0.524\n1810 1000   513   487 0.513\n1811 1000   503   497 0.503\n1812 1000   493   507 0.493\n1813 1000   495   505 0.495\n1814 1000   506   494 0.506\n1815 1000   513   487 0.513\n1816 1000   485   515 0.485\n1817 1000   498   502 0.498\n1818 1000   483   517 0.483\n1819 1000   502   498 0.502\n1820 1000   501   499 0.501\n1821 1000   498   502 0.498\n1822 1000   505   495 0.505\n1823 1000   495   505 0.495\n1824 1000   517   483 0.517\n1825 1000   504   496 0.504\n1826 1000   499   501 0.499\n1827 1000   496   504 0.496\n1828 1000   499   501 0.499\n1829 1000   481   519 0.481\n1830 1000   496   504 0.496\n1831 1000   488   512 0.488\n1832 1000   492   508 0.492\n1833 1000   495   505 0.495\n1834 1000   528   472 0.528\n1835 1000   520   480 0.520\n1836 1000   516   484 0.516\n1837 1000   496   504 0.496\n1838 1000   493   507 0.493\n1839 1000   511   489 0.511\n1840 1000   491   509 0.491\n1841 1000   469   531 0.469\n1842 1000   487   513 0.487\n1843 1000   490   510 0.490\n1844 1000   475   525 0.475\n1845 1000   491   509 0.491\n1846 1000   510   490 0.510\n1847 1000   491   509 0.491\n1848 1000   512   488 0.512\n1849 1000   503   497 0.503\n1850 1000   485   515 0.485\n1851 1000   508   492 0.508\n1852 1000   497   503 0.497\n1853 1000   512   488 0.512\n1854 1000   511   489 0.511\n1855 1000   506   494 0.506\n1856 1000   516   484 0.516\n1857 1000   499   501 0.499\n1858 1000   499   501 0.499\n1859 1000   490   510 0.490\n1860 1000   488   512 0.488\n1861 1000   499   501 0.499\n1862 1000   522   478 0.522\n1863 1000   464   536 0.464\n1864 1000   487   513 0.487\n1865 1000   512   488 0.512\n1866 1000   504   496 0.504\n1867 1000   504   496 0.504\n1868 1000   501   499 0.501\n1869 1000   526   474 0.526\n1870 1000   534   466 0.534\n1871 1000   503   497 0.503\n1872 1000   496   504 0.496\n1873 1000   497   503 0.497\n1874 1000   517   483 0.517\n1875 1000   508   492 0.508\n1876 1000   501   499 0.501\n1877 1000   482   518 0.482\n1878 1000   498   502 0.498\n1879 1000   510   490 0.510\n1880 1000   503   497 0.503\n1881 1000   502   498 0.502\n1882 1000   476   524 0.476\n1883 1000   507   493 0.507\n1884 1000   500   500 0.500\n1885 1000   493   507 0.493\n1886 1000   507   493 0.507\n1887 1000   500   500 0.500\n1888 1000   509   491 0.509\n1889 1000   510   490 0.510\n1890 1000   500   500 0.500\n1891 1000   512   488 0.512\n1892 1000   527   473 0.527\n1893 1000   484   516 0.484\n1894 1000   458   542 0.458\n1895 1000   497   503 0.497\n1896 1000   502   498 0.502\n1897 1000   496   504 0.496\n1898 1000   505   495 0.505\n1899 1000   513   487 0.513\n1900 1000   543   457 0.543\n1901 1000   506   494 0.506\n1902 1000   508   492 0.508\n1903 1000   528   472 0.528\n1904 1000   472   528 0.472\n1905 1000   492   508 0.492\n1906 1000   493   507 0.493\n1907 1000   482   518 0.482\n1908 1000   501   499 0.501\n1909 1000   504   496 0.504\n1910 1000   504   496 0.504\n1911 1000   499   501 0.499\n1912 1000   491   509 0.491\n1913 1000   507   493 0.507\n1914 1000   463   537 0.463\n1915 1000   499   501 0.499\n1916 1000   486   514 0.486\n1917 1000   483   517 0.483\n1918 1000   515   485 0.515\n1919 1000   475   525 0.475\n1920 1000   495   505 0.495\n1921 1000   495   505 0.495\n1922 1000   504   496 0.504\n1923 1000   484   516 0.484\n1924 1000   523   477 0.523\n1925 1000   491   509 0.491\n1926 1000   472   528 0.472\n1927 1000   498   502 0.498\n1928 1000   514   486 0.514\n1929 1000   473   527 0.473\n1930 1000   485   515 0.485\n1931 1000   502   498 0.502\n1932 1000   491   509 0.491\n1933 1000   499   501 0.499\n1934 1000   498   502 0.498\n1935 1000   492   508 0.492\n1936 1000   502   498 0.502\n1937 1000   477   523 0.477\n1938 1000   518   482 0.518\n1939 1000   520   480 0.520\n1940 1000   469   531 0.469\n1941 1000   500   500 0.500\n1942 1000   509   491 0.509\n1943 1000   482   518 0.482\n1944 1000   519   481 0.519\n1945 1000   488   512 0.488\n1946 1000   488   512 0.488\n1947 1000   517   483 0.517\n1948 1000   510   490 0.510\n1949 1000   519   481 0.519\n1950 1000   486   514 0.486\n1951 1000   496   504 0.496\n1952 1000   503   497 0.503\n1953 1000   503   497 0.503\n1954 1000   528   472 0.528\n1955 1000   506   494 0.506\n1956 1000   484   516 0.484\n1957 1000   504   496 0.504\n1958 1000   494   506 0.494\n1959 1000   492   508 0.492\n1960 1000   487   513 0.487\n1961 1000   518   482 0.518\n1962 1000   475   525 0.475\n1963 1000   498   502 0.498\n1964 1000   473   527 0.473\n1965 1000   509   491 0.509\n1966 1000   459   541 0.459\n1967 1000   508   492 0.508\n1968 1000   499   501 0.499\n1969 1000   514   486 0.514\n1970 1000   511   489 0.511\n1971 1000   504   496 0.504\n1972 1000   490   510 0.490\n1973 1000   518   482 0.518\n1974 1000   487   513 0.487\n1975 1000   498   502 0.498\n1976 1000   515   485 0.515\n1977 1000   521   479 0.521\n1978 1000   492   508 0.492\n1979 1000   522   478 0.522\n1980 1000   498   502 0.498\n1981 1000   510   490 0.510\n1982 1000   495   505 0.495\n1983 1000   529   471 0.529\n1984 1000   483   517 0.483\n1985 1000   505   495 0.505\n1986 1000   497   503 0.497\n1987 1000   493   507 0.493\n1988 1000   491   509 0.491\n1989 1000   525   475 0.525\n1990 1000   490   510 0.490\n1991 1000   498   502 0.498\n1992 1000   524   476 0.524\n1993 1000   506   494 0.506\n1994 1000   485   515 0.485\n1995 1000   502   498 0.502\n1996 1000   491   509 0.491\n1997 1000   479   521 0.479\n1998 1000   524   476 0.524\n1999 1000   505   495 0.505\n2000 1000   507   493 0.507\n\n\n\nmean(coin_flips_2000_1000$heads)\n\n[1] 499.9055\n\n\n\nggplot(coin_flips_2000_1000, aes(x = heads)) +\n    geom_histogram(binwidth = 10, boundary = 500)\n\n\n\n\nAnd now the same histogram, but with proportions:\n\nggplot(coin_flips_2000_1000, aes(x = prop)) +\n    geom_histogram(binwidth = 0.01, boundary = 0.5)\n\n\n\n\n\nExercise 4\nComment on the histogram above. Describe its shape using the vocabulary of the three important features (modes, symmetry, outliers). Why do you think it’s shaped like this?\n\nPlease write up your answer here.\n\n\n\nExercise 5\nGiven the amount of randomness involved (each person is tossing coins which randomly come up heads or tails), why do we see so much structure and orderliness in the histograms?\n\nPlease write up your answer here."
  },
  {
    "objectID": "08-intro_to_randomization_1-web.html#randomization1-who-cares",
    "href": "08-intro_to_randomization_1-web.html#randomization1-who-cares",
    "title": "8  Introduction to randomization, Part 1",
    "section": "8.7 But who cares about coin flips?",
    "text": "8.7 But who cares about coin flips?\nIt’s fair to ask why we go to all this trouble to talk about coin flips. The most pressing research questions of our day do not involve people sitting around and flipping coins, either physically or virtually.\nBut now substitute “heads” and “tails” with “cancer” and “no cancer”. Or “guilty” and “not guilty”. Or “shot” and “not shot”. The fact is that many important issues are measured as variables with two possible outcomes. There is some underlying “probability” of seeing one outcome over the other. (It doesn’t have to be 50% like the coin.) Statistical methods—including simulation—can say a lot about what we “expect” to see if these outcomes are truly random. More importantly, when we see outcomes that aren’t consistent with our simulations, we may wonder if there is some underlying mechanism that may be not so random after all. It may not look like it on first blush, but this idea is at the core of the scientific method.\nFor example, let’s suppose that 85% of U.S. adults support some form of background checks for gun buyers.2 Now, imagine we went out and surveyed a random group of people and asked them a simple yes/no question about their support for background checks. What might we see?\nLet’s simulate. Imagine flipping a coin, but instead of coming up heads 50% of the time, suppose it were possible for the coin to come up heads 85% of the time.3 A sequence of heads and tails with this weird coin would be much like randomly surveying people and asking them about background checks.\nWe can make a “virtual” weird coin with the rflip command by specifying how often we want heads to come up.\n\nset.seed(1234)\nrflip(1, prob = 0.85)\n\n\nFlipping 1 coin [ Prob(Heads) = 0.85 ] ...\n\nH\n\nNumber of Heads: 1 [Proportion Heads: 1]\n\n\nIf we flip our weird coin a bunch of times, we can see that our coin is not fair. Indeed, it appears to come up heads way more often than not:\n\nset.seed(1234)\nrflip(100, prob = 0.85)\n\n\nFlipping 100 coins [ Prob(Heads) = 0.85 ] ...\n\nH H H H T H H H H H H H H T H H H H H H H H H H H H H T H H H H H H H H\nH H T H H H H H H H H H H H H H H H H H H H H H T H H H H H H H H H H T\nH H H H H H H H T H H H H T H H H T H T H H H H H H H H\n\nNumber of Heads: 90 [Proportion Heads: 0.9]\n\n\nThe results from the above code can be thought of as a survey of 100 random U.S. adults about their support for background checks for purchasing guns. “Heads” means “supports” and “tails” means “opposes.” If the majority of Americans support background checks, then we will come across more people in our survey who tell us they support background checks. This shows up in our simulation as the appearance of more heads than tails.\nNote that there is no guarantee that our sample will have exactly 85% heads. In fact, it doesn’t; it has 90% heads.\nAgain, keep in mind that we’re simulating the act of obtaining a random sample of 100 U.S. adults. If we get a different sample, we’ll get different results. (We set a different seed here. That ensures that this code chunk is randomly different from the one above.)\n\nset.seed(123456)\nrflip(100, prob = 0.85)\n\n\nFlipping 100 coins [ Prob(Heads) = 0.85 ] ...\n\nH H H H H H H H T H H H T T T T T H H H H H H H H H T T T H H T H H H H\nT T H H H H T H H H H H H H H H H T H T H H H H H H H H H H H H H H H H\nT H H H T H H H H H H T H H H H H H H H H H H H T H H H\n\nNumber of Heads: 81 [Proportion Heads: 0.81]\n\n\nSee, this time, only 81% came up heads, even though we expected 85%. That’s how randomness works.\n\nExercise 6(a)\nNow imagine that 2000 people all go out and conduct surveys of 100 random U.S. adults, asking them about their support for background checks. Write some R code that simulates this. Plot a histogram of the results. (Hint: you’ll need do(2000) * in there.) Use the proportion of supporters (prop), not the raw count of supporters (heads).\n\n\nset.seed(1234)\n# Add code here to simulate 2000 surveys of 100 U.S. adults.\n\n\n# Plot the results in a histogram using proportions.\n\n\n\n\nExercise 6(b)\nRun another simulation, but this time, have each person survey 1000 adults and not just 100.\n\n\nset.seed(1234)\n# Add code here to simulate 2000 surveys of 1000 U.S. adults.\n\n\n# Plot the results in a histogram using proportions.\n\n\n\n\nExercise 6(c)\nWhat changed when you surveyed 1000 people instead of 100?\n\nPlease write up your answer here."
  },
  {
    "objectID": "08-intro_to_randomization_1-web.html#randomization1-sampling-var",
    "href": "08-intro_to_randomization_1-web.html#randomization1-sampling-var",
    "title": "8  Introduction to randomization, Part 1",
    "section": "8.8 Sampling variability",
    "text": "8.8 Sampling variability\nWe’ve seen that taking repeated samples (using the do command) leads to lots of different outcomes. That is randomness in action. We don’t expect the results of each survey to be exactly the same every time the survey is administered.\nBut despite this randomness, there is an interesting pattern that we can observe. It has to do with the number of times we flip the coin. Since we’re using coin flips to simulate the act of conducting a survey, the number of coin flips is playing the role of the sample size. In other words, if we want to simulate a survey of U.S. adults with a sample size of 100, we simulate that by flipping 100 coins.\n\nExercise 7\nGo back and look at all the examples above. What do you notice about the range of values on the x-axis when the sample size is small versus large? (In other words, in what way are the histograms different when using rflip(10, prob = ...) or rflip(100, prob = ...) versus rflip(1000, prob = ...)? It’s easier to compare histograms one to another when looking at the proportions instead of the raw head counts because proportions are always on the same scale from 0 to 1.)\n\nPlease write up your answer here."
  },
  {
    "objectID": "08-intro_to_randomization_1-web.html#randomization1-conclusion",
    "href": "08-intro_to_randomization_1-web.html#randomization1-conclusion",
    "title": "8  Introduction to randomization, Part 1",
    "section": "8.9 Conclusion",
    "text": "8.9 Conclusion\nSimulation is a tool for understanding what happens when a statistical process is repeated many times in a randomized way. The availability of fast computer processing makes simulation easy and accessible. Eventually, the goal will be to use simulation to answer important questions about data and the processes in the world that generate data. This is possible because, despite the ubiquitous presence of randomness, a certain order emerges when the number of samples is large enough. Even though there is sampling variability (different random outcomes each time we sample), there are patterns in that variability that can be exploited to make predictions."
  },
  {
    "objectID": "08-intro_to_randomization_1-web.html#footnotes",
    "href": "08-intro_to_randomization_1-web.html#footnotes",
    "title": "8  Introduction to randomization, Part 1",
    "section": "",
    "text": "There is some theory behind choosing the number of times we need to simulate, but we’re not going to get into all that.↩︎\nThis is likely close to the truth. See this article: https://iop.harvard.edu/get-involved/harvard-political-review/vast-majority-americans-support-universal-background-checks↩︎\nThe idea of a “weighted” coin that can do this comes up all the time in probability and statistics courses, but it seems that it’s not likely one could actually manufacture a coin that came up heads more or less than 50% of the time when flipped. See this paper for more details: http://www.stat.columbia.edu/~gelman/research/published/diceRev2.pdf↩︎"
  },
  {
    "objectID": "09-intro_to_randomization_2-web.html#randomization2-intro",
    "href": "09-intro_to_randomization_2-web.html#randomization2-intro",
    "title": "9  Introduction to randomization, Part 2",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nIn this chapter, we’ll learn more about randomization and simulation. Instead of flipping coins, though, we’ll randomly shuffle data around in order to explore the effects of randomizing a predictor variable.\n\n9.1.1 Install new packages\nIf you are using RStudio Workbench, you do not need to install any packages. (Any packages you need should already be installed by the server administrators.)\nIf you are using R and RStudio on your own machine instead of accessing RStudio Workbench through a browser, you’ll need to type the following commands at the Console:\ninstall.packages(\"openintro\")\ninstall.packages(\"infer\")\n\n\n9.1.2 Download the R notebook file\nCheck the upper-right corner in RStudio to make sure you’re in your intro_stats project. Then click on the following link to download this chapter as an R notebook file (.Rmd).\nhttps://vectorposse.github.io/intro_stats/chapter_downloads/09-intro_to_randomization_2.Rmd\nOnce the file is downloaded, move it to your project folder in RStudio and open it there.\n\n\n9.1.3 Restart R and run all chunks\nIn RStudio, select “Restart R and Run All Chunks” from the “Run” menu."
  },
  {
    "objectID": "09-intro_to_randomization_2-web.html#randomization2-load",
    "href": "09-intro_to_randomization_2-web.html#randomization2-load",
    "title": "9  Introduction to randomization, Part 2",
    "section": "9.2 Load packages",
    "text": "9.2 Load packages\nWe’ll load tidyverse as usual along with the janitor package to make tables (with tabyl). The openintro package has a data set called sex_discrimination that we will explore. Finally, the infer package will provide tools that we will use in nearly every chapter for the remainder of the book.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(openintro)\n\nLoading required package: airports\nLoading required package: cherryblossom\nLoading required package: usdata\n\nlibrary(infer)"
  },
  {
    "objectID": "09-intro_to_randomization_2-web.html#randomization2-question",
    "href": "09-intro_to_randomization_2-web.html#randomization2-question",
    "title": "9  Introduction to randomization, Part 2",
    "section": "9.3 Our research question",
    "text": "9.3 Our research question\nAn interesting study was conducted in the 1970s that investigated gender discrimination in hiring.1 The researchers brought in 48 male bank supervisors and asked them to evaluate personnel files. Based on their review, they were to determine if the person was qualified for promotion to branch manager. The trick is that all the files were identical, but half listed the candidate as male and half listed the candidate as female. The files were randomly assigned to the 48 supervisors.\nThe research question is whether the files supposedly belonging to males were recommended for promotion more than the files supposedly belonging to females.\n\nExercise 1\nIs the study described above an observational study or an experiment? How do you know?\n\nPlease write up your answer here.\n\n\n\nExercise 2(a)\nIdentify the sample in the study. In other words, how many people were in the sample and what are the important characteristics common to those people.\n\nPlease write up your answer here.\n\n\n\nExercise 2(b)\nIdentify the population of interest in the study. In other words, who is the sample supposed to represent? That is, what group of people that this study is trying to learn about?\n\nPlease write up your answer here.\n\n\n\nExercise 2(c)\nIn your opinion, does the sample from this study truly represent the population you identified above?\n\nPlease write up your answer here."
  },
  {
    "objectID": "09-intro_to_randomization_2-web.html#randomization2-eda",
    "href": "09-intro_to_randomization_2-web.html#randomization2-eda",
    "title": "9  Introduction to randomization, Part 2",
    "section": "9.4 Exploratory data analysis",
    "text": "9.4 Exploratory data analysis\nHere is the data:\n\nsex_discrimination\n\n# A tibble: 48 × 2\n   sex   decision\n   &lt;fct&gt; &lt;fct&gt;   \n 1 male  promoted\n 2 male  promoted\n 3 male  promoted\n 4 male  promoted\n 5 male  promoted\n 6 male  promoted\n 7 male  promoted\n 8 male  promoted\n 9 male  promoted\n10 male  promoted\n# ℹ 38 more rows\n\n\n\nglimpse(sex_discrimination)\n\nRows: 48\nColumns: 2\n$ sex      &lt;fct&gt; male, male, male, male, male, male, male, male, male, male, m…\n$ decision &lt;fct&gt; promoted, promoted, promoted, promoted, promoted, promoted, p…\n\n\n\nExercise 3\nWhich variable is the response variable and which variable is the predictor variable?\n\nPlease write up your answer here.\n\n\nHere is a contingency table with decision as the row variable and sex as the column variable. (Recall that we always list the response variable first. That way, the column sums will show us how many are in each of the predictor groups.)\n\ntabyl(sex_discrimination, decision, sex) %&gt;%\n    adorn_totals()\n\n     decision male female\n     promoted   21     14\n not promoted    3     10\n        Total   24     24\n\n\n\n\nExercise 4\nCreate another contingency table of decision and sex, this time with percentages (not proportions) instead of counts. You’ll probably have to go back to the “Categorical data” to review the syntax. (Hint: you should have three separate adorn functions on the lines following the tabyl command.)\n\n\n# Add code here to create a contingency table of percentages\n\n\n\nAlthough we can read off the percentages in the contingency table, we need to do computations using the proportions. (Remember that we use percentages to communicate with other human beings, but we do math with proportions.) Fortunately, the output of tabyl is a tibble! So we can manipulate and grab the elements we need.\nLet’s create and store the tabyl output with proportions. We don’t need the marginal distribution, so we can dispense with adorn_totals.\n\ndecision_sex_tabyl &lt;- tabyl(sex_discrimination, decision, sex) %&gt;%\n    adorn_percentages(\"col\")\ndecision_sex_tabyl\n\n     decision  male    female\n     promoted 0.875 0.5833333\n not promoted 0.125 0.4166667\n\n\n\n\nExercise 5\nInterpret these proportions in the context of the data. In other words, what do these proportions say about the male files that were recommended for promotion versus the female files recommended for promotion?\n\nPlease write up your answer here.\n\n\nThe real statistic of interest to us is the difference between these proportions. We can use the mutate command from dplyr variable compute the difference for us.\n\ndecision_sex_tabyl %&gt;%\n    mutate(diff = male - female)\n\n     decision  male    female       diff\n     promoted 0.875 0.5833333  0.2916667\n not promoted 0.125 0.4166667 -0.2916667\n\n\nAs a matter of fact, once we know the difference in promotion rates, we don’t really need the individual proportions anymore. The transmute verb is a version of mutate that gives us exactly what we want. It will create a new column just like mutate, but then it keeps only that new column. We’ll call the resulting output decision_sex_diff.\n\ndecision_sex_diff &lt;- decision_sex_tabyl %&gt;%\n    transmute(diff = male - female)\ndecision_sex_diff\n\n       diff\n  0.2916667\n -0.2916667\n\n\nNotice the order of subtraction: we’re doing the men’s rates minus the women’s rates.\nThis computes both the difference in promotion rates (in the first row) and the difference in not-promoted rates (in the second row). Let’s just keep the first row, since we care more about promotion rates. (That’s our success category.) We can use slice to grab the first row:\n\ndecision_sex_diff %&gt;%\n    slice(1)\n\n      diff\n 0.2916667\n\n\nThis means that there is a 29% difference between the male files that were promoted and the female files that were promoted. The difference was computed as males minus females, so the fact that the number is positive means that male files were more likely to recommended for promotion."
  },
  {
    "objectID": "09-intro_to_randomization_2-web.html#randomization2-permuting",
    "href": "09-intro_to_randomization_2-web.html#randomization2-permuting",
    "title": "9  Introduction to randomization, Part 2",
    "section": "9.5 Permuting",
    "text": "9.5 Permuting\nOne way to see if there is evidence of an association between promotion decisions and sex is to assume, temporarily, that there is no association. If there were truly no association, then the difference between the promotion rates between the male files and female files should be 0%. Of course, the number of people promoted in the data was 35, an odd number, so the number of male files promoted and female files promoted cannot be the same. Therefore, the difference in proportions can’t be exactly 0 in this data. Nevertheless, we would expect—under the assumption of no association—the number of male files promoted to be close to the number of female files promoted, giving a difference around 0%.\nNow, we saw a difference of about 29% between the two groups in the data. Then again, non-zero differences—sometimes even large ones— can just come about by pure chance alone. We may have accidentally sampled more bank managers who just happened to prefer the male candidates. This could happen for sexist reasons; it’s possible our sample of bank managers are, by chance, more sexist than bank managers in the general population during the 1970s. Or it might be for more benign reasons; perhaps the male applications got randomly steered to bank managers who were more likely to be impressed with any application, and therefore, they were more likely to promote anyone regardless of the gender listed. We have to consider the possibility that our observed difference seems large even though there may have been no association between promotion and sex in the general population.\nSo how do we test the range of values that could arise from just chance alone? In other words, how do we explore sampling variability?\nOne way to force the variables to be independent is to “permute”—in other words, shuffle—the values of sex in our data. If we ignore the sex listed in the file and give it a random label (independent of the actual sex listed in the file), we know for sure that such an assignment is random and not due to any actual evidence of sexism. In that case, promotion is equally likely to occur in both groups.\nLet’s see how permuting works in R. To begin with, look at the actual values of sex in our data:\n\nsex_discrimination$sex\n\n [1] male   male   male   male   male   male   male   male   male   male  \n[11] male   male   male   male   male   male   male   male   male   male  \n[21] male   male   male   male   female female female female female female\n[31] female female female female female female female female female female\n[41] female female female female female female female female\nLevels: male female\n\n\nAll the males happen to be listed first, followed by all the females.\nNow we permute all the values around (using the sample command). As explained in an earlier chapter, we will set the seed so that our results are reproducible.\n\nset.seed(3141593)\nsample(sex_discrimination$sex)\n\n [1] male   female male   male   female female female female female female\n[11] female female female female male   male   female male   female male  \n[21] female female male   male   female female male   female male   male  \n[31] male   male   male   female male   female male   male   male   male  \n[41] female female female male   male   male   female male  \nLevels: male female\n\n\nDo it again without the seed, just to make sure it’s truly random:\n\nsample(sex_discrimination$sex)\n\n [1] male   male   male   female male   female male   female female female\n[11] female male   female male   female female female female male   male  \n[21] female female female male   male   female male   male   male   female\n[31] male   male   male   male   male   female female female female male  \n[41] female female male   male   male   female female male  \nLevels: male female"
  },
  {
    "objectID": "09-intro_to_randomization_2-web.html#randomization2-randomization",
    "href": "09-intro_to_randomization_2-web.html#randomization2-randomization",
    "title": "9  Introduction to randomization, Part 2",
    "section": "9.6 Randomization",
    "text": "9.6 Randomization\nThe idea here is to keep the promotion status the same for each file, but randomly permute the sex labels. There will still be the same number of male and female files, but now they will be randomly matched with promoted files and not promoted files. Since this new grouping into “males” and “females” is completely random and arbitrary, we expect the likelihood of promotion to be equal for both groups.\nA more precise way of saying this is that the expected difference under the assumption of independent variables is 0%. If there were truly no association, then the percentage of people promoted would be independent of sex. However, sampling variability means that we are not likely to see an exact difference of 0%. (Also, as we mentioned earlier, the odd number of promotions means the difference will never be exactly 0% anyway in this data.) The real question, then, is how different could the difference be from 0% and still be reasonably possible due to random chance.\nLet’s perform a few random simulations. We’ll walk through the steps one line at a time. The first thing we do is permute the sex column:\n\nset.seed(3141593)\nsex_discrimination %&gt;%\n    mutate(sex = sample(sex))\n\n# A tibble: 48 × 2\n   sex    decision\n   &lt;fct&gt;  &lt;fct&gt;   \n 1 male   promoted\n 2 female promoted\n 3 male   promoted\n 4 male   promoted\n 5 female promoted\n 6 female promoted\n 7 female promoted\n 8 female promoted\n 9 female promoted\n10 female promoted\n# ℹ 38 more rows\n\n\nThen we follow the steps from earlier, generating a contingency table with proportions. This is accomplished by simply adding two lines of code to the previous code:\n\nset.seed(3141593)\nsex_discrimination %&gt;%\n    mutate(sex = sample(sex)) %&gt;%\n    tabyl(decision, sex) %&gt;%\n    adorn_percentages(\"col\")\n\n     decision      male    female\n     promoted 0.6666667 0.7916667\n not promoted 0.3333333 0.2083333\n\n\nNote that the proportions in this table are different from the ones in the real data.\nThen we calculate the difference between the male and female columns by adding a line with transmute:\n\nset.seed(3141593)\nsex_discrimination %&gt;%\n    mutate(sex = sample(sex)) %&gt;%\n    tabyl(decision, sex) %&gt;%\n    adorn_percentages(\"col\") %&gt;%\n    transmute(diff = male - female)\n\n   diff\n -0.125\n  0.125\n\n\nIn this case, the first row happens to be negative, but that’s okay. This particular random shuffling had more females promoted than males. (Remember, though, that the permuted sex labels are now meaningless.)\nFinally, we grab the entry in the first row with slice:\n\nset.seed(3141593)\nsex_discrimination %&gt;%\n    mutate(sex = sample(sex)) %&gt;%\n    tabyl(decision, sex) %&gt;%\n    adorn_percentages(\"col\") %&gt;%\n    transmute(diff = male - female) %&gt;%\n    slice(1)\n\n   diff\n -0.125\n\n\nWe’ll repeat this code a few more times, but without the seed, to get new random observations.\n\nsex_discrimination %&gt;%\n    mutate(sex = sample(sex)) %&gt;%\n    tabyl(decision, sex) %&gt;%\n    adorn_percentages(\"col\") %&gt;%\n    transmute(diff = male - female) %&gt;%\n    slice(1)\n\n       diff\n 0.04166667\n\n\n\nsex_discrimination %&gt;%\n    mutate(sex = sample(sex)) %&gt;%\n    tabyl(decision, sex) %&gt;%\n    adorn_percentages(\"col\") %&gt;%\n    transmute(diff = male - female) %&gt;%\n    slice(1)\n\n  diff\n 0.125\n\n\n\nsex_discrimination %&gt;%\n    mutate(sex = sample(sex)) %&gt;%\n    tabyl(decision, sex) %&gt;%\n    adorn_percentages(\"col\") %&gt;%\n    transmute(diff = male - female) %&gt;%\n    slice(1)\n\n  diff\n 0.125\n\n\n\nsex_discrimination %&gt;%\n    mutate(sex = sample(sex)) %&gt;%\n    tabyl(decision, sex) %&gt;%\n    adorn_percentages(\"col\") %&gt;%\n    transmute(diff = male - female) %&gt;%\n    slice(1)\n\n       diff\n -0.2916667\n\n\nThink carefully about what these random numbers mean. Each time we randomize, we get a simulated difference in the proportion of promotions between male files and female files. The sample part ensures that there is no actual relationship between promotion and sex among these randomized values. We expect each simulated difference to be close to zero, but we also expect deviations from zero due to randomness and chance."
  },
  {
    "objectID": "09-intro_to_randomization_2-web.html#randomization2-infer",
    "href": "09-intro_to_randomization_2-web.html#randomization2-infer",
    "title": "9  Introduction to randomization, Part 2",
    "section": "9.7 The infer package",
    "text": "9.7 The infer package\nThe above code examples show the nuts and bolts of permuting data around to break any association that might exist between two variables. However, to do a proper randomization, we need to repeat this process many, many times (just like how we flipped thousands of “coins” in the last chapter).\nHere we introduce some code from the infer package that will help us automate this procedure. The added benefit of introducing infer now is that we will continue to use it in nearly every chapter of the book that follows.\nHere is the code template, starting with setting the seed:\n\nset.seed(3141593)\nsims &lt;- sex_discrimination %&gt;%\n    specify(decision ~ sex, success = \"promoted\") %&gt;%\n    hypothesize(null = \"independence\") %&gt;%\n    generate(reps = 1000, type = \"permute\") %&gt;%\n    calculate(stat = \"diff in props\", order = c(\"male\", \"female\"))\nsims\n\nResponse: decision (factor)\nExplanatory: sex (factor)\nNull Hypothesis: independence\n# A tibble: 1,000 × 2\n   replicate    stat\n       &lt;int&gt;   &lt;dbl&gt;\n 1         1 -0.125 \n 2         2 -0.125 \n 3         3 -0.0417\n 4         4  0.0417\n 5         5  0.125 \n 6         6 -0.0417\n 7         7 -0.0417\n 8         8  0.125 \n 9         9  0.125 \n10        10  0.208 \n# ℹ 990 more rows\n\n\nWe will learn more about all these lines of code in future chapters. By the end of the course, running this type of analysis will be second nature. For now, you can copy and paste the code chunk above and make minor changes as you need. Here are the three things you will need to look out for for doing this with different data sets in the future:\n\nThe second line (after setting the seed) will be your new data set.\nIn the specify line, you will have a different response variable, predictor variable, and success condition that will depend on the context of your new data.\nIn the calculate line, you will have two different levels that you want to compare. Be careful to list them in the order in which you want to subtract them."
  },
  {
    "objectID": "09-intro_to_randomization_2-web.html#randomization2-plot",
    "href": "09-intro_to_randomization_2-web.html#randomization2-plot",
    "title": "9  Introduction to randomization, Part 2",
    "section": "9.8 Plot results",
    "text": "9.8 Plot results\nA histogram will show us the range of possible values under the assumption of independence of the two variables. We can get one from our infer output using visualize. (This is a lot easier than building a histogram with ggplot!)\n\nsims %&gt;%\n    visualize()\n\n\n\n\nThe bins aren’t great in the picture above. There is no way currently to set the binwidth or boundary as we’ve done before, but we can experiment with the total number of bins. 9 seems to be a good number.\n\nsims %&gt;%\n    visualize(bins = 9)\n\n\n\n\n\nExercise 6\nWhy is the mode of the graph above at 0? This has been explained several different times in this chapter, but put it into your own words to make sure you understand the logic behind the randomization.\n\nPlease write up your answer here.\n\n\nLet’s compare these simulated values to the observed difference in the real data. We’ve computed the latter already, but let’s use infer tools to find it. We’ll give the answer a name, obs_diff.\n\nobs_diff &lt;- sex_discrimination %&gt;%\n    observe(decision ~ sex, success = \"promoted\",\n            stat = \"diff in props\", order = c(\"male\", \"female\"))\nobs_diff\n\nResponse: decision (factor)\nExplanatory: sex (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 0.292\n\n\nNow we can graph the observed difference in the data alongside the simulated values under the assumption of independent variables. The name of the function shade_p_value is a little cryptic for now, but it will become clear within a few chapters.\n\nsims %&gt;%\n    visualize(bins = 9) +\n    shade_p_value(obs_stat = obs_diff, direction = \"greater\")"
  },
  {
    "objectID": "09-intro_to_randomization_2-web.html#randomization2-chance",
    "href": "09-intro_to_randomization_2-web.html#randomization2-chance",
    "title": "9  Introduction to randomization, Part 2",
    "section": "9.9 By chance?",
    "text": "9.9 By chance?\nHow likely is it that the observed difference (or a difference even more extreme) could have resulted from chance alone? Because sims contains simulated results after permuting, the values in the stat column assume that promotion is independent of sex. In order to assess how plausible our observed difference is under that assumption, we want to find out how many of the simulated values are at least as big, if not bigger, than the observed difference, 0.292.\nLook at the randomized differences sorted in decreasing order:\n\nsims %&gt;%\n    arrange(desc(stat))\n\nResponse: decision (factor)\nExplanatory: sex (factor)\nNull Hypothesis: independence\n# A tibble: 1,000 × 2\n   replicate  stat\n       &lt;int&gt; &lt;dbl&gt;\n 1       133 0.375\n 2       181 0.375\n 3       568 0.375\n 4       619 0.375\n 5        50 0.292\n 6        68 0.292\n 7        77 0.292\n 8        93 0.292\n 9       111 0.292\n10       119 0.292\n# ℹ 990 more rows\n\n\nOf the 1000 simulations, the most extreme difference of 37.5% occurred four times, just by chance. That seems like a pretty extreme value when expecting a value of 0%, but the laws of probability tell us that extreme values will be observed from time to time, even if rarely. Also recall that the observed difference in the actual data was 29.2%. This specific value came up quite a bit in our simulated data. In fact, the 31st entry of the sorted data above is the last occurrence of the value 0.292. After that, the next higher larger value is 0.208.\nSo let’s return to the original question. How many simulated values are as large—if not larger—than the observed difference? Apparently, 31 out of 1000, which is 0.031. In other words 3% of the simulated data is as extreme or more extreme than the actual difference in promotion rates between male files and female files in the real data. That’s not very large. In other words, a difference like 29.2% could occur just by chance—like flipping 10 out of 10 heads or something like that. But it doesn’t happen very often.\nWe can automate this calculation using the function get_p_value (similar to shade_p_value above) even though we don’t yet know what “p value” means.\n\nsims %&gt;%\n    get_p_value(obs_stat = obs_diff, direction = \"greater\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.031\n\n\nCOPY/PASTE WARNING: If the observed difference were negative, then extreme values of interest would be less than, say, -0.292, not greater than 0.292. You must note if the observed difference is positive or negative and then use “greater” or “less” as appropriate!\nAgain, 0.031 is a small number. This shows us that if there were truly no association between promotion and sex, then our data is a rare event. (An observed difference this extreme or more extreme would only occur about 3% of the time by chance.)\nBecause the probability above is so small, it seems unlikely that our variables are independent. Therefore, it seems more likely that there is an association between promotion and sex. We have evidence of a statistically significant difference between the chance of getting recommended for promotion if the file indicates male versus female.\nBecause this is an experiment, it’s possible that a causal claim could be made. If everything in the application files was identical except the indication of gender, then it stands to reason that gender explains why more male files were promoted over female files. But all that depends on the experiment being a well-designed experiment.\n\nExercise 7\nAlthough we are not experts in experimental design, what concerns do you have about generalizing the results of this experiment to broad conclusions about sexism in the 1970s? (To be clear, I’m not saying that sexism wasn’t a broad problem in the 1970s. It surely was—and still is. I’m only asking you to opine as to why the results of this one study might not be conclusive in making an overly broad statement.)\n\nPlease write up your answer here."
  },
  {
    "objectID": "09-intro_to_randomization_2-web.html#randomization2-your-turn",
    "href": "09-intro_to_randomization_2-web.html#randomization2-your-turn",
    "title": "9  Introduction to randomization, Part 2",
    "section": "9.10 Your turn",
    "text": "9.10 Your turn\nIn this section, you’ll explore another famous data set related to the topic of gender discrimination. (Also from the 1970s!)\nThe following code will download admissions data from the six largest graduate departments at the University of California, Berkeley in 1973. We’ve seen the read_csv command before, but we’ve added some extra stuff in there to make sure all the columns get imported as factor variables (rather than having to convert them ourselves later).\n\nucb_admit &lt;- read_csv(\"https://vectorposse.github.io/intro_stats/data/ucb_admit.csv\",\n                      col_types = list(\n                          Admit = col_factor(),\n                          Gender = col_factor(),\n                          Dept = col_factor()))\n\n\nucb_admit\n\n# A tibble: 4,526 × 3\n   Admit    Gender Dept \n   &lt;fct&gt;    &lt;fct&gt;  &lt;fct&gt;\n 1 Admitted Male   A    \n 2 Admitted Male   A    \n 3 Admitted Male   A    \n 4 Admitted Male   A    \n 5 Admitted Male   A    \n 6 Admitted Male   A    \n 7 Admitted Male   A    \n 8 Admitted Male   A    \n 9 Admitted Male   A    \n10 Admitted Male   A    \n# ℹ 4,516 more rows\n\n\n\nglimpse(ucb_admit)\n\nRows: 4,526\nColumns: 3\n$ Admit  &lt;fct&gt; Admitted, Admitted, Admitted, Admitted, Admitted, Admitted, Adm…\n$ Gender &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, Male, Male, Mal…\n$ Dept   &lt;fct&gt; A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, …\n\n\nAs you go through the exercises below, you should carefully copy and paste commands from earlier in the chapter, making the necessary changes.\nRemember that R is case sensitive! In the sex_discrimination data, all the variables and levels started with lowercase letters. In the ucb_admit data, they all start with uppercase letters, so you’ll need to be careful to change that after you copy and paste code examples from above.\n\nExercise 8(a)\nIs this data observational or experimental? How do you know?\n\nPlease write up your answer here.\n\n\n\nExercise 8(b)\nExploratory data analysis: make two contingency tables with Admit as the response variable and Gender as the explanatory variable. One table should have counts and the other table should have percentages. (Both tables should include the marginal distribution at the bottom.)\n\n\n# Add code here to make a contingency table with counts.\n\n\n# Add code here to make a contingency table with percentages.\n\n\n\n\nExercise 8(c)\nUse observe from the infer package to calculate the observed difference in proportions between males who were admitted and females who were admitted. Do the subtraction in that order: males minus females. Store your output as obs_diff2 so that it doesn’t overwrite the variable obs_diff we created earlier.\n\n\n# Add code here to calculate the observed difference.\n# Store this as obs_diff2.\n\n\n\n\nExercise 8(d)\nSimulate 1000 outcomes under the assumption that admission is independent of gender. Use the specify, hypothesize, generate, and calculate sequence from the infer package as above. Call the simulated data frame sims2 so that it doesn’t conflict with the earlier sims. Don’t touch the set.seed command. That will ensure that all students get the same randomization.\n\n\nset.seed(10101)\n# Add code here to simulate 1000 outcomes\n# under the independence assumption\n# and store the simulations in a data frame called sims2.\n\n\n\n\nExercise 8(e)\nPlot the simulated values in a histogram using the visualize verb from infer. When you first run the code, remove the bins = 9 we had earlier and let visualize choose the number of bins. If you are satisfied with the graph, you don’t need to specify a number of bins. If you are not satisfied, you can experiment with the number of bins until you find a number that seems reasonable.\nBe sure to include a vertical line at the value of the observed difference using the shade_p_value command. Don’t forget that the location of that line is obs_diff2 now.\n\n\n# Add code here to plot the results.\n\n\n\n\nExercise 8(f)\nFinally, comment on what you see. Based on the histogram above, is the observed difference in the data rare? In other words, under the assumption that admission and gender are independent, are we likely to see an observed difference as far away from zero as we actually see in the data? So what is your conclusion then? Do you believe there was an association between admission and gender in the UC Berkeley admissions process in 1973?\n\nPlease write up your answer here."
  },
  {
    "objectID": "09-intro_to_randomization_2-web.html#randomization2-simpson",
    "href": "09-intro_to_randomization_2-web.html#randomization2-simpson",
    "title": "9  Introduction to randomization, Part 2",
    "section": "9.11 Simpson’s paradox",
    "text": "9.11 Simpson’s paradox\nThe example above from UC Berkeley seems like an open and shut case. Male applicants were clearly admitted at a greater rate than female applicants. While we never expect the application rates to be exactly equal—even under the assumption that admission and gender are independent—the randomization exercise showed us that the observed data was way outside the range of possible differences that could have occurred just by chance.\nBut we also know this is observational data. Association is not causation.\n\nExercise 9\nNote that we didn’t say “correlation is not causation”. The latter is also true, but why does it not apply in this case? (Think about the conditions for correlation.)\n\nPlease write up your answer here.\n\n\nSince we don’t have data from a carefully controlled experiment, we always have to be worried about lurking variables. Could there be a third variable apart from admission and gender that could be driving the association between them? In other words, the fact that males were admitted at a higher rate than females might be sexism, or it might be spurious.\nSince we have access to a third variable, Dept, let’s analyze it as well. The tabyl command will happily take a third variable and create a set of contingency tables, one for each department.\nHere are the tables with counts:\n\ntabyl(ucb_admit, Admit, Gender, Dept) %&gt;%\n    adorn_totals()\n\n$A\n    Admit Male Female\n Admitted  512     89\n Rejected  313     19\n    Total  825    108\n\n$B\n    Admit Male Female\n Admitted  353     17\n Rejected  207      8\n    Total  560     25\n\n$C\n    Admit Male Female\n Admitted  120    202\n Rejected  205    391\n    Total  325    593\n\n$D\n    Admit Male Female\n Admitted  138    131\n Rejected  279    244\n    Total  417    375\n\n$E\n    Admit Male Female\n Admitted   53     94\n Rejected  138    299\n    Total  191    393\n\n$F\n    Admit Male Female\n Admitted   22     24\n Rejected  351    317\n    Total  373    341\n\n\nAnd here are the tables with percentages:\n\ntabyl(ucb_admit, Admit, Gender, Dept) %&gt;%\n    adorn_totals() %&gt;%\n    adorn_percentages(\"col\") %&gt;%\n    adorn_pct_formatting()\n\n$A\n    Admit   Male Female\n Admitted  62.1%  82.4%\n Rejected  37.9%  17.6%\n    Total 100.0% 100.0%\n\n$B\n    Admit   Male Female\n Admitted  63.0%  68.0%\n Rejected  37.0%  32.0%\n    Total 100.0% 100.0%\n\n$C\n    Admit   Male Female\n Admitted  36.9%  34.1%\n Rejected  63.1%  65.9%\n    Total 100.0% 100.0%\n\n$D\n    Admit   Male Female\n Admitted  33.1%  34.9%\n Rejected  66.9%  65.1%\n    Total 100.0% 100.0%\n\n$E\n    Admit   Male Female\n Admitted  27.7%  23.9%\n Rejected  72.3%  76.1%\n    Total 100.0% 100.0%\n\n$F\n    Admit   Male Female\n Admitted   5.9%   7.0%\n Rejected  94.1%  93.0%\n    Total 100.0% 100.0%\n\n\n\n\nExercise 10\nLook at the contingency tables with percentages. Examine each department individually. What do you notice about the admit rates (as percentages) between males and females for most of the departments listed? Identify the four departments where female admission rates were higher than male admission rates.\n\nPlease write up your answer here.\n\n\nThis is completely counterintuitive. How can males be admitted at a higher rate overall, and yet in most departments, females were admitted at a higher rate.\nThis phenomenon is often called Simpson’s Paradox. Like almost everything in statistics, this is named after a person (Edward H. Simpson) who got the popular credit for writing about the phenomenon, but not being the person who actually discovered the phenomenon. (There does not appear to be a primeval reference for the first person to have studied it. Similar observations had appeared in various sources more than 50 years before Simpson wrote his paper.)\n\n\nExercise 11\nLook at the contingency tables with counts. Focus on the four departments you identified above. What is true of the total number of male and female applicants for those four department (and not for the other two departments)?\n\nPlease write up your answer here.\n\n\n\nExercise 12(a)\nNow create a contingency table with percentages that uses Admit for the row variable and Dept as the column variable.\n\n\n# Add code here to create a contingency table with percentages\n# for Dept and Admit\n\n\n\n\nExercise 12(b)\nAccording to the contingency table above, which two departments were (by far) the least selective? (In other words, which two departments admitted a vast majority of their applicants?)\n\nPlease write up your answer here.\n\n\n\nExercise 12(c)\nEarlier, you identified four departments where male applicants outnumbered female applicants. (These were the same departments that had higher admission rates for females.) But for which two departments was the difference between the number of male and female applicants the largest?\n\nPlease write up your answer here.\n\n\nYour work in the previous exercises begins to paint a picture that explains what’s going on with this “paradox”. Males applied in much greater numbers to a few departments with high acceptance rates. As a result, more male students overall got in to graduate school. Females applied in greater numbers to departments that were more selective. Overall, then, fewer females got in to graduate school. But on a department-by-department basis, female applicants were usually more likely to get accepted.\nNone of this suggests that sexism fails to exist. It doesn’t even prove that sexism wasn’t a factor in some departmental admission procedures. What it does suggest is that when we don’t take into account possible lurking variables, we run the risk of oversimplifying issues that are potentially complex.\nIn our analysis of the UC Berkeley data, we’ve exhausted all the variables available to us in the data set. There remains the potential for unmeasured confounders, or variables that could still act as lurking variables, but we have no idea about them because they aren’t in our data. This is an unavoidable peril of working with observational data. If we aren’t careful to “control” for a reasonable set of possible lurking variables, we must be very careful when trying to make broad conclusions."
  },
  {
    "objectID": "09-intro_to_randomization_2-web.html#randomization2-conclusion",
    "href": "09-intro_to_randomization_2-web.html#randomization2-conclusion",
    "title": "9  Introduction to randomization, Part 2",
    "section": "9.12 Conclusion",
    "text": "9.12 Conclusion\nHere we used randomization to explore the idea of two variables being independent or associated. When we assume they are independent, we can explore the sampling variability of the differences that could occur by pure chance alone. We expect the difference to be zero, but we know that randomness will cause the simulated differences to have a range of values. Is the difference in the observed data far away from zero? In that case, we can say we have evidence that the variables are not independent; in other words, it is more likely that our variables are associated.\n\n9.12.1 Preparing and submitting your assignment\n\nFrom the “Run” menu, select “Restart R and Run All Chunks”.\nDeal with any code errors that crop up. Repeat steps 1–-2 until there are no more code errors.\nSpell check your document by clicking the icon with “ABC” and a check mark.\nHit the “Preview” button one last time to generate the final draft of the .nb.html file.\nProofread the HTML file carefully. If there are errors, go back and fix them, then repeat steps 1–5 again.\n\nIf you have completed this chapter as part of a statistics course, follow the directions you receive from your professor to submit your assignment."
  },
  {
    "objectID": "09-intro_to_randomization_2-web.html#footnotes",
    "href": "09-intro_to_randomization_2-web.html#footnotes",
    "title": "9  Introduction to randomization, Part 2",
    "section": "",
    "text": "Rosen B and Jerdee T. 1974. Influence of sex role stereotypes on personnel decisions. Journal of Applied Psychology 59(1):9-14.↩︎"
  },
  {
    "objectID": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-intro",
    "href": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-intro",
    "title": "10  Hypothesis testing with randomization, Part 1",
    "section": "10.1 Introduction",
    "text": "10.1 Introduction\nUsing a sample to deduce something about a population is called “statistical inference”. In this chapter, we’ll learn about one form of statistical inference called “hypothesis testing”. The focus will be on walking through the example from Part 2 of “Introduction to randomization” and recasting it here as a formal hypothesis test.\nThere are no new R commands here, but there are many new ideas that will require careful reading. You are not expected to be an expert on hypothesis testing after this one chapter. However, within the next few chapters, as we learn more about hypothesis testing and work through many more examples, the hope is that you will begin to assimilate and internalize the logic of inference and the steps of a hypothesis test.\n\n10.1.1 Install new packages\nThere are no new packages used in this chapter.\n\n\n10.1.2 Download the R notebook file\nCheck the upper-right corner in RStudio to make sure you’re in your intro_stats project. Then click on the following link to download this chapter as an R notebook file (.Rmd).\nhttps://vectorposse.github.io/intro_stats/chapter_downloads/10-hypothesis_testing_with_randomization_1.Rmd\nOnce the file is downloaded, move it to your project folder in RStudio and open it there.\n\n\n10.1.3 Restart R and run all chunks\nIn RStudio, select “Restart R and Run All Chunks” from the “Run” menu."
  },
  {
    "objectID": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-load",
    "href": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-load",
    "title": "10  Hypothesis testing with randomization, Part 1",
    "section": "10.2 Load packages",
    "text": "10.2 Load packages\nWe load tidyverse and janitor. We’ll continue to explore the infer package for investigating statistical claims. We load the openintro package to access the sex_discrimination data (the one with the male bank managers promoting male files versus female files).\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(infer)\nlibrary(openintro)\n\nLoading required package: airports\nLoading required package: cherryblossom\nLoading required package: usdata"
  },
  {
    "objectID": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-question",
    "href": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-question",
    "title": "10  Hypothesis testing with randomization, Part 1",
    "section": "10.3 Our research question",
    "text": "10.3 Our research question\nWe return to the sex discrimination experiment from the last chapter. We are interested in finding out if there is an association between the recommendation to promote a candidate for branch manager and the gender listed on the file being evaluated by the male bank manager."
  },
  {
    "objectID": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-hypothesis-testing",
    "href": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-hypothesis-testing",
    "title": "10  Hypothesis testing with randomization, Part 1",
    "section": "10.4 Hypothesis testing",
    "text": "10.4 Hypothesis testing\nThe approach we used in Part 2 of “Introduction to randomization” was to assume that the two variables decision and sex were independent. From that assumption, we were able to compare the observed difference in promotion percentages between males and females from the actual data to the distribution of random values obtained by randomization. When the observed difference was far enough away from zero, we concluded that the assumption of independence was probably false, giving us evidence that the two variables were associated after all.\nThis logic is formalized into a sequence of steps known as a hypothesis test. In this section, we will introduce a rubric for conducting a full and complete hypothesis test for the sex discrimination example. (This rubric also appears in the Appendix. If you need the rubric as a file, you can also download copies either as an .Rmd file here or as an .nb.html file here.)\nA hypothesis test can be organized into five parts:\n\nExploratory data analysis\nHypotheses\nModel\nMechanics\nConclusion\n\nBelow, I’ll address each of these steps.\n\n10.4.1 Exploratory data analysis\nBefore we can answer questions using data, we need to understand our data.\nMost data sets come with some information about the provenance and structure of the data. (Often this is called “metadata”.) Data provenance is the story of how the data was collected and for what purpose. Together with some information about the types of variables recorded, this is the who, what, when, where, why, and how. Without context, data is just a bunch of letters and numbers. You must understand the nature of the data in order to use the data. Information about the structure of the data is often recorded in a “code book”.\nFor data that you collect yourself, you’ll already know all about it, although should probably write that stuff down in case other people want to use your data (or in case “future you” wants to use the data). For other data sets, you hope that other people have recorded information about how the data was collected and what is described in the data. When working with data sets in R as we do for these chapters, we’ve already seen that there are help files—sometimes more or less helpful. In some cases, you’ll need to go beyond the brief explanations in the help file to investigate the data provenance. And for files we download from other places on the internet, we may have a lot of work to do.\n\nExercise 1\nWhat are some ethical issues you might want to consider when looking into the provenance of data? Have a discussion with a classmate and/or do some internet sleuthing to see if you can identify one or two key issues that should be considered before you access or analyze data.\n\nPlease write up your answer here.\n\nFor exploring the raw data in front of us, we can use commands like View from the Console to see the data in spreadsheet form, although if we’re using R Notebooks, we can just type the name of the data frame in a code chunk and run it to print the data in a form we can navigate and explore. There is also glimpse to explore the structure of the data (the variables and how they’re coded), as well as other summary functions to get a quick sense of the variables.\nSometimes you have to prepare your data for analysis. A common example is converting categorical variables that should be coded as factor variables, but often are coded as character vectors, or are coded numerically (like “1” and “0” instead of “Yes” and “No”). Sometimes missing data is coded unusually (like “999”) and that has to be fixed before trying to calculate statistics. “Cleaning” data is often a task that takes more time than analyzing it!\nFinally, once the data is in a suitably tidy form, we can use visualizations like tables, graphs, and charts to understand the data better. Often, there are conditions about the shape of our data that have to be met before inference is appropriate, and this step can help diagnose problems that could arise in the inferential procedure. This is a good time to look for outliers, for example.\n\n\n\n10.4.2 Hypotheses\nWe are trying to ask some question about a population of interest. However, all we have in our data is a sample of that population. The word inference comes from the verb “infer”: we are trying to infer what might be true of a population just from examining a sample. It’s also possible that our question involves comparing two or more populations to each other. In this case, we’ll have multiple samples, one from each of our populations. For example, in our sex discrimination example, we are comparing two populations: male bank managers who consider male files for promotion, and male bank managers who consider female files for promotion. Our data gives us two samples who form only a part of the larger populations of interest.\nTo convince our audience that our analysis is correct, it makes sense to take a skeptical position. If we are trying to prove that there is an association between promotion and sex, we don’t just declare it to be so. We start with a “null hypothesis”, or an expression of the belief that there is no association. A null hypothesis always represents the “default” position that a skeptic might take. It codifies the idea that “there’s nothing to see here.”\nOur job is to gather evidence to show that there is something interesting going on. The statement of interest to us is called the “alternative hypothesis”. This is usually the thing we’re trying to prove related to our research question.\nWe can perform one-sided tests or two-sided tests. A one-sided test is when we have a specific direction in mind for the effect. For example, if we are trying to prove that male files are more likely to be promoted than female files, then we would perform a one-sided test. On the other hand, if we only care about proving an association, then male files could be either more likely or less likely to be promoted than female files. (This is contrasted to the null that states that male files are equally likely to be promoted as female files.) If it seems weird to run a two-sided test, keep in mind that we want to give our statistical analysis a chance to prove an association regardless of the direction of the association. Wouldn’t you be interested to know if it turned out that male files are, in fact, less likely to be promoted?\nYou can’t cheat and look at the data first. In a normal research study out there in the real world, you develop hypotheses long before you collect data. So you have to decide to do a one-sided or two-sided test before you have the luxury of seeing your data pointing in one direction or the other.\nRunning a two-sided test is often a good default option. Again, this is because our analysis will allow us to show interesting effects in any direction.\nWe typically express hypotheses in two ways. First, we write down full sentences that express in the context of the problem what our null and alternative hypotheses are stating. Then, we express the same ideas as mathematical statements. This translation from words to math is important as it gives us the connection to the quantitative statistical analysis we need to perform. The null hypothesis will always be that some quantity is equal to (=) the null value. The alternative hypothesis depends on whether we are conducting a one-sided test or a two-sided test. A one-sided test is mathematically saying that the quantity of interest is either greater than (&gt;) or less than (&lt;) the null value. A two-sided test always states that the quantity of interest is not equal to (\\(\\neq\\)) the null value. (Notice the math symbol enclosed in dollar signs in the previous sentence. In the HTML file, these symbols will appear correctly. In the R Notebook, you can hover the cursor anywhere between the dollar signs and the math symbol will show up. Alternatively, you can click somewhere between the dollar signs and hit Ctrl-Enter or Cmd-Enter, just like with inline R code.)\nThe most important thing to know is that the entire hypothesis test up until you reach the conclusion is conducted under the assumption that the null hypothesis is true. In other words, we pretend the whole time that our alternative hypothesis is false, and we carry out our analysis working under that assumption. This may seem odd, but it makes sense when you remember that the goal of inference is to try to convince a skeptic. Others will only believe your claim after you present evidence that suggests that the data is inconsistent with the claims made in the null.\n\n\n10.4.3 Model\nA model is an approximation—usually a simplification—of reality. In a hypothesis test, when we say “model” we are talking specifically about the “null model”. In other words, what is true about the population under the assumption of the null? If we sample from the population repeatedly, we find that there is some kind of distribution of values that can occur by pure chance alone. This is called the sampling distribution model. We have been learning about how to use randomization to understand the sampling distribution and how much sampling variability to expect, even when the null hypothesis is true.\nBuilding a model is contingent upon certain assumptions being true. We cannot usually demonstrate directly that these assumptions are conclusively met; however, there are often conditions that can be checked with our data that can give us some confidence in saying that the assumptions are probably met. For example, there is no hope that we can infer anything from our sample unless that sample is close to a random sample of the population. There is rarely any direct evidence of having a properly random sample, and often, random samples are too much to ask for. There is almost never such a thing as a truly random sample of the population. Nevertheless, it is up to us to make the case that our sample is as representative of the population as possible. Additionally, we have to know that our sample comprises less than 10% of the size of the population. The reasons for this are somewhat technical and the 10% figure is just a rough guideline, but we should think carefully about this whenever we want our inference to be correct.\nThose are just two examples. For the randomization tests we are running, those are the only two conditions we need to check. For other hypothesis tests in the future that use different types of models, we will need to check more conditions that correspond to the modeling assumptions we will need to make.\n\n\n10.4.4 Mechanics\nThis is the nitty-gritty, nuts-and-bolts part of a hypothesis test. Once we have a model that tells us how data should behave under the assumption of the null hypothesis, we need to check how our data actually behaved. The measure of where our data is relative to the null model is called the test statistic. For example, if the null hypothesis states that there should be a difference of zero between promotion rates for males and females, then the test statistic would be the actual observed difference in our data between males and females.\nOnce we have a test statistic, we can plot it in the same graph as the null model. This gives us a visual sense of how rare or unusual our observed data is. The further our test statistic is from the center of the null model, the more evidence we have that our data would be very unusual if the null model were true. And that, in turn, gives us a reason not to believe the null model. When conducting a two-sided test, we will actually graph locations on both side of the null value: the test statistic on one side of the null value and a point the same distance on the other side of the null value. This will acknowledge that we’re interested in evidence of an effect in either direction.\nFinally, we convert the visual evidence explained in the previous paragraph to a number called a P-value. This measures how likely it is to see our observed data—or data even more extreme—under the assumption of the null. A small P-value, then, means that if the null were really true, we wouldn’t be very likely at all to see data like ours. That leaves us with little confidence that the null model is really true. (After all, we did see the data we gathered!) If the P-value is large—in other words, if the test statistic is closer to the middle of the null distribution—then our data is perfectly consistent with the null hypothesis. That doesn’t mean the null is true, but it certainly does not give us evidence against the null.\nA one-sided test will give us a P-value that only counts data more extreme than the observed data in the direction that we explicitly hypothesized. For example, if our alternative hypothesis was that male files are more likely to be promoted, then we would only look at the part of the model that showed differences with as many or more male promotions as our data showed. A two-sided P-value, by contrast, will count data that is extreme in either direction. This will include values on both sides of the distribution, which is why it’s called a two-sided test. Computationally, it is usually easiest to calculate the one-sided P-value and just double it.1\nRemember the statement made earlier that throughout the hypothesis testing process, we work under the assumption that the null hypothesis is true. The P-value is no exception. It tells us under the assumption of the null how likely we are to to see data at least as extreme (if not even more extreme) as the data we actually saw.\n\n\n10.4.5 Conclusion\nThe P-value we calculate in the Mechanics section allows us to determine what our decision will be relative to the null hypothesis. As explained above, when the P-value is small, that means we had data that would be very unlikely had the null been true. The sensible conclusion is then to “reject the null hypothesis.” On the other hand, if the data is consistent with the null hypothesis, then we “fail to reject the null hypothesis.”\nHow small does the P-value need to be before we are willing to reject the null hypothesis? That is a decision we have to make based on how much we are willing to risk an incorrect conclusion. A value that is widely used is 0.05; in other words, if \\(P &lt; 0.05\\) we reject the null, and if \\(P &gt; 0.05\\), we fail to reject the null. However, for situations where we want to be conservative, we could choose this threshold to be much smaller. If we insist that the P-value be less than 0.01, for example, then we will only reject the null when we have a lot more evidence. The threshold we choose is called the “significance level”, denoted by the Greek letter alpha: \\(\\alpha\\). The value of \\(\\alpha\\) must be chosen long before we compute our P-value so that we’re not tempted to cheat and change the value of \\(\\alpha\\) to suit our P-value (and by doing so, quite literally, move the goalposts).\nNote that we never accept the null hypothesis. The hypothesis testing procedure gives us no evidence in favor of the null. All we can say is that the evidence is either strong enough to warrant rejection of the null, or else it isn’t, in which case we can conclude nothing. If we can’t prove the null false, we are left not knowing much of anything at all.\nThe phrases “reject the null” or “fail to reject the null” are very statsy. Your audience may not be statistically trained. Besides, the real conclusion you care about concerns the research question of interest you posed at the beginning of this process, and that is built into the alternative hypothesis, not the null. Therefore, we need some statement that addresses the alternative hypothesis in words that a general audience will understand. I recommend the following templates:\n\nWhen you reject the null, you can safely say, “We have sufficient evidence that [restate the alternative hypothesis].”\nWhen you fail to reject the null, you can safely say, “We have insufficient evidence that [restate the alternative hypothesis].”\n\nThe last part of your conclusion should be an acknowledgement of the uncertainty in this process. Statistics tries to tame randomness, but in the end, randomness is always somewhat unpredictable. It is possible that we came to the wrong conclusion, not because we made mistakes in our computation, but because statistics just can’t be right 100% of the time when randomness is involved. Therefore, we need to explain to our audience that we may have made an error.\nA Type I error is what happens when the null hypothesis is actually true, but our procedure rejects it anyway. This happens when we get an unrepresentative extreme sample for some reason. For example, perhaps there really is no association between promotion and sex. Even if that were true, we could accidentally survey a group of bank managers who—by pure chance alone—happen to recommend promotion more often for the male files. Our test statistic will be “accidentally” far from the null value, and we will mistakenly reject the null. Whenever we reject the null, we are at risk of making a Type I error. Given that we are conclusively stating a statistically significant finding, if that finding is wrong, this is a false positive, a term that is synonymous with a Type I error. The significance level \\(\\alpha\\) discussed above is, in fact, the probability of making a Type I error. (If the null is true, we will still reject the null if our P-value happens to be less than \\(\\alpha\\).)\nOn the other hand, the null may actually be false, and yet, we may not manage to gather enough evidence to disprove it. This can also happen due to an unusual sample—a sample that doesn’t conform to the “truth”. But there are other ways this can happen as well, most commonly when you have a small sample size (which doesn’t allow you to prove much of anything at all) or when the effect you’re trying to measure exists, but is so small that it is hard to distinguish from no effect at all (which is what the null postulates). In these cases, we are at risk of making a Type II error. Anytime we say that we fail to reject the null, we have to worry about the possibility of making a Type II error, also called a false negative."
  },
  {
    "objectID": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-example",
    "href": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-example",
    "title": "10  Hypothesis testing with randomization, Part 1",
    "section": "10.5 Example",
    "text": "10.5 Example\nBelow, we’ll model the process of walking through a complete hypothesis test, showing how we would address each step. Then, you’ll have a turn at doing the same thing for a different question. Unless otherwise stated, we will always assume a significance level of \\(\\alpha = 0.05\\). (In other words, we will reject the null if our computed P-value is less than 0.05, and we will fail to reject the null if our P-value is greater than or equal to 0.05.)\nNote that there is some mathematical formatting. As mentioned before, this is done by enclosing such math in dollar signs. Don’t worry too much about the syntax; just mimic what you see in the example."
  },
  {
    "objectID": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-ex-eda",
    "href": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-ex-eda",
    "title": "10  Hypothesis testing with randomization, Part 1",
    "section": "10.6 Exploratory data analysis",
    "text": "10.6 Exploratory data analysis\n\n10.6.1 Use data documentation (help files, code books, Google, etc.) to determine as much as possible about the data provenance and structure.\nYou can look at the help file by typing ?sex_discrimination at the Console. (However, do not put that command here in a code chunk. The R Notebook has no way of displaying a help file when it’s processed.) You can also type that into the Help tab in the lower-right panel in RStudio.\nThe help file doesn’t say too much, but there is a “Source” at the bottom. We can do an internet search for “Rosen Jerdee Influence of sex role stereotypes on personnel decisions”. As many academics articles on the internet are, this one is pay-walled, so we can’t read it for free. If you go to school or work for an institution with a library, though, you may be able to access articles through your library services. Talk to a librarian if you’d like to access research articles. As long as you have the citation details, librarians can often track down articles, and many are already accessible through library databases.\nIn this case, we can read the abstract for free. This tells us that the data we have is only one part of a larger set of experiments done.\nThis is also the place to comment on any ethical concerns you may have. For example, how was the data collected? Did the researchers follow ethical guidelines in the treatment of their subjects, like obtaining consent? Without accessing the full article, it’s hard to know in this case. But do your best in each data analysis task you have to try to find out as much as possible about the data.\nIn this section, we’ll also print the data set and use glimpse to summarize the variables.\n\nsex_discrimination\n\n# A tibble: 48 × 2\n   sex   decision\n   &lt;fct&gt; &lt;fct&gt;   \n 1 male  promoted\n 2 male  promoted\n 3 male  promoted\n 4 male  promoted\n 5 male  promoted\n 6 male  promoted\n 7 male  promoted\n 8 male  promoted\n 9 male  promoted\n10 male  promoted\n# ℹ 38 more rows\n\n\n\nglimpse(sex_discrimination)\n\nRows: 48\nColumns: 2\n$ sex      &lt;fct&gt; male, male, male, male, male, male, male, male, male, male, m…\n$ decision &lt;fct&gt; promoted, promoted, promoted, promoted, promoted, promoted, p…\n\n\n\n\n10.6.2 Prepare the data for analysis.\nIn this section, we do any tasks required to clean the data. This will often involve using mutate, either to convert other variable types to factors, or compute additional variables using existing columns. It may involve using filter to analyze only one part of the data we care about.\nIf there is missing data, this is the place to identify it and decide if you need to address it before starting your analysis. It’s always important to check for missing data. It’s not always necessary to address it now as many of the R functions we use will ignore rows with missing data.\nThe easiest way to detect missing data is to try deleting rows that are missing some data with drop_na and see if the number of rows changes:\n\nsex_discrimination %&gt;%\n  drop_na()\n\n# A tibble: 48 × 2\n   sex   decision\n   &lt;fct&gt; &lt;fct&gt;   \n 1 male  promoted\n 2 male  promoted\n 3 male  promoted\n 4 male  promoted\n 5 male  promoted\n 6 male  promoted\n 7 male  promoted\n 8 male  promoted\n 9 male  promoted\n10 male  promoted\n# ℹ 38 more rows\n\n\nSince the result still has 48 rows, there are no missing values.\nThe sex_discimination data is already squeaky clean, so we don’t need to do anything here.\n\n\n10.6.3 Make tables or plots to explore the data visually.\nAs we have two categorical variables, a contingency table is a good way of visualizing the distribution of both variables together. (Don’t forget to include the marginal distribution and create two tables: one with counts and one with percentages!)\n\ntabyl(sex_discrimination, decision, sex) %&gt;%\n  adorn_totals()\n\n     decision male female\n     promoted   21     14\n not promoted    3     10\n        Total   24     24\n\n\n\ntabyl(sex_discrimination, decision, sex) %&gt;%\n  adorn_totals() %&gt;%\n  adorn_percentages(\"col\") %&gt;%\n  adorn_pct_formatting()\n\n     decision   male female\n     promoted  87.5%  58.3%\n not promoted  12.5%  41.7%\n        Total 100.0% 100.0%"
  },
  {
    "objectID": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-ex-hypotheses",
    "href": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-ex-hypotheses",
    "title": "10  Hypothesis testing with randomization, Part 1",
    "section": "10.7 Hypotheses",
    "text": "10.7 Hypotheses\n\n10.7.1 Identify the sample (or samples) and a reasonable population (or populations) of interest.\nThere are technically two samples of interest here. All the data comes from a group of 48 bank managers recruited for the study, but one group of interest are bank managers who are evaluating male files, and the other group of interest are bank managers who are evaluating female files.\nOne of the contingency tables above shows the sample sizes for each group in the marginal distribution along the bottom of the table (i.e., the column sums). There are 24 mangers with male files and 24 managers with female files.\nThe populations of interest are probably all bank managers evaluating male candidates and all bank managers evaluating female candidates, probably only in in the U.S. (where the two researchers were based) and only during the 1970s.\n\n\n10.7.2 Express the null and alternative hypotheses as contextually meaningful full sentences.\n(Note: The null hypothesis is indicated by the symbol \\(H_{0}\\), often pronounced “H naught” or “H sub zero.” The alternative hypothesis is indicated by \\(H_{A}\\), pronounced “H sub A.”)\n\\(H_{0}:\\) There is no association between decision and sex in hiring branch managers for banks in the 1970s.\n\\(H_{A}:\\) There is an association between decision and sex in hiring branch managers for banks in the 1970s.\n\n\n10.7.3 Express the null and alternative hypotheses in symbols (when possible).\n\\(H_{0}: p_{promoted, male} - p_{promoted, female} = 0\\)\n\\(H_{A}: p_{promoted, male} - p_{promoted, female} \\neq 0\\)\nNote: First, pay attention to the “success” condition (in this case, “promoted”). We could choose to measure either those promoted or those not promoted. The difference will be positive for one and negative for the other, so it really doesn’t matter which one we choose. Just make a choice and be consistent. Also pay close attention here to the order of the subtraction. Again, while it doesn’t matter conceptually, we need to make sure that the code we include later agrees with this order."
  },
  {
    "objectID": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-ex-model",
    "href": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-ex-model",
    "title": "10  Hypothesis testing with randomization, Part 1",
    "section": "10.8 Model",
    "text": "10.8 Model\n\n10.8.1 Identify the sampling distribution model.\nWe will randomize to simulate the sampling distribution.\n\n\n10.8.2 Check the relevant conditions to ensure that model assumptions are met.\n\nRandom (for both groups)\n\nWe have no evidence that these are random samples of bank managers. We hope that they are representative. If the populations of interest are all bank managers in the U.S. evaluating either male candidates or female candidates, then we have some doubts as to how representative these samples are. It is likely that the bank managers were recruited from limited geographic areas based on the location of the researchers, and we know that geography could easily be a confounder for sex discrimination (because some areas of the country might be more prone to it than others). Despite our misgivings, we will proceed on with the analysis, but we will temper our expectations for grand, sweeping conclusions.\n\n10% (for both groups)\n\nRegardless of the intended populations, 24 bank managers evaluating male files and 24 bank managers evaluating female files are surely less than 10% of all bank managers under consideration."
  },
  {
    "objectID": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-ex-mechanics",
    "href": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-ex-mechanics",
    "title": "10  Hypothesis testing with randomization, Part 1",
    "section": "10.9 Mechanics",
    "text": "10.9 Mechanics\n\n10.9.1 Compute the test statistic.\nWe let infer do the work here:\n\nobs_diff &lt;- sex_discrimination %&gt;%\n    observe(decision ~ sex, success = \"promoted\",\n            stat = \"diff in props\", order = c(\"male\", \"female\"))\nobs_diff\n\nResponse: decision (factor)\nExplanatory: sex (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 0.292\n\n\nNote: obs_diff is a tibble, albeit a small one, having only one column and one row. That tibble is what we need to feed into the visualization later. However, for reporting the value by itself, we have to pull it out of the tibble. We will do this below using the pull function. See the inline code in the next subsection.\n\n\n10.9.2 Report the test statistic in context (when possible).\nThe observed difference in the proportion of promotion recommendations for male files versus female files is 0.2916667 (subtracting males minus females). Or, another way to say this: there is a 29.1666667% difference in the promotion rates between male files and female files.\n\n\n10.9.3 Plot the null distribution.\nNote: In this section, we will use the series of verbs from infer to generate all the information we need about the hypothesis test. We call that output decision_sex_test here, but you’ll want to change it to another name for a different test. The recommended pattern is response_predictor_test.\nDon’t forget to set the seed. We are using randomization to permute the values of the predictor variable in order to break any association that might exist in the data. This will allow us to explore the sampling distribution created under the assumption of the null hypothesis.\nWhen you get to the visualize step, leave the number of bins out. (Just type visualize() with empty parentheses.) If you determine that the default binning is not optimal, you can add back bins and experiment with the number. We know from the previous chapter that 9 bins is good here.\n\nset.seed(9999)\ndecision_sex_test &lt;- sex_discrimination %&gt;%\n    specify(decision ~ sex, success = \"promoted\") %&gt;%\n    hypothesize(null = \"independence\") %&gt;%\n    generate(reps = 1000, type = \"permute\") %&gt;%\n    calculate(stat = \"diff in props\", order = c(\"male\", \"female\"))\ndecision_sex_test\n\nResponse: decision (factor)\nExplanatory: sex (factor)\nNull Hypothesis: independence\n# A tibble: 1,000 × 2\n   replicate    stat\n       &lt;int&gt;   &lt;dbl&gt;\n 1         1 -0.0417\n 2         2  0.208 \n 3         3  0.0417\n 4         4 -0.125 \n 5         5 -0.0417\n 6         6 -0.208 \n 7         7 -0.208 \n 8         8  0.0417\n 9         9 -0.292 \n10        10  0.125 \n# ℹ 990 more rows\n\n\n\ndecision_sex_test %&gt;%\n    visualize(bins = 9) +\n    shade_p_value(obs_stat = obs_diff, direction = \"two-sided\")\n\n\n\n\n(You’ll note that there is light gray shading in both tails above. This is because we are conducting a two-sided test, which means that we’re interested in values that are more extreme than our observed difference in both directions.)\n\n\n10.9.4 Calculate the P-value.\n\nP &lt;- decision_sex_test %&gt;%\n  get_p_value(obs_stat = obs_diff, direction = \"two-sided\")\nP\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.048\n\n\nNote: as with the test statistic above, the P-value appears above in a 1x1 tibble. That’s fine for this step, but in the inline code below, we will need to use pull again to extract the value.\n\n\n10.9.5 Interpret the P-value as a probability given the null.\nThe P-value is 0.048. If there were no association between decision and sex, there would be a 4.8% chance of seeing data at least as extreme as we saw.\nSome important things here:\n\nWe include an interpretation for our P-value. Remember that the P-value is the probability—under the assumption of the null hypothesis—of seeing results as extreme or even more extreme than the data we saw.\nThe P-value is less than 0.05 (just barely). Remember that as we talk about the conclusion in the next section of the rubric."
  },
  {
    "objectID": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-ex-ht-conclusion",
    "href": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-ex-ht-conclusion",
    "title": "10  Hypothesis testing with randomization, Part 1",
    "section": "10.10 Conclusion",
    "text": "10.10 Conclusion\n\n10.10.1 State the statistical conclusion.\nWe reject the null hypothesis.\n\n\n10.10.2 State (but do not overstate) a contextually meaningful conclusion.\nThere is sufficient evidence to suggest that there is an an association between decision and sex in hiring branch managers for banks in the 1970s.\nNote: the easiest thing to do here is just restate the alternative hypothesis. If we reject the null, then we have sufficient evidence for the alternative hypothesis. If we fail to reject the null, we have insufficient evidence for the alternative hypothesis. Either way, though, this contextually meaningful conclusion is all about the alternative hypothesis.\n\n\n10.10.3 Express reservations or uncertainty about the generalizability of the conclusion.\nWe have some reservations about how generalizable this conclusion is due to the fact that we are lacking information about how representative our samples of bank managers were. We also point out that this experiment was conducted in the 1970s, so its conclusions are not valid for today.\nNote: This would also be the place to point out any possible sources of bias or confounding that might be present, especially for observational studies.\n\n\n10.10.4 Identify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.\nAs we rejected the null, we run the risk of committing a Type I error. It is possible that there is no association between decision and sex, but we’ve come across a sample in which male files were somehow more likely to be recommended for promotion.\n\nAfter writing up your conclusions and acknowledging the possibility of a Type I or Type II error, the hypothesis test is complete. (At least for now. In the future, we will add one more step of computing a confidence interval.)"
  },
  {
    "objectID": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-one-sided-two-sided",
    "href": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-one-sided-two-sided",
    "title": "10  Hypothesis testing with randomization, Part 1",
    "section": "10.11 More on one-sided and two-sided tests",
    "text": "10.11 More on one-sided and two-sided tests\nI want to emphasize again the difference between conducting a one-sided versus a two-sided test. You may recall that in “Introduction to simulation, Part 2”, we calculated this:\n\nset.seed(9999)\nsex_discrimination %&gt;%\n    specify(decision ~ sex, success = \"promoted\") %&gt;%\n    hypothesize(null = \"independence\") %&gt;%\n    generate(reps = 1000, type = \"permute\") %&gt;%\n    calculate(stat = \"diff in props\", order = c(\"male\", \"female\")) %&gt;%\n    get_p_value(obs_stat = obs_diff, direction = \"greater\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.024\n\n\nThe justification was that, back then, we already suspected that male files were more likely to be promoted, and it appears that our evidence (the test statistic, or our observed difference) was pretty far in that direction. (Actually, we may get a slightly different number each time. Remember that we are randomizing. Therefore, we won’t expect to get the exact same numbers each time.)\nBy way of contrast, in this chapter we computed the two-sided P-value:\n\nset.seed(9999)\nsex_discrimination %&gt;%\n    specify(decision ~ sex, success = \"promoted\") %&gt;%\n    hypothesize(null = \"independence\") %&gt;%\n    generate(reps = 1000, type = \"permute\") %&gt;%\n    calculate(stat = \"diff in props\", order = c(\"male\", \"female\")) %&gt;%\n    get_p_value(obs_stat = obs_diff, direction = \"two-sided\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.048\n\n\nThe only change to the code is the word “two-sided” (versus “greater”) in the last line.\nOur P-value in this chapter is twice as large as it could have been if we had run a one-sided test.\nDoubling the P-value might mean that it no longer falls under the significance threshold \\(\\alpha = 0.05\\) (although in this case, we still came in under 0.05). This raises an obvious question: why use two-sided tests at all? If the P-values are higher, that makes it less likely that we will reject the null, which means we won’t be able to prove our alternative hypothesis. Isn’t that a bad thing?\nAs a matter of fact, there are many researchers in the world who do think it’s a bad thing, and routinely do things like use one-sided tests to give them a better chance of getting small P-values. But this is not ethical. The point of research is to do good science, not prove your pet theories correct. There are many incentives in the world for a researcher to prove their theories correct (money, awards, career advancement, fame and recognition, legacy, etc.), but these should be secondary to the ultimate purpose of advancing knowledge. Sadly, many researchers out there have these priorities reversed. I do not claim that researchers set out to cheat; I suspect that the vast majority of researchers act in good faith. Nevertheless, the rewards associated with “successful” research cause cognitive biases that are hard to overcome. And “success” is often very narrowly defined as research that produces small P-values.\nA better approach is to be conservative. For example, a two-sided test is not only more conservative because it produces higher P-values, but also because it answers a more general question. That is, it is scientifically interesting when an association goes in either direction (e.g. more male promotions, but also possibly more female promotions). This is why we recommended above using two-sided tests by default, and only using a one-sided test when there is a very strong research hypothesis that justifies it."
  },
  {
    "objectID": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-fail-to-reject",
    "href": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-fail-to-reject",
    "title": "10  Hypothesis testing with randomization, Part 1",
    "section": "10.12 A reminder about failing to reject the null",
    "text": "10.12 A reminder about failing to reject the null\nIt’s also important to remember that when we fail to reject the null hypothesis, we are not saying that the null hypothesis is true. Neither are we saying it’s false. Failure to reject the null is really a failure to conclude anything at all. But rather than looking at it as a failure, a more productive viewpoint is to see it as an opportunity for more research, possibly with larger sample sizes.\nEven when we do reject the null, it is important not to see that as the end of the conversation. Too many times, a researcher publishes a “statistically significant” finding in a peer-reviewed journal, and then that result is taken as “Truth”. We should, instead, view statistical inference as incremental knowledge that works slowly to refine our state of scientific knowledge, as opposed to a collection of “facts” and “non-facts”."
  },
  {
    "objectID": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-your-turn",
    "href": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-your-turn",
    "title": "10  Hypothesis testing with randomization, Part 1",
    "section": "10.13 Your turn",
    "text": "10.13 Your turn\nNow it’s your turn to run a complete hypothesis test. Determine if males were admitted to the top six UC Berkeley grad programs at a higher rate than females. For purposes of this exercise, we will not take into account the Dept variable as we did in the last chapter when we discussed Simpson’s Paradox. But as that is a potential source of confounding, be sure to mention it in the part of the rubric where you discuss reservations about your conclusion.\nAs always, use a significance level of \\(\\alpha = 0.05\\).\nHere is the data import:\n\nucb_admit &lt;- read_csv(\"https://vectorposse.github.io/intro_stats/data/ucb_admit.csv\",\n                      col_types = list(\n                          Admit = col_factor(),\n                          Gender = col_factor(),\n                          Dept = col_factor()))\n\nI have copied the template below. You need to fill in each step. Some of the steps will be the same or similar to steps in the example above. It is perfectly okay to copy and paste R code, making the necessary changes. It is not okay to copy and paste text. You need to put everything into your own words. Also, don’t copy and paste the parts that are labeled as “Notes”. That is information to help you understand each step, but it’s not part of the statistical analysis itself.\nThe template below is exactly the same as in the Appendix up to the part about confidence intervals which we haven’t learned yet.\n\nExploratory data analysis\n\nUse data documentation (help files, code books, Google, etc.) to determine as much as possible about the data provenance and structure.\n\nPlease write up your answer here\n\n# Add code here to print the data\n\n\n# Add code here to glimpse the variables\n\n\n\n\nPrepare the data for analysis. [Not always necessary.]\n\n\n# Add code here to prepare the data for analysis.\n\n\n\n\nMake tables or plots to explore the data visually.\n\n\n# Add code here to make tables or plots.\n\n\n\n\n\nHypotheses\n\nIdentify the sample (or samples) and a reasonable population (or populations) of interest.\n\nPlease write up your answer here.\n\n\n\nExpress the null and alternative hypotheses as contextually meaningful full sentences.\n\n\\(H_{0}:\\) Null hypothesis goes here.\n\\(H_{A}:\\) Alternative hypothesis goes here.\n\n\n\nExpress the null and alternative hypotheses in symbols (when possible).\n\n\\(H_{0}: math\\)\n\\(H_{A}: math\\)\n\n\n\n\nModel\n\nIdentify the sampling distribution model.\n\nPlease write up your answer here.\n\n\n\nCheck the relevant conditions to ensure that model assumptions are met.\n\nPlease write up your answer here. (Some conditions may require R code as well.)\n\n\n\n\nMechanics\n\nCompute the test statistic.\n\n\n# Add code here to compute the test statistic.\n\n\n\n\nReport the test statistic in context (when possible).\n\nPlease write up your answer here.\n\n\n\nPlot the null distribution.\n\n\nset.seed(9999)\n# Add code here to simulate the null distribution.\n# Run 1000 reps like in the earlier example.\n\n\n# Add code here to plot the null distribution.\n\n\n\n\nCalculate the P-value.\n\n\n# Add code here to calculate the P-value.\n\n\n\n\nInterpret the P-value as a probability given the null.\n\nPlease write up your answer here.\n\n\n\n\nConclusion\n\nState the statistical conclusion.\n\nPlease write up your answer here.\n\n\n\nState (but do not overstate) a contextually meaningful conclusion.\n\nPlease write up your answer here.\n\n\n\nExpress reservations or uncertainty about the generalizability of the conclusion.\n\nPlease write up your answer here.\n\n\n\nIdentify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.\n\nPlease write up your answer here."
  },
  {
    "objectID": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-conclusion",
    "href": "10-hypothesis_testing_with_randomization_1-web.html#hypothesis1-conclusion",
    "title": "10  Hypothesis testing with randomization, Part 1",
    "section": "10.14 Conclusion",
    "text": "10.14 Conclusion\nA hypothesis test is a formal set of steps—a procedure, if you will—for implementing the logic of inference. We take a skeptical position and assume a null hypothesis in contrast to the question of interest, the alternative hypothesis. We build a model under the assumption of the null hypothesis to see if our data is consistent with the null (in which case we fail to reject the null) or unusual/rare relative to the null (in which case we reject the null). We always work under the assumption of the null so that we can convince a skeptical audience using evidence. We also take care to acknowledge that statistical procedures can be wrong, and not to put too much credence in the results of any single set of data or single hypothesis test.\n\n10.14.1 Preparing and submitting your assignment\n\nFrom the “Run” menu, select “Restart R and Run All Chunks”.\nDeal with any code errors that crop up. Repeat steps 1–-2 until there are no more code errors.\nSpell check your document by clicking the icon with “ABC” and a check mark.\nHit the “Preview” button one last time to generate the final draft of the .nb.html file.\nProofread the HTML file carefully. If there are errors, go back and fix them, then repeat steps 1–5 again.\n\nIf you have completed this chapter as part of a statistics course, follow the directions you receive from your professor to submit your assignment."
  },
  {
    "objectID": "10-hypothesis_testing_with_randomization_1-web.html#footnotes",
    "href": "10-hypothesis_testing_with_randomization_1-web.html#footnotes",
    "title": "10  Hypothesis testing with randomization, Part 1",
    "section": "",
    "text": "This is not technically the most mathematically appropriate thing to do, but it’s a reasonable approximation in many common situations.↩︎"
  },
  {
    "objectID": "11-hypothesis_testing_with_randomization_2-web.html#hypothesis2-intro",
    "href": "11-hypothesis_testing_with_randomization_2-web.html#hypothesis2-intro",
    "title": "11  Hypothesis testing with randomization, Part 2",
    "section": "11.1 Introduction",
    "text": "11.1 Introduction\nNow that we have learned about hypothesis testing, we’ll explore a different example. Although the rubric for performing the hypothesis test will not change, the individual steps will be implemented in a different way due to the research question we’re asking and the type of data used to answer it.\n\n11.1.1 Install new packages\nIf you are using RStudio Workbench, you do not need to install any packages. (Any packages you need should already be installed by the server administrators.)\nIf you are using R and RStudio on your own machine instead of accessing RStudio Workbench through a browser, you’ll need to type the following command at the Console:\ninstall.packages(\"MASS\")\n\n\n11.1.2 Download the R notebook file\nCheck the upper-right corner in RStudio to make sure you’re in your intro_stats project. Then click on the following link to download this chapter as an R notebook file (.Rmd).\nhttps://vectorposse.github.io/intro_stats/chapter_downloads/11-hypothesis_testing_with_randomization_2.Rmd\nOnce the file is downloaded, move it to your project folder in RStudio and open it there.\n\n\n11.1.3 Restart R and run all chunks\nIn RStudio, select “Restart R and Run All Chunks” from the “Run” menu."
  },
  {
    "objectID": "11-hypothesis_testing_with_randomization_2-web.html#hypothesis2-load",
    "href": "11-hypothesis_testing_with_randomization_2-web.html#hypothesis2-load",
    "title": "11  Hypothesis testing with randomization, Part 2",
    "section": "11.2 Load packages",
    "text": "11.2 Load packages\nIn additional to tidyverse and janitor, we load the MASS package to access the Melanoma data on patients in Denmark with malignant melanoma, and the infer package for inference tools.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(infer)"
  },
  {
    "objectID": "11-hypothesis_testing_with_randomization_2-web.html#hypothesis2-question",
    "href": "11-hypothesis_testing_with_randomization_2-web.html#hypothesis2-question",
    "title": "11  Hypothesis testing with randomization, Part 2",
    "section": "11.3 Our research question",
    "text": "11.3 Our research question\nWe know that certain types of cancer are more common among females or males. Is there a sex bias among patients with malignant melanoma?\nLet’s jump into the “Exploratory data analysis” part of the rubric first."
  },
  {
    "objectID": "11-hypothesis_testing_with_randomization_2-web.html#hypothesis2ex-eda",
    "href": "11-hypothesis_testing_with_randomization_2-web.html#hypothesis2ex-eda",
    "title": "11  Hypothesis testing with randomization, Part 2",
    "section": "11.4 Exploratory data analysis",
    "text": "11.4 Exploratory data analysis\n\n11.4.1 Use data documentation (help files, code books, Google, etc.) to determine as much as possible about the data provenance and structure.\nYou can look at the help file by typing ?Melanoma at the Console. However, do not put that command here in a code chunk. The R Notebook has no way of displaying a help file when it’s processed. Be careful: there’s another data set called melanoma with a lower-case “m”. Make sure you are using an uppercase “M”.\nThere is a reference at the bottom of the help file.\n\nExercise 1\nUsing the reference in the help file, do an internet search to find the source of this data. How can you tell that this reference is not, in fact, a reference to a study of cancer patients in Denmark?\n\nPlease write up your answer here.\n\n\nFrom the exercise above, we can see that it will be very difficult, if not impossible, to discover anything useful about the true provenance of the data (unless you happen to have a copy of that textbook, which in theory provided another more primary source). We will not know, for example, how the data was collected and if the patients consented to having their data shared publicly. The data is suitably anonymized, though, so we don’t have any serious concerns about the privacy of the data. Having said that, if a condition is rare enough, a dedicated research can often “de-anonymize” data by cross-referencing information in the data to other kinds of public records. But melanoma is not particularly rare. At any rate, all we can do is assume that the textbook authors obtained the data from a source that used proper procedures for collecting and handling the data.\nWe print the data frame:\n\nMelanoma\n\n    time status sex age year thickness ulcer\n1     10      3   1  76 1972      6.76     1\n2     30      3   1  56 1968      0.65     0\n3     35      2   1  41 1977      1.34     0\n4     99      3   0  71 1968      2.90     0\n5    185      1   1  52 1965     12.08     1\n6    204      1   1  28 1971      4.84     1\n7    210      1   1  77 1972      5.16     1\n8    232      3   0  60 1974      3.22     1\n9    232      1   1  49 1968     12.88     1\n10   279      1   0  68 1971      7.41     1\n11   295      1   0  53 1969      4.19     1\n12   355      3   0  64 1972      0.16     1\n13   386      1   0  68 1965      3.87     1\n14   426      1   1  63 1970      4.84     1\n15   469      1   0  14 1969      2.42     1\n16   493      3   1  72 1971     12.56     1\n17   529      1   1  46 1971      5.80     1\n18   621      1   1  72 1972      7.06     1\n19   629      1   1  95 1968      5.48     1\n20   659      1   1  54 1972      7.73     1\n21   667      1   0  89 1968     13.85     1\n22   718      1   1  25 1967      2.34     1\n23   752      1   1  37 1973      4.19     1\n24   779      1   1  43 1967      4.04     1\n25   793      1   1  68 1970      4.84     1\n26   817      1   0  67 1966      0.32     0\n27   826      3   0  86 1965      8.54     1\n28   833      1   0  56 1971      2.58     1\n29   858      1   0  16 1967      3.56     0\n30   869      1   0  42 1965      3.54     0\n31   872      1   0  65 1968      0.97     0\n32   967      1   1  52 1970      4.83     1\n33   977      1   1  58 1967      1.62     1\n34   982      1   0  60 1970      6.44     1\n35  1041      1   1  68 1967     14.66     0\n36  1055      1   0  75 1967      2.58     1\n37  1062      1   1  19 1966      3.87     1\n38  1075      1   1  66 1971      3.54     1\n39  1156      1   0  56 1970      1.34     1\n40  1228      1   1  46 1973      2.24     1\n41  1252      1   0  58 1971      3.87     1\n42  1271      1   0  74 1971      3.54     1\n43  1312      1   0  65 1970     17.42     1\n44  1427      3   1  64 1972      1.29     0\n45  1435      1   1  27 1969      3.22     0\n46  1499      2   1  73 1973      1.29     0\n47  1506      1   1  56 1970      4.51     1\n48  1508      2   1  63 1973      8.38     1\n49  1510      2   0  69 1973      1.94     0\n50  1512      2   0  77 1973      0.16     0\n51  1516      1   1  80 1968      2.58     1\n52  1525      3   0  76 1970      1.29     1\n53  1542      2   0  65 1973      0.16     0\n54  1548      1   0  61 1972      1.62     0\n55  1557      2   0  26 1973      1.29     0\n56  1560      1   0  57 1973      2.10     0\n57  1563      2   0  45 1973      0.32     0\n58  1584      1   1  31 1970      0.81     0\n59  1605      2   0  36 1973      1.13     0\n60  1621      1   0  46 1972      5.16     1\n61  1627      2   0  43 1973      1.62     0\n62  1634      2   0  68 1973      1.37     0\n63  1641      2   1  57 1973      0.24     0\n64  1641      2   0  57 1973      0.81     0\n65  1648      2   0  55 1973      1.29     0\n66  1652      2   0  58 1973      1.29     0\n67  1654      2   1  20 1973      0.97     0\n68  1654      2   0  67 1973      1.13     0\n69  1667      1   0  44 1971      5.80     1\n70  1678      2   0  59 1973      1.29     0\n71  1685      2   0  32 1973      0.48     0\n72  1690      1   1  83 1971      1.62     0\n73  1710      2   0  55 1973      2.26     0\n74  1710      2   1  15 1973      0.58     0\n75  1726      1   0  58 1970      0.97     1\n76  1745      2   0  47 1973      2.58     1\n77  1762      2   0  54 1973      0.81     0\n78  1779      2   1  55 1973      3.54     1\n79  1787      2   1  38 1973      0.97     0\n80  1787      2   0  41 1973      1.78     1\n81  1793      2   0  56 1973      1.94     0\n82  1804      2   0  48 1973      1.29     0\n83  1812      2   1  44 1973      3.22     1\n84  1836      2   0  70 1972      1.53     0\n85  1839      2   0  40 1972      1.29     0\n86  1839      2   1  53 1972      1.62     1\n87  1854      2   0  65 1972      1.62     1\n88  1856      2   1  54 1972      0.32     0\n89  1860      3   1  71 1969      4.84     1\n90  1864      2   0  49 1972      1.29     0\n91  1899      2   0  55 1972      0.97     0\n92  1914      2   0  69 1972      3.06     0\n93  1919      2   1  83 1972      3.54     0\n94  1920      2   1  60 1972      1.62     1\n95  1927      2   1  40 1972      2.58     1\n96  1933      1   0  77 1972      1.94     0\n97  1942      2   0  35 1972      0.81     0\n98  1955      2   0  46 1972      7.73     1\n99  1956      2   0  34 1972      0.97     0\n100 1958      2   0  69 1972     12.88     0\n101 1963      2   0  60 1972      2.58     0\n102 1970      2   1  84 1972      4.09     1\n103 2005      2   0  66 1972      0.64     0\n104 2007      2   1  56 1972      0.97     0\n105 2011      2   0  75 1972      3.22     1\n106 2024      2   0  36 1972      1.62     0\n107 2028      2   1  52 1972      3.87     1\n108 2038      2   0  58 1972      0.32     1\n109 2056      2   0  39 1972      0.32     0\n110 2059      2   1  68 1972      3.22     1\n111 2061      1   1  71 1968      2.26     0\n112 2062      1   0  52 1965      3.06     0\n113 2075      2   1  55 1972      2.58     1\n114 2085      3   0  66 1970      0.65     0\n115 2102      2   1  35 1972      1.13     0\n116 2103      1   1  44 1966      0.81     0\n117 2104      2   0  72 1972      0.97     0\n118 2108      1   0  58 1969      1.76     1\n119 2112      2   0  54 1972      1.94     1\n120 2150      2   0  33 1972      0.65     0\n121 2156      2   0  45 1972      0.97     0\n122 2165      2   1  62 1972      5.64     0\n123 2209      2   0  72 1971      9.66     0\n124 2227      2   0  51 1971      0.10     0\n125 2227      2   1  77 1971      5.48     1\n126 2256      1   0  43 1971      2.26     1\n127 2264      2   0  65 1971      4.83     1\n128 2339      2   0  63 1971      0.97     0\n129 2361      2   1  60 1971      0.97     0\n130 2387      2   0  50 1971      5.16     1\n131 2388      1   1  40 1966      0.81     0\n132 2403      2   0  67 1971      2.90     1\n133 2426      2   0  69 1971      3.87     0\n134 2426      2   0  74 1971      1.94     1\n135 2431      2   0  49 1971      0.16     0\n136 2460      2   0  47 1971      0.64     0\n137 2467      1   0  42 1965      2.26     1\n138 2492      2   0  54 1971      1.45     0\n139 2493      2   1  72 1971      4.82     1\n140 2521      2   0  45 1971      1.29     1\n141 2542      2   1  67 1971      7.89     1\n142 2559      2   0  48 1970      0.81     1\n143 2565      1   1  34 1970      3.54     1\n144 2570      2   0  44 1970      1.29     0\n145 2660      2   0  31 1970      0.64     0\n146 2666      2   0  42 1970      3.22     1\n147 2676      2   0  24 1970      1.45     1\n148 2738      2   0  58 1970      0.48     0\n149 2782      1   1  78 1969      1.94     0\n150 2787      2   1  62 1970      0.16     0\n151 2984      2   1  70 1969      0.16     0\n152 3032      2   0  35 1969      1.29     0\n153 3040      2   0  61 1969      1.94     0\n154 3042      1   0  54 1967      3.54     1\n155 3067      2   0  29 1969      0.81     0\n156 3079      2   1  64 1969      0.65     0\n157 3101      2   1  47 1969      7.09     0\n158 3144      2   1  62 1969      0.16     0\n159 3152      2   0  32 1969      1.62     0\n160 3154      3   1  49 1969      1.62     0\n161 3180      2   0  25 1969      1.29     0\n162 3182      3   1  49 1966      6.12     0\n163 3185      2   0  64 1969      0.48     0\n164 3199      2   0  36 1969      0.64     0\n165 3228      2   0  58 1969      3.22     1\n166 3229      2   0  37 1969      1.94     0\n167 3278      2   1  54 1969      2.58     0\n168 3297      2   0  61 1968      2.58     1\n169 3328      2   1  31 1968      0.81     0\n170 3330      2   1  61 1968      0.81     1\n171 3338      1   0  60 1967      3.22     1\n172 3383      2   0  43 1968      0.32     0\n173 3384      2   0  68 1968      3.22     1\n174 3385      2   0   4 1968      2.74     0\n175 3388      2   1  60 1968      4.84     1\n176 3402      2   1  50 1968      1.62     0\n177 3441      2   0  20 1968      0.65     0\n178 3458      3   0  54 1967      1.45     0\n179 3459      2   0  29 1968      0.65     0\n180 3459      2   1  56 1968      1.29     1\n181 3476      2   0  60 1968      1.62     0\n182 3523      2   0  46 1968      3.54     0\n183 3667      2   0  42 1967      3.22     0\n184 3695      2   0  34 1967      0.65     0\n185 3695      2   0  56 1967      1.03     0\n186 3776      2   1  12 1967      7.09     1\n187 3776      2   0  21 1967      1.29     1\n188 3830      2   1  46 1967      0.65     0\n189 3856      2   0  49 1967      1.78     0\n190 3872      2   0  35 1967     12.24     1\n191 3909      2   1  42 1967      8.06     1\n192 3968      2   0  47 1967      0.81     0\n193 4001      2   0  69 1967      2.10     0\n194 4103      2   0  52 1966      3.87     0\n195 4119      2   1  52 1966      0.65     0\n196 4124      2   0  30 1966      1.94     1\n197 4207      2   1  22 1966      0.65     0\n198 4310      2   1  55 1966      2.10     0\n199 4390      2   0  26 1965      1.94     1\n200 4479      2   0  19 1965      1.13     1\n201 4492      2   1  29 1965      7.06     1\n202 4668      2   0  40 1965      6.12     0\n203 4688      2   0  42 1965      0.48     0\n204 4926      2   0  50 1964      2.26     0\n205 5565      2   0  41 1962      2.90     0\n\n\nUse glimpse to examine the structure of the data:\n\nglimpse(Melanoma)\n\nRows: 205\nColumns: 7\n$ time      &lt;int&gt; 10, 30, 35, 99, 185, 204, 210, 232, 232, 279, 295, 355, 386,…\n$ status    &lt;int&gt; 3, 3, 2, 3, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, …\n$ sex       &lt;int&gt; 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, …\n$ age       &lt;int&gt; 76, 56, 41, 71, 52, 28, 77, 60, 49, 68, 53, 64, 68, 63, 14, …\n$ year      &lt;int&gt; 1972, 1968, 1977, 1968, 1965, 1971, 1972, 1974, 1968, 1971, …\n$ thickness &lt;dbl&gt; 6.76, 0.65, 1.34, 2.90, 12.08, 4.84, 5.16, 3.22, 12.88, 7.41…\n$ ulcer     &lt;int&gt; 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n\n\n\n\n\n11.4.2 Prepare the data for analysis.\nIt appears that sex is coded as an integer. You will recall that we need to convert it to a factor variable since it is categorical, not numerical.\n\nExercise 2\nAccording to the help file, which number corresponds to which sex?\n\nPlease write up your answer here.\n\n\nWe can convert a numerical variable a couple of different ways. In Chapter 3, we used the as_factor command. That command works fine, but it doesn’t give you a way to change the levels of the variable. In other words, if we used as_factor here, we would get a factor variable that still contained zeroes and ones.\nInstead, we will use the factor command. It allows us to manually relabel the levels. The levels argument requires a vector (with c) of the current levels, and the labels argument requires a vector listing the new names you want to assign, as follows:\n\nMelanoma &lt;- Melanoma %&gt;%\n    mutate(sex_fct = factor(sex, levels = c(0, 1), labels = c(\"female\", \"male\")))\nglimpse(Melanoma)\n\nRows: 205\nColumns: 8\n$ time      &lt;int&gt; 10, 30, 35, 99, 185, 204, 210, 232, 232, 279, 295, 355, 386,…\n$ status    &lt;int&gt; 3, 3, 2, 3, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, …\n$ sex       &lt;int&gt; 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, …\n$ age       &lt;int&gt; 76, 56, 41, 71, 52, 28, 77, 60, 49, 68, 53, 64, 68, 63, 14, …\n$ year      &lt;int&gt; 1972, 1968, 1977, 1968, 1965, 1971, 1972, 1974, 1968, 1971, …\n$ thickness &lt;dbl&gt; 6.76, 0.65, 1.34, 2.90, 12.08, 4.84, 5.16, 3.22, 12.88, 7.41…\n$ ulcer     &lt;int&gt; 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ sex_fct   &lt;fct&gt; male, male, male, female, male, male, male, female, male, fe…\n\n\nYou should check to make sure the first few entries of sex_fct agree with the numbers in the sex variable according to the labels explained in the help file. (If not, it means that you put the levels in one order and the labels in a different order.)\n\n\n\n11.4.3 Make tables or plots to explore the data visually.\nWe only have one categorical variable, so we only need a frequency table. Since we are concerned with proportions, we’ll also look at a relative frequency table which the tabyl command provides for free.\n\ntabyl(Melanoma, sex_fct) %&gt;%\n    adorn_totals()\n\n sex_fct   n   percent\n  female 126 0.6146341\n    male  79 0.3853659\n   Total 205 1.0000000"
  },
  {
    "objectID": "11-hypothesis_testing_with_randomization_2-web.html#hypothesis2-logic",
    "href": "11-hypothesis_testing_with_randomization_2-web.html#hypothesis2-logic",
    "title": "11  Hypothesis testing with randomization, Part 2",
    "section": "11.5 The logic of inference and randomization",
    "text": "11.5 The logic of inference and randomization\nThis is a good place to pause and remember why statistical inference is important. There are certainly more females than males in this data set. So why don’t we just show the table above, declare females are more likely to have malignant melanoma, and then go home?\nThink back to coin flips. Even though there was a 50% chance of seeing heads, did that mean that exactly half of our flips came up heads? No. We have to acknowledge sampling variability: even if the truth were 50%, when w sample, we could accidentally get more or less than 50%, just by pure chance alone. Perhaps these 205 patients just happen to have more females than average.\nThe key, then, is to figure out if 61.5% is significantly larger than 50%, or if a number like 61.5% (or one even more extreme) could easily come about from random chance.\nAs we know from the last chapter, we can run a formal hypothesis test to find out. As we do so, make note of the things that are the same and the things that have changed from the last hypothesis tests you ran. For example, we are not comparing two groups anymore. We have one group of patients, and all we’re doing is measuring the percentage of this group that is female. It’s tempting to think that we’re comparing males and females, but that’s not the case. We are not using sex to divide our data into two groups for the purpose of exploring whether some other variable differs between men and women. We just have one sample. “Female” and “Male” are simply categories in a single categorical variable. Also, because we are only asking about one variable (sex_fct), the mathematical form of the hypotheses will look a little different.\nBecause this is no longer a question about two variables being independent or associated, the “permuting” idea we’ve been using no longer makes sense. So what does make sense?\nIt helps to start by figuring out what our null hypothesis is. Remember, our question of interest is whether there is a sex bias in malignant melanoma. In other words, are there more or fewer females than males with malignant melanoma? As this is our research question, it will be the alternative hypothesis. So what is the null? What is the “default” situation in which nothing interesting is going on? Well, there would be no sex bias. In other words, there would be the same number of females and males with malignant melanoma. Or another way of saying that—with respect to the “success” condition of being female that we discussed earlier—is that females comprise 50% of all patients with malignant melanoma.\nOkay, given our philosophy about the null hypothesis, let’s take the skeptical position and assume that, indeed, 50% of all malignant melanoma patients in our population are female. Then let’s take a sample of 205 patients. We can’t get exactly 50% females from a sample of 205 (that would be 102.5 females!), so what numbers can we get?\nRandomization will tell us. What kind of randomization? As we come across each patient in our sample, there is a 50% chance of them being female. So instead of sampling real patients, what if we just flipped a coin? A simulated coin flip will come up heads just as often as our patients will be female under the assumption of the null.\nThis brings us full circle, back to the first randomization idea we explored. We can simulate coin flips, graph our results, and calculate a P-value. More specifically, we’ll flip a coin 205 times to represent sampling 205 patients. Then we’ll repeat this procedure a bunch of times and establish a range of plausible percentages that can come about by chance from this procedure. Instead of doing coin flips with the rflip command as we did then, however, we’ll use our new favorite friend, the infer package.\nLet’s dive back into the remaining steps of the formal hypothesis test."
  },
  {
    "objectID": "11-hypothesis_testing_with_randomization_2-web.html#hypothesis2-ex-hypotheses",
    "href": "11-hypothesis_testing_with_randomization_2-web.html#hypothesis2-ex-hypotheses",
    "title": "11  Hypothesis testing with randomization, Part 2",
    "section": "11.6 Hypotheses",
    "text": "11.6 Hypotheses\n\n11.6.1 Identify the sample (or samples) and a reasonable population (or populations) of interest.\nThe sample consists of 205 patients from Denmark with malignant melanoma. Our population is presumably all patients with malignant melanoma, although in checking conditions below, we’ll take care to discuss whether patients in Denmark are representative of patients elsewhere.\n\n\n11.6.2 Express the null and alternative hypotheses as contextually meaningful full sentences.\n\\(H_{0}:\\) Half of malignant melanoma patients are female.\n\\(H_{A}:\\) There is a sex bias among patients with malignant melanoma (meaning that females are either over-represented or under-represented).\n\n\n11.6.3 Express the null and alternative hypotheses in symbols (when possible).\n\\(H_{0}: p_{female} = 0.5\\)\n\\(H_{A}: p_{female} \\neq 0.5\\)"
  },
  {
    "objectID": "11-hypothesis_testing_with_randomization_2-web.html#hypothesis2-ex-model",
    "href": "11-hypothesis_testing_with_randomization_2-web.html#hypothesis2-ex-model",
    "title": "11  Hypothesis testing with randomization, Part 2",
    "section": "11.7 Model",
    "text": "11.7 Model\n\n11.7.1 Identify the sampling distribution model.\nWe will randomize to simulate the sampling distribution.\n\n\n11.7.2 Check the relevant conditions to ensure that model assumptions are met.\n\nRandom\n\nAs mentioned above, these 205 patients are not a random sample of all people with malignant melanoma. We don’t even have any evidence that they are a random sample of melanoma patients in Denmark. Without such evidence, we have to hope that these 205 patients are representative of all patients who have malignant melanoma. Unless there’s something special about Danes in terms of their genetics or diet or something like that, one could imagine that their physiology makes them just as susceptible to melanoma as anyone else. More specifically, though, our question is about females and males getting malignant melanoma. Perhaps there are more female sunbathers in Denmark than in other countries. That might make Danes unrepresentative in terms of the gender balance among melanoma patients. We should be cautious in interpreting any conclusion we might reach in light of these doubts.\n\n10%\n\nWhether in Denmark or not, given that melanoma is a fairly common form of cancer, I assume 205 is less than 10% of all patients with malignant melanoma."
  },
  {
    "objectID": "11-hypothesis_testing_with_randomization_2-web.html#hypothesis2-ex-mechanics",
    "href": "11-hypothesis_testing_with_randomization_2-web.html#hypothesis2-ex-mechanics",
    "title": "11  Hypothesis testing with randomization, Part 2",
    "section": "11.8 Mechanics",
    "text": "11.8 Mechanics\n\n11.8.1 Compute the test statistic.\n\nfemale_prop &lt;- Melanoma %&gt;%\n    observe(response = sex_fct, success = \"female\",\n            stat = \"prop\")\nfemale_prop\n\nResponse: sex_fct (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 0.615\n\n\nNote: Pay close attention to the difference in the observe command above. Unlike in the last chapter, we don’t have any tildes. That’s because there are not two variables involved. There is only one variable, which observe needs to see as the “response” variable. (Don’t forget to use the factor version sex_fct and not sex!) We still have to specify a “success” condition. Since the hypotheses are about measuring females, we have to tell observe to calculate the proportion of females. Finally, the stat is no longer “diff in props” There are not two proportions with which to find a difference. There is just one proportion, hence, “prop”.\n\n\n11.8.2 Report the test statistic in context (when possible).\nThe observed percentage of females with melanoma in our sample is 61.4634146%.\nNote: As explained in the last chapter, we have to use pull to pull out the number from the female_prop tibble.\n\n\n11.8.3 Plot the null distribution.\nSince this is the first step for which we need the simulated values, it will be convenient to run the simulation here. We’ll need to set the seed as well.\n\nset.seed(42)\nmelanoma_test &lt;- Melanoma %&gt;%\n    specify(response = sex_fct, success = \"female\") %&gt;%\n    hypothesize(null = \"point\", p = 0.5) %&gt;%\n    generate(reps = 1000, type = \"draw\") %&gt;%\n    calculate(stat = \"prop\")\nmelanoma_test\n\nResponse: sex_fct (factor)\nNull Hypothesis: point\n# A tibble: 1,000 × 2\n   replicate  stat\n   &lt;fct&gt;     &lt;dbl&gt;\n 1 1         0.444\n 2 2         0.585\n 3 3         0.551\n 4 4         0.502\n 5 5         0.561\n 6 6         0.493\n 7 7         0.527\n 8 8         0.488\n 9 9         0.512\n10 10        0.454\n# ℹ 990 more rows\n\n\nThis list of proportions is the sampling distribution. It represents possible sample proportions of females with melanoma under the assumption that the null is true. In other words, even if the “true” proportion of female melanoma patients were 0.5, these are all values that can result from random samples.\nIn the hypothesize command, we use “point” to tell infer that we want the null to be centered at the point 0.5. In the generate command, we need to specify the type as “draw” instead of “permute”. We are not shuffling any values here; we are “drawing” values from a probability distribution like coin flips. Everything else in the command is pretty self-explanatory.\nThe value of our test statistic, female_prop, is 0.6146341. It appears in the right tail:\n\nmelanoma_test %&gt;%\n    visualize() +\n    shade_p_value(obs_stat = female_prop, direction = \"two-sided\")\n\n\n\n\nAlthough the line only appears on the right, keep in mind that we are conducting a two-sided test, so we are interested in values more extreme than the red line on the right, but also more extreme than a similarly placed line on the left.\n\nExercise 3\nThe red line sits at about 0.615. If you were to draw a red line on the above histogram that represented a value equally distant from 0.5, but on the left instead of the right, where would that line be? Do a little arithmetic to figure it out and show your work.\n\nPlease write up your answer here.\n\n\n\n\n11.8.4 Calculate the P-value.\n\nmelanoma_test %&gt;%\n    get_p_value(obs_stat = female_prop, direction = \"two-sided\")\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an\napproximation based on the number of `reps` chosen in the `generate()` step.\nSee `?get_p_value()` for more information.\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\nThe P-value appears to be zero. Indeed, among the 1000 simulated values, we saw none that exceeded 0.615 and none that were less than 0.385. However, a true P-value can never be zero. If you did millions or billions of simulations (please don’t try!), surely there would be one or two with even more extreme values. In cases when the P-value is really, really tiny, it is traditional to report \\(P &lt; 0.001\\). It is incorrect to say \\(P = 0\\).\n\n\n11.8.5 Interpret the P-value as a probability given the null.\n\\(P &lt; 0.001\\). If there were no sex bias in malignant melanoma patients, there would be less than a 0.1% chance of seeing a percentage of females at least as extreme as the one we saw in our data.\nNote: Don’t forget to interpret the P-value in a contextually meaningful way. The P-value is the probability under the assumption of the null hypothesis of seeing data at least as extreme as the data we saw. In this context, that means that if we assume 50% of patients are female, it would be extremely rare to see more than 61.5% or less than 38.5% females in a sample of size 205."
  },
  {
    "objectID": "11-hypothesis_testing_with_randomization_2-web.html#hypothesis2-ex-ht-conclusion",
    "href": "11-hypothesis_testing_with_randomization_2-web.html#hypothesis2-ex-ht-conclusion",
    "title": "11  Hypothesis testing with randomization, Part 2",
    "section": "11.9 Conclusion",
    "text": "11.9 Conclusion\n\n11.9.1 State the statistical conclusion.\nWe reject the null hypothesis.\n\n\n11.9.2 State (but do not overstate) a contextually meaningful conclusion.\nThere is sufficient evidence that there is a sex bias in patients who suffer from malignant melanoma.\n\n\n11.9.3 Express reservations or uncertainty about the generalizability of the conclusion.\nWe have no idea how these patients were sampled. Are these all the patients in Denmark with malignant melanoma over a certain period of time? Were they part of a convenience sample? As a result of our uncertainly about the sampling process, we can’t be sure if the results generalize to a larger population, either in Denmark or especially outside of Denmark.\n\nExercise 4\nCan you find on the internet any evidence that females do indeed suffer from malignant melanoma more often than males (not just in Denmark, but anywhere)?\n\nPlease write up your answer here.\n\n\n\n\n11.9.4 Identify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.\nAs we rejected the null, we run the risk of making a Type I error. If we have made such an error, that would mean that patients with malignant melanoma are equally likely to be male or female, but that we got a sample with an unusual number of female patients."
  },
  {
    "objectID": "11-hypothesis_testing_with_randomization_2-web.html#hypothesis2-your-turn",
    "href": "11-hypothesis_testing_with_randomization_2-web.html#hypothesis2-your-turn",
    "title": "11  Hypothesis testing with randomization, Part 2",
    "section": "11.10 Your turn",
    "text": "11.10 Your turn\nDetermine if the percentage of patients in Denmark with malignant melanoma who also have an ulcerated tumor (measured with the ulcer variable) is significantly different from 50%.\nAs before, you have the outline of the rubric for inference below. Some of the steps will be the same or similar to steps in the example above. It is perfectly okay to copy and paste R code, making the necessary changes. It is not okay to copy and paste text. You need to put everything into your own words.\nThe template below is exactly the same as in the appendix (Rubric for inference) up to the part about confidence intervals which we haven’t learned yet.\n\nExploratory data analysis\n\nUse data documentation (help files, code books, Google, etc.) to determine as much as possible about the data provenance and structure.\n\n\n# Add code here to understand the data.\n\n\n\n\nPrepare the data for analysis. [Not always necessary.]\n\n\n# Add code here to prepare the data for analysis.\n\n\n\n\nMake tables or plots to explore the data visually.\n\n\n# Add code here to make tables or plots.\n\n\n\n\n\nHypotheses\n\nIdentify the sample (or samples) and a reasonable population (or populations) of interest.\n\nPlease write up your answer here.\n\n\n\nExpress the null and alternative hypotheses as contextually meaningful full sentences.\n\n\\(H_{0}:\\) Null hypothesis goes here.\n\\(H_{A}:\\) Alternative hypothesis goes here.\n\n\n\nExpress the null and alternative hypotheses in symbols (when possible).\n\n\\(H_{0}: math\\)\n\\(H_{A}: math\\)\n\n\n\n\nModel\n\nIdentify the sampling distribution model.\n\nPlease write up your answer here.\n\n\n\nCheck the relevant conditions to ensure that model assumptions are met.\n\nPlease write up your answer here. (Some conditions may require R code as well.)\n\n\n\n\nMechanics\n\nCompute the test statistic.\n\n\n# Add code here to compute the test statistic.\n\n\n\n\nReport the test statistic in context (when possible).\n\nPlease write up your answer here.\n\n\n\nPlot the null distribution.\n\n\nset.seed(42)\n# Add code here to simulate the null distribution.\n# Run 1000 simulations like in the earlier example.\n\n\n# Add code here to plot the null distribution.\n\n\n\n\nCalculate the P-value.\n\n\n# Add code here to calculate the P-value.\n\n\n\n\nInterpret the P-value as a probability given the null.\n\nPlease write up your answer here.\n\n\n\n\nConclusion\n\nState the statistical conclusion.\n\nPlease write up your answer here.\n\n\n\nState (but do not overstate) a contextually meaningful conclusion.\n\nPlease write up your answer here.\n\n\n\nExpress reservations or uncertainty about the generalizability of the conclusion.\n\nPlease write up your answer here.\n\n\n\nIdentify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.\n\nPlease write up your answer here."
  },
  {
    "objectID": "11-hypothesis_testing_with_randomization_2-web.html#hypothesis2-conclusion",
    "href": "11-hypothesis_testing_with_randomization_2-web.html#hypothesis2-conclusion",
    "title": "11  Hypothesis testing with randomization, Part 2",
    "section": "11.11 Conclusion",
    "text": "11.11 Conclusion\nNow you have seen two fully-worked examples of hypothesis tests using randomization, and you have created two more examples on your own. Hopefully, the logic of inference and the process of running a formal hypothesis test are starting to make sense.\nKeep in mind that the outline of steps will not change. However, the way each step is carried out will vary from problem to problem. Not only does the context change (one example involved sex discrimination, the other melanoma patients), but the statistics you compute also change (one example compared proportions from two samples and the other only had one proportion from a single sample). Pay close attention to the research question and the data that will be used to answer that question. That will be the only information you have to help you know which hypothesis test applies.\n\n11.11.1 Preparing and submitting your assignment\n\nFrom the “Run” menu, select “Restart R and Run All Chunks”.\nDeal with any code errors that crop up. Repeat steps 1–-2 until there are no more code errors.\nSpell check your document by clicking the icon with “ABC” and a check mark.\nHit the “Preview” button one last time to generate the final draft of the .nb.html file.\nProofread the HTML file carefully. If there are errors, go back and fix them, then repeat steps 1–5 again.\n\nIf you have completed this chapter as part of a statistics course, follow the directions you receive from your professor to submit your assignment."
  },
  {
    "objectID": "12-confidence_intervals-web.html#ci-intro",
    "href": "12-confidence_intervals-web.html#ci-intro",
    "title": "12  Confidence intervals",
    "section": "12.1 Introduction",
    "text": "12.1 Introduction\nSampling variability means that we can never trust a single sample to identify a population parameter exactly. Instead of simply trusting a point estimate, we can look at the entire sampling distribution to create an interval of plausible values called a confidence interval. By making our intervals wide enough, we hope to have some chance of capturing the true population value. Like hypothesis tests, confidence intervals are a form of inference because they use a sample to deduce something about the population. Along the way, we will also learn about a new form of randomization called bootstrapping.\n\n12.1.1 Install new packages\nThere are no new packages used in this chapter.\n\n\n12.1.2 Download the R notebook file\nCheck the upper-right corner in RStudio to make sure you’re in your intro_stats project. Then click on the following link to download this chapter as an R notebook file (.Rmd).\nhttps://vectorposse.github.io/intro_stats/chapter_downloads/12-confidence_intervals.Rmd\nOnce the file is downloaded, move it to your project folder in RStudio and open it there.\n\n\n12.1.3 Restart R and run all chunks\nIn RStudio, select “Restart R and Run All Chunks” from the “Run” menu."
  },
  {
    "objectID": "12-confidence_intervals-web.html#ci-load",
    "href": "12-confidence_intervals-web.html#ci-load",
    "title": "12  Confidence intervals",
    "section": "12.2 Load packages",
    "text": "12.2 Load packages\nWe load the standard tidyverse, janitor, and infer packages. We’ll also need the openintro package later in the chapter for the hsb2 and the smoking data set.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(infer)\nlibrary(openintro)\n\nLoading required package: airports\nLoading required package: cherryblossom\nLoading required package: usdata"
  },
  {
    "objectID": "12-confidence_intervals-web.html#ci-boot",
    "href": "12-confidence_intervals-web.html#ci-boot",
    "title": "12  Confidence intervals",
    "section": "12.3 Bootstrapping",
    "text": "12.3 Bootstrapping\nImagine you obtain a random sample of 200 high school seniors from across the U.S. Suppose 32 of them attend private school. As a sample statistic, we have\n\\[\n\\hat{p} = 32/200 = 0.16\n\\] In other words, 16% of the students in the sample attended private school.\nIf our sample is representative, we might guess that the true population parameter \\(p\\) is also close to 0.16, but we’re not really sure:\n\\[\np \\approx 0.16?\n\\]\nAnd what about the sampling variability? A few chapters ago, we flipped coins. A “weighted” coin flipped 200 times can give us a “new” (fake) sample, and doing that a thousand times (or even more) can give us a lot of new samples to see what range of values is possible. But what would we use as the probability of heads for the weighted coin? It would be a bad idea to use 0.16 because that would assume that the population proportion agreed exactly with the one sample we happen to have. It worked in a hypothesis test because we had a value of \\(p\\) we assumed was true in the guise of a null hypothesis. But in general, if I simply want to estimate a population parameter with a sample statistic, I have no such information to use. So coin flipping is out.\nAn alternative that is available to us is a procedure called bootstrapping. The idea sounds weird, but it’s pretty simple: instead of building fake samples, what if we tried to build a fake population? And then, what if we took repeated samples from it?\nHow would we build a fake population? Imagine making many, many copies of our sample until we had thousands or even millions of students. In fact, we can think of an infinite number of copies of our sample if we want. Sure, this fake population isn’t exactly like the real population of all high school seniors. But if our sample is representative, we might hope that lots of copies of our sample would approximate the population we care about.\nComputationally, it’s a lot of work to copy our sample thousands or millions of times. And we certainly can’t work with an infinite number of copies. Fortunately, we can use a shortcut. It’s called sampling with replacement.\nNormal sampling is usually without replacement, meaning that once we have sampled an individual, they are not eligible to be sampled again. We don’t want to survey Billy and then later in our study, survey Billy again.\nIn sampling with replacement, we put Billy back in the pool and make him eligible to be sampled again. This is the same thing as having access to an infinite population. Remember that our fake population is just many, many copies of our sample. So in that fake population, there are many, many Billy clones that could end up in our sample. So rather than cloning Billy many, many times, let’s just put Billy back in the group any time he’s sampled.\nWe need to see this in action. We have a random sample of 200 students obtained by the National Center of Education Statistics in their “High School and Beyond” survey. This is stored in the hsb2 data set from the openintro package. Here are the school types for these students, stored in the variable schtyp:\n\nhsb2$schtyp\n\n  [1] public  public  public  public  public  public  public  public  public \n [10] public  public  public  public  public  public  public  public  private\n [19] public  public  public  public  public  public  public  public  public \n [28] private private public  public  public  private public  private public \n [37] private public  public  public  private private public  public  public \n [46] public  public  public  private public  public  public  public  private\n [55] public  public  public  public  private public  private public  public \n [64] public  private public  public  public  public  public  public  public \n [73] public  public  public  public  public  public  public  public  public \n [82] public  private public  public  public  public  public  public  public \n [91] public  public  public  public  public  public  public  public  public \n[100] private public  public  public  public  public  public  public  public \n[109] private private public  public  public  public  private public  public \n[118] public  public  public  private public  public  public  public  public \n[127] public  public  public  public  public  public  public  public  public \n[136] public  private public  public  private public  public  public  public \n[145] public  private public  private public  public  public  public  public \n[154] public  public  public  public  public  public  public  public  public \n[163] private public  public  public  public  public  private public  public \n[172] public  public  public  public  public  public  public  public  public \n[181] public  private public  public  public  public  public  public  private\n[190] public  public  private private public  private private public  private\n[199] public  public \nLevels: public private\n\n\nLet’s sample an individual from our sample:\n\nset.seed(6)\nsample(hsb2$schtyp, size = 1)\n\n[1] public\nLevels: public private\n\n\nThat was one of the public school students from among the 200 students in our sample. Here’s another one:\n\nset.seed(7)\nsample(hsb2$schtyp, size = 1)\n\n[1] private\nLevels: public private\n\n\nThat was one of the private school students.\nWe can do this 200 times. Now, if we sample without replacement, all we get back are the original students, just listed in a different order. Think about why: we’re just picking one student at a time. But since they don’t get replaced, eventually, every student will get chosen. We’re choosing 200 students, but there are only 200 students from which to choose.\n\nset.seed(8)\nsample_without_replacement1 &lt;- sample(hsb2$schtyp, size = 200)\nsample_without_replacement1\n\n  [1] public  public  public  public  public  public  public  public  public \n [10] public  public  public  public  public  public  public  private public \n [19] public  public  public  public  public  public  public  public  public \n [28] public  public  private public  public  public  public  public  public \n [37] public  public  public  public  public  public  public  private public \n [46] public  public  public  public  private private private public  public \n [55] private private public  public  public  public  public  public  private\n [64] public  public  private public  public  public  public  public  public \n [73] public  public  public  public  public  public  public  public  public \n [82] public  public  public  public  public  public  public  public  public \n [91] public  public  public  public  public  public  public  public  public \n[100] public  public  private public  public  public  public  public  public \n[109] public  public  public  public  public  public  public  public  public \n[118] public  public  public  private public  public  private public  public \n[127] private private public  public  public  public  private public  private\n[136] private public  public  public  public  public  public  public  public \n[145] public  public  public  public  public  public  public  public  private\n[154] public  public  public  public  public  private public  private private\n[163] public  public  public  public  private public  public  private public \n[172] private private public  public  public  public  public  public  public \n[181] public  private public  private public  public  public  private private\n[190] public  public  public  public  public  public  public  public  private\n[199] public  private\nLevels: public private\n\n\n\ntabyl(sample_without_replacement1)\n\n sample_without_replacement1   n percent\n                      public 168    0.84\n                     private  32    0.16\n\n\n\nset.seed(9)\nsample_without_replacement2 &lt;- sample(hsb2$schtyp, size = 200)\nsample_without_replacement2\n\n  [1] public  public  public  private public  private public  private public \n [10] public  public  private private private private private public  public \n [19] private public  public  public  public  public  public  public  public \n [28] public  public  public  public  public  public  public  public  public \n [37] public  public  public  public  public  public  public  private public \n [46] public  public  public  public  private public  private public  public \n [55] public  private public  public  public  public  public  public  public \n [64] private public  public  public  public  public  public  public  public \n [73] public  public  public  public  private public  public  public  public \n [82] private private public  public  public  public  public  public  private\n [91] public  private public  public  public  private public  public  public \n[100] public  public  private private public  public  public  public  public \n[109] public  private public  public  private public  private public  public \n[118] public  public  public  public  public  private public  public  public \n[127] public  public  public  public  public  public  public  public  public \n[136] public  public  public  public  public  public  private public  public \n[145] public  private public  public  public  public  public  public  public \n[154] public  public  public  public  public  public  public  public  private\n[163] private public  public  public  public  public  public  public  public \n[172] public  public  public  public  public  public  public  public  public \n[181] private public  public  public  public  public  private public  public \n[190] public  public  public  public  public  public  public  public  public \n[199] public  public \nLevels: public private\n\n\n\ntabyl(sample_without_replacement2)\n\n sample_without_replacement2   n percent\n                      public 168    0.84\n                     private  32    0.16\n\n\nThe two lists above consist of the same 200 students, just drawn in a different order.\nOn the other hand, if we sample with replacement, then students can get chosen more than once. (Remember, we’re equating “getting chosen more than once” with “sampling from an infinite population and choosing a clone”.) Now, the number of private school students we see might not be 32.\nEach of the following samples is called a bootstrap sample. Notice that we’ve added the argument replace = TRUE to the sample function:\n\nset.seed(10)\nsample_with_replacement1 &lt;- sample(hsb2$schtyp, size = 200, replace = TRUE)\nsample_with_replacement1\n\n  [1] private public  public  public  public  private public  public  public \n [10] public  public  public  public  public  public  public  public  public \n [19] private public  public  public  public  private private private public \n [28] public  private public  public  public  private public  public  public \n [37] public  public  public  public  public  public  private public  public \n [46] public  public  public  public  public  public  private public  public \n [55] public  public  public  public  public  public  public  public  public \n [64] public  public  private public  private public  public  public  private\n [73] public  public  public  public  public  public  public  public  public \n [82] public  public  public  public  private public  public  public  public \n [91] public  private public  private public  private public  public  public \n[100] public  public  public  private private public  public  public  public \n[109] public  public  public  public  public  private private public  public \n[118] private public  public  private public  public  private public  public \n[127] public  public  public  private private private public  public  private\n[136] public  public  public  public  public  public  public  public  public \n[145] public  public  public  public  public  public  public  public  public \n[154] public  public  public  public  public  public  public  private public \n[163] public  public  public  private private public  private private private\n[172] public  public  public  public  public  public  private public  public \n[181] public  public  public  public  public  public  private public  public \n[190] public  public  public  public  public  public  public  public  public \n[199] public  public \nLevels: public private\n\n\n\ntabyl(sample_with_replacement1)\n\n sample_with_replacement1   n percent\n                   public 164    0.82\n                  private  36    0.18\n\n\nThat bootstrap sample proportion is 0.18, not 0.16.\n\nset.seed(11)\nsample_with_replacement2 &lt;- sample(hsb2$schtyp, size = 200, replace = TRUE)\nsample_with_replacement2\n\n  [1] public  public  public  public  public  private public  public  private\n [10] public  public  private public  public  public  public  public  public \n [19] public  public  public  public  public  public  public  public  public \n [28] public  public  public  public  public  public  public  public  public \n [37] public  public  public  public  public  public  public  public  public \n [46] public  public  public  public  public  public  public  private public \n [55] public  public  public  public  public  public  public  public  public \n [64] public  public  public  public  public  public  public  public  public \n [73] public  public  public  public  private public  public  public  public \n [82] public  private private public  public  private public  public  public \n [91] public  private public  public  public  public  private public  private\n[100] public  public  private public  public  public  public  public  public \n[109] public  public  private public  public  private public  public  public \n[118] private private public  public  public  public  private public  private\n[127] public  private public  private public  public  public  private public \n[136] private public  public  public  private private private public  private\n[145] public  public  private public  public  private public  public  public \n[154] private private public  public  public  public  public  public  private\n[163] private public  public  public  public  public  private public  private\n[172] public  public  public  private private public  private public  public \n[181] public  public  public  public  public  public  public  public  public \n[190] public  private public  private public  public  public  private public \n[199] public  public \nLevels: public private\n\n\n\ntabyl(sample_with_replacement2)\n\n sample_with_replacement2   n percent\n                   public 160     0.8\n                  private  40     0.2\n\n\nThat bootstrap sample proportion is 0.2.\nNow we’re getting some sampling variability!\nIf we do this many, many times, we get a whole collection of sample proportions. The distribution of all those sample proportions, obtained with bootstrap samples (samples drawn with replacement), is called the bootstrap sampling distribution."
  },
  {
    "objectID": "12-confidence_intervals-web.html#ci-computing-boot",
    "href": "12-confidence_intervals-web.html#ci-computing-boot",
    "title": "12  Confidence intervals",
    "section": "12.4 Computing a bootstrap sampling distribution",
    "text": "12.4 Computing a bootstrap sampling distribution\nThe infer package can compute bootstrap samples and, hence, produce a bootstrap sampling distribution. The code looks a whole like the code you already know for hypothesis testing:\n\nprivate_boot &lt;- hsb2 %&gt;%\n    specify(response = schtyp, success = \"private\") %&gt;%\n    generate(reps = 1000, type = \"bootstrap\") %&gt;%\n    calculate(stat = \"prop\")\nprivate_boot\n\nResponse: schtyp (factor)\n# A tibble: 1,000 × 2\n   replicate  stat\n       &lt;int&gt; &lt;dbl&gt;\n 1         1 0.185\n 2         2 0.185\n 3         3 0.15 \n 4         4 0.135\n 5         5 0.145\n 6         6 0.175\n 7         7 0.15 \n 8         8 0.195\n 9         9 0.18 \n10        10 0.185\n# ℹ 990 more rows\n\n\nWe simply changed the type to “bootstrap”.\nNow we visualize like normal:\n\nprivate_boot %&gt;%\n    visualize()\n\n\n\n\n(We can change the number of bins if we want, but this number looks pretty good.)"
  },
  {
    "objectID": "12-confidence_intervals-web.html#ci-ci",
    "href": "12-confidence_intervals-web.html#ci-ci",
    "title": "12  Confidence intervals",
    "section": "12.5 Confidence intervals",
    "text": "12.5 Confidence intervals\nThe histogram above simulates what might happen if we took many samples from our infinite “fake” population consisting of many copies of our original, actual sample data. On the lower end, we might see something like 8% private school students. On the upper end, we could see 25% or more private school students.\nIn the chapter about numerical data, we computed the IQR (interquartile range), which was the difference between the 25th percentile and the 75th percentile. The IQR was then the range of the middle 50% of the data. Let’s use infer tools to calculate the middle 50% of the above distribution:\n\nprivate_50 &lt;- private_boot %&gt;%\n    get_confidence_interval(level = 0.5)\nprivate_50\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     0.14    0.175\n\n\nThe middle 50% ranges from 14% up to 17.5%. We can also visualize this:\n\nprivate_boot %&gt;%\n    visualise() +\n    shade_confidence_interval(endpoints = private_50)\n\n\n\n\nIn other words, when we go out to gather a sample from our (fake infinite) population of high school seniors, about half of the time, we expect the percentage of private students to be somewhere between 14% and 17.5%. The other half of the time, we will sample a value outside that range.\nThis is a confidence interval. More specifically, this is a 50% confidence interval. This is the range of values we expect sample proportions to be in approximately half of the samples we might gather from our (fake infinite) population.\nNow don’t forget the goal. What we are really trying to find is the value \\(p\\), the true population parameter. We want to know what proportion of high school seniors attend private school in the whole population of all high school seniors in the U.S.\nFor mathematical reasons that are outside the scope of this course, it turns out that the sampling variability in the bootstrap distribution around \\(\\hat{p}\\) is very similar to the sampling variability of the sample proportion \\(\\hat{p}\\) around the true value \\(p\\). We bootstrapped our way to the picture above using one actual sample with about 16% private school students. A different sample of high school seniors would give us different bootstrap samples, producing a slightly different bootstrap distribution from the one above. But it, too, will have a shaded region like the histogram above. Every actual sample we might obtain in the real world would give us a bootstrap distribution with a different shaded region. But the amazing fact is this: about half of those shaded regions will actually contain the true population parameter \\(p\\).\nThink about the value \\(p\\) like a fish hidden in a murky lake. The sample proportion \\(\\hat{p}\\) is our attempt at fishing. We drop a hook down at the value \\(\\hat{p}\\) and pull it right back up. It’s not very likely that we caught the fish, although we hope that we were close. Alas, the sample proportion is almost never exactly equal to the true proportion \\(p\\). But what if we cast a net instead? That net is the shaded range of values in our confidence interval. That range of values might catch the fish.\nThe difference between statistics and fishing is that, in the latter, when we pull up the net, we can see if we successfully caught the fish. In the former, all we can say is that there is some probability that the net caught the fish, but you’re not able to look inside the net to know for sure.\nSo the confidence interval we created above might have caught the true value \\(p\\). But then again, it might not have. There’s only a 50% chance we captured the true value in the range 14% to 17.5% that we computed from our specific sample with its accompanying bootstrap samples. Most researchers would be displeased with only a 50% success rate. So can we do better?\nHow much better do we want to do? This is a subjective question with no definitive answer. Many people say they want to be 95% confident that the confidence interval they build will capture the true population parameter. Let’s modify our code to do that:\n\nprivate_95 &lt;- private_boot %&gt;%\n    get_confidence_interval(level = 0.95)\nprivate_95\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     0.11    0.215\n\n\nThe middle 95% ranges from 11% up to 21.5%. We can also visualize this:\n\nprivate_boot %&gt;%\n    visualise() +\n    shade_confidence_interval(endpoints = private_95)\n\n\n\n\nThe interpretation is that when you go collect many samples, the confidence intervals you produce using the bootstrap procedure described above will capture the true population proportion 95% of the time.\n\nExercise 1\nWhy is a 95% confidence interval wider than a 50% confidence interval? In other words, why should our desire to be 95% confident in capturing the true value of \\(p\\) result in an interval that is wider than if we only wanted to be 50% confident?\n\nPlease write up your answer here.\n\n\n\nExercise 2\nBeing more confident seems like a good thing. In fact, we might want a 99% confidence interval. Compute and visualize a 99% confidence interval for proportion of private school students.\n\n\n# Add code here to compute a 99% confidence interval\n\n\n# Add code here to visualize a 99% confidence interval\n\n\n\n\nExercise 3\nCan you think of any downside to using higher and higher confidence levels? As a hint, think about the following completely true sentence: “I am 100% confident that the true proportion of high school seniors attending private school is somewhere between 0% and 100%.”\n\nPlease write up your answer here.\n\n\nWhile 50% is clearly too low for a confidence level, as seen above, there is no particular reason that we need to compute a 95% confidence interval either. There is some consensus in the scientific community here: 95% has evolved to become a generally agreed-upon standard. But we could compute a 90% confidence interval or a 99% confidence interval (as you did above), or any other type of interval. Having said that, if you choose other intervals besides these three, people might wonder if you’re up to something.1"
  },
  {
    "objectID": "12-confidence_intervals-web.html#ci-conditions",
    "href": "12-confidence_intervals-web.html#ci-conditions",
    "title": "12  Confidence intervals",
    "section": "12.6 Conditions",
    "text": "12.6 Conditions\nDon’t forget that there are always assumptions we make when relying on any kind of statistical inference. Before computing a confidence interval for a proportion, we must verify that certain conditions are satisfied. But these conditions are not new. We already know from hypothesis testing what is required for good inference from a sample. These are the “Random” and the “10%” conditions.\n\nRandom\n\nThe sample must be random (or hopefully representative).\n\n10%\n\nThe sample size must be less than 10% of the size of the population.\n\n\nBoth conditions are met for the data in the High School and Beyond survey."
  },
  {
    "objectID": "12-confidence_intervals-web.html#ci-rubric",
    "href": "12-confidence_intervals-web.html#ci-rubric",
    "title": "12  Confidence intervals",
    "section": "12.7 Rubric for confidence intervals",
    "text": "12.7 Rubric for confidence intervals\nTypically, you will be asked to report a confidence interval after performing a hypothesis test. Whereas a hypothesis test gives you a “decision criterion” (using data to make a decision to reject the null or fail to reject the null), a confidence interval gives you an estimate of the “effect size” (a range of plausible values for the population parameter).\nAs such, there is a section in the Rubric for inference that shows the steps of calculating and reporting a confidence interval. They are as follows:\n\nCheck the relevant conditions to ensure that model assumptions are met.\nCalculate and graph the confidence interval.\nState (but do not overstate) a contextually meaningful interpretation.\nIf running a two-sided test, explain how the confidence interval reinforces the conclusion of the hypothesis test.\nWhen comparing two groups, comment on the effect size and the practical significance of the result."
  },
  {
    "objectID": "12-confidence_intervals-web.html#ci-ex",
    "href": "12-confidence_intervals-web.html#ci-ex",
    "title": "12  Confidence intervals",
    "section": "12.8 Example",
    "text": "12.8 Example\nHere is a worked example. (Unless otherwise stated, we always use a 95% confidence level.)\nSome of the students in the “High School and Beyond” survey attended vocational programs. This data is stored in the prog variable. Using a confidence interval, estimate what percentage of all high school seniors attend vocational programs.\nWe will need to do a little data cleaning before we can address this question. There are actually three types of programs: “general”, “academic”, and “vocational”. The infer commands will only work when a categorical variable has two levels. We are thinking of “general” and “academic” together as more like a combined “other” category. We can fix this by creating a new factor variable with mutate. Inside that mutate, we will use the fct_collapse function to collapse two of the levels into one as follows:\n\nhsb2 &lt;- hsb2 %&gt;%\n    mutate(prog2 = fct_collapse(prog,\n                                vocational = \"vocational\",\n                                other = c(\"general\", \"academic\")))\nglimpse(hsb2)\n\nRows: 200\nColumns: 12\n$ id      &lt;int&gt; 70, 121, 86, 141, 172, 113, 50, 11, 84, 48, 75, 60, 95, 104, 3…\n$ gender  &lt;chr&gt; \"male\", \"female\", \"male\", \"male\", \"male\", \"male\", \"male\", \"mal…\n$ race    &lt;chr&gt; \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"african…\n$ ses     &lt;fct&gt; low, middle, high, high, middle, middle, middle, middle, middl…\n$ schtyp  &lt;fct&gt; public, public, public, public, public, public, public, public…\n$ prog    &lt;fct&gt; general, vocational, general, vocational, academic, academic, …\n$ read    &lt;int&gt; 57, 68, 44, 63, 47, 44, 50, 34, 63, 57, 60, 57, 73, 54, 45, 42…\n$ write   &lt;int&gt; 52, 59, 33, 44, 52, 52, 59, 46, 57, 55, 46, 65, 60, 63, 57, 49…\n$ math    &lt;int&gt; 41, 53, 54, 47, 57, 51, 42, 45, 54, 52, 51, 51, 71, 57, 50, 43…\n$ science &lt;int&gt; 47, 63, 58, 53, 53, 63, 53, 39, 58, 50, 53, 63, 61, 55, 31, 50…\n$ socst   &lt;int&gt; 57, 61, 31, 56, 61, 61, 61, 36, 51, 51, 61, 61, 71, 46, 56, 56…\n$ prog2   &lt;fct&gt; other, vocational, other, vocational, other, other, other, oth…\n\n\nInspect the variables prog and prog2 above to make sure that the recoding was successful. Then be sure to use prog2 and not prog everywhere.\n\n12.8.1 Check the relevant conditions to ensure that model assumptions are met.\n\nRandom\n\nThe sample is a random sample of high school seniors from the U.S. as the survey was conducted by the National Center of Education Statistics, a reputable government organization.\n\n10%\n\nThe sample size is 200, which is much less than 10% of the population of all U.S. high school seniors.\n\n\n\n\n12.8.2 Calculate and graph the confidence interval.\n\nvocational_boot &lt;- hsb2 %&gt;%\n    specify(response = prog2, success = \"vocational\") %&gt;%\n    generate(reps = 1000, type = \"bootstrap\") %&gt;%\n    calculate(stat = \"prop\")\nvocational_boot\n\nResponse: prog2 (factor)\n# A tibble: 1,000 × 2\n   replicate  stat\n       &lt;int&gt; &lt;dbl&gt;\n 1         1 0.335\n 2         2 0.25 \n 3         3 0.17 \n 4         4 0.24 \n 5         5 0.245\n 6         6 0.245\n 7         7 0.2  \n 8         8 0.24 \n 9         9 0.265\n10        10 0.25 \n# ℹ 990 more rows\n\n\n\nvocational_ci &lt;- vocational_boot %&gt;%\n    get_confidence_interval(level = 0.95)\nvocational_ci\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     0.19     0.31\n\n\n\nvocational_boot %&gt;%\n    visualize() +\n    shade_confidence_interval(endpoints = vocational_ci)\n\n\n\n\n\n\n12.8.3 State (but do not overstate) a contextually meaningful interpretation.\nWe are 95% confident that the true percentage of U.S. high school seniors who attend a vocational program is captured in the interval (19%, 31%).\nNote: we use inline code to grab the values of the endpoints of the confidence interval. We also multiply by 100 to report percentages instead of proportions.\n\n\n12.8.4 If running a two-sided test, explain how the confidence interval reinforces the conclusion of the hypothesis test.\nIn this chapter, we haven’t run a hypothesis test, so this step is irrelevant for us here. However, in future chapters, we will incorporate this step into the rubric and see how the confidence interval relates to the conclusion of a hypothesis test.\n\n\n12.8.5 When comparing two groups, comment on the effect size and the practical significance of the result.\nThis step will also become more clear in future chapters. It only applies to situations where you are attempting to find a difference between two groups. In this example, we’re simply using a sample statistic to estimate a single population parameter."
  },
  {
    "objectID": "12-confidence_intervals-web.html#ci-your-turn",
    "href": "12-confidence_intervals-web.html#ci-your-turn",
    "title": "12  Confidence intervals",
    "section": "12.9 Your turn",
    "text": "12.9 Your turn\nUse the smoking data set from the openintro package. What percentage of the population of the U.K. smokes tobacco? (The information you need is in the smoke variable.) Use a 95% confidence interval.\n\nCheck the relevant conditions to ensure that model assumptions are met.\n\n\nRandom\n\n[Check condition here.]\n\n10%\n\n[Check condition here.]\n\n\n\n\n\nCalculate and graph the confidence interval.\n\n\n# Add code here to create the bootstrap sampling distribution.\n\n\n# Add code here to calculate the confidence interval.\n\n\n# Add code here to graph the confidence interval.\n\n\n\n\nState (but do not overstate) a contextually meaningful interpretation.\n\nPlease write up your answer here.\n\n\n(We will ignore the last two last steps in the rubric. We haven’t run a hypothesis test and we’re not comparing smoking between two groups.)"
  },
  {
    "objectID": "12-confidence_intervals-web.html#ci-interpret",
    "href": "12-confidence_intervals-web.html#ci-interpret",
    "title": "12  Confidence intervals",
    "section": "12.10 Interpreting confidence intervals",
    "text": "12.10 Interpreting confidence intervals\nConfidence intervals are notoriously difficult to interpret.2\nHere are several wrong interpretations of a 95% confidence interval:\n\n95% of the data lies in the interval.\nThere is a 95% chance that the sample proportion lies in the interval.\nThere is a 95% chance that the population parameter lies in the interval.\n\nWe’ll take a closer look at these incorrect claims in a moment. First, let’s see how confidence intervals work using simulation.\nIn order to simulate, we’ll have to pretend temporarily that we know a true population parameter. Let’s use the example of a candidate who has the support of 64% of voters. In other words, \\(p = 0.64\\). We go out and get a sample of voters, let’s say 50. From that sample we construct a 95% confidence interval by bootstrapping. Most of the time, 64% (the true value!) should be in our interval. But sometimes it won’t be. We can get an unusual sample that is far away from 64%, just by pure chance alone. (Perhaps we accidentally run into a bunch of people who oppose our candidate.)\nOkay, let’s do it again. Get a new sample and calculate a new confidence interval. This sample will likely result in a different sample proportion than the first sample. Therefore, the confidence interval will be located in a different place. Does it contain 64%? Most of the time, we expect it to. Occasionally, it will not.\nWe can do this over and over again through the magic of simulation! Here’s what this simulation looks like in R. The following code is quite technical, although you will recognize bits and pieces of it. Don’t worry about it. You won’t need to generate code like this on your own. Just look at the pretty picture in the output below below the code.\n\nset.seed(11111)\n\n# The true population proportion is 0.64\ntrue_val &lt;- 0.64\n# The sample size is 50\nsample_size &lt;- 50\n# Set confidence level\nour_level &lt;- 0.95\n# Set number of intervals to simulate\nsim_num &lt;- 100\n\n# Get a random sample of size n.\n# Compute the test statistic and the bootstrap confidence interval.\n# Put both into a single tibble.\nsimulate_ci &lt;- function(n, level = 0.95) {\n    sample_data &lt;-\n        factor(rbinom(n , size = 1, prob = true_val)) %&gt;%\n        tibble(data = .)\n    stat &lt;- sample_data %&gt;%\n        observe(response = data, success = \"1\", stat = \"prop\")\n    ci &lt;- sample_data %&gt;%\n        specify(response = data, success = \"1\") %&gt;%\n        generate(reps = 1000, type = \"bootstrap\") %&gt;%\n        calculate(stat = \"prop\") %&gt;%\n        get_confidence_interval(level = our_level)\n    bind_cols(stat, ci) %&gt;%\n        return()\n}\n\n# Simulate 100 random samples (each of size 50)\n# Assign a color based on whether the intervals contain the true proportion\nci &lt;- map_dfr(rep(sample_size, times = sim_num), simulate_ci, level = our_level) %&gt;%\n    mutate(row_num = row_number()) %&gt;%\n    mutate(color = ifelse(lower_ci &lt;= true_val & true_val &lt;= upper_ci,\n                          \"black\", \"red\"),\n           alpha = ifelse(color == \"black\", 0.5, 1))\n\n# Plot all the simulated intervals\nggplot(ci, aes(x = stat, y = row_num,\n                   color = color, alpha = alpha)) +\n    geom_point() +\n    scale_color_manual(values = c(\"black\", \"red\"), guide = \"none\") +\n    geom_segment(aes(x = lower_ci, xend = upper_ci, yend = row_num)) +\n    geom_vline(xintercept = true_val, color = \"blue\") +\n    scale_alpha_identity() +\n    labs(y = \"Simulation\", x = \"Estimates with confidence intervals\")\n\n\n\n\nEach sample gives us a slightly different estimate, and therefore, a different confidence interval as well.\nFor each of the 100 simulated intervals, most of them (the black ones) do capture the true value of 0.64 (the blue vertical line). Occasionally they don’t (the red ones). We expect 5 red intervals, but since randomness is involved, it won’t necessarily be exactly 5. (Here there were only 3 bad intervals.)\nThis is the key to interpreting confidence intervals. The “95%” in a 95% confidence interval means that if we were to collect many random samples, about 95% of them would contain the true population parameter and about 5% would not.\nSo let’s revisit the erroneous statements from the beginning of this section and correct the misconceptions.\n\n95% of the data lies in the interval.\n\nThis doesn’t even make sense. Our data is categorical. The confidence interval is a range of plausible values for the proportion of successes in the sample.\n\nThere is a 95% chance that the sample proportion lies in the interval.\n\nNo. There is essentially a 100% chance that the sample proportion lies in the interval. Most of the time, the sample proportion is very close to the center of the interval. When we bootstrap, the “infinite population” we are simulating has the same population proportion as the sample we started with. (After all, the infinite population is just many copies of the sample we started with.) Therefore, samples from that infinite population should be more or less centered around the sample proportion.\n\nThere is a 95% chance that the population parameter lies in the interval.\n\nThis is wrong in a more subtle way. The problem here as that it takes our interval as being fixed and special, and then tries to declare that of all possible population parameters, we have a 95% chance of the true one landing in our interval. The logic is backwards. The population parameter is the fixed truth. It doesn’t wander around and land in our interval sometimes and not at other times. It is our confidence interval that wanders; it is just one of many intervals we could have obtained from random sampling. When we say, “We are 95% confident that…,” we are just using a convenient shorthand for, “If we were to repeat the process of sampling and creating confidence intervals many times, about 95% of those times would produce an interval that happens to capture the actual population proportion.” But we’re lazy and we don’t want to say that every time."
  },
  {
    "objectID": "12-confidence_intervals-web.html#ci-conclusion",
    "href": "12-confidence_intervals-web.html#ci-conclusion",
    "title": "12  Confidence intervals",
    "section": "12.11 Conclusion",
    "text": "12.11 Conclusion\nA confidence interval is a form of statistical inference that gives us a range of numbers in which we hope to capture the true population parameter. Of course, we can’t be certain of that. If we repeatedly collect samples, the expectation is that 95% of those samples will produce confidence intervals that capture the true population parameter, but that also means that 5% will not. We’ll never know if our sample was one of the 95% that worked, or one of the 5% that did not. And even if we get one of the intervals that worked, all we have is a range of values and it’s impossible to determine which of those values is the true population parameter. Because it’s statistics, we just have to live with that uncertainty.\n\n12.11.1 Preparing and submitting your assignment\n\nFrom the “Run” menu, select “Restart R and Run All Chunks”.\nDeal with any code errors that crop up. Repeat steps 1–-2 until there are no more code errors.\nSpell check your document by clicking the icon with “ABC” and a check mark.\nHit the “Preview” button one last time to generate the final draft of the .nb.html file.\nProofread the HTML file carefully. If there are errors, go back and fix them, then repeat steps 1–5 again.\n\nIf you have completed this chapter as part of a statistics course, follow the directions you receive from your professor to submit your assignment."
  },
  {
    "objectID": "12-confidence_intervals-web.html#footnotes",
    "href": "12-confidence_intervals-web.html#footnotes",
    "title": "12  Confidence intervals",
    "section": "",
    "text": "A contrary position is proffered by Richard McElreath, an evolutionary ecologist and author of the amazing book Statistical Rethinking. He uses 89% and 97% intervals to highlight the absurdity of regarding 95% as a magic number that has some kind of deep, special meaning.↩︎\nSeveral studies have given surveys to statistics students, teachers, and researchers, and find that even these people often misinterpret confidence intervals. See, for example, this paper: http://www.ejwagenmakers.com/inpress/HoekstraEtAlPBR.pdf↩︎"
  },
  {
    "objectID": "13-normal_models-web.html#normal-intro",
    "href": "13-normal_models-web.html#normal-intro",
    "title": "13  Normal models",
    "section": "13.1 Introduction",
    "text": "13.1 Introduction\nIn this chapter we will learn how to work with normal models. In addition to learning about theoretical normal distributions, we will also develop QQ plots to assess the normality of data.\n\n13.1.1 Install new packages\nThere are no new packages used in this chapter.\n\n\n13.1.2 Download the R notebook file\nCheck the upper-right corner in RStudio to make sure you’re in your intro_stats project. Then click on the following link to download this chapter as an R notebook file (.Rmd).\nhttps://vectorposse.github.io/intro_stats/chapter_downloads/13-normal_models.Rmd\nOnce the file is downloaded, move it to your project folder in RStudio and open it there.\n\n\n13.1.3 Restart R and run all chunks\nIn RStudio, select “Restart R and Run All Chunks” from the “Run” menu."
  },
  {
    "objectID": "13-normal_models-web.html#normal-load",
    "href": "13-normal_models-web.html#normal-load",
    "title": "13  Normal models",
    "section": "13.2 Load packages",
    "text": "13.2 Load packages\nIn addition to tidyverse, we return to the mosaic package to produce some nice visualizations of normal models.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nWarning: package 'mosaic' was built under R version 4.3.1\n\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum"
  },
  {
    "objectID": "13-normal_models-web.html#normal-clt",
    "href": "13-normal_models-web.html#normal-clt",
    "title": "13  Normal models",
    "section": "13.3 The Central Limit Theorem",
    "text": "13.3 The Central Limit Theorem\nAn important aspect of all the simulations that we’ve done so far—assuming that we’ve run a large enough number of them—is that their histograms all look like bell curves. This fact is known as the “Central Limit Theorem”. Under some basic assumptions that we’ll discuss in a later chapter, this will be typical of many of our simulated null distributions.\nSo rather than running a simulation each time we want to conduct a hypothesis test, we could also assume that the null distribution is a bell curve. The rest of this chapter will teach you how to work with the “normal distribution,” which is just the mathematically correct term for a bell curve."
  },
  {
    "objectID": "13-normal_models-web.html#normal-normal",
    "href": "13-normal_models-web.html#normal-normal",
    "title": "13  Normal models",
    "section": "13.4 Normal models",
    "text": "13.4 Normal models\nThe normal distribution looks like this:\n\n# Don't worry about the syntax here.\n# You won't need to know how to do this on your own.\nggplot(data.frame(x = c(-4, 4)), aes(x)) +\n    stat_function(fun = dnorm) +\n    scale_x_continuous(breaks = -3:3)\n\n\n\n\nThe curve pictured above is called the standard normal distribution. It has a mean of 0 and a standard deviation of 1. Mathematically, this is written as\n\\[\nN(\\mu = 0, \\sigma = 1),\n\\]\nor usually just\n\\[\nN(0, 1).\n\\]\nWe use this bell curve shape to model data that is unimodal, symmetric, and without outliers. A statistical “model” is a simplification or an idealization. Reality is, of course, never perfectly bell-shaped. Real data is not exactly symmetric with one clear peak in the middle. Nevertheless, an abstract model can give us good answers if used properly.\nAs an example of this, systolic blood pressure (SBP, measured in millimeters of mercury, or mmHg) is more-or-less normally distributed in women ages 30–44 in the U.S. and Canada, with a mean of 114 and a standard deviation of 14.1\nIf we were to plot a histogram with the SBP of every woman between the ages of 30 and 44 in the U.S. and Canada, it would have the shape of a normal distribution, but instead of being centered at 0 like the graph above, this one would be centered at 114. Mathematically, we write\n\\[\nN(\\mu = 114, \\sigma = 14),\n\\]\nor\n\\[\nN(114, 14).\n\\]\nThe graph now looks like this:\n\n# Again, don't worry about the syntax here.\nggplot(data.frame(x = c(58, 170)), aes(x)) +\n    stat_function(fun = dnorm, args = list(mean = 114, sd = 14)) +\n    scale_x_continuous(breaks = c(72, 86, 100, 114, 128, 142, 156))"
  },
  {
    "objectID": "13-normal_models-web.html#normal-predictions",
    "href": "13-normal_models-web.html#normal-predictions",
    "title": "13  Normal models",
    "section": "13.5 Predictions using normal models",
    "text": "13.5 Predictions using normal models\nUsing this information, we can estimate the percentage of such women who are expected to have any range of SBP without having access to all such data.\nFor example, what percentage of women ages 30–44 in the U.S. and Canada are expected to have SBP under 130 mmHg? The pdist command from the mosaic package will not only help us with this calculation, but it also offers a nice visual representation depending on the arguments we supply to the function:\n\npdist(\"norm\", q = 130, mean = 114, sd = 14)\n\n\n\n\n[1] 0.873451\n\n\nIn the notebook view, you have to switch back and forth between the two boxes below the code chunk (above the graph) to see the number versus the graph. In the HTML output, however, both the number and the plot are visible.\nFor situations where we really just want to see the number, we can always add plot = FALSE to the function:\n\npdist(\"norm\", q = 130, mean = 114, sd = 14, plot = FALSE)\n\n[1] 0.873451\n\n\nThe other pieces of the pdist function are pretty intuitive: \"norm\" (and it has to be in quotes) indicates that we want a normal model, q is the value of interest to us, and mean and sd are self-evident. The numerical output gives the area under the curve to the left of our value of interest. This area is 0.873451; in other words, about 87.3% of women are expected to have SBP less than 130.\nIf you use this command inline, the pretty picture is not generated, just the value. For example, look at the following sentence (remembering that you can click anywhere inside the inline R code and hit Ctrl-Enter or Cmd-Enter):\n\nThe model predicts that 87.3451046% of women ages 30–44 in the U.S. and Canada will have systolic blood pressure under 130 mmHg.\n\nNote that the above code multiplied the result of the pdist command by 100. This is important because the full sentence interpretation is meant to be read by human beings, and human beings tend to report these kinds of numbers as percentages and not decimals.2\nIt’s also important that you include the phrase, “The model predicts…” or something like that. Without that part, the claim is likely false. It would be too definitive. Remember that a model is just an approximation or simplification of reality. We’re not claiming we’ve found the “True” number. All we know is that if the model is roughly correct, we can predict the true value.\nHere’s another question: how many women are predicted to have SBP greater than 130? If 87.3% of women have SBP under 130, then 12.7% must have SBP over 130. Why? Because all women have to add up to 100%!\nTherefore, all we have to do to solve this problem is subtract the number we obtained in the previous question from 1. (Remember that 1 = 100%.)\n\nThe model predicts that 12.6548954% of women ages 30–44 in the U.S. and Canada will have systolic blood pressure over 130 mmHg.\n\nDon’t forget to include parentheses. We need to multiply the whole expression by 100.\nNow, here’s a more complicated question: what percentage of women are predicted to have SBP between 110 mmHg and 130 mmHg?\nRecall that the proportion of women predicted to have SBP less than 130 mmHg was 0.873. But this is also counting women with SBP under 110 mmHg, whom we now want to exclude. The proportion of women with SBP under 110 is found with the following code:\n\npdist(\"norm\", q = 110, mean = 114, sd = 14, plot = FALSE)\n\n[1] 0.3875485\n\n\nTherefore, all we have to do is calculate 0.873 minus 0.388:\n\nThe model predicts that 48.5902564% of women ages 30–44 in the U.S. and Canada will have systolic blood pressure between 110 mmHg and 130 mmHg.\n\n(Again, don’t forget the parentheses.)\nWhat about the pretty picture? Unfortunately, this doesn’t work so well:\n\npdist(\"norm\", q = 130, mean = 114, sd = 14) - \n    pdist(\"norm\", q = 110, mean = 114, sd = 14)\n\n\n\n\n\n\n\n[1] 0.4859026\n\n\nThe code is bulky and it prints two pictures, neither of which are quite right for our question.\nInstead, let’s observe that the pdist command can include both values (110 and 130) using the vector notation c:\n\npdist(\"norm\", q = c(110, 130), mean = 114, sd = 14)\n\n\n\n\n[1] 0.3875485 0.8734510\n\n\nNow the picture looks great and you can see the proportion you desire in the area between the two lines at 110 and 130.\nThis doesn’t work so well for the numerical output though. Observe:\n\npdist(\"norm\", q = c(110, 130), mean = 114, sd = 14, plot = FALSE)\n\n[1] 0.3875485 0.8734510\n\n\nThere are two numbers shown, but neither is the correct answer. This command shows the percentages below 110 and below 130, respectively, but not the area in between 110 and 130. We still have to subtract. However, R can do this for us easily with the diff command:\n\npdist(\"norm\", q = c(110, 130), mean = 114, sd = 14, plot = FALSE) %&gt;%\n    diff()\n\n[1] 0.4859026\n\n\nAgain, for inline R code, you don’t need to specify plot = FALSE:\n\nThe model predicts that 48.5902564% of women ages 30–44 in the U.S. and Canada will have systolic blood pressure between 110 mmHg and 130 mmHg.\n\nFor the following exercises, we’ll use a running example of IQ scores. Keep in mind that, at best, IQ scores fail to measure anything like “intelligence” (https://www.sciencedaily.com/releases/2012/12/121219133334.htm). At worse, IQ tests (and other forms of standardized testing) have been used to perpetuate systemic racism and inequality (https://www.nea.org/advocating-for-change/new-from-nea/racist-beginnings-standardized-testing).\nIQ scores—whatever they actually measure—are standardized so that they have a mean of 100 and a standard deviation of 16. For each exercise, use the pdist to draw the right picture and then state your answer in a contextually meaningful full sentence using inline R code. Don’t forget to use the phrase “The model predicts…” and report numbers as percentages, not decimals.\n\nExercise 1(a)\nWhat percentage of people would you expect to have IQ scores over 80?\n\n\n# Add code here to draw the model.\n\nPlease write up your answer here.\n\n\n\nExercise 1(b)\nWhat percentage of people would you expect to have IQ scores under 90?\n\n\n# Add code here to draw the model.\n\nPlease write up your answer here.\n\n\n\nExercise 1(c)\nWhat percentage of people would you expect to have IQ scores between 112 and 132?\n\n\n# Add code here to draw the model.\n\nPlease write up your answer here."
  },
  {
    "objectID": "13-normal_models-web.html#normal-percentiles",
    "href": "13-normal_models-web.html#normal-percentiles",
    "title": "13  Normal models",
    "section": "13.6 Percentiles",
    "text": "13.6 Percentiles\nOften, the question is reversed: instead of getting a value and being asked what percentage of the population falls above or below it, we are given a percentile and asked about the value to which it corresponds.\nHere is an example using systolic blood pressure: what is the cutoff value of SBP for the lowest 25% of women ages 30–44 in the U.S. and Canada? In other words, what is the 25th percentile of SBP for this group of women?\nThe command we need is qdist. It looks a lot like pdist. Observe:\n\nqdist(\"norm\", p = 0.25, mean = 114, sd = 14)\n\n\n\n\n[1] 104.5571\n\n\nThe only change here is that one of the arguments is p instead of q, and the value of p is a proportion (between 0 and 1) instead of a value of SBP. The output is now an SBP value.\nHere it is inline:\n\nThe model predicts that the 25th percentile for SBP in women ages 30–44 in the U.S. and Canada is 104.5571435 mmHg.\n\nWhat if we asked about the highest 10% of women? All you have to do is remember that the top 10% is actually the 90th percentile.\n\nqdist(\"norm\", p = 0.9, mean = 114, sd = 14)\n\n\n\n\n[1] 131.9417\n\n\n\nThe model predicts that the top 10% of SBP in women ages 30–44 in the U.S. and Canada have SBP higher than 131.9417219 mmHg.\n\nFinally, what if we want the middle 50%? This is trickier. The middle 50% lies between the 25th percentile and the 75th percentile. Observe the syntax below:\n\nqdist(\"norm\", p = c(0.25, 0.75), mean = 114, sd = 14)\n\n\n\n\n[1] 104.5571 123.4429\n\n\n\nTherefore, the model predicts that the middle 50% of SBP for women ages 30–44 in the U.S. and Canada lies between 104.5571435 mmHg and 123.4428565 mmHg.\n\nWe did something tricky in the inline code above. Because the qdist command produces two values (one at the 25th percentile and one at the 75th percentile), we can grab each value separately by appending [1] or [2] to the end of the command.\nFor the exercises below, we’ll continue to use IQ scores (mean of 100 and standard deviation of 16). Use the qdist command to draw the right picture and then state your answer in a contextually meaningful full sentence. Don’t forget to use the phrase “The model predicts…”\n\nExercise 2(a)\nWhat cutoff value bounds the highest 5% of IQ scores?\n\n\n# Add code here to draw the model.\n\nPlease write up your answer here.\n\n\n\nExercise 2(b)\nWhat cutoff value bounds the lowest 30% of IQ scores?\n\n\n# Add code here to draw the model.\n\nPlease write up your answer here.\n\n\n\nExercise 2(c)\nWhat cutoff values bound the middle 80% of IQ scores?\n\n\n# Add code here to draw the model.\n\nPlease write up your answer here."
  },
  {
    "objectID": "13-normal_models-web.html#normal-z",
    "href": "13-normal_models-web.html#normal-z",
    "title": "13  Normal models",
    "section": "13.7 Z scores",
    "text": "13.7 Z scores\nSometimes it is easier to refer to a value in terms of how many standard deviations it lies from the mean. For example, a systolic blood pressure of 100 is 14 mmHg below the mean, but since the standard deviation is 14 mmHg, this means that 100 is one standard deviation below the mean. This distance from the mean in terms of standard deviations is called a z score.\nWe calculate z scores using the following formula:\n\\[\nz = \\frac{x - \\mu}{\\sigma}.\n\\]\nIn our example, if we wanted to know the z score for an SBP of 100, we just plug all the numbers into the formula above:\n\\[\nz = \\frac{100 - 114}{14} = -1.\n\\]\nWhat is the z score for an SBP of 132? Look at the graph of the normal model \\(N(114, 14)\\):\n\n# Don't worry about the syntax here.\n# You won't need to know how to do this on your own.\nggplot(data.frame(x = c(58, 170)), aes(x)) +\n    stat_function(fun = dnorm, args = list(mean = 114, sd = 14)) +\n    scale_x_continuous(breaks = c(72, 86, 100, 114, 128, 142, 156)) +\n    geom_vline(xintercept = 132, color = \"blue\")\n\n\n\n\nWe can see that 132 lies between 128 and 142, which are 1 and 2 standard deviations above the mean, respectively. The exact z score is\n\\[\nz = \\frac{132 - 114}{14} = 1.285714.\n\\]\nThe scale function from R also computes z scores. Just note that the function takes arguments center and scale, not mean and sd.\n\nscale(x = 100, center = 114, scale = 14)\n\n     [,1]\n[1,]   -1\nattr(,\"scaled:center\")\n[1] 114\nattr(,\"scaled:scale\")\n[1] 14\n\n\n\nscale(x = 132, center = 114, scale = 14)\n\n         [,1]\n[1,] 1.285714\nattr(,\"scaled:center\")\n[1] 114\nattr(,\"scaled:scale\")\n[1] 14\n\n\nAlso note that the function spits about a bunch of extra crap we don’t care about. This goes away for inline code. Go ahead and preview the HTML file now so you can see the effect in the following sentence:\n\nThe z score for 100 is -1 and the z score for 132 is 1.2857143.\n\n\nExercise 3\nIf IQ scores have a mean of 100 and a standard deviation of 16, what are the z scores for the following IQ scores? Write up your answers as full sentences using inline R code.\n\n80\n\n\nPlease write up your answer here.\n\n\n102\n\n\nPlease write up your answer here.\n\n\n130\n\n\nPlease write up your answer here.\n\n\nWorking with z scores also makes it easier to work with normal models. The default settings for pdist and qdist are mean = 0 and sd = 1. That saves you some typing. So, for example, we calculated above that an SBP of 100 has a z score of -1. What percentage of women are expected to have SBP lower than 100?\n\npdist(\"norm\", q = -1)\n\n\n\n\n[1] 0.1586553\n\n\n\nThe model predicts that 15.8655254% of women ages 30–44 in the U.S. and Canada will have SBP less than 100.\n\n\n\nExercise 4\nAlbert Einstein supposedly had an IQ of 160. Calculate the z score for his IQ and then use that z score to figure out what percentage of the population is predicted to have higher IQ than Einstein. Use full sentences and inline R code to express your answer.\n\nPlease write up your answer here."
  },
  {
    "objectID": "13-normal_models-web.html#normal-qq",
    "href": "13-normal_models-web.html#normal-qq",
    "title": "13  Normal models",
    "section": "13.8 QQ plots",
    "text": "13.8 QQ plots\nAll of the work we do with normal models assumes that a normal model is appropriate. When we want to summarize data using a normal model, this means that the data distribution should be reasonably unimodal, symmetric, and with no serious outliers.\nWe can, of course, use a histogram to check this. But a histogram can be highly sensitive to the choice of bins. Furthermore, for small sample sizes, histograms look “chunky”, making it hard to test this assumption.\nAn easier way to check normality is to use a quantile-quantile plot, typically called a QQ plot or sometimes a normal probability plot. We won’t get into the technicalities of how this plot works. Suffice it to say that if data is normally distributed, the points of a QQ plot should lie along a diagonal line.\nHere is an example. The total snowfall in Grand Rapids, Michigan has been recorded every year since 1893. This data is included with the mosaic package in the data frame SnowGR. A histogram (with reasonable binning) shows that the data is nearly normal.\n\nggplot(SnowGR, aes(x = Total)) +\n    geom_histogram(binwidth = 10, boundary = 50)\n\nWarning: Removed 1 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nHere is the QQ plot for the same data. Notice that the aesthetics are a little different; instead of x, we have to use sample.\n\nggplot(SnowGR, aes(sample = Total)) +\n    geom_qq() +\n    geom_qq_line()\n\nWarning: Removed 1 rows containing non-finite values (`stat_qq()`).\n\n\nWarning: Removed 1 rows containing non-finite values (`stat_qq_line()`).\n\n\n\n\n\n(The warning is because there is one missing value in the data.)\nThe geom_qq() layer plots the dots and the geom_qq_line() layer plots a diagonal line that the dots should more or less follow.\nOther than a few points here and there, the bulk of the data is lined up nicely. There’s a minor outlier, and that can be seen in both the histogram and the QQ plot.\nContrast that with skewed data. For example, the Alcohol data set contains per capita consumption (in liters) of alcohol for various countries over several years. The alcohol consumption variable is highly skewed, as one can see in the histogram.\n\nggplot(Alcohol, aes(x = alcohol)) +\n    geom_histogram(binwidth = 2, boundary = 0)\n\n\n\n\nIt is also apparent in the QQ plot that the data is not normally distributed.\n\nggplot(Alcohol, aes(sample = alcohol)) +\n    geom_qq() +\n    geom_qq_line()\n\n\n\n\nThe path of dots is sharply curved, indicating a lack of normality.\n\nExercise 5(a)\nFind a data set with a numerical variable that is nearly normal in its distribution. (It can be something we’ve already seen in a past chapter, or if you’re really ambitious, you’re welcome to find a new data set.) Plot both a histogram and a QQ plot to demonstrate that the data is nearly normal. No need for a written response. Just plot the graphs.\nBe aware that if you use a data set from a package, you may have to add library(PACKAGE) to your code. (You replace the word PACKAGE with whatever package you need.)\n\n\n# Add code here to plot a histogram.\n\n\n# Add code here to plot a QQ plot.\n\n\n\n\nExercise 5(b)\nNow find a data set with a numerical variable that is skewed in its distribution. Plot both a histogram and a QQ plot to demonstrate that the data is not normal. Again, no need for a written response. Just plot the graphs.\n\n\n# Add code here to plot a histogram.\n\n\n# Add code here to plot a QQ plot."
  },
  {
    "objectID": "13-normal_models-web.html#normal-conclusion",
    "href": "13-normal_models-web.html#normal-conclusion",
    "title": "13  Normal models",
    "section": "13.9 Conclusion",
    "text": "13.9 Conclusion\nThe normal model is ubiquitous in statistics, so understanding how to use it to make predictions is critical. When certain assumptions are met (that will be discussed in a future chapter), we can use the normal model to make predictions. The use of z scores allows us to measure distances from the mean in terms of standard deviations, giving us a scale in which data from different contexts are comparable as long as such measurements are normally distributed. A QQ plot helps us check that assumption.\n\n13.9.1 Preparing and submitting your assignment\n\nFrom the “Run” menu, select “Restart R and Run All Chunks”.\nDeal with any code errors that crop up. Repeat steps 1–-2 until there are no more code errors.\nSpell check your document by clicking the icon with “ABC” and a check mark.\nHit the “Preview” button one last time to generate the final draft of the .nb.html file.\nProofread the HTML file carefully. If there are errors, go back and fix them, then repeat steps 1–5 again.\n\nIf you have completed this chapter as part of a statistics course, follow the directions you receive from your professor to submit your assignment."
  },
  {
    "objectID": "13-normal_models-web.html#footnotes",
    "href": "13-normal_models-web.html#footnotes",
    "title": "13  Normal models",
    "section": "",
    "text": "Statistics from the World Health Organization: http://www.who.int/publications/cra/chapters/volume1/0281-0390.pdf↩︎\nWhen you preview this in HTML, you’ll see a ridiculous number of decimal places that R reports. It’s a bit of a hassle to try to change it, so we’ll just ignore the issue.↩︎"
  },
  {
    "objectID": "14-sampling_distribution_models-web.html#samp-dist-models-intro",
    "href": "14-sampling_distribution_models-web.html#samp-dist-models-intro",
    "title": "14  Sampling distribution models",
    "section": "14.1 Introduction",
    "text": "14.1 Introduction\nIn this chapter, we’ll revisit the idea of a sampling distribution model. We’ve already seen how useful it can be to simulate the process of simulating samples from a population and looking at the distribution of values that can occur by chance (i.e., sampling variability). We’ve also had some experience working with normal models. Under certain assumptions, we can use normal models to approximate our simulated sampling distributions.\n\n14.1.1 Install new packages\nThere are no new packages used in this chapter.\n\n\n14.1.2 Download the R notebook file\nCheck the upper-right corner in RStudio to make sure you’re in your intro_stats project. Then click on the following link to download this chapter as an R notebook file (.Rmd).\nhttps://vectorposse.github.io/intro_stats/chapter_downloads/14-sampling_distribution_models.Rmd\nOnce the file is downloaded, move it to your project folder in RStudio and open it there.\n\n\n14.1.3 Restart R and run all chunks\nIn RStudio, select “Restart R and Run All Chunks” from the “Run” menu."
  },
  {
    "objectID": "14-sampling_distribution_models-web.html#samp-dist-models-load",
    "href": "14-sampling_distribution_models-web.html#samp-dist-models-load",
    "title": "14  Sampling distribution models",
    "section": "14.2 Load packages",
    "text": "14.2 Load packages\nWe load the standard tidyvese package. The mosaic package will provide coin flips.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nWarning: package 'mosaic' was built under R version 4.3.1\n\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum"
  },
  {
    "objectID": "14-sampling_distribution_models-web.html#samp-dist-models-sampling-variability",
    "href": "14-sampling_distribution_models-web.html#samp-dist-models-sampling-variability",
    "title": "14  Sampling distribution models",
    "section": "14.3 Sampling variability and sample size",
    "text": "14.3 Sampling variability and sample size\nWe know that when we sample from a population, our sample is “wrong”: even when the sample is representative of the population, we don’t actually expect our sample statistic to agree exactly with the population parameter of interest. Our prior simulations have demonstrated this. They are centered on the “true” value (for example, in a hypothesis test, the “true” value is the assumed null value), but there is some spread due to sampling variability.\nLet’s explore this idea a little further, this time considering how sample size plays a role in sampling variability.\nSuppose that a certain candidate in an election actually has 64% of the support of registered voters. We conduct a poll of 10 random people, gathering a representative (though not very large) sample of voters.\nWe can simulate this task in R by using the rflip command from the mosaic package. Remember that the default for a coin flip is a 50% probability of heads, so we have to change that if we want to model a candidate with 64% support.\n\nset.seed(13579)\nrflip(10, prob = 0.64)\n\n\nFlipping 10 coins [ Prob(Heads) = 0.64 ] ...\n\nH T H T H H H T T H\n\nNumber of Heads: 6 [Proportion Heads: 0.6]\n\n\nYou can think of the above command as taking one random sample of size 10 and getting a certain number of “successes”, where a “success” is a person who votes for our candidate—here encoded as “heads”. In other words, of the 10 people in this particular sample, we surveyed 6 people who said they were voting for our candidate and 4 people who were not.\nUsing the do command, we can simulate many samples, all of size 10. Let’s take 1000 samples and store them in a variable called sims_1000_10.\n\nset.seed(13579)\nsims_1000_10 &lt;- do(1000) * rflip(10, prob = 0.64)\nsims_1000_10\n\n      n heads tails prop\n1    10     5     5  0.5\n2    10     6     4  0.6\n3    10     8     2  0.8\n4    10     7     3  0.7\n5    10     8     2  0.8\n6    10     7     3  0.7\n7    10     5     5  0.5\n8    10     7     3  0.7\n9    10     6     4  0.6\n10   10     6     4  0.6\n11   10     6     4  0.6\n12   10     7     3  0.7\n13   10     6     4  0.6\n14   10     8     2  0.8\n15   10     6     4  0.6\n16   10     9     1  0.9\n17   10     5     5  0.5\n18   10     6     4  0.6\n19   10     9     1  0.9\n20   10     5     5  0.5\n21   10     5     5  0.5\n22   10     5     5  0.5\n23   10     4     6  0.4\n24   10     6     4  0.6\n25   10     9     1  0.9\n26   10     4     6  0.4\n27   10     8     2  0.8\n28   10     8     2  0.8\n29   10     8     2  0.8\n30   10     3     7  0.3\n31   10     8     2  0.8\n32   10     8     2  0.8\n33   10     5     5  0.5\n34   10     4     6  0.4\n35   10     7     3  0.7\n36   10     6     4  0.6\n37   10     5     5  0.5\n38   10     5     5  0.5\n39   10     6     4  0.6\n40   10     8     2  0.8\n41   10     7     3  0.7\n42   10     6     4  0.6\n43   10     8     2  0.8\n44   10     7     3  0.7\n45   10     5     5  0.5\n46   10     9     1  0.9\n47   10     8     2  0.8\n48   10     9     1  0.9\n49   10     8     2  0.8\n50   10     6     4  0.6\n51   10     5     5  0.5\n52   10     7     3  0.7\n53   10     9     1  0.9\n54   10     7     3  0.7\n55   10     7     3  0.7\n56   10     7     3  0.7\n57   10     5     5  0.5\n58   10     8     2  0.8\n59   10     4     6  0.4\n60   10     7     3  0.7\n61   10     5     5  0.5\n62   10     6     4  0.6\n63   10     5     5  0.5\n64   10     8     2  0.8\n65   10     6     4  0.6\n66   10     7     3  0.7\n67   10     7     3  0.7\n68   10     4     6  0.4\n69   10     7     3  0.7\n70   10     7     3  0.7\n71   10     7     3  0.7\n72   10     3     7  0.3\n73   10     6     4  0.6\n74   10     6     4  0.6\n75   10     5     5  0.5\n76   10     7     3  0.7\n77   10     6     4  0.6\n78   10     5     5  0.5\n79   10     4     6  0.4\n80   10     9     1  0.9\n81   10     5     5  0.5\n82   10     8     2  0.8\n83   10     5     5  0.5\n84   10     7     3  0.7\n85   10     8     2  0.8\n86   10     4     6  0.4\n87   10     6     4  0.6\n88   10     6     4  0.6\n89   10     8     2  0.8\n90   10     8     2  0.8\n91   10     6     4  0.6\n92   10     8     2  0.8\n93   10     8     2  0.8\n94   10     5     5  0.5\n95   10     7     3  0.7\n96   10     9     1  0.9\n97   10     8     2  0.8\n98   10     5     5  0.5\n99   10     8     2  0.8\n100  10     8     2  0.8\n101  10     6     4  0.6\n102  10     6     4  0.6\n103  10     5     5  0.5\n104  10     5     5  0.5\n105  10     8     2  0.8\n106  10     5     5  0.5\n107  10     6     4  0.6\n108  10     8     2  0.8\n109  10     5     5  0.5\n110  10     6     4  0.6\n111  10     7     3  0.7\n112  10     9     1  0.9\n113  10     8     2  0.8\n114  10     6     4  0.6\n115  10     9     1  0.9\n116  10     7     3  0.7\n117  10     8     2  0.8\n118  10     4     6  0.4\n119  10     9     1  0.9\n120  10     6     4  0.6\n121  10     6     4  0.6\n122  10     8     2  0.8\n123  10     5     5  0.5\n124  10     6     4  0.6\n125  10     7     3  0.7\n126  10     7     3  0.7\n127  10     5     5  0.5\n128  10     4     6  0.4\n129  10     4     6  0.4\n130  10     4     6  0.4\n131  10     5     5  0.5\n132  10     5     5  0.5\n133  10     7     3  0.7\n134  10     5     5  0.5\n135  10     8     2  0.8\n136  10     7     3  0.7\n137  10     6     4  0.6\n138  10     5     5  0.5\n139  10     8     2  0.8\n140  10     5     5  0.5\n141  10     8     2  0.8\n142  10     6     4  0.6\n143  10     3     7  0.3\n144  10     5     5  0.5\n145  10     5     5  0.5\n146  10     7     3  0.7\n147  10     7     3  0.7\n148  10     8     2  0.8\n149  10     7     3  0.7\n150  10     6     4  0.6\n151  10    10     0  1.0\n152  10     8     2  0.8\n153  10     7     3  0.7\n154  10     4     6  0.4\n155  10     5     5  0.5\n156  10     9     1  0.9\n157  10     6     4  0.6\n158  10    10     0  1.0\n159  10     6     4  0.6\n160  10     7     3  0.7\n161  10     8     2  0.8\n162  10     7     3  0.7\n163  10     6     4  0.6\n164  10     7     3  0.7\n165  10     6     4  0.6\n166  10     8     2  0.8\n167  10     4     6  0.4\n168  10     7     3  0.7\n169  10     6     4  0.6\n170  10     8     2  0.8\n171  10     6     4  0.6\n172  10     7     3  0.7\n173  10     4     6  0.4\n174  10     5     5  0.5\n175  10     6     4  0.6\n176  10     7     3  0.7\n177  10     4     6  0.4\n178  10     4     6  0.4\n179  10     7     3  0.7\n180  10     8     2  0.8\n181  10     7     3  0.7\n182  10     4     6  0.4\n183  10     7     3  0.7\n184  10     5     5  0.5\n185  10     4     6  0.4\n186  10     3     7  0.3\n187  10     5     5  0.5\n188  10     6     4  0.6\n189  10     6     4  0.6\n190  10     7     3  0.7\n191  10     7     3  0.7\n192  10     6     4  0.6\n193  10     6     4  0.6\n194  10     6     4  0.6\n195  10     8     2  0.8\n196  10     9     1  0.9\n197  10     7     3  0.7\n198  10     4     6  0.4\n199  10     6     4  0.6\n200  10     8     2  0.8\n201  10     5     5  0.5\n202  10     8     2  0.8\n203  10     5     5  0.5\n204  10     6     4  0.6\n205  10     9     1  0.9\n206  10     6     4  0.6\n207  10     6     4  0.6\n208  10     3     7  0.3\n209  10     4     6  0.4\n210  10     5     5  0.5\n211  10     6     4  0.6\n212  10     8     2  0.8\n213  10     7     3  0.7\n214  10     6     4  0.6\n215  10     7     3  0.7\n216  10     6     4  0.6\n217  10     6     4  0.6\n218  10     7     3  0.7\n219  10     5     5  0.5\n220  10     6     4  0.6\n221  10     7     3  0.7\n222  10     9     1  0.9\n223  10     6     4  0.6\n224  10     9     1  0.9\n225  10     4     6  0.4\n226  10     7     3  0.7\n227  10     5     5  0.5\n228  10     6     4  0.6\n229  10     6     4  0.6\n230  10     7     3  0.7\n231  10     6     4  0.6\n232  10     6     4  0.6\n233  10     8     2  0.8\n234  10     6     4  0.6\n235  10     7     3  0.7\n236  10     6     4  0.6\n237  10     8     2  0.8\n238  10     5     5  0.5\n239  10     7     3  0.7\n240  10     6     4  0.6\n241  10     4     6  0.4\n242  10     4     6  0.4\n243  10     7     3  0.7\n244  10     7     3  0.7\n245  10     6     4  0.6\n246  10     2     8  0.2\n247  10     7     3  0.7\n248  10     7     3  0.7\n249  10     6     4  0.6\n250  10     7     3  0.7\n251  10     8     2  0.8\n252  10     7     3  0.7\n253  10     7     3  0.7\n254  10     8     2  0.8\n255  10     7     3  0.7\n256  10     6     4  0.6\n257  10     8     2  0.8\n258  10     7     3  0.7\n259  10     7     3  0.7\n260  10     5     5  0.5\n261  10     7     3  0.7\n262  10     5     5  0.5\n263  10     5     5  0.5\n264  10     7     3  0.7\n265  10     5     5  0.5\n266  10     4     6  0.4\n267  10     7     3  0.7\n268  10     8     2  0.8\n269  10     8     2  0.8\n270  10     4     6  0.4\n271  10     8     2  0.8\n272  10     6     4  0.6\n273  10     7     3  0.7\n274  10     9     1  0.9\n275  10     8     2  0.8\n276  10     4     6  0.4\n277  10     8     2  0.8\n278  10     6     4  0.6\n279  10     6     4  0.6\n280  10     7     3  0.7\n281  10     9     1  0.9\n282  10    10     0  1.0\n283  10     8     2  0.8\n284  10     9     1  0.9\n285  10     9     1  0.9\n286  10     7     3  0.7\n287  10     6     4  0.6\n288  10     8     2  0.8\n289  10     6     4  0.6\n290  10     5     5  0.5\n291  10     7     3  0.7\n292  10     7     3  0.7\n293  10     5     5  0.5\n294  10     6     4  0.6\n295  10     5     5  0.5\n296  10     5     5  0.5\n297  10     4     6  0.4\n298  10     8     2  0.8\n299  10     9     1  0.9\n300  10     6     4  0.6\n301  10     5     5  0.5\n302  10     5     5  0.5\n303  10     9     1  0.9\n304  10     5     5  0.5\n305  10     5     5  0.5\n306  10     6     4  0.6\n307  10     6     4  0.6\n308  10     9     1  0.9\n309  10     9     1  0.9\n310  10     6     4  0.6\n311  10     7     3  0.7\n312  10     8     2  0.8\n313  10     7     3  0.7\n314  10     8     2  0.8\n315  10     3     7  0.3\n316  10     7     3  0.7\n317  10     6     4  0.6\n318  10     7     3  0.7\n319  10     7     3  0.7\n320  10     8     2  0.8\n321  10     8     2  0.8\n322  10     9     1  0.9\n323  10     8     2  0.8\n324  10     7     3  0.7\n325  10     7     3  0.7\n326  10     8     2  0.8\n327  10     7     3  0.7\n328  10     7     3  0.7\n329  10     4     6  0.4\n330  10     5     5  0.5\n331  10     7     3  0.7\n332  10     7     3  0.7\n333  10     5     5  0.5\n334  10     6     4  0.6\n335  10     8     2  0.8\n336  10     5     5  0.5\n337  10     6     4  0.6\n338  10     7     3  0.7\n339  10     9     1  0.9\n340  10     7     3  0.7\n341  10     6     4  0.6\n342  10     4     6  0.4\n343  10     5     5  0.5\n344  10     7     3  0.7\n345  10     7     3  0.7\n346  10     7     3  0.7\n347  10     6     4  0.6\n348  10     7     3  0.7\n349  10     6     4  0.6\n350  10     8     2  0.8\n351  10     5     5  0.5\n352  10    10     0  1.0\n353  10     5     5  0.5\n354  10     7     3  0.7\n355  10     7     3  0.7\n356  10     5     5  0.5\n357  10     7     3  0.7\n358  10     7     3  0.7\n359  10     5     5  0.5\n360  10     8     2  0.8\n361  10     8     2  0.8\n362  10     6     4  0.6\n363  10     6     4  0.6\n364  10     6     4  0.6\n365  10     5     5  0.5\n366  10     6     4  0.6\n367  10     5     5  0.5\n368  10     7     3  0.7\n369  10     8     2  0.8\n370  10     4     6  0.4\n371  10     4     6  0.4\n372  10     6     4  0.6\n373  10     7     3  0.7\n374  10     6     4  0.6\n375  10     6     4  0.6\n376  10     8     2  0.8\n377  10     5     5  0.5\n378  10     7     3  0.7\n379  10     6     4  0.6\n380  10     6     4  0.6\n381  10     4     6  0.4\n382  10     4     6  0.4\n383  10     6     4  0.6\n384  10     8     2  0.8\n385  10     5     5  0.5\n386  10     6     4  0.6\n387  10     7     3  0.7\n388  10     6     4  0.6\n389  10     8     2  0.8\n390  10     8     2  0.8\n391  10     6     4  0.6\n392  10     5     5  0.5\n393  10     8     2  0.8\n394  10     5     5  0.5\n395  10     6     4  0.6\n396  10     6     4  0.6\n397  10     5     5  0.5\n398  10     4     6  0.4\n399  10     7     3  0.7\n400  10     7     3  0.7\n401  10     9     1  0.9\n402  10     6     4  0.6\n403  10     6     4  0.6\n404  10     5     5  0.5\n405  10     8     2  0.8\n406  10     5     5  0.5\n407  10     9     1  0.9\n408  10     7     3  0.7\n409  10     6     4  0.6\n410  10     6     4  0.6\n411  10     9     1  0.9\n412  10     4     6  0.4\n413  10     4     6  0.4\n414  10     7     3  0.7\n415  10     7     3  0.7\n416  10     6     4  0.6\n417  10     5     5  0.5\n418  10     6     4  0.6\n419  10     6     4  0.6\n420  10     6     4  0.6\n421  10     7     3  0.7\n422  10     8     2  0.8\n423  10     6     4  0.6\n424  10     7     3  0.7\n425  10     8     2  0.8\n426  10     5     5  0.5\n427  10     8     2  0.8\n428  10     8     2  0.8\n429  10     6     4  0.6\n430  10     5     5  0.5\n431  10     4     6  0.4\n432  10     7     3  0.7\n433  10     6     4  0.6\n434  10     6     4  0.6\n435  10     9     1  0.9\n436  10     5     5  0.5\n437  10     5     5  0.5\n438  10     6     4  0.6\n439  10     6     4  0.6\n440  10     7     3  0.7\n441  10     6     4  0.6\n442  10     8     2  0.8\n443  10     6     4  0.6\n444  10     5     5  0.5\n445  10     7     3  0.7\n446  10     6     4  0.6\n447  10     5     5  0.5\n448  10     7     3  0.7\n449  10     6     4  0.6\n450  10     5     5  0.5\n451  10     9     1  0.9\n452  10     8     2  0.8\n453  10     8     2  0.8\n454  10     5     5  0.5\n455  10     6     4  0.6\n456  10     5     5  0.5\n457  10     8     2  0.8\n458  10     8     2  0.8\n459  10     8     2  0.8\n460  10     5     5  0.5\n461  10     7     3  0.7\n462  10     5     5  0.5\n463  10     5     5  0.5\n464  10     8     2  0.8\n465  10     4     6  0.4\n466  10     6     4  0.6\n467  10     6     4  0.6\n468  10     8     2  0.8\n469  10     8     2  0.8\n470  10     6     4  0.6\n471  10     6     4  0.6\n472  10    10     0  1.0\n473  10     4     6  0.4\n474  10     8     2  0.8\n475  10     6     4  0.6\n476  10     6     4  0.6\n477  10     9     1  0.9\n478  10     7     3  0.7\n479  10     7     3  0.7\n480  10     5     5  0.5\n481  10     7     3  0.7\n482  10     5     5  0.5\n483  10     5     5  0.5\n484  10     8     2  0.8\n485  10     7     3  0.7\n486  10     7     3  0.7\n487  10     6     4  0.6\n488  10     6     4  0.6\n489  10     6     4  0.6\n490  10     8     2  0.8\n491  10     8     2  0.8\n492  10     2     8  0.2\n493  10     5     5  0.5\n494  10     8     2  0.8\n495  10     7     3  0.7\n496  10     8     2  0.8\n497  10     5     5  0.5\n498  10     7     3  0.7\n499  10     7     3  0.7\n500  10     9     1  0.9\n501  10     6     4  0.6\n502  10     4     6  0.4\n503  10     6     4  0.6\n504  10     5     5  0.5\n505  10     4     6  0.4\n506  10     7     3  0.7\n507  10     7     3  0.7\n508  10     5     5  0.5\n509  10     6     4  0.6\n510  10     6     4  0.6\n511  10     7     3  0.7\n512  10     6     4  0.6\n513  10     3     7  0.3\n514  10     7     3  0.7\n515  10     7     3  0.7\n516  10     6     4  0.6\n517  10     6     4  0.6\n518  10     6     4  0.6\n519  10     6     4  0.6\n520  10     8     2  0.8\n521  10     6     4  0.6\n522  10     8     2  0.8\n523  10     8     2  0.8\n524  10     7     3  0.7\n525  10     8     2  0.8\n526  10     7     3  0.7\n527  10     7     3  0.7\n528  10     5     5  0.5\n529  10     6     4  0.6\n530  10     8     2  0.8\n531  10     6     4  0.6\n532  10     4     6  0.4\n533  10     5     5  0.5\n534  10     5     5  0.5\n535  10     4     6  0.4\n536  10     7     3  0.7\n537  10     6     4  0.6\n538  10     9     1  0.9\n539  10     7     3  0.7\n540  10     4     6  0.4\n541  10     7     3  0.7\n542  10     3     7  0.3\n543  10    10     0  1.0\n544  10     5     5  0.5\n545  10     7     3  0.7\n546  10     8     2  0.8\n547  10     5     5  0.5\n548  10     6     4  0.6\n549  10     7     3  0.7\n550  10     7     3  0.7\n551  10     5     5  0.5\n552  10     7     3  0.7\n553  10     5     5  0.5\n554  10     7     3  0.7\n555  10     6     4  0.6\n556  10     7     3  0.7\n557  10     6     4  0.6\n558  10     5     5  0.5\n559  10     6     4  0.6\n560  10     7     3  0.7\n561  10     5     5  0.5\n562  10     6     4  0.6\n563  10     5     5  0.5\n564  10     7     3  0.7\n565  10     7     3  0.7\n566  10     6     4  0.6\n567  10     4     6  0.4\n568  10     5     5  0.5\n569  10     6     4  0.6\n570  10     4     6  0.4\n571  10     8     2  0.8\n572  10     7     3  0.7\n573  10     7     3  0.7\n574  10     7     3  0.7\n575  10     8     2  0.8\n576  10     6     4  0.6\n577  10     5     5  0.5\n578  10     8     2  0.8\n579  10     5     5  0.5\n580  10     6     4  0.6\n581  10     6     4  0.6\n582  10     7     3  0.7\n583  10     7     3  0.7\n584  10     8     2  0.8\n585  10     7     3  0.7\n586  10     7     3  0.7\n587  10     6     4  0.6\n588  10     5     5  0.5\n589  10     8     2  0.8\n590  10     8     2  0.8\n591  10     8     2  0.8\n592  10     6     4  0.6\n593  10     7     3  0.7\n594  10     6     4  0.6\n595  10     7     3  0.7\n596  10     5     5  0.5\n597  10     6     4  0.6\n598  10     6     4  0.6\n599  10     8     2  0.8\n600  10    10     0  1.0\n601  10     5     5  0.5\n602  10     4     6  0.4\n603  10     9     1  0.9\n604  10     7     3  0.7\n605  10     8     2  0.8\n606  10     7     3  0.7\n607  10     5     5  0.5\n608  10     4     6  0.4\n609  10     7     3  0.7\n610  10     7     3  0.7\n611  10     7     3  0.7\n612  10     8     2  0.8\n613  10     6     4  0.6\n614  10     7     3  0.7\n615  10     7     3  0.7\n616  10     7     3  0.7\n617  10     7     3  0.7\n618  10     5     5  0.5\n619  10     6     4  0.6\n620  10     7     3  0.7\n621  10     6     4  0.6\n622  10     6     4  0.6\n623  10     6     4  0.6\n624  10     6     4  0.6\n625  10     8     2  0.8\n626  10     7     3  0.7\n627  10     4     6  0.4\n628  10     6     4  0.6\n629  10     5     5  0.5\n630  10     4     6  0.4\n631  10     8     2  0.8\n632  10     5     5  0.5\n633  10     7     3  0.7\n634  10     6     4  0.6\n635  10     5     5  0.5\n636  10     6     4  0.6\n637  10     7     3  0.7\n638  10     8     2  0.8\n639  10     6     4  0.6\n640  10     5     5  0.5\n641  10     6     4  0.6\n642  10     9     1  0.9\n643  10     9     1  0.9\n644  10     4     6  0.4\n645  10     8     2  0.8\n646  10     8     2  0.8\n647  10     7     3  0.7\n648  10     8     2  0.8\n649  10     9     1  0.9\n650  10     7     3  0.7\n651  10     5     5  0.5\n652  10     5     5  0.5\n653  10     6     4  0.6\n654  10     8     2  0.8\n655  10     5     5  0.5\n656  10     8     2  0.8\n657  10     9     1  0.9\n658  10     8     2  0.8\n659  10     9     1  0.9\n660  10     7     3  0.7\n661  10     6     4  0.6\n662  10     8     2  0.8\n663  10     6     4  0.6\n664  10     7     3  0.7\n665  10     7     3  0.7\n666  10     8     2  0.8\n667  10     6     4  0.6\n668  10     7     3  0.7\n669  10     6     4  0.6\n670  10    10     0  1.0\n671  10     5     5  0.5\n672  10     7     3  0.7\n673  10     7     3  0.7\n674  10     8     2  0.8\n675  10     7     3  0.7\n676  10     4     6  0.4\n677  10     5     5  0.5\n678  10     7     3  0.7\n679  10     3     7  0.3\n680  10     6     4  0.6\n681  10     6     4  0.6\n682  10     6     4  0.6\n683  10     6     4  0.6\n684  10     7     3  0.7\n685  10     7     3  0.7\n686  10     4     6  0.4\n687  10     6     4  0.6\n688  10     6     4  0.6\n689  10     6     4  0.6\n690  10     6     4  0.6\n691  10     8     2  0.8\n692  10     8     2  0.8\n693  10     7     3  0.7\n694  10     6     4  0.6\n695  10     8     2  0.8\n696  10     7     3  0.7\n697  10     8     2  0.8\n698  10     8     2  0.8\n699  10     5     5  0.5\n700  10     9     1  0.9\n701  10     6     4  0.6\n702  10     7     3  0.7\n703  10     7     3  0.7\n704  10     6     4  0.6\n705  10     7     3  0.7\n706  10     8     2  0.8\n707  10     5     5  0.5\n708  10     7     3  0.7\n709  10     6     4  0.6\n710  10     6     4  0.6\n711  10     7     3  0.7\n712  10     7     3  0.7\n713  10     8     2  0.8\n714  10     4     6  0.4\n715  10     6     4  0.6\n716  10     5     5  0.5\n717  10     8     2  0.8\n718  10     6     4  0.6\n719  10     6     4  0.6\n720  10     4     6  0.4\n721  10     7     3  0.7\n722  10     6     4  0.6\n723  10     9     1  0.9\n724  10     7     3  0.7\n725  10     5     5  0.5\n726  10     7     3  0.7\n727  10     6     4  0.6\n728  10     6     4  0.6\n729  10     5     5  0.5\n730  10     8     2  0.8\n731  10     7     3  0.7\n732  10     6     4  0.6\n733  10     5     5  0.5\n734  10     6     4  0.6\n735  10     5     5  0.5\n736  10     4     6  0.4\n737  10     7     3  0.7\n738  10     7     3  0.7\n739  10     4     6  0.4\n740  10     7     3  0.7\n741  10     8     2  0.8\n742  10     6     4  0.6\n743  10     6     4  0.6\n744  10     7     3  0.7\n745  10    10     0  1.0\n746  10     4     6  0.4\n747  10     8     2  0.8\n748  10     7     3  0.7\n749  10     7     3  0.7\n750  10     4     6  0.4\n751  10     9     1  0.9\n752  10     7     3  0.7\n753  10     7     3  0.7\n754  10     9     1  0.9\n755  10     5     5  0.5\n756  10     8     2  0.8\n757  10     5     5  0.5\n758  10     8     2  0.8\n759  10     4     6  0.4\n760  10     8     2  0.8\n761  10     7     3  0.7\n762  10     8     2  0.8\n763  10     6     4  0.6\n764  10     8     2  0.8\n765  10     3     7  0.3\n766  10     9     1  0.9\n767  10     7     3  0.7\n768  10     6     4  0.6\n769  10     3     7  0.3\n770  10     4     6  0.4\n771  10     6     4  0.6\n772  10     6     4  0.6\n773  10     5     5  0.5\n774  10     4     6  0.4\n775  10     5     5  0.5\n776  10     7     3  0.7\n777  10     5     5  0.5\n778  10     8     2  0.8\n779  10     8     2  0.8\n780  10     6     4  0.6\n781  10     7     3  0.7\n782  10     6     4  0.6\n783  10     6     4  0.6\n784  10     6     4  0.6\n785  10     7     3  0.7\n786  10     7     3  0.7\n787  10     6     4  0.6\n788  10     6     4  0.6\n789  10     8     2  0.8\n790  10     6     4  0.6\n791  10     9     1  0.9\n792  10     5     5  0.5\n793  10     8     2  0.8\n794  10     4     6  0.4\n795  10     6     4  0.6\n796  10     5     5  0.5\n797  10     6     4  0.6\n798  10     6     4  0.6\n799  10     7     3  0.7\n800  10     3     7  0.3\n801  10     4     6  0.4\n802  10     6     4  0.6\n803  10     5     5  0.5\n804  10     7     3  0.7\n805  10     8     2  0.8\n806  10     7     3  0.7\n807  10     7     3  0.7\n808  10     4     6  0.4\n809  10     6     4  0.6\n810  10     8     2  0.8\n811  10     4     6  0.4\n812  10     7     3  0.7\n813  10     9     1  0.9\n814  10     7     3  0.7\n815  10     7     3  0.7\n816  10     6     4  0.6\n817  10     5     5  0.5\n818  10     8     2  0.8\n819  10     6     4  0.6\n820  10     6     4  0.6\n821  10     5     5  0.5\n822  10     8     2  0.8\n823  10     6     4  0.6\n824  10     4     6  0.4\n825  10     5     5  0.5\n826  10     3     7  0.3\n827  10     7     3  0.7\n828  10     9     1  0.9\n829  10     8     2  0.8\n830  10     7     3  0.7\n831  10     6     4  0.6\n832  10     5     5  0.5\n833  10     8     2  0.8\n834  10     6     4  0.6\n835  10     8     2  0.8\n836  10     5     5  0.5\n837  10    10     0  1.0\n838  10     5     5  0.5\n839  10     4     6  0.4\n840  10     7     3  0.7\n841  10     7     3  0.7\n842  10     7     3  0.7\n843  10     4     6  0.4\n844  10     7     3  0.7\n845  10     7     3  0.7\n846  10     7     3  0.7\n847  10     6     4  0.6\n848  10     8     2  0.8\n849  10     6     4  0.6\n850  10     5     5  0.5\n851  10     7     3  0.7\n852  10     7     3  0.7\n853  10     4     6  0.4\n854  10     7     3  0.7\n855  10     8     2  0.8\n856  10     2     8  0.2\n857  10     9     1  0.9\n858  10     6     4  0.6\n859  10     7     3  0.7\n860  10     5     5  0.5\n861  10     7     3  0.7\n862  10     6     4  0.6\n863  10     5     5  0.5\n864  10     7     3  0.7\n865  10     8     2  0.8\n866  10     4     6  0.4\n867  10     4     6  0.4\n868  10     5     5  0.5\n869  10     4     6  0.4\n870  10     4     6  0.4\n871  10     5     5  0.5\n872  10     6     4  0.6\n873  10     4     6  0.4\n874  10     5     5  0.5\n875  10     7     3  0.7\n876  10    10     0  1.0\n877  10     6     4  0.6\n878  10     7     3  0.7\n879  10     5     5  0.5\n880  10     9     1  0.9\n881  10     7     3  0.7\n882  10     5     5  0.5\n883  10     5     5  0.5\n884  10     8     2  0.8\n885  10     6     4  0.6\n886  10     5     5  0.5\n887  10     7     3  0.7\n888  10     7     3  0.7\n889  10     6     4  0.6\n890  10     7     3  0.7\n891  10     9     1  0.9\n892  10     7     3  0.7\n893  10     5     5  0.5\n894  10     8     2  0.8\n895  10     6     4  0.6\n896  10     5     5  0.5\n897  10     6     4  0.6\n898  10     6     4  0.6\n899  10     6     4  0.6\n900  10     8     2  0.8\n901  10     8     2  0.8\n902  10     7     3  0.7\n903  10     7     3  0.7\n904  10     3     7  0.3\n905  10     9     1  0.9\n906  10     4     6  0.4\n907  10     6     4  0.6\n908  10     9     1  0.9\n909  10     7     3  0.7\n910  10     7     3  0.7\n911  10     8     2  0.8\n912  10     4     6  0.4\n913  10     6     4  0.6\n914  10     7     3  0.7\n915  10     8     2  0.8\n916  10     5     5  0.5\n917  10     7     3  0.7\n918  10     5     5  0.5\n919  10     9     1  0.9\n920  10     7     3  0.7\n921  10     6     4  0.6\n922  10     6     4  0.6\n923  10     8     2  0.8\n924  10     6     4  0.6\n925  10     7     3  0.7\n926  10     5     5  0.5\n927  10     5     5  0.5\n928  10     5     5  0.5\n929  10     4     6  0.4\n930  10     6     4  0.6\n931  10     3     7  0.3\n932  10     5     5  0.5\n933  10     7     3  0.7\n934  10     7     3  0.7\n935  10     9     1  0.9\n936  10     7     3  0.7\n937  10     6     4  0.6\n938  10     6     4  0.6\n939  10     7     3  0.7\n940  10     7     3  0.7\n941  10     7     3  0.7\n942  10     7     3  0.7\n943  10     9     1  0.9\n944  10     8     2  0.8\n945  10     7     3  0.7\n946  10     7     3  0.7\n947  10     5     5  0.5\n948  10     5     5  0.5\n949  10     7     3  0.7\n950  10     7     3  0.7\n951  10     6     4  0.6\n952  10     4     6  0.4\n953  10     7     3  0.7\n954  10     5     5  0.5\n955  10     8     2  0.8\n956  10     6     4  0.6\n957  10     8     2  0.8\n958  10     6     4  0.6\n959  10     7     3  0.7\n960  10     6     4  0.6\n961  10     9     1  0.9\n962  10     6     4  0.6\n963  10     5     5  0.5\n964  10     5     5  0.5\n965  10     6     4  0.6\n966  10     7     3  0.7\n967  10     7     3  0.7\n968  10     8     2  0.8\n969  10     7     3  0.7\n970  10     7     3  0.7\n971  10     7     3  0.7\n972  10     4     6  0.4\n973  10     9     1  0.9\n974  10     6     4  0.6\n975  10     6     4  0.6\n976  10     8     2  0.8\n977  10     7     3  0.7\n978  10     7     3  0.7\n979  10     8     2  0.8\n980  10     3     7  0.3\n981  10     9     1  0.9\n982  10     4     6  0.4\n983  10     5     5  0.5\n984  10     6     4  0.6\n985  10     9     1  0.9\n986  10     5     5  0.5\n987  10     4     6  0.4\n988  10     8     2  0.8\n989  10     6     4  0.6\n990  10     5     5  0.5\n991  10     9     1  0.9\n992  10     7     3  0.7\n993  10     6     4  0.6\n994  10     5     5  0.5\n995  10     6     4  0.6\n996  10     6     4  0.6\n997  10     5     5  0.5\n998  10    10     0  1.0\n999  10     6     4  0.6\n1000 10     7     3  0.7\n\n\nNote that with 10 people, it is impossible to get a 64% success rate in any given sample. (That would be 6.4 people!) Nevertheless, we can see that many of the samples gave us around 5–8 successes, as we’d expect when the true population rate is 64%. Also, the mean number of successes across all simulations is 6.414, which is very close to 6.4.\nInstead of focusing on the total number of successes, let’s use the proportion of successes in each sample. We can graph our simulated proportions, just as we’ve done in previous chapters. (The fancy stuff in scale_x_continuous is just making sure that the x-axis goes from 0 to 1 and that the tick marks appear as multiples of 0.1.)\n\nggplot(sims_1000_10, aes(x = prop)) +\n    geom_histogram(binwidth = 0.05) +\n    scale_x_continuous(limits = c(0, 1.1),\n                       breaks = seq(0, 1, 0.1))\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\nBecause each sample has size 10, the proportion of successes can only be multiples of 0.1. Although the distribution is somewhat normally shaped, it is discrete (no values in between the bars) and there is an appreciable left skew.\nWhat happens if we increase the sample size to 20? (The binwidth has to change to see the discrete bars.)\n\nset.seed(13579)\nsims_1000_20 &lt;- do(1000) * rflip(20, prob = 0.64)\n\n\nggplot(sims_1000_20, aes(x = prop)) +\n    geom_histogram(binwidth = 0.025) +\n    scale_x_continuous(limits = c(0, 1.1),\n                       breaks = seq(0, 1, 0.1))\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\n\nExercise 1\nExplain how the distribution of simulations has changed going from a sample size of 10 to a sample size of 20.\n\nPlease write up your answer here.\n\n\n\nExercise 2(a)\nRun a set of simulations yourself, this time with samples of size 50. Use the same number of simulations (1000) and the same ggplot code from above (especially the scale_x_continuous option) so that the x-axis is scaled identically to the previous cases, but change the binwidth to 0.01.\n\n\nset.seed(13579)\n# Add code here to simulate 1000 random samples of size 50 and plot them.\n\n\n\n\nExercise 2(b)\nExplain how the distribution of simulations has changed going from a sample size of 10 to 20 to 50.\n\nPlease write up your answer here."
  },
  {
    "objectID": "14-sampling_distribution_models-web.html#samp-dist-models-se",
    "href": "14-sampling_distribution_models-web.html#samp-dist-models-se",
    "title": "14  Sampling distribution models",
    "section": "14.4 The sampling distribution model and the standard error",
    "text": "14.4 The sampling distribution model and the standard error\nIn the last chapter on normal models, we mentioned briefly the Central Limit Theorem and the fact that under certain assumptions, our simulations would look normally distributed. More concretely, the Central Limit Theorem tells us that as our sample size increases, the distribution of sample proportions looks more and more like a normal model. This model is called the sampling distribution model because it describes how many different samples from a population should be distributed.\nWhich normal model do we use? In other words, what is the mean and standard deviation of a normal model that describes a simulation of repeated samples?\nThe simulations above are all centered at the same place, 0.64. This is no surprise. If the true population proportion is 0.64, then we expect most of our samples to be around 64% (even if, as above, it is actually impossible to get exactly 64% in any given sample).\nBut what about the standard deviation? It seems to be changing with each sample size.\n\nExercise 3\nLooking at your simulations above, how does the standard deviation appear to change as the sample size increases? Intuitively, why do you think this happens? (Hint: think about the relationship between larger sample sizes and accuracy.)\n\nPlease write up your answer here.\n\n\nThe standard deviation of a sampling distribution is usually called the standard error. (The use of the word “error” in statistics does not mean that anyone made a mistake. A better word for error would be “uncertainty” or even just “variability”.)\nThere is some complicated mathematics involved in figuring out the standard error, so I’ll just tell you what it is. If \\(p\\) is the true population proportion, then the standard error is\n\\[\n\\sqrt{\\frac{p(1 - p)}{n}}.\n\\]\nTherefore, if the sample size is large enough, the sampling distribution model is nearly normal, and the correct normal model is\n\\[\nN\\left(p, \\sqrt{\\frac{p(1 - p)}{n}}\\right).\n\\]\nIn our election example, we can calculate the standard error for a sample of size 10:\n\\[\n\\sqrt{\\frac{p(1 - p)}{n}} = \\sqrt{\\frac{0.64(1 - 0.64)}{10}} = 0.152.\n\\]\nWe can do this easily using inline R code. (Remember that R is nothing more than a glorified calculator.) If a candidate has 64% of the vote and we take a sample of size 10, the standard error is 0.1517893. In other words, the sampling distribution model is\n\\[\nN(0.64, 0.152).\n\\]\nFor a sample of size 20, the standard error is 0.1073313 and the sampling distribution model is\n\\[\nN(0.64, 0.107).\n\\]\n\n\nExercise 4\nCalculate the standard error for the example above, but this time using a sample size of 50. Give your answer as a contextually meaningful full sentence using inline R code.\n\nPlease write up your answer here."
  },
  {
    "objectID": "14-sampling_distribution_models-web.html#samp-dist-models-conditions",
    "href": "14-sampling_distribution_models-web.html#samp-dist-models-conditions",
    "title": "14  Sampling distribution models",
    "section": "14.5 Conditions",
    "text": "14.5 Conditions\nLike anything in statistics, there are assumptions that have to be met before applying any technique. We must check that certain conditions are true before we can reasonably make the necessary assumptions required by our model.\nWhen we want to use a normal model, we have to make sure the sampling distribution model is truly normal (or nearly normal).\nFirst, we need our samples to be random. Clearly, when samples are not random, there is a danger of bias, and then all bets are off. Of course, in real life hardly any sample will be truly random, so being representative is the most we can usually hope for.\nSecond, our sample size must be less than 10% of the population size. The reasons for this are somewhat technical, and 10% is a rough guideline. The idea is that if we are sampling, we need our sample not to be a significant chunk of the population.\nThese two conditions are always important when sampling. Together, they help ensure that the mathematical assumption of independence is met. In other words, when these two conditions are met, there is a better chance that the data from one member of our sample will not influence nor be influenced by the data from another member.\nFor applying normal models, there is one more condition. It is called the “success/failure” condition. We need for the total number of successes to be at least 10 and, similarly, for the total number of failures to be at least 10.\nGo back and consider our first simulated sample. The true rate of success in the population was presumed to be 64%. Given that we were sampling only 10 individuals, this implies that, on average, we would expect 6.4 people out of 10 to vote for the candidate. And likewise, that means that we would expect 3.6 people to vote against the candidate. (Clearly, it is impossible in any given sample to get 6.4 votes for, or 3.6 votes against. But on average, this is what we expect.) In fact, since the sample size was 10, there was no way that we could meet the success/failure condition. When we plotted the histogram of simulated proportions, we saw the problem: with such small numbers, the histogram was skewed, and not normal.\nWe check the success/failure condition by calculating \\(np\\) and \\(n(1 - p)\\): \\(n\\) is the sample size and \\(p\\) is the proportion of successes. Therefore, \\(np\\) is the total number of successes. Since \\(1 - p\\) is the proportion of failures, \\(n(1 - p)\\) is the total number of failures. Each of the numbers \\(np\\) and \\(n(1 - p)\\) needs to be bigger than 10.\nIn our example, \\(n = 10\\) (the sample size), and \\(p = 0.64\\) (the probability of success). So\n\\[\nnp = 10(0.64) = 6.4\n\\]\nand\n\\[\nn(1 - p) = 10(1 - 0.64) = 10(0.36) = 3.6.\n\\]\nNeither of these numbers is bigger than 10.\nNotice that when \\(n\\) is large, the quantities \\(np\\) and \\(n(1 - p)\\) will also tend to be large. This is the content of the Central Limit Theorem: when sample sizes grow, the sampling distribution model becomes more and more normal.\nThere is something else going on too. Suppose that \\(n = 100\\) but \\(p = 0.01\\). The sample seems quite large, but let’s look at the sampling distribution through a simulation.\n\nset.seed(13579)\nsims_1000_100 &lt;- do(1000) * rflip(100, prob = 0.01)\n\n\nggplot(sims_1000_100, aes(x = prop)) +\n    geom_histogram(binwidth = 0.005) +\n    scale_x_continuous(limits = c(-0.01, 0.1),\n                       breaks = seq(0, 0.1, 0.01))\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\n(Note that the x-axis scale is much smaller than it was before.)\n\nExercise 5\nWhat’s the problem here? Despite having a fairly large sample size, why is this distribution so skewed?\n\nPlease write up your answer here.\n\n\nIn this scenario, the success/failure condition fails because\n\\[\nnp = (100)(0.01) = 1 \\ngeq 10.\n\\]\nIn other words, in a typical sample, we expect 1 success and 99 failures.\n\n\nExercise 6\nGoing back to the election example (in which the candidate has 64% of the vote), check that a sample size of 50 does satisfy the success/failure condition.\n\nPlease write up your answer here."
  },
  {
    "objectID": "14-sampling_distribution_models-web.html#samp-dist-models-predictions",
    "href": "14-sampling_distribution_models-web.html#samp-dist-models-predictions",
    "title": "14  Sampling distribution models",
    "section": "14.6 Using the model to make predictions",
    "text": "14.6 Using the model to make predictions\nOnce we know that a normal model is appropriate, we can employ all the tools we’ve previously developed to work with normal models, notably pdist and qdist.\nFor example, we know that samples can be “wrong” due to sampling variability. Even though we know the candidate has 64% support, most surveys are not going to give us back that exact number.\nCould a survey of 50 random voters accidentally predict defeat for the candidate even though the candidate will actually win with 64% support?\nLet’s simulate:\n\nset.seed(13579)\nsurvey_sim &lt;- do(1000) * rflip(50, prob = 0.64)\n\n\nggplot(survey_sim, aes(x = prop)) +\n    geom_histogram(binwidth = 0.01) +\n    geom_vline(xintercept = 0.5, color = \"blue\")\n\n\n\n\nIt looks like there are at least a few simulated samples that could come in less than 50% by chance.\nLet’s check the conditions to see if we can use a normal model:\n\nRandom\n\nWe are told that our 50 voters are a random sample.\n\n10%\n\nIt is safe to assume there are more than 500 voters for this election.\n\nSuccess/failure\n\nThe number of expected successes is 32 and the expected number of failures is 18. These are both greater than 10.\n\n\nSince the conditions are satisfied, our sampling distribution model can be approximated with a normal model. The standard error is 0.0678823. Therefore, our normal model is\n\\[\nN(0.64, 0.068).\n\\]\nBack to our original question. How likely is it that a random survey of 50 voters predicts defeat for the candidate? Well, any survey that comes in less than 50% will make it look like the candidate is going to lose. So we simply need to figure out how much of the sampling distribution lies below 50%. This is made simple with the pdist command. Note that we’ll get a more accurate answer if we include the formula for the standard error, rather than rounding it off as 0.068.\n\npdist(\"norm\", q = 0.5,\n      mean = 0.64, sd = sqrt(0.64 * (1 - 0.64) / 50))\n\n\n\n\n[1] 0.01958508\n\n\nFrom the picture, we can see that there is only about a 2% chance that one of our surveys of 50 voters could predict defeat. Using inline code, we calculate it as 1.9585083%. The vast majority of the time, then, when we go out and take such a survey, the results will show the candidate in the lead. It will likely not say exactly 64%; there is still a relatively wide range of values that seem to be possible outcomes of such surveys. Nevertheless, this range of values is mostly above 50%. Nevertheless, there is a small chance that the survey will give us the “wrong” answer and predict defeat for the candidate.1\n\nExercise 7(a)\nSuppose we are testing a new drug that is intended to reduce cholesterol levels in patients with high cholesterol. Also suppose that the drug works for 83% of such patients. When testing our drug, we use a suitably random sample of 143 individuals with high cholesterol.\nFirst, simulate the sampling distribution using 1000 samples, each of size 143. Plot the resulting sampling distribution.\n\n\nset.seed(13579)\n# Add code here to simulate 1000 samples of size 143\n# and plot the resulting distribution.\n\n\n\n\nExercise 7(b)\nNext, check the conditions that would allow you to use a normal model as a sampling distribution model. I’ve given you an outline below:\n\n\nRandom\n\n[Check condition here.]\n\n10%\n\n[Check condition here.]\n\nSuccess/failure\n\n[Check condition here.]\n\n\n\n\n\nExercise 7(c)\nIf the conditions are met, we can use a normal model as the sampling distribution model. What are the mean and standard error of this model? (You should use inline R code to calculate and report the standard error.)\n\nPlease write up your answer here.\n\n\n\nExercise 7(d)\nMarket analysis shows that unless the drug is effective in more than 85% of patients, doctors won’t prescribe it. Secretly, we know that the true rate of effectiveness is 83%, but the manufacturer doesn’t know that yet. They only have access to their drug trial data in which they had 143 patients with high cholesterol.\nUsing the normal model you just developed, determine how likely the drug trial data will be to show the drug as “effective” according to the 85% standard. In other words, how often will our sample give us a result that is 85% or higher (even though secretly we know the true effectiveness is only 83%)? Report your answer in a contextually-meaningful full sentence using inline R code. (Hint: you’ll need to use the pdist command.)\n\nPlease write up your answer here."
  },
  {
    "objectID": "14-sampling_distribution_models-web.html#samp-dist-models-conclusion",
    "href": "14-sampling_distribution_models-web.html#samp-dist-models-conclusion",
    "title": "14  Sampling distribution models",
    "section": "14.7 Conclusion",
    "text": "14.7 Conclusion\nIt is very easy to work with normal models. Therefore, when we want to study sampling variability, it is useful to have a normal model as a sampling distribution model. The standard error is a measure of how variable random samples can be. Such variability naturally decreases as our sample size grows. (This makes sense: larger samples give us more precise estimates of the true population, so they should be “closer” to the true population value.) Once conditions are checked, we can use normal models to make predictions about what we are likely to see when we sample from the population.\n\n14.7.1 Preparing and submitting your assignment\n\nFrom the “Run” menu, select “Restart R and Run All Chunks”.\nDeal with any code errors that crop up. Repeat steps 1–-2 until there are no more code errors.\nSpell check your document by clicking the icon with “ABC” and a check mark.\nHit the “Preview” button one last time to generate the final draft of the .nb.html file.\nProofread the HTML file carefully. If there are errors, go back and fix them, then repeat steps 1–5 again.\n\nIf you have completed this chapter as part of a statistics course, follow the directions you receive from your professor to submit your assignment."
  },
  {
    "objectID": "14-sampling_distribution_models-web.html#footnotes",
    "href": "14-sampling_distribution_models-web.html#footnotes",
    "title": "14  Sampling distribution models",
    "section": "",
    "text": "Most polls in the 2016 presidential election predicted a win for Hillary Clinton, so they also gave the wrong answer. It’s possible that some of them were accidentally wrong due to sampling variability, but a much more likely explanation for their overall failure was bias.↩︎"
  },
  {
    "objectID": "15-inference_for_one_proportion-web.html#one-prop-intro",
    "href": "15-inference_for_one_proportion-web.html#one-prop-intro",
    "title": "15  Inference for one proportion",
    "section": "15.1 Introduction",
    "text": "15.1 Introduction\nOur earlier work with simulations showed us that when the number of successes and failures is large enough, we can use a normal model as our sampling distribution model.\nWe revisit hypothesis tests for a single proportion, but now, instead of running a simulation to compute the P-value, we take the shortcut of computing the P-value directly from a normal model.\nThere are no new concepts here. All we are doing is revisiting the rubric for inference and making the necessary changes.\n\n15.1.1 Install new packages\nThere are no new packages used in this chapter.\n\n\n15.1.2 Download the R notebook file\nCheck the upper-right corner in RStudio to make sure you’re in your intro_stats project. Then click on the following link to download this chapter as an R notebook file (.Rmd).\nhttps://vectorposse.github.io/intro_stats/chapter_downloads/15-inference_for_one_proportion.Rmd\nOnce the file is downloaded, move it to your project folder in RStudio and open it there.\n\n\n15.1.3 Restart R and run all chunks\nIn RStudio, select “Restart R and Run All Chunks” from the “Run” menu."
  },
  {
    "objectID": "15-inference_for_one_proportion-web.html#one-prop-load",
    "href": "15-inference_for_one_proportion-web.html#one-prop-load",
    "title": "15  Inference for one proportion",
    "section": "15.2 Load packages",
    "text": "15.2 Load packages\nWe load the standard tidyverse, janitor and infer packages as well as the openintro package to access data on heart transplant candidates. We’ll include mosaic for one spot below when we compare the results of infer to the results of graphing a normal distribution using qdist.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(infer)\nlibrary(openintro)\n\nLoading required package: airports\nLoading required package: cherryblossom\nLoading required package: usdata\n\nlibrary(mosaic)\n\nWarning: package 'mosaic' was built under R version 4.3.1\n\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following object is masked from 'package:openintro':\n\n    dotPlot\n\nThe following objects are masked from 'package:infer':\n\n    prop_test, t_test\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum"
  },
  {
    "objectID": "15-inference_for_one_proportion-web.html#one-prop-rubric",
    "href": "15-inference_for_one_proportion-web.html#one-prop-rubric",
    "title": "15  Inference for one proportion",
    "section": "15.3 Revisiting the rubric for inference",
    "text": "15.3 Revisiting the rubric for inference\nInstead of running a simulation, we are going to assume that the sampling distribution can be modeled with a normal model as long as the conditions for using a normal model are met.\nAlthough the rubric has not changed, the use of a normal model changes quite a bit about the way we go through the other steps. For example, we won’t have simulated values to give us a histogram of the null model. Instead, we’ll go straight to graphing a normal model. We won’t compute the percent of our simulated samples that are at least as extreme as our test statistic to get the P-value. The P-value from a normal model is found directly from shading the model.\nWhat follows is a fully-worked example of inference for one proportion. After the hypothesis test (sometimes called a one-proportion z-test for reasons that will become clear), we also follow up by computing a confidence interval. From now on, we will consider inference to consist of a hypothesis test and a confidence interval. Whenever you’re asked a question that requires statistical inference, you should follow both the rubric steps for a hypothesis test and for a confidence interval.\nThe example below will pause frequently for commentary on the steps, especially where their execution will be different from what you’ve seen before when you used simulation. When it’s your turn to work through another example on your own, you should follow the outline of the rubric, but you should not copy and paste the commentary that accompanies it."
  },
  {
    "objectID": "15-inference_for_one_proportion-web.html#one-prop-question",
    "href": "15-inference_for_one_proportion-web.html#one-prop-question",
    "title": "15  Inference for one proportion",
    "section": "15.4 Research question",
    "text": "15.4 Research question\nData from the Stanford University Heart Transplant Study is located in the openintro package in a data frame called heart_transplant. From the help file we learn, “Each patient entering the program was designated officially a heart transplant candidate, meaning that he was gravely ill and would most likely benefit from a new heart.” Survival rates are not good for this population, although they are better for those who receive a heart transplant. Do heart transplant recipients still have less than a 50% chance of survival?"
  },
  {
    "objectID": "15-inference_for_one_proportion-web.html#one-prop-ex-eda",
    "href": "15-inference_for_one_proportion-web.html#one-prop-ex-eda",
    "title": "15  Inference for one proportion",
    "section": "15.5 Exploratory data analysis",
    "text": "15.5 Exploratory data analysis\n\n15.5.1 Use data documentation (help files, code books, Google, etc.) to determine as much as possible about the data provenance and structure.\nStart by typing ?heart_transplant at the Console or searching for heart_translplant in the Help tab to read the help file.\n\nExercise 1\nClick on the link under “Source” in the help file. Why is this not helpful for determining the provenance of the data?\nNow try to do an internet search to find the original research article from 1974. Why is this search process also not likely to help you determine the provenance of the data?\n\nPlease write up your answer here.\n\n\nNow that we have learned everything we can reasonably learn about the data, we print it out and look at the variables.\n\nheart_transplant\n\n# A tibble: 103 × 8\n      id acceptyear   age survived survtime prior transplant  wait\n   &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;       &lt;int&gt; &lt;fct&gt; &lt;fct&gt;      &lt;int&gt;\n 1    15         68    53 dead            1 no    control       NA\n 2    43         70    43 dead            2 no    control       NA\n 3    61         71    52 dead            2 no    control       NA\n 4    75         72    52 dead            2 no    control       NA\n 5     6         68    54 dead            3 no    control       NA\n 6    42         70    36 dead            3 no    control       NA\n 7    54         71    47 dead            3 no    control       NA\n 8    38         70    41 dead            5 no    treatment      5\n 9    85         73    47 dead            5 no    control       NA\n10     2         68    51 dead            6 no    control       NA\n# ℹ 93 more rows\n\n\n\nglimpse(heart_transplant)\n\nRows: 103\nColumns: 8\n$ id         &lt;int&gt; 15, 43, 61, 75, 6, 42, 54, 38, 85, 2, 103, 12, 48, 102, 35,…\n$ acceptyear &lt;int&gt; 68, 70, 71, 72, 68, 70, 71, 70, 73, 68, 67, 68, 71, 74, 70,…\n$ age        &lt;int&gt; 53, 43, 52, 52, 54, 36, 47, 41, 47, 51, 39, 53, 56, 40, 43,…\n$ survived   &lt;fct&gt; dead, dead, dead, dead, dead, dead, dead, dead, dead, dead,…\n$ survtime   &lt;int&gt; 1, 2, 2, 2, 3, 3, 3, 5, 5, 6, 6, 8, 9, 11, 12, 16, 16, 16, …\n$ prior      &lt;fct&gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no, no,…\n$ transplant &lt;fct&gt; control, control, control, control, control, control, contr…\n$ wait       &lt;int&gt; NA, NA, NA, NA, NA, NA, NA, 5, NA, NA, NA, NA, NA, NA, NA, …\n\n\nCommentary: The variable of interest is survived, which is coded as a factor variable with two categories, “alive” and “dead”. Keep in mind that because we are interested in survival rates, the “alive” condition will be considered the “success” condition.\nThere are 103 patients, but we are not considering all these patients. Our sample should consist of only those patients who actually received the transplant. The following table shows that only 69 patients were in the “treatment” group (meaning that they received a heart transplant).\n\ntabyl(heart_transplant, transplant) %&gt;%\n    adorn_totals()\n\n transplant   n   percent\n    control  34 0.3300971\n  treatment  69 0.6699029\n      Total 103 1.0000000\n\n\n\n\n\n15.5.2 Prepare the data for analysis.\nCAUTION: If you are copying and pasting from this example to use for another research question, the following code chunk is specific to this research question and not applicable in other contexts.\nWe need to use filter so we get only the patients who actually received the heart transplant.\n\n# Do not copy and paste this code for future work\nheart_transplant2 &lt;- heart_transplant %&gt;%\n    filter(transplant == \"treatment\")\nheart_transplant2\n\n# A tibble: 69 × 8\n      id acceptyear   age survived survtime prior transplant  wait\n   &lt;int&gt;      &lt;int&gt; &lt;int&gt; &lt;fct&gt;       &lt;int&gt; &lt;fct&gt; &lt;fct&gt;      &lt;int&gt;\n 1    38         70    41 dead            5 no    treatment      5\n 2    95         73    40 dead           16 no    treatment      2\n 3     3         68    54 dead           16 no    treatment      1\n 4    74         72    29 dead           17 no    treatment      5\n 5    20         69    55 dead           28 no    treatment      1\n 6    70         72    52 dead           30 no    treatment      5\n 7     4         68    40 dead           39 no    treatment     36\n 8   100         74    35 alive          39 yes   treatment     38\n 9    16         68    56 dead           43 no    treatment     20\n10    45         71    36 dead           45 no    treatment      1\n# ℹ 59 more rows\n\n\nCommentary: don’t forget the double equal sign (==) that checks whether the treatment variable is equal to the value “treatment”. (See the Chapter 5 if you’ve forgotten how to use filter.)\nAgain, this step isn’t something you need to do for other research questions. This question is peculiar because it asks only about patients who received a heart transplant, and that only involves a subset of the data we have in the heart_transplant data frame.\n\n\n15.5.3 Make tables or plots to explore the data visually.\nMaking sure that we refer from now on to the heart_transplant2 data frame and not the original heart_transplant data frame:\n\ntabyl(heart_transplant2, survived) %&gt;%\n    adorn_totals()\n\n survived  n   percent\n    alive 24 0.3478261\n     dead 45 0.6521739\n    Total 69 1.0000000"
  },
  {
    "objectID": "15-inference_for_one_proportion-web.html#one-prop-ex-hypotheses",
    "href": "15-inference_for_one_proportion-web.html#one-prop-ex-hypotheses",
    "title": "15  Inference for one proportion",
    "section": "15.6 Hypotheses",
    "text": "15.6 Hypotheses\n\n15.6.1 Identify the sample (or samples) and a reasonable population (or populations) of interest.\nThe sample consists of 69 heart transplant recipients in a study at Stanford University. The population of interest is presumably all heart transplants recipients.\n\n\n15.6.2 Express the null and alternative hypotheses as contextually meaningful full sentences.\n\\(H_{0}:\\) Heart transplant recipients have a 50% chance of survival.\n\\(H_{A}:\\) Heart transplant recipients have less than a 50% chance of survival.\nCommentary: It is slightly unusual that we are conducting a one-sided test. The standard default is typically a two-sided test. However, it is not for us to choose: the proposed research question is unequivocal in hypothesizing “less than 50%” survival.\n\n\n15.6.3 Express the null and alternative hypotheses in symbols (when possible).\n\\(H_{0}: p_{alive} = 0.5\\)\n\\(H_{A}: p_{alive} &lt; 0.5\\)"
  },
  {
    "objectID": "15-inference_for_one_proportion-web.html#one-prop-ex-model",
    "href": "15-inference_for_one_proportion-web.html#one-prop-ex-model",
    "title": "15  Inference for one proportion",
    "section": "15.7 Model",
    "text": "15.7 Model\n\n15.7.1 Identify the sampling distribution model.\nWe will use a normal model.\nCommentary: In past chapters, we have simulated the sampling distribution or applied some kind of randomization to simulate the effect of the null hypothesis. The point of this chapter is that we can—when the conditions are met—substitute a normal model to replace the unimodal and symmetric histogram that resulted from randomization and simulation.\n\n\n15.7.2 Check the relevant conditions to ensure that model assumptions are met.\n\nRandom\n\nSince the 69 patients are from a study at Stanford, we do not have a random sample of all heart transplant recipients. We hope that the patients recruited to this study were physiologically similar to other heart patients so that they are a representative sample. Without more information, we have no real way of knowing.\n\n10%\n\n69 patients are definitely less than 10% of all heart transplant recipients.\n\nSuccess/failure\n\n\\[\nnp_{alive} = 69(0.5) = 34.5 \\geq 10\n\\]\n\\[\nn(1 - p_{alive}) = 69(0.5) = 34.5 \\geq 10\n\\]\nCommentary: Notice something interesting here. Why did we not use the 24 patients who survived and the 45 who died as the successes and failures? In other words, why did we use \\(np_{alive}\\) and \\(n(1 - p_{alive})\\) instead of \\(n \\hat{p}_{alive}\\) and \\(n(1 - \\hat{p}_{alive})\\)?\nRemember the logic of inference and the philosophy of the null hypothesis. To convince the skeptics, we must assume the null hypothesis throughout the process. It’s only after we present sufficient evidence that can we reject the null and fall back on the alternative hypothesis that encapsulates our research question.\nTherefore, under the assumption of the null, the sampling distribution is the null distribution, meaning that it’s centered at 0.5. All work we do with the normal model, including checking conditions, must use the null model with \\(p_{alive}= 0.5\\).\nThat’s also why the numbers don’t have to be whole numbers. If the null states that of the 69 patients, 50% are expected to survive, then we expect 50% of 69, or 34.5, to survive. Of course, you can’t have half of a survivor. But these are not actual survivors. Rather, they are the expected number of survivors in a group of 69 patients on average under the assumption of the null."
  },
  {
    "objectID": "15-inference_for_one_proportion-web.html#one-prop-ex-mechanics",
    "href": "15-inference_for_one_proportion-web.html#one-prop-ex-mechanics",
    "title": "15  Inference for one proportion",
    "section": "15.8 Mechanics",
    "text": "15.8 Mechanics\n\n15.8.1 Compute the test statistic.\n\nalive_prop &lt;- heart_transplant2 %&gt;%\n    specify(response = survived, succes = \"alive\") %&gt;%\n    calculate(stat = \"prop\")\nalive_prop\n\nResponse: survived (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 0.348\n\n\nWe’ll also compute the corresponding z score.\n\nalive_z &lt;- heart_transplant2 %&gt;%\n    specify(response = survived, success = \"alive\") %&gt;%\n    hypothesize(null = \"point\", p = 0.5) %&gt;%\n    calculate(stat = \"z\")\nalive_z\n\nResponse: survived (factor)\nNull Hypothesis: point\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 -2.53\n\n\nCommentary: The sample proportion code is straightforward and we’ve seen it before. To get the z score, we also have to tell infer what the null hypothesis is so that it knows where the center of our normal distribution will be. In the hypothesize function, we tell infer to use a “point” null hypothesis with p = 0.5. All this means is that the null is a specific point: 0.5. (Contrast this to hypothesis tests with two variables when we had null = \"independence\".)\nWe can confirm the calculation of the z score manually. It’s easiest to compute the standard error first. Recall that the standard error is\n\\[\nSE = \\sqrt{\\frac{p_{alive}(1 - p_{alive})}{n}} = \\sqrt{\\frac{0.5(1 - 0.5)}{69}}\n\\]\nRemember that are working under the assumption of the null hypothesis. This means that we use \\(p_{alive} = 0.5\\) everywhere in the formula for the standard error.\nWe can do the math in R and store our result as SE.\n\nSE &lt;- sqrt(0.5*(1 - 0.5)/69)\nSE\n\n[1] 0.06019293\n\n\nThen our z score is\n\\[\nz = \\frac{(\\hat{p}_{alive} - p_{alive})}{SE} =  \\frac{(\\hat{p}_{alive} - p_{alive})}{\\sqrt{\\frac{p_{alive} (1 - p_{alive})}{n}}} = \\frac{(0.348 - 0.5)}{\\sqrt{\\frac{0.5 (1 - 0.5)}{69}}} =  -2.53.\n\\]\nUsing the values of alive_prop and SE:\n\nz &lt;- (alive_prop - 0.5)/SE\nz\n\n       stat\n1 -2.528103\n\n\nBoth the sample proportion \\(\\hat{p}_{alive}\\) (stored above as alive_prop) and the corresponding z-score can be considered the “test statistic”. If we use \\(\\hat{p}_{alive}\\) as the test statistic, then we’re considering the null model to be\n\\[\nN\\left(0.5, \\sqrt{\\frac{0.5 (1 - 0.5)}{69}}\\right).\n\\]\nIf we use z as the test statistic, then we’re considering the null model to be the standard normal model:\n\\[\nN(0, 1).\n\\]\nThe standard normal model is more intuitive and easier to work with, both conceptually and in R. Generally, then, we will consider z as the test statistic so that we can consider our null model to be the standard normal model. For example, knowing that our test statistic is two and a half standard deviations to the left of the null value already tells us a lot. We can anticipate a small P-value leading to rejection of the null. Nevertheless, for this type of hypothesis test, we’ll compute both in this section of the rubric.\n\n\n15.8.2 Report the test statistic in context (when possible).\nThe test statistic is 0.3478261. In other words, 34.7826087% of heart transplant recipients were alive at the end of the study.\nThe z score is -2.5281029. The proportion of survivors is about 2.5 standard errors below the null value.\n\n\n15.8.3 Plot the null distribution.\n\nalive_test &lt;- heart_transplant2 %&gt;%\n    specify(response = survived, success = \"alive\") %&gt;%\n    hypothesize(null = \"point\", p = 0.5) %&gt;%\n    assume(distribution = \"z\")\nalive_test\n\nA Z distribution.\n\n\n\nalive_test %&gt;%\n    visualize() +\n    shade_p_value(obs_stat = alive_z, direction = \"less\")\n\n\n\n\nCommentary: In past chapters, we have used the generate verb to get many repetitions (usually 1000) of some kind of random process to simulate the sampling distribution model. In this chapter, we have used the verb assume instead to assume that the sampling distribution is a normal model. As long as the conditions hold, this is a reasonable assumption. This also means that we don’t have to use set.seed as there is no random process to reproduce.\nCompare the graph above to what we would see if we simulated the sampling distribution. (Now we do need set.seed!)\n\nset.seed(6789)\nalive_test_draw &lt;- heart_transplant2 %&gt;%\n    specify(response = survived, success = \"alive\") %&gt;%\n    hypothesize(null = \"point\", p = 0.5) %&gt;%\n    generate(reps = 1000, type = \"draw\") %&gt;%\n    calculate(stat = \"prop\")\nalive_test_draw\n\nResponse: survived (factor)\nNull Hypothesis: point\n# A tibble: 1,000 × 2\n   replicate  stat\n   &lt;fct&gt;     &lt;dbl&gt;\n 1 1         0.493\n 2 2         0.406\n 3 3         0.435\n 4 4         0.580\n 5 5         0.522\n 6 6         0.507\n 7 7         0.580\n 8 8         0.435\n 9 9         0.551\n10 10        0.435\n# ℹ 990 more rows\n\n\n\nalive_test_draw %&gt;%\n    visualize() +\n    shade_p_value(obs_stat = alive_prop, direction = \"less\")\n\n\n\n\nThis is essentially the same picture, although the model above is centered on the null value 0.5 instead of the z score of 0. This also means that the obs_stat had to be the sample proportion alive_prop and not the z score alive_z.\n\n\n15.8.4 Calculate the P-value.\n\nalive_test_p &lt;- alive_test %&gt;%\n    get_p_value(obs_stat = alive_z, direction = \"less\")\nalive_test_p\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1 0.00573\n\n\nCommentary: compare this to the P-value we get from simulating random draws:\n\nalive_test_draw %&gt;%\n    get_p_value(obs_stat = alive_prop, direction = \"less\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.007\n\n\nThe values are not exactly the same. And a new simulation with a different seed would likely give another slightly different P-value. The takeaway here is that the P-value itself has some uncertainty, so you should never take the value too seriously.\n\n\n15.8.5 Interpret the P-value as a probability given the null.\nThe P-value is 0.005734. If there were truly a 50% chance of survival among heart transplant patients, there would only be a 0.5734037% chance of seeing data at least as extreme as we saw."
  },
  {
    "objectID": "15-inference_for_one_proportion-web.html#one-prop-ex-ht-conclusion",
    "href": "15-inference_for_one_proportion-web.html#one-prop-ex-ht-conclusion",
    "title": "15  Inference for one proportion",
    "section": "15.9 Conclusion",
    "text": "15.9 Conclusion\n\n15.9.1 State the statistical conclusion.\nWe reject the null hypothesis.\n\n\n15.9.2 State (but do not overstate) a contextually meaningful conclusion.\nWe have sufficient evidence that heart transplant recipients have less than a 50% chance of survival.\n\n\n15.9.3 Express reservations or uncertainty about the generalizability of the conclusion.\nBecause we know nearly nothing about the provenance of the data, it’s hard to generalize the conclusion. We know the data is from 1974, so it’s also very likely that survival rates for heart transplant patients then are not the same as they are today. The most we could hope for is that the Stanford data was representative for heart transplant patients in 1974. Our sample size (69) is also quite small.\n\n\n15.9.4 Identify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.\nAs we rejected the null, we run the risk of making a Type I error. It is possible that the null is true and that there is a 50% chance of survival for these patients, but we got an unusual sample that appears to have a much smaller chance of survival."
  },
  {
    "objectID": "15-inference_for_one_proportion-web.html#one-prop-ex-ci",
    "href": "15-inference_for_one_proportion-web.html#one-prop-ex-ci",
    "title": "15  Inference for one proportion",
    "section": "15.10 Confidence interval",
    "text": "15.10 Confidence interval\n\n15.10.1 Check the relevant conditions to ensure that model assumptions are met.\n\nRandom\n\nSame as above.\n\n10%\n\nSame as above.\n\nSuccess/failure\n\nThere were 24 patients who survived and 45 who died in our sample. Both are larger than 10.\n\n\nCommentary: In the “Confidence interval” section of the rubric, there is no need to recheck conditions that have already been checked. The sample has not changed; if it met the “Random” and “10%” conditions before, it will meet them now.\nSo why recheck the success/failure condition?\nKeep in mind that in a hypothesis test, we temporarily assume the null is true. The null states that \\(p = 0.5\\) and the resulting null distribution is, therefore, centered at \\(p = 0.5\\). The success/failure condition is a condition that applies to the normal model we’re using, and for a hypothesis test, that’s the null model.\nBy contrast, a confidence interval is making no assumption about the “true” value of \\(p\\). The inferential goal of a confidence interval is to try to capture the true value of \\(p\\), so we certainly cannot make any assumptions about it. Therefore, we go back to the original way we learned about the success/failure condition. That is, we check the actual number of successes and failures.\n\n\n15.10.2 Calculate and graph the confidence interval.\n\nalive_ci &lt;- alive_test %&gt;%\n    get_confidence_interval(point_estimate = alive_prop, level = 0.95)\nalive_ci\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    0.235    0.460\n\n\n\nalive_test %&gt;%\n    visualize() +\n    shade_confidence_interval(endpoints = alive_ci)\n\n\n\n\nCommentary: when we use a theoretical normal distribution, we have to compute the confidence interval a different way.\nWhen we bootstrapped, we had many repetitions of a process that resulted in a sampling distribution. From all those, we could find the 2.5th percentile and the 97.5th percentile. Although we let the computer do it for us, the process is straightforward enough that we could do it by hand if we needed to. Just put all 1000 bootstrapped values in order, then go to the 25th and 975th position in the list.\nWe don’t have a list of 1000 values when we use an abstract curve to represent our sampling distribution. Nevertheless, we can find the 2.5th percentile and the 97.5th percentile using the area under the normal curve as we saw in the last two chapters. We can do this “manually” with the qdist command, but we need the standard error first.\nDidn’t we calculate this earlier?\n\\[\nSE = \\sqrt{\\frac{p_{alive}(1 - p_{alive})}{n}} = \\sqrt{\\frac{0.5(1 - 0.5)}{69}}\n\\]\nWell…sort of. The value of \\(p_{alive}\\) here is the value of the null hypothesis from the hypothesis test above. However, the hypothesis test is done. For a confidence interval, we have no information about any “null” value. There is no null anymore. It’s irrelevant.\nSo what is the standard error for a confidence interval? Since we don’t have \\(p_{alive}\\), the best we can do is replace it with \\(\\hat{p}_{alive}\\):\n\\[\nSE = \\sqrt{\\frac{\\hat{p}_{alive} (1 - \\hat{p}_{alive})}{n}} = \\sqrt{\\frac{0.3478261 (1 - 0.3478261 )}{69}}.\n\\]\nWe can let R do the heavy lifting here:\n\nSE2 &lt;- sqrt(alive_prop * (1 - alive_prop) / 69)\nSE2\n\n        stat\n1 0.05733743\n\n\nAnd now this number can go into qdist as our standard deviation:\n\nqdist(\"norm\", p = c(0.025, 0.975), mean = 0.3478261, sd = 0.05733743, plot = FALSE)\n\n[1] 0.2354468 0.4602054\n\n\nThe numbers above are identical to the ones computed by the infer commands.\n\n\n15.10.3 State (but do not overstate) a contextually meaningful interpretation.\nWe are 95% confident that the true percentage of heart transplant recipients who survive is captured in the interval (23.5446784%, 46.020539%).\nCommentary: Note that when we state our contextually meaningful conclusion, we also convert the decimal proportions to percentages. Humans like percentages a lot better.\n\n\n15.10.4 If running a two-sided test, explain how the confidence interval reinforces the conclusion of the hypothesis test.\nWe are not running a two-sided test, so this step is not applicable.\n\n\n15.10.5 When comparing two groups, comment on the effect size and the practical significance of the result.\nThis is not applicable here because we are not comparing two groups. We are looking at the survival percentage in only one group of patients, those who had a heart transplant."
  },
  {
    "objectID": "15-inference_for_one_proportion-web.html#one-prop-your-turn",
    "href": "15-inference_for_one_proportion-web.html#one-prop-your-turn",
    "title": "15  Inference for one proportion",
    "section": "15.11 Your turn",
    "text": "15.11 Your turn\nFollow the rubric to answer the following research question:\nSome heart transplant candidates have already had a prior surgery. Use the variable prior in the heart_transplant data set to determine if fewer than 50% of patients have had a prior surgery. (To be clear, you are being asked to perform a one-sided test again.) Be sure to use the full heart_transplant data, not the modified heart_transplant2 from the previous example.\nThe rubric outline is reproduced below. You may refer to the worked example above and modify it accordingly. Remember to strip out all the commentary. That is just exposition for your benefit in understanding the steps, but is not meant to form part of the formal inference process.\nAnother word of warning: the copy/paste process is not a substitute for your brain. You will often need to modify more than just the names of the tibbles and variables to adapt the worked examples to your own work. For example, if you run a two-sided test instead of a one-sided test, there are a few places that have to be adjusted accordingly. Understanding the sampling distribution model and the computation of the P-value goes a long way toward understanding the changes that must be made. Do not blindly copy and paste code without understanding what it does. And you should never copy and paste text. All the sentences and paragraphs you write are expressions of your own analysis. They must reflect your own understanding of the inferential process.\nAlso, so that your answers here don’t mess up the code chunks above, use new variable names everywhere. In particular, you should use prior_test(instead of alive_test) to store the results of your hypothesis test. Make other corresponding changes as necessary, like prior_test_p instead of alive_test_p, for example.\n\nExploratory data analysis\n\nUse data documentation (help files, code books, Google, etc.) to determine as much as possible about the data provenance and structure.\n\nPlease write up your answer here.\n\n# Add code here to print the data\n\n\n# Add code here to glimpse the variables\n\n\n\n\nPrepare the data for analysis. [Not always necessary.]\n\n\n# Add code here to prepare the data for analysis.\n\n\n\n\nMake tables or plots to explore the data visually.\n\n\n# Add code here to make tables or plots.\n\n\n\n\n\nHypotheses\n\nIdentify the sample (or samples) and a reasonable population (or populations) of interest.\n[Remember that you are using the full heart_transplant data, so your sample size should be larger here than in the example above.]\n\nPlease write up your answer here.\n\n\n\nExpress the null and alternative hypotheses as contextually meaningful full sentences.\n\n\\(H_{0}:\\) Null hypothesis goes here.\n\\(H_{A}:\\) Alternative hypothesis goes here.\n\n\n\nExpress the null and alternative hypotheses in symbols (when possible).\n\n\\(H_{0}: math\\)\n\\(H_{A}: math\\)\n\n\n\n\nModel\n\nIdentify the sampling distribution model.\n\nPlease write up your answer here.\n\n\n\nCheck the relevant conditions to ensure that model assumptions are met.\n[Remember that you are using the full heart_transplant data, so the number of successes and failures will be different here than in the example above.]\n\nPlease write up your answer here. (Some conditions may require R code as well.)\n\n\n\n\nMechanics\n[Be sure to use heart_transplant everywhere and not heart_transplant2!]\n\nCompute the test statistic.\n\n\n# Add code here to compute the test statistic.\n\n\n\n\nReport the test statistic in context (when possible).\n\nPlease write up your answer here.\n\n\n\nPlot the null distribution.\n\n\n# Add code here to plot the null distribution.\n\n\n\n\nCalculate the P-value.\n\n\n# Add code here to calculate the P-value.\n\n\n\n\nInterpret the P-value as a probability given the null.\n\nPlease write up your answer here.\n\n\n\n\nConclusion\n\nState the statistical conclusion.\n\nPlease write up your answer here.\n\n\n\nState (but do not overstate) a contextually meaningful conclusion.\n\nPlease write up your answer here.\n\n\n\nExpress reservations or uncertainty about the generalizability of the conclusion.\n\nPlease write up your answer here.\n\n\n\nIdentify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.\n\nPlease write up your answer here.\n\n\n\n\nConfidence interval\n\nCheck the relevant conditions to ensure that model assumptions are met.\n\nPlease write up your answer here. (Some conditions may require R code as well.)\n\n\n\nCalculate the confidence interval.\n\n\n# Add code here to calculate the confidence interval.\n\n\n\n\nState (but do not overstate) a contextually meaningful interpretation.\n\nPlease write up your answer here.\n\n\n\nIf running a two-sided test, explain how the confidence interval reinforces the conclusion of the hypothesis test. [Not always applicable.]\n\nPlease write up your answer here.\n\n\n\nWhen comparing two groups, comment on the effect size and the practical significance of the result. [Not always applicable.]\n\nPlease write up your answer here."
  },
  {
    "objectID": "15-inference_for_one_proportion-web.html#one-prop-conclusion",
    "href": "15-inference_for_one_proportion-web.html#one-prop-conclusion",
    "title": "15  Inference for one proportion",
    "section": "15.12 Conclusion",
    "text": "15.12 Conclusion\nWhen certain conditions are met, we can use a theoretical normal model—a perfectly symmetric bell curve—as a sampling distribution model in hypothesis testing. Because this does not require drawing many samples, it is faster and cleaner than simulation. Of course, on modern computing devices, drawing even thousands of simulated samples is not very time consuming, and the code we write doesn’t really change much. Given the additional success/failure condition that has to met, it’s worth considering the pros and cons of using a normal model instead of simulating the sampling distribution. Similarly, confidence intervals can be obtained directly from the percentiles of the normal model without the need to obtain bootstrapped samples.\n\n15.12.1 Preparing and submitting your assignment\n\nFrom the “Run” menu, select “Restart R and Run All Chunks”.\nDeal with any code errors that crop up. Repeat steps 1–-2 until there are no more code errors.\nSpell check your document by clicking the icon with “ABC” and a check mark.\nHit the “Preview” button one last time to generate the final draft of the .nb.html file.\nProofread the HTML file carefully. If there are errors, go back and fix them, then repeat steps 1–5 again.\n\nIf you have completed this chapter as part of a statistics course, follow the directions you receive from your professor to submit your assignment."
  },
  {
    "objectID": "16-inference_for_two_proportions-web.html#introduction",
    "href": "16-inference_for_two_proportions-web.html#introduction",
    "title": "16  Inference for two proportions",
    "section": "16.1 Introduction",
    "text": "16.1 Introduction\nIn this chapter, we revisit the idea of inference for two proportions, but this time using a normal model as the sampling distribution model.\n\n16.1.1 Install new packages\nThere are no new packages used in this chapter.\n\n\n16.1.2 Download the R notebook file\nCheck the upper-right corner in RStudio to make sure you’re in your intro_stats project. Then click on the following link to download this chapter as an R notebook file (.Rmd).\nhttps://vectorposse.github.io/intro_stats/chapter_downloads/16-inference_for_two_proportions.Rmd\nOnce the file is downloaded, move it to your project folder in RStudio and open it there.\n\n\n16.1.3 Restart R and run all chunks\nIn RStudio, select “Restart R and Run All Chunks” from the “Run” menu."
  },
  {
    "objectID": "16-inference_for_two_proportions-web.html#load-packages",
    "href": "16-inference_for_two_proportions-web.html#load-packages",
    "title": "16  Inference for two proportions",
    "section": "16.2 Load packages",
    "text": "16.2 Load packages\nWe load the standard tidyverse, janitor and infer packages as well as the MASS package for the Melanoma data.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(infer)\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select"
  },
  {
    "objectID": "16-inference_for_two_proportions-web.html#research-question",
    "href": "16-inference_for_two_proportions-web.html#research-question",
    "title": "16  Inference for two proportions",
    "section": "16.3 Research question",
    "text": "16.3 Research question\nIn an earlier chapter, we used the data set Melanoma from the MASS package to explore the possibility of a sex bias among patients with melanoma. A related question is whether male or females are more likely to die from melanoma. In this case, we are thinking of status as the response variable and sex as the predictor variable."
  },
  {
    "objectID": "16-inference_for_two_proportions-web.html#the-sampling-distribution-model-for-two-proportions",
    "href": "16-inference_for_two_proportions-web.html#the-sampling-distribution-model-for-two-proportions",
    "title": "16  Inference for two proportions",
    "section": "16.4 The sampling distribution model for two proportions",
    "text": "16.4 The sampling distribution model for two proportions\nWhen we simulated a sampling distribution using randomization (shuffling the values of the predictor variable), it looked like the simulated sampling distribution was roughly normal. Therefore, we should be able to use a normal model in place of randomization when we want to perform statistical inference.\nThe question is, “Which normal model?” In other words, what is the mean and standard deviation we should use?\nSince we have two groups, let’s call the true proportion of success \\(p_{1}\\) for group 1 and \\(p_{2}\\) for group 2. Therefore, the true difference between groups 1 and 2 in the population is \\(p_{1} - p_{2}\\). If we sample repeatedly from groups 1 and 2 and form many sample differences \\(\\hat{p}_{1} - \\hat{p}_{2}\\), we should expect most of the values \\(\\hat{p}_{1} - \\hat{p}_{2}\\) to be close to the true difference \\(p_{1} - p_{2}\\). In other words, the sampling distribution is centered at a mean of \\(p_{1} - p_{2}\\).\nWhat about the standard error? This is much more technical and complicated. Here is the formula, whose derivation is outside the scope of the course:\n\\[\n\\sqrt{\\frac{p_{1} (1 - p_{1})}{n_{1}} + \\frac{p_{2} (1 - p_{2})}{n_{2}}}.\n\\]\nSo the somewhat complicated normal model is\n\\[\nN\\left( p_{1} - p_{2}, \\sqrt{\\frac{p_{1} (1 - p_{1})}{n_{1}} + \\frac{p_{2} (1 - p_{2})}{n_{2}}} \\right).\n\\]\nWhen we ran hypothesis tests for one proportion, the true proportion \\(p\\) was assumed to be known, set equal to some null value. Therefore, we could calculate the standard error \\(\\sqrt{\\frac{p(1 - p)}{n}}\\) under the assumption of the null.\nWe also have a null hypothesis for two proportions. When comparing two groups, the default assumption is that the two groups are the same. This translates into the mathematical statement \\(p_{1} - p_{2} = 0\\) (i.e., there is no difference between \\(p_{1}\\) and \\(p_{2}\\)).\nBut there is a problem here. Although we are assuming something about the difference \\(p_{1} - p_{2}\\), we are not assuming anything about the actual values of \\(p_{1}\\) and \\(p_{2}\\). For example, both groups could be 0.3, or 0.6, or 0.92, or whatever, and the difference between the groups would still be zero.\nWithout values of \\(p_{1}\\) and \\(p_{2}\\), we cannot plug anything into the standard error formula above. One easy “cheat” is to just use the sample values \\(\\hat{p}_{1}\\) and \\(\\hat{p}_{2}\\):\n\\[\nSE = \\sqrt{\\frac{\\hat{p}_{1} (1 - \\hat{p}_{1})}{n_{1}} + \\frac{\\hat{p}_{2} (1 - \\hat{p}_{2})}{n_{2}}}.\n\\]\nThere is a more sophisticated way to address this called “pooling”. This more advanced concept is covered in an optional appendix to this chapter."
  },
  {
    "objectID": "16-inference_for_two_proportions-web.html#inference-for-two-proportions",
    "href": "16-inference_for_two_proportions-web.html#inference-for-two-proportions",
    "title": "16  Inference for two proportions",
    "section": "16.5 Inference for two proportions",
    "text": "16.5 Inference for two proportions\nBelow is a fully-worked example of inference (hypothesis test and confidence interval) for two proportions. When you work your own example, you can thoughtfully copy and paste the R code, making changes as necessary.\nThe example below will pause frequently for commentary on the steps, especially where their execution will be different from what you’ve seen before when you used randomization. When it’s your turn to work through another example on your own, you should follow the outline of the rubric, but you should not copy and paste the commentary that accompanies it."
  },
  {
    "objectID": "16-inference_for_two_proportions-web.html#exploratory-data-analysis",
    "href": "16-inference_for_two_proportions-web.html#exploratory-data-analysis",
    "title": "16  Inference for two proportions",
    "section": "16.6 Exploratory data analysis",
    "text": "16.6 Exploratory data analysis\n\n16.6.1 Use data documentation (help files, code books, Google, etc.) to determine as much as possible about the data provenance and structure.\nType ?Melanoma at the Console to read the help file. We discussed this data back in Chapter 11 and determined that it was difficult, if not impossible, to discover anything useful about the true provenance of the data. We can, at least, print the data out and examine the variables\n\nMelanoma\n\n    time status sex age year thickness ulcer\n1     10      3   1  76 1972      6.76     1\n2     30      3   1  56 1968      0.65     0\n3     35      2   1  41 1977      1.34     0\n4     99      3   0  71 1968      2.90     0\n5    185      1   1  52 1965     12.08     1\n6    204      1   1  28 1971      4.84     1\n7    210      1   1  77 1972      5.16     1\n8    232      3   0  60 1974      3.22     1\n9    232      1   1  49 1968     12.88     1\n10   279      1   0  68 1971      7.41     1\n11   295      1   0  53 1969      4.19     1\n12   355      3   0  64 1972      0.16     1\n13   386      1   0  68 1965      3.87     1\n14   426      1   1  63 1970      4.84     1\n15   469      1   0  14 1969      2.42     1\n16   493      3   1  72 1971     12.56     1\n17   529      1   1  46 1971      5.80     1\n18   621      1   1  72 1972      7.06     1\n19   629      1   1  95 1968      5.48     1\n20   659      1   1  54 1972      7.73     1\n21   667      1   0  89 1968     13.85     1\n22   718      1   1  25 1967      2.34     1\n23   752      1   1  37 1973      4.19     1\n24   779      1   1  43 1967      4.04     1\n25   793      1   1  68 1970      4.84     1\n26   817      1   0  67 1966      0.32     0\n27   826      3   0  86 1965      8.54     1\n28   833      1   0  56 1971      2.58     1\n29   858      1   0  16 1967      3.56     0\n30   869      1   0  42 1965      3.54     0\n31   872      1   0  65 1968      0.97     0\n32   967      1   1  52 1970      4.83     1\n33   977      1   1  58 1967      1.62     1\n34   982      1   0  60 1970      6.44     1\n35  1041      1   1  68 1967     14.66     0\n36  1055      1   0  75 1967      2.58     1\n37  1062      1   1  19 1966      3.87     1\n38  1075      1   1  66 1971      3.54     1\n39  1156      1   0  56 1970      1.34     1\n40  1228      1   1  46 1973      2.24     1\n41  1252      1   0  58 1971      3.87     1\n42  1271      1   0  74 1971      3.54     1\n43  1312      1   0  65 1970     17.42     1\n44  1427      3   1  64 1972      1.29     0\n45  1435      1   1  27 1969      3.22     0\n46  1499      2   1  73 1973      1.29     0\n47  1506      1   1  56 1970      4.51     1\n48  1508      2   1  63 1973      8.38     1\n49  1510      2   0  69 1973      1.94     0\n50  1512      2   0  77 1973      0.16     0\n51  1516      1   1  80 1968      2.58     1\n52  1525      3   0  76 1970      1.29     1\n53  1542      2   0  65 1973      0.16     0\n54  1548      1   0  61 1972      1.62     0\n55  1557      2   0  26 1973      1.29     0\n56  1560      1   0  57 1973      2.10     0\n57  1563      2   0  45 1973      0.32     0\n58  1584      1   1  31 1970      0.81     0\n59  1605      2   0  36 1973      1.13     0\n60  1621      1   0  46 1972      5.16     1\n61  1627      2   0  43 1973      1.62     0\n62  1634      2   0  68 1973      1.37     0\n63  1641      2   1  57 1973      0.24     0\n64  1641      2   0  57 1973      0.81     0\n65  1648      2   0  55 1973      1.29     0\n66  1652      2   0  58 1973      1.29     0\n67  1654      2   1  20 1973      0.97     0\n68  1654      2   0  67 1973      1.13     0\n69  1667      1   0  44 1971      5.80     1\n70  1678      2   0  59 1973      1.29     0\n71  1685      2   0  32 1973      0.48     0\n72  1690      1   1  83 1971      1.62     0\n73  1710      2   0  55 1973      2.26     0\n74  1710      2   1  15 1973      0.58     0\n75  1726      1   0  58 1970      0.97     1\n76  1745      2   0  47 1973      2.58     1\n77  1762      2   0  54 1973      0.81     0\n78  1779      2   1  55 1973      3.54     1\n79  1787      2   1  38 1973      0.97     0\n80  1787      2   0  41 1973      1.78     1\n81  1793      2   0  56 1973      1.94     0\n82  1804      2   0  48 1973      1.29     0\n83  1812      2   1  44 1973      3.22     1\n84  1836      2   0  70 1972      1.53     0\n85  1839      2   0  40 1972      1.29     0\n86  1839      2   1  53 1972      1.62     1\n87  1854      2   0  65 1972      1.62     1\n88  1856      2   1  54 1972      0.32     0\n89  1860      3   1  71 1969      4.84     1\n90  1864      2   0  49 1972      1.29     0\n91  1899      2   0  55 1972      0.97     0\n92  1914      2   0  69 1972      3.06     0\n93  1919      2   1  83 1972      3.54     0\n94  1920      2   1  60 1972      1.62     1\n95  1927      2   1  40 1972      2.58     1\n96  1933      1   0  77 1972      1.94     0\n97  1942      2   0  35 1972      0.81     0\n98  1955      2   0  46 1972      7.73     1\n99  1956      2   0  34 1972      0.97     0\n100 1958      2   0  69 1972     12.88     0\n101 1963      2   0  60 1972      2.58     0\n102 1970      2   1  84 1972      4.09     1\n103 2005      2   0  66 1972      0.64     0\n104 2007      2   1  56 1972      0.97     0\n105 2011      2   0  75 1972      3.22     1\n106 2024      2   0  36 1972      1.62     0\n107 2028      2   1  52 1972      3.87     1\n108 2038      2   0  58 1972      0.32     1\n109 2056      2   0  39 1972      0.32     0\n110 2059      2   1  68 1972      3.22     1\n111 2061      1   1  71 1968      2.26     0\n112 2062      1   0  52 1965      3.06     0\n113 2075      2   1  55 1972      2.58     1\n114 2085      3   0  66 1970      0.65     0\n115 2102      2   1  35 1972      1.13     0\n116 2103      1   1  44 1966      0.81     0\n117 2104      2   0  72 1972      0.97     0\n118 2108      1   0  58 1969      1.76     1\n119 2112      2   0  54 1972      1.94     1\n120 2150      2   0  33 1972      0.65     0\n121 2156      2   0  45 1972      0.97     0\n122 2165      2   1  62 1972      5.64     0\n123 2209      2   0  72 1971      9.66     0\n124 2227      2   0  51 1971      0.10     0\n125 2227      2   1  77 1971      5.48     1\n126 2256      1   0  43 1971      2.26     1\n127 2264      2   0  65 1971      4.83     1\n128 2339      2   0  63 1971      0.97     0\n129 2361      2   1  60 1971      0.97     0\n130 2387      2   0  50 1971      5.16     1\n131 2388      1   1  40 1966      0.81     0\n132 2403      2   0  67 1971      2.90     1\n133 2426      2   0  69 1971      3.87     0\n134 2426      2   0  74 1971      1.94     1\n135 2431      2   0  49 1971      0.16     0\n136 2460      2   0  47 1971      0.64     0\n137 2467      1   0  42 1965      2.26     1\n138 2492      2   0  54 1971      1.45     0\n139 2493      2   1  72 1971      4.82     1\n140 2521      2   0  45 1971      1.29     1\n141 2542      2   1  67 1971      7.89     1\n142 2559      2   0  48 1970      0.81     1\n143 2565      1   1  34 1970      3.54     1\n144 2570      2   0  44 1970      1.29     0\n145 2660      2   0  31 1970      0.64     0\n146 2666      2   0  42 1970      3.22     1\n147 2676      2   0  24 1970      1.45     1\n148 2738      2   0  58 1970      0.48     0\n149 2782      1   1  78 1969      1.94     0\n150 2787      2   1  62 1970      0.16     0\n151 2984      2   1  70 1969      0.16     0\n152 3032      2   0  35 1969      1.29     0\n153 3040      2   0  61 1969      1.94     0\n154 3042      1   0  54 1967      3.54     1\n155 3067      2   0  29 1969      0.81     0\n156 3079      2   1  64 1969      0.65     0\n157 3101      2   1  47 1969      7.09     0\n158 3144      2   1  62 1969      0.16     0\n159 3152      2   0  32 1969      1.62     0\n160 3154      3   1  49 1969      1.62     0\n161 3180      2   0  25 1969      1.29     0\n162 3182      3   1  49 1966      6.12     0\n163 3185      2   0  64 1969      0.48     0\n164 3199      2   0  36 1969      0.64     0\n165 3228      2   0  58 1969      3.22     1\n166 3229      2   0  37 1969      1.94     0\n167 3278      2   1  54 1969      2.58     0\n168 3297      2   0  61 1968      2.58     1\n169 3328      2   1  31 1968      0.81     0\n170 3330      2   1  61 1968      0.81     1\n171 3338      1   0  60 1967      3.22     1\n172 3383      2   0  43 1968      0.32     0\n173 3384      2   0  68 1968      3.22     1\n174 3385      2   0   4 1968      2.74     0\n175 3388      2   1  60 1968      4.84     1\n176 3402      2   1  50 1968      1.62     0\n177 3441      2   0  20 1968      0.65     0\n178 3458      3   0  54 1967      1.45     0\n179 3459      2   0  29 1968      0.65     0\n180 3459      2   1  56 1968      1.29     1\n181 3476      2   0  60 1968      1.62     0\n182 3523      2   0  46 1968      3.54     0\n183 3667      2   0  42 1967      3.22     0\n184 3695      2   0  34 1967      0.65     0\n185 3695      2   0  56 1967      1.03     0\n186 3776      2   1  12 1967      7.09     1\n187 3776      2   0  21 1967      1.29     1\n188 3830      2   1  46 1967      0.65     0\n189 3856      2   0  49 1967      1.78     0\n190 3872      2   0  35 1967     12.24     1\n191 3909      2   1  42 1967      8.06     1\n192 3968      2   0  47 1967      0.81     0\n193 4001      2   0  69 1967      2.10     0\n194 4103      2   0  52 1966      3.87     0\n195 4119      2   1  52 1966      0.65     0\n196 4124      2   0  30 1966      1.94     1\n197 4207      2   1  22 1966      0.65     0\n198 4310      2   1  55 1966      2.10     0\n199 4390      2   0  26 1965      1.94     1\n200 4479      2   0  19 1965      1.13     1\n201 4492      2   1  29 1965      7.06     1\n202 4668      2   0  40 1965      6.12     0\n203 4688      2   0  42 1965      0.48     0\n204 4926      2   0  50 1964      2.26     0\n205 5565      2   0  41 1962      2.90     0\n\n\n\nglimpse(Melanoma)\n\nRows: 205\nColumns: 7\n$ time      &lt;int&gt; 10, 30, 35, 99, 185, 204, 210, 232, 232, 279, 295, 355, 386,…\n$ status    &lt;int&gt; 3, 3, 2, 3, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, …\n$ sex       &lt;int&gt; 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, …\n$ age       &lt;int&gt; 76, 56, 41, 71, 52, 28, 77, 60, 49, 68, 53, 64, 68, 63, 14, …\n$ year      &lt;int&gt; 1972, 1968, 1977, 1968, 1965, 1971, 1972, 1974, 1968, 1971, …\n$ thickness &lt;dbl&gt; 6.76, 0.65, 1.34, 2.90, 12.08, 4.84, 5.16, 3.22, 12.88, 7.41…\n$ ulcer     &lt;int&gt; 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n\n\n\n\n16.6.2 Prepare the data for analysis.\nThe two variables of interest are status and sex. We are considering them as categorical variables, but they are recorded numerically in the data frame. We convert them to proper factor variables and put them in their own data frame using the help file to identify the levels and labels we need.\nThere is a minor hitch with status. The help file shows three categories: 1. died from melanoma, 2. alive, 3. dead from other causes. For two-proportion inference, it would be better to have two categories only, a success category and a failure category. Since our research question asks about deaths due to melanoma, the “success” condition is the one numbered 1 in the help file, “died from melanoma”. That means we need to combine the other two categories into a single failure category. Perhaps we should call it “other”. You can accomplish this by simply repeating the “other” label more than once in the factor command:\n\nMelanoma &lt;- Melanoma %&gt;%\n    mutate(sex_fct = factor(sex,\n                            levels = c(0, 1),\n                            labels = c(\"female\", \"male\")),\n           status_fct = factor(status,\n                               levels = c(1, 2, 3),\n                               labels = c(\"died from melanoma\", \"other\", \"other\")))\nglimpse(Melanoma)\n\nRows: 205\nColumns: 9\n$ time       &lt;int&gt; 10, 30, 35, 99, 185, 204, 210, 232, 232, 279, 295, 355, 386…\n$ status     &lt;int&gt; 3, 3, 2, 3, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1,…\n$ sex        &lt;int&gt; 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1,…\n$ age        &lt;int&gt; 76, 56, 41, 71, 52, 28, 77, 60, 49, 68, 53, 64, 68, 63, 14,…\n$ year       &lt;int&gt; 1972, 1968, 1977, 1968, 1965, 1971, 1972, 1974, 1968, 1971,…\n$ thickness  &lt;dbl&gt; 6.76, 0.65, 1.34, 2.90, 12.08, 4.84, 5.16, 3.22, 12.88, 7.4…\n$ ulcer      &lt;int&gt; 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ sex_fct    &lt;fct&gt; male, male, male, female, male, male, male, female, male, f…\n$ status_fct &lt;fct&gt; other, other, other, other, died from melanoma, died from m…\n\n\n\nExercise 1\nObserve the new variables sex_fct and status_fct in the glimpse output above. How can we check that the categories got assigned correctly and match the original sex and status variables?\n\nPlease write up your answer here.\n\n\n\n\n16.6.3 Make tables or plots to explore the data visually.\nAs these are two categorical variables, we should look at contingency tables (both counts and percentages). The variable status is the response and sex is the predictor.\n\ntabyl(Melanoma, status_fct, sex_fct) %&gt;%\n    adorn_totals()\n\n         status_fct female male\n died from melanoma     28   29\n              other     98   50\n              Total    126   79\n\n\n\ntabyl(Melanoma, status_fct, sex_fct) %&gt;%\n    adorn_totals() %&gt;%\n    adorn_percentages(\"col\") %&gt;%\n    adorn_pct_formatting()\n\n         status_fct female   male\n died from melanoma  22.2%  36.7%\n              other  77.8%  63.3%\n              Total 100.0% 100.0%\n\n\nCommentary: You can see why column percentages are necessary in a contingency table. There are 28 females and 29 males who died from melanoma, almost a tie. However, there are more females (126) than there are males (79) who have melanoma in this data set. So the proportion of males who died from melanoma is quite a bit larger."
  },
  {
    "objectID": "16-inference_for_two_proportions-web.html#hypotheses",
    "href": "16-inference_for_two_proportions-web.html#hypotheses",
    "title": "16  Inference for two proportions",
    "section": "16.7 Hypotheses",
    "text": "16.7 Hypotheses\n\n16.7.1 Identify the sample (or samples) and a reasonable population (or populations) of interest.\nThere are two samples: 126 female patients and 79 male patients in Denmark with malignant melanoma. In order for these samples to be representative of their respective populations, we should probably restrict our conclusions to the population of all females and males in Denmark with malignant melanoma, although we might be able to make the case that these females and males could be representative of people in other countries who have malignant melanoma.\n\n\n16.7.2 Express the null and alternative hypotheses as contextually meaningful full sentences.\n\\(H_{0}:\\) There is no difference between the rate at which women and men in Denmark die from malignant melanoma.\n\\(H_{A}:\\) There is a difference between the rate at which women and men in Denmark die from malignant melanoma.\nOR\n\\(H_{0}:\\) In Denmark, death from malignant melanoma is independent of sex.\n\\(H_{A}:\\) In Denmark, death from malignant melanoma is associated with sex.\nCommentary: Either of these forms is correct. The former makes it a little easier to figure out how to express the hypotheses mathematically in the next step. The latter reminds us that the hypothesize step of the infer pipeline will require a null of independence.\n\n\n16.7.3 Express the null and alternative hypotheses in symbols (when possible).\n\\(H_{0}: p_{died, F} - p_{died, M} = 0\\)\n\\(H_{A}: p_{died, F} - p_{died, M} \\neq 0\\)\nCommentary: The order in which you subtract is irrelevant to the inferential process. However, you should be sure that any future steps respect the order you choose here. To be on the safe side, it’s always best to subtract in the order in which the factor was created. So in the contingency tables above, females are listed first, and that’s because “female” was the first label we used when we created the sex_fct variable. So we’ll subtract females minus males throughout the remaining steps."
  },
  {
    "objectID": "16-inference_for_two_proportions-web.html#model",
    "href": "16-inference_for_two_proportions-web.html#model",
    "title": "16  Inference for two proportions",
    "section": "16.8 Model",
    "text": "16.8 Model\n\n16.8.1 Identify the sampling distribution model.\nWe will use a normal model.\n\n\n16.8.2 Check the relevant conditions to ensure that model assumptions are met.\n\nRandom\n\nAs observed in a previous chapter when we used this data set before, We have no information about how these samples were obtained. We hope the 126 female patients and 79 male patients are representative of other Danish patients with malignant melanoma.\n\n10%\n\nWe don’t know exactly how many people in Denmark suffer from malignant melanoma, but we could imagine over time it’s more than 1260 females and 790 males.\n\nSuccess/Failure\n\nChecking the contingency table above (the one with counts), we see the numbers 28 and 98 (the successes and failures among females), and 29 and 50 (the successes and failures among males). These are all larger than 10.\n\n\nCommentary: Ideally, for the success/failure condition we would like to check \\(n_{1} p_{1}\\), \\(n_{1} (1 - p_{1})\\), \\(n_{2} p_{2}\\), and \\(n_{2} (1 - p_{2})\\); however, the null makes no claim about the values of \\(p_{1}\\) and \\(p_{2}\\). We do the next best thing and estimate these by substituting the sample proportions \\(\\hat{p}_{1}\\) and \\(\\hat{p}_{2}\\). But \\(n_{1} \\hat{p}_{1}\\) and \\(n_{2} \\hat{p}_{2}\\) are just the raw counts of successes in each group. Likewise, \\(n_{1} (1 - \\hat{p}_{1})\\) and \\(n_{2} (1 - \\hat{p}_{2})\\) are just the raw counts of failures in each group. That’s why we can just read them off the contingency table.\nFor a more sophisticated approach, one could also use “pooled proportions”. See the optional appendix to this chapter for more information."
  },
  {
    "objectID": "16-inference_for_two_proportions-web.html#mechanics",
    "href": "16-inference_for_two_proportions-web.html#mechanics",
    "title": "16  Inference for two proportions",
    "section": "16.9 Mechanics",
    "text": "16.9 Mechanics\n\n16.9.1 Compute the test statistic.\n\nobs_diff &lt;- Melanoma %&gt;%\n    observe(status_fct ~ sex_fct, success = \"died from melanoma\",\n            stat = \"diff in props\", order = c(\"female\", \"male\"))\nobs_diff\n\nResponse: status_fct (factor)\nExplanatory: sex_fct (factor)\n# A tibble: 1 × 1\n    stat\n   &lt;dbl&gt;\n1 -0.145\n\n\nThe test statistic is the difference of proportions in the sample, \\(\\hat{p}_{died, F} - \\hat{p}_{died, M}\\):\n\\[\n\\hat{p}_{died, F} - \\hat{p}_{died, M} = 0.222 - 0.367 = -0.145\n\\]\nAs a z-score:\n\nobs_diff_z &lt;- Melanoma %&gt;%\n    observe(status_fct ~ sex_fct, success = \"died from melanoma\",\n            stat = \"z\", order = c(\"female\", \"male\"))\nobs_diff_z\n\nResponse: status_fct (factor)\nExplanatory: sex_fct (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1 -2.25\n\n\nCommentary: We can confirm the value of the z-score manually just to make sure we understand where it comes from.\nThe standard error looks like the following:\n\\[\nSE = \\sqrt{\\frac{\\hat{p}_{died, F} (1 - \\hat{p}_{died, F})}{n_{F}} + \\frac{\\hat{p}_{died, M} (1 - \\hat{p}_{died, M})}{n_M}}\n\\]\nPlugging in the numbers from the exploratory data analysis output:\n\\[\nSE = \\sqrt{\\frac{0.222 (1 - 0.222)}{126} + \\frac{0.367 (1 - 0.367)}{79}}\n\\]\nIn R,\n\nsqrt(0.222 * (1 - 0.222) / 126 + 0.367 * (1 - 0.367) / 79)\n\n[1] 0.06566131\n\n\nNow our z-score formula is\n\\[\nz = \\frac{(\\hat{p}_{died, F} - \\hat{p}_{died, M}) - (p_{died, F} - p_{died, M})}{SE}\n\\]\nThe first term in the numerator \\((\\hat{p}_{died, F} - \\hat{p}_{died, M})\\) is our test statistic, -0.145. The second term in the numerator \\((p_{died, F} - p_{died, M})\\) is zero according to the null hypothesis. Plugging all that in, along with the value of SE, gives\n\\[\nz = \\frac{-0.145 - 0}{0.066} \\approx -2.2\n\\]\nOther than a little rounding error (since we rounded everything in sight to three decimal places instead of keeping more precision), this is what the infer output also reported.\n\n\n16.9.2 Report the test statistic in context (when possible).\nIn our sample, there is a -14.4866385% difference between the rate at which women and men in Denmark die from malignant melanoma (meaning that males died at a higher rate).\nThe test statistic has a z score of -2.2530721. The difference in proportions between the rate at which women and men in Denmark die from malignant melanoma lies a bit more than 2 standard errors to the left of the null value.\nCommentary: Note the phrase “meaning that males died at a higher rate”. If you are looking at a difference, you must indicate the direction of the difference. Without that, we would know that there was a difference, but we would have no idea whether women or men die more from malignant melanoma. Once we know that we are subtracting female minus male, then given the values are negative, we can infer that males die from malignant melanoma more often than females in these samples.\n\n\n16.9.3 Plot the null distribution.\n\nstatus_sex_test &lt;- Melanoma %&gt;%\n    specify(status_fct ~ sex_fct, success = \"died from melanoma\") %&gt;%\n    hypothesize(null = \"independence\") %&gt;%\n    assume(distribution = \"z\")\nstatus_sex_test\n\nA Z distribution.\n\n\n\nstatus_sex_test %&gt;%\n    visualize() +\n    shade_p_value(obs_stat = obs_diff_z, direction = \"two-sided\")\n\n\n\n\nCommentary: Remember that this is a two-sided test.The red line above is the location of the test statistic, but both tails are shaded and count toward the P-value.\n\n\n16.9.4 Calculate the P-value.\n\nstatus_sex_test_p &lt;- status_sex_test %&gt;%\n    get_p_value(obs_stat = obs_diff_z, direction = \"two-sided\")\nstatus_sex_test_p\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1  0.0243\n\n\n\n\n16.9.5 Interpret the P-value as a probability given the null.\nThe P-value is 0.0242546. If there were truly no difference between the rate at which women and men in Denmark die from malignant melanoma, there is only a 2.4254604% chance of seeing a difference in our data at least as extreme as what we saw."
  },
  {
    "objectID": "16-inference_for_two_proportions-web.html#conclusion",
    "href": "16-inference_for_two_proportions-web.html#conclusion",
    "title": "16  Inference for two proportions",
    "section": "16.10 Conclusion",
    "text": "16.10 Conclusion\n\n16.10.1 State the statistical conclusion.\nWe reject the null hypothesis.\n\n\n16.10.2 State (but do not overstate) a contextually meaningful conclusion.\nWe have sufficient evidence to suggest that there is a difference between the rate at which women and men in Denmark die from malignant melanoma.\n\n\n16.10.3 Express reservations or uncertainty about the generalizability of the conclusion.\nWe echo the same concerns we had back in Chapter 11 when we first saw this data. We have no idea how these patients were sampled. Are these all the patients in Denmark with malignant melanoma over a certain period of time? Were they part of a convenience sample? As a result of our uncertainly about the sampling process, we can’t be sure if the results generalize to a larger population, either in Denmark or especially outside of Denmark.\n\n\n16.10.4 Identify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.\nIf we have made a Type I error, then there would actually be no difference between the rate at which women and men in Denmark die from malignant melanoma, but our samples showed a significant difference."
  },
  {
    "objectID": "16-inference_for_two_proportions-web.html#confidence-interval",
    "href": "16-inference_for_two_proportions-web.html#confidence-interval",
    "title": "16  Inference for two proportions",
    "section": "16.11 Confidence interval",
    "text": "16.11 Confidence interval\n\n16.11.1 Check the relevant conditions to ensure that model assumptions are met.\nNone of the conditions have changed, so they don’t need to be rechecked.\n\n\n16.11.2 Calculate and graph the confidence interval.\n\nstatus_sex_ci &lt;- status_sex_test %&gt;%\n    get_confidence_interval(point_estimate = obs_diff, level = 0.95)\nstatus_sex_ci\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1   -0.274  -0.0162\n\n\n\nstatus_sex_test %&gt;%\n    visualize() +\n    shade_confidence_interval(endpoints = status_sex_ci)\n\n\n\n\n\n\n16.11.3 State (but do not overstate) a contextually meaningful interpretation.\nWe are 95% confident that the true difference between the rate at which women and men die from malignant melanoma is captured in the interval (-27.3579265%, -1.6153506%). (This difference is measured by calculating female minus male.)\nCommentary: Note the addition of that last sentence. As we mentioned before, if you are looking at a difference, you must indicate the direction of the difference. We know that we are subtracting female minus male, So given that the values are negative, we can infer that males die from malignant melanoma more often than females—at least according to this confidence interval.\n\n\n16.11.4 If running a two-sided test, explain how the confidence interval reinforces the conclusion of the hypothesis test.\nThe confidence interval does not contain the null value of zero. Since zero is not a plausible value for the true difference between the rate at which women and men die from malignant melanoma, it makes sense that we rejected the null hypothesis.\n\n\n16.11.5 When comparing two groups, comment on the effect size and the practical significance of the result.\nAt the most extreme end of the confidence interval, -27.3579265% is a very large difference between females and males. If this outer value is close to the truth, males are at much more risk of melanoma than females (at least in Denmark at the time of the study). The other end of the confidence interval, -1.6153506%, is a negligible difference. If that number were close to the truth, it’s not clear that the true difference would have practical significance in the real world.\nCommentary: The P-value for the hypothesis test indicated that the results are statistically significant. But what does that really mean? It means that if the null were true, the probability of getting samples of females and males whose melanoma rates differed by -14.4866385%—or something more extreme in either direction—would be quite small. Our conclusion to reject the null follows as a logical consequence.\nSo we can be somewhat confident that there is a difference between females and males. But how much of a difference? A small difference can be statistically significant, and yet be completely irrelevant in the real world. A 1% difference in melanoma rates might not be enough to enact extra preventative measures for men, for example. On the other hand, a 27% difference is huge, and might result in a campaign targeted at men specifically due to the extra risk.\nIn other words, we cannot just rest on a conclusion of statistical significance. A difference might exist, but so what? We also need to know if that difference is practically significant? Are there any practical, real-world consequences due to the magnitude of the difference? There is no cutoff for practical significance. This is determined in the context of the problem, preferably using expert guidance. There are policy considerations, cost-benefit analyses, risk assessments, and a host of other considerations that are made when determining if a result is practically significant.\nA big part of this process that is often neglected is the role of uncertainty. Our point estimate was -14.4866385%. But that number, by itself, is not that meaningful. That is but one estimate coming from one set of samples. The range of plausible values, according to the confidence interval, is -27.3579265% to -1.6153506% . This is a huge range, and there are very different consequences to society is the difference is -27.3579265% versus -1.6153506%."
  },
  {
    "objectID": "16-inference_for_two_proportions-web.html#your-turn",
    "href": "16-inference_for_two_proportions-web.html#your-turn",
    "title": "16  Inference for two proportions",
    "section": "16.12 Your turn",
    "text": "16.12 Your turn\nGo through the rubric to determine if females and males in Denmark who are diagnosed with malignant melanoma suffer from ulcerated tumors at different rates.\nThe rubric outline is reproduced below. You may refer to the worked example above and modify it accordingly. Remember to strip out all the commentary. That is just exposition for your benefit in understanding the steps, but is not meant to form part of the formal inference process.\nAnother word of warning: the copy/paste process is not a substitute for your brain. You will often need to modify more than just the names of the data frames and variables to adapt the worked examples to your own work. Do not blindly copy and paste code without understanding what it does. And you should never copy and paste text. All the sentences and paragraphs you write are expressions of your own analysis. They must reflect your own understanding of the inferential process.\nAlso, so that your answers here don’t mess up the code chunks above, use new variable names everywhere. In particular, you should use ulcer_sex everywhere instead of status_sex\n\nExploratory data analysis\n\nUse data documentation (help files, code books, Google, etc.) to determine as much as possible about the data provenance and structure.\n\nPlease write up your answer here\n\n# Add code here to print the data\n\n\n# Add code here to glimpse the variables\n\n\n\n\nPrepare the data for analysis. [Not always necessary.]\n\n\n# Add code here to prepare the data for analysis.\n\n\n\n\nMake tables or plots to explore the data visually.\n\n\n# Add code here to make tables or plots.\n\n\n\n\n\nHypotheses\n\nIdentify the sample (or samples) and a reasonable population (or populations) of interest.\n\nPlease write up your answer here.\n\n\n\nExpress the null and alternative hypotheses as contextually meaningful full sentences.\n\n\\(H_{0}:\\) Null hypothesis goes here.\n\\(H_{A}:\\) Alternative hypothesis goes here.\n\n\n\nExpress the null and alternative hypotheses in symbols (when possible).\n\n\\(H_{0}: math\\)\n\\(H_{A}: math\\)\n\n\n\n\nModel\n\nIdentify the sampling distribution model.\n\nPlease write up your answer here.\n\n\n\nCheck the relevant conditions to ensure that model assumptions are met.\n\nPlease write up your answer here. (Some conditions may require R code as well.)\n\n\n\n\nMechanics\n\nCompute the test statistic.\n\n\n# Add code here to compute the test statistic.\n\n\n\n\nReport the test statistic in context (when possible).\n\nPlease write up your answer here.\n\n\n\nPlot the null distribution.\n\n\n# Add code here to plot the null distribution.\n\n\n\n\nCalculate the P-value.\n\n\n# Add code here to calculate the P-value.\n\n\n\n\nInterpret the P-value as a probability given the null.\n\nPlease write up your answer here.\n\n\n\n\nConclusion\n\nState the statistical conclusion.\n\nPlease write up your answer here.\n\n\n\nState (but do not overstate) a contextually meaningful conclusion.\n\nPlease write up your answer here.\n\n\n\nExpress reservations or uncertainty about the generalizability of the conclusion.\n\nPlease write up your answer here.\n\n\n\nIdentify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.\n\nPlease write up your answer here.\n\n\n\n\nConfidence interval\n\nCheck the relevant conditions to ensure that model assumptions are met.\n\nPlease write up your answer here. (Some conditions may require R code as well.)\n\n\n\nCalculate and graph the confidence interval.\n\n\n# Add code here to calculate the confidence interval.\n\n\n# Add code here to graph the confidence interval.\n\n\n\n\nState (but do not overstate) a contextually meaningful interpretation.\n\nPlease write up your answer here.\n\n\n\nIf running a two-sided test, explain how the confidence interval reinforces the conclusion of the hypothesis test. [Not always applicable.]\n\nPlease write up your answer here.\n\n\n\nWhen comparing two groups, comment on the effect size and the practical significance of the result. [Not always applicable.]\n\nPlease write up your answer here."
  },
  {
    "objectID": "16-inference_for_two_proportions-web.html#conclusion-2",
    "href": "16-inference_for_two_proportions-web.html#conclusion-2",
    "title": "16  Inference for two proportions",
    "section": "16.13 Conclusion",
    "text": "16.13 Conclusion\nJust like with one proportion, when certain conditions are met, the difference between two proportions follow a normal model. Rather than simulating a bunch of different sample differences under the assumption of independent variables, we can just replace all that with a relatively simple normal model with mean zero and a standard error based on the sample proportions of successes and failures in the two samples. From that normal model, we obtain P-values and confidence intervals as before.\n\n16.13.1 Preparing and submitting your assignment\n\nFrom the “Run” menu, select “Restart R and Run All Chunks”.\nDeal with any code errors that crop up. Repeat steps 1–-2 until there are no more code errors.\nSpell check your document by clicking the icon with “ABC” and a check mark.\nHit the “Preview” button one last time to generate the final draft of the .nb.html file.\nProofread the HTML file carefully. If there are errors, go back and fix them, then repeat steps 1–5 again.\n\nIf you have completed this chapter as part of a statistics course, follow the directions you receive from your professor to submit your assignment."
  },
  {
    "objectID": "16-inference_for_two_proportions-web.html#pooling",
    "href": "16-inference_for_two_proportions-web.html#pooling",
    "title": "16  Inference for two proportions",
    "section": "16.14 Optional appendix: Pooling",
    "text": "16.14 Optional appendix: Pooling\nEarlier, we mentioned that that we cannot calculate the “true” standard error directly because the null hypothesis does not give us \\(p_{1}\\) and \\(p_{2}\\). (The null only addresses the value of the difference \\(p_{1} - p_{2}\\).) We dealt with this by simply substituting \\(\\hat{p}_{1}\\) for \\(p_{1}\\) and \\(\\hat{p}_{2}\\) for \\(p_{2}\\).\nThere is, however, one assumption from the null we can still salvage that will improve our test. Since the null hypothesis assumes that the two groups are the same, let’s compute a single overall success rate for both samples together. In other words, if the two groups aren’t different, let’s just pool them into one single group and calculate the successes for the whole group.\nThis is called a pooled proportion. It’s straightforward to compute: just take the total number of successes in both groups and divide by the total size of both groups. Here is the formula:\n\\[\\hat{p}_{pooled} = \\frac{successes_{1} + successes_{2}}{n_{1} + n_{2}}.\\]\nOccasionally, we are not given the raw number of successes in each group, but rather, the proportion of successes in each group, \\(\\hat{p}_{1}\\) and \\(\\hat{p}_{2}\\). The simple fix is to recompute the raw count of successes as \\(n_{1} \\hat{p}_{1}\\) and \\(n_{2} \\hat{p}_{2}\\). Here is what it looks like in the formula:\n\\[\\hat{p}_{pooled} = \\frac{n_{1} \\hat{p}_{1} + n_{2} \\hat{p}_{2}}{n_{1} + n_{2}}.\\] The normal model can still have a mean of \\(p_{1} - p_{2}\\). (We usually assume this is 0 in the null hypothesis.) But its standard error will use the pooled proportion:\n\\[N\\left( p_{1} - p_{2}, \\sqrt{\\frac{\\hat{p}_{pooled} (1 - \\hat{p}_{pooled})}{n_{1}} + \\frac{\\hat{p}_{pooled} (1 - \\hat{p}_{pooled})}{n_{2}}} \\right).\\]\nNot only can we use the pooled proportion in the standard error, but in fact we can use it anywhere we assume the null. For example, the success/failure condition is also subject to the assumption of the null, so we could use the pooled proportion there too.\nFor a confidence interval, things are different. There is no null hypothesis in effect while computing a confidence interval, so there is no assumption that would justify pooling.\nThe standard error in the one-proportion interval is\n\\[\n\\sqrt{\\frac{\\hat{p}(1 - \\hat{p})}{n}}\n\\]\nwhich just substitutes \\(\\hat{p}\\) for \\(p\\). We do the same for the standard error in the two-proportion case:\n\\[\nSE = \\sqrt{\\frac{\\hat{p}_{1} (1 - \\hat{p}_{1})}{n_{1}} + \\frac{\\hat{p}_{2} (1 - \\hat{p}_{2})}{n_{2}}}.\n\\]"
  },
  {
    "objectID": "17-chi_square_goodness_of_fit-web.html#introduction",
    "href": "17-chi_square_goodness_of_fit-web.html#introduction",
    "title": "17  Chi-square goodness-of-fit test",
    "section": "17.1 Introduction",
    "text": "17.1 Introduction\nIn this assignment we will learn how to run the chi-square goodness-of-fit test. A chi-square goodness-of-fit test is similar to a test for a single proportion except, instead of two categories (success/failure), we now try to understand the distribution among three or more categories.\n\n17.1.1 Install new packages\nThere are no new packages used in this chapter.\n\n\n17.1.2 Download the R notebook file\nCheck the upper-right corner in RStudio to make sure you’re in your intro_stats project. Then click on the following link to download this chapter as an R notebook file (.Rmd).\nhttps://vectorposse.github.io/intro_stats/chapter_downloads/17-chi_square_goodness_of_fit.Rmd\nOnce the file is downloaded, move it to your project folder in RStudio and open it there.\n\n\n17.1.3 Restart R and run all chunks\nIn RStudio, select “Restart R and Run All Chunks” from the “Run” menu."
  },
  {
    "objectID": "17-chi_square_goodness_of_fit-web.html#load-packages",
    "href": "17-chi_square_goodness_of_fit-web.html#load-packages",
    "title": "17  Chi-square goodness-of-fit test",
    "section": "17.2 Load packages",
    "text": "17.2 Load packages\nWe load the standard tidyverse, janitor, and infer packages and the openintro package for the hsb2 data.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(infer)\nlibrary(openintro)\n\nLoading required package: airports\nLoading required package: cherryblossom\nLoading required package: usdata"
  },
  {
    "objectID": "17-chi_square_goodness_of_fit-web.html#research-question",
    "href": "17-chi_square_goodness_of_fit-web.html#research-question",
    "title": "17  Chi-square goodness-of-fit test",
    "section": "17.3 Research question",
    "text": "17.3 Research question\nWe use a classic data set mtcars from a 1974 Motor Trend magazine to examine the distribution of the number of engine cylinders (with values 4, 6, or 8). We’ll assume that this data set is representative of all cars from 1974.\nIn recent years, 4-cylinder vehicles and 6-cylinder vehicles have comprised about 38% of the market each, with nearly all the rest (24%) being 8-cylinder cars. (This ignores a very small number of cars manufactured with 3- or 5-cylinder engines.) Were car engines in 1974 manufactured according to the same distribution?\nHere is the structure of the data:\n\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\nNote that the variable of interest cyl is not coded as a factor variable. Let’s convert cyl to a factor variable first and add it to a new data frame called mtcars2. (Since the levels are already called 4, 6, and 8, we do not need to specify levels or labels.) Be sure to remember to use mtcars2 from here on out, and not the original mtcars.\n\nmtcars2 &lt;- mtcars %&gt;%\n  mutate(cyl_fct = factor(cyl))\nmtcars2\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb cyl_fct\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4       6\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4       6\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1       4\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1       6\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2       8\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1       6\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4       8\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2       4\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2       4\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4       6\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4       6\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3       8\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3       8\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3       8\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4       8\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4       8\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4       8\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1       4\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2       4\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1       4\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1       4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2       8\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2       8\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4       8\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2       8\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1       4\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2       4\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2       4\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4       8\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6       6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8       8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2       4\n\n\n\nglimpse(mtcars2)\n\nRows: 32\nColumns: 12\n$ mpg     &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17…\n$ cyl     &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4,…\n$ disp    &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8,…\n$ hp      &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, …\n$ drat    &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.…\n$ wt      &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150,…\n$ qsec    &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90,…\n$ vs      &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,…\n$ am      &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,…\n$ gear    &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3,…\n$ carb    &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1,…\n$ cyl_fct &lt;fct&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4,…"
  },
  {
    "objectID": "17-chi_square_goodness_of_fit-web.html#chi-squared",
    "href": "17-chi_square_goodness_of_fit-web.html#chi-squared",
    "title": "17  Chi-square goodness-of-fit test",
    "section": "17.4 Chi-squared",
    "text": "17.4 Chi-squared\nWhen we have three or more categories in a categorical variable, it is natural to ask how the observed counts in each category compare to the counts that we expect to see under the assumption of some null hypothesis. In other words, we’re assuming that there is some “true” distribution to which we are going to compare our data. Sometimes, this null comes from substantive expert knowledge. (For example, we will be comparing the 1974 distribution to a known distribution from recent years.) Sometimes we’re interested to see if our data deviates from a null distribution that predicts an equal number of observations in each category.\nFirst of all, what is the actual distribution of cylinders in our data? Here’s a frequency table.\n\ntabyl(mtcars2, cyl_fct) %&gt;%\n    adorn_totals() %&gt;%\n    adorn_pct_formatting()\n\n cyl_fct  n percent\n       4 11   34.4%\n       6  7   21.9%\n       8 14   43.8%\n   Total 32  100.0%\n\n\nThe counts of our frequency table are the “observed” values, usually denoted by the letter \\(O\\) (uppercase “O”, which is a little unfortunate, because it also looks like a zero).\nWhat are the expected counts? Well, since there are 32 cars, we need to multiply 32 by the percentages listed in the research question. For 4-cylinder and 6-cylinder cars, if the distribution of engines in 1974 were the same as today, there would be \\(32 * 0.38\\) or about 12.2 cars we would expect to see in our sample that have 4-cylinder engines, and the same for 6-cylinder cars. For 8-cylinder cars, we expect \\(32 * 0.24\\) or about 7.7 cars in our sample to have 8-cylinder engines. These “expected” counts are usually denoted by the letter \\(E\\).\nWhy aren’t the expected counts whole numbers? In any given data set, of course, we will see a whole number of cars with 4, 6, or 8 cylinders. However, since we’re looking only at expected counts, they are the average over lots of possible sets of 32 cars under the assumption of the null. We don’t need for these averages to be whole numbers.\nHow should the deviation between the data and the null distribution be measured? We could simply look at the difference between the observed counts and the expected counts \\(O - E\\). However, there will be some positive values (cells where we have more than the expected number of cars) and some negative values (cells where we have fewer than the expected number of cars). These will all cancel out.\nIf this sounds vaguely familiar, it is because we encountered the same problem with the formula for the standard deviation. The differences \\(y - \\bar{y}\\) had the same issue. Do you recall the solution in that case? It was to square these values, making them all positive.\nSo instead of \\(O - E\\), we will consider \\((O - E)^{2}\\). Finally, to make sure that cells with large expected values don’t dominate, we divide by \\(E\\):\n\\[\n\\frac{(O - E)^{2}}{E}.\n\\]\nThis puts each cell on equal footing. Now that we have a reasonable measure of the deviation between observed and expected counts for each cell, we define \\(\\chi^{2}\\) (“chi-squared”, pronounced “kye-squared”—rhymes with “die-scared”, or if that’s too dark, how about “pie-shared”1) as the sum of all these fractions, one for each cell:\n\\[\n\\chi^{2} = \\sum \\frac{(O - E)^{2}}{E}.\n\\]\nA \\(\\chi^{2}\\) value of zero would indicate perfect agreement between observed and expected values. As the \\(\\chi^{2}\\) value gets larger and larger, this indicates more and more deviation between observed and expected values.\nAs an example, for our data, we calculate chi-squared as follows:\n\\[\n\\chi^{2} = \\frac{(11 - 12.2)^{2}}{12.2} + \\frac{(7 - 12.2)^{2}}{12.2} + \\frac{(14 - 7.7)^{2}}{7.7} \\approx 7.5.\n\\]\nOr we could just do it in R with the infer package. To do so, we have to state explicitly the proportions that correspond to the null hypothesis. In this case, since the order of entries in the frequency table is 4-cylinder, 6-cylinder, then 8-cylinder, we need to give infer a vector of entries c(\"4\" = 0.38, \"6\" = 0.38, \"8\" = 0.24) that represents the 38%, 38%, and 24% expected for 4, 6, and 8 cylinders respectively.\n\nobs_chisq &lt;- mtcars2 %&gt;%\n  specify(response = cyl_fct) %&gt;%\n  hypothesize(null = \"point\",\n              p = c(\"4\" = 0.38,\n                    \"6\" = 0.38,\n                    \"8\" = 0.24)) %&gt;%\n  calculate(stat = \"chisq\")\nobs_chisq\n\nResponse: cyl_fct (factor)\nNull Hypothesis: point\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  7.50"
  },
  {
    "objectID": "17-chi_square_goodness_of_fit-web.html#the-chi-square-distribution",
    "href": "17-chi_square_goodness_of_fit-web.html#the-chi-square-distribution",
    "title": "17  Chi-square goodness-of-fit test",
    "section": "17.5 The chi-square distribution",
    "text": "17.5 The chi-square distribution\nWe know that even if the true distribution were 38%, 38%, 24%, we would not see exactly 12.2, 12.2, 7.7 in a sample of 32 cars. (In fact, the “true” distribution is physically impossible because these are not whole numbers!) So what kinds of numbers could we get?\nLet’s do a quick simulation to find out.\nUnder the assumption of the null, there should be a 38%, 38%, and 24% chance of seeing 4, 6, or 8 cylinders, respectively. To get a sense of the extent of sampling variability, we could use the sample command to see what happens in a sample of size 32 taken from a population where the true percentages are 38%, 38%, and 24%.\n\nset.seed(99999)\nsample1 &lt;- sample(c(4, 6, 8), size = 32, replace = TRUE,\n       prob = c(0.38, 0.38, 0.24))\nsample1\n\n [1] 6 8 4 8 6 6 8 4 8 6 8 6 6 4 6 8 4 6 6 8 8 6 6 8 4 8 6 4 4 4 6 4\n\n\n\nsample1 %&gt;%\n  table()\n\n.\n 4  6  8 \n 9 13 10 \n\n\n\nsample2 &lt;- sample(c(4, 6, 8), size = 32, replace = TRUE,\n       prob = c(0.38, 0.38, 0.24))\nsample2\n\n [1] 6 8 8 8 4 4 8 4 8 6 8 4 4 6 6 6 6 4 4 4 6 4 4 4 8 4 4 8 4 4 4 8\n\n\n\nsample2 %&gt;%\n  table()\n\n.\n 4  6  8 \n16  7  9 \n\n\n\nsample3 &lt;- sample(c(4, 6, 8), size = 32, replace = TRUE,\n       prob = c(0.38, 0.38, 0.24))\nsample3\n\n [1] 8 6 4 6 6 6 6 4 6 6 6 4 6 4 8 8 6 8 8 8 4 6 8 4 8 6 6 6 6 8 4 6\n\n\n\nsample3 %&gt;%\n  table()\n\n.\n 4  6  8 \n 7 16  9 \n\n\nWe can calculate the chi-squared value for each of these samples to get a sense of the possibilities. The chisq.test command from base R is a little unusual because it requires a frequency table (generated from the table command) as input. We will never use the chisq.test command directly because we will always use infer to do this work. But just to see some examples:\n\nsample1 %&gt;%\n  table() %&gt;%\n  chisq.test()\n\n\n    Chi-squared test for given probabilities\n\ndata:  .\nX-squared = 0.8125, df = 2, p-value = 0.6661\n\n\n\nsample2 %&gt;%\n  table() %&gt;%\n  chisq.test()\n\n\n    Chi-squared test for given probabilities\n\ndata:  .\nX-squared = 4.1875, df = 2, p-value = 0.1232\n\n\n\nsample3 %&gt;%\n  table() %&gt;%\n  chisq.test()\n\n\n    Chi-squared test for given probabilities\n\ndata:  .\nX-squared = 4.1875, df = 2, p-value = 0.1232\n\n\n\nExercise 1\nLook more carefully at the three random samples above. Why does sample 1 have a chi-squared closer to 0 while samples 2 and 3 have a chi-squared values that are a little larger? (Hint: look at the counts of 4s, 6s, and 8s in those samples. How do those counts compare to the expected number of 4s, 6s, and 8s?)\n\nPlease write up your answer here.\n\n\nThe infer pipeline below (the generate command specifically) takes the values “4”, “6”, or “8” and grabs them at random according to the probabilities specified until it has 32 values. In other words, it will randomly select “4” about 38% of the time, “6” about 38% of the time, and “8” about 24% of the time, until it gets a list of 32 total cars. Then it will calculate the chi-squared value for that simulated set of 32 cars. But because randomness is involved, the simulated samples are subject to sampling variability and the chi-square values obtained will differ from each other. This is exactly what we did above with the sample command and the chisq command, but the benefit now is that we get 1000 random samples very quickly.\n\nset.seed(99999)\ncyl_test_sim &lt;- mtcars2 %&gt;%\n  specify(response = cyl_fct) %&gt;%\n  hypothesize(null = \"point\",\n              p = c(\"4\" = 0.38,\n                    \"6\" = 0.38,\n                    \"8\" = 0.24)) %&gt;%\n  generate(reps = 1000, type = \"draw\") %&gt;%\n  calculate(stat = \"chisq\")\ncyl_test_sim\n\nResponse: cyl_fct (factor)\nNull Hypothesis: point\n# A tibble: 1,000 × 2\n   replicate  stat\n   &lt;fct&gt;     &lt;dbl&gt;\n 1 1         1.58 \n 2 2         3.63 \n 3 3         3.63 \n 4 4         0.669\n 5 5         2.31 \n 6 6         0.648\n 7 7         4.13 \n 8 8         7.08 \n 9 9         0.648\n10 10        0.669\n# ℹ 990 more rows\n\n\nThe “stat” column above contains 1000 random values of \\(\\chi^{2}\\). Let’s graph these values and include the chi-squared value for our actual data in the same graph.\n\ncyl_test_sim %&gt;%\n  visualize() +\n  shade_p_value(obs_chisq, direction = \"greater\")\n\n\n\n\nA few things are apparent:\n\nThe values are all positive. (The leftmost bar is sitting at 0, but it represents values greater than zero.) This makes sense when you remember that each piece of the \\(\\chi^{2}\\) calculation was positive. This is different from our earlier simulations that looked like normal models. (Z scores can be positive or negative, but not \\(\\chi^{2}\\).)\nThis is a severely right-skewed graph. Although most values are near zero, the occasional unusual sample can have a large value of \\(\\chi^{2}\\).\nYou can see that our sample (the red line) is pretty far to the right. It is an unusual value given the assumption of the null hypothesis. In fact, we can count the proportion of sampled values that are to the right of the red line:\n\n\ncyl_test_sim %&gt;%\n  get_p_value(obs_chisq, direction = \"greater\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.021\n\n\nThis is the simulated P-value. Keep this number in mind when we calculate the P-value using a sampling distribution model below."
  },
  {
    "objectID": "17-chi_square_goodness_of_fit-web.html#chi-square-as-a-sampling-distribution-model",
    "href": "17-chi_square_goodness_of_fit-web.html#chi-square-as-a-sampling-distribution-model",
    "title": "17  Chi-square goodness-of-fit test",
    "section": "17.6 Chi-square as a sampling distribution model",
    "text": "17.6 Chi-square as a sampling distribution model\nJust like there was a mathematical model for our simulated data before (the normal model back then), there is also a mathematical model for this type of simulated data. It’s called (not surprisingly) the chi-square distribution.\nThere is one new idea, though. Although all normal models have the same bell shape, there are many different chi-square models. This is because the number of cells can change the sampling distribution. Our engine cylinder example has three cells (corresponding to the categories “4”, “6”, and “8”). But what if there were 10 categories? The shape of the chi-square model would be different.\nThe terminology used by statisticians to distinguish these models is degrees of freedom, abbreviated \\(df\\). The reason for this name and the mathematics behind it are somewhat technical. Suffice it to say for now that if there are \\(c\\) cells, you use \\(c - 1\\) degrees of freedom. For our car example, there are 3 cylinder categories, so \\(df = 2\\).\nLook at the graph below that shows the theoretical chi-square models for varying degrees of freedom.\n\n# Don't worry about the syntax here.\n# You won't need to know how to do this on your own.\nggplot(data.frame(x = c(0, 20)), aes(x)) +\n    stat_function(fun = dchisq, args = list(df = 2),\n                  aes(color = \"2\")) +\n    stat_function(fun = dchisq, args = list(df = 5),\n                  aes(color = \"5\" )) +\n    stat_function(fun = dchisq, args = list(df = 10),\n                  aes(color = \"10\")) +\n    scale_color_manual(name = \"df\",\n                       values = c(\"2\" = \"red\",\n                                  \"5\" = \"blue\",\n                                  \"10\" = \"green\"),\n                       breaks =  c(\"2\", \"5\", \"10\"))\n\n\n\n\nThe red curve (corresponding to \\(df = 2\\)) looks a lot like our simulation above. But as the degrees of freedom increase, the mode shifts further to the right."
  },
  {
    "objectID": "17-chi_square_goodness_of_fit-web.html#chi-square-goodness-of-fit-test",
    "href": "17-chi_square_goodness_of_fit-web.html#chi-square-goodness-of-fit-test",
    "title": "17  Chi-square goodness-of-fit test",
    "section": "17.7 Chi-square goodness-of-fit test",
    "text": "17.7 Chi-square goodness-of-fit test\nThe formal inferential procedure for examining whether data from a categorical variable fits a proposed distribution in the population is called a chi-square goodness-of-fit test.\nWe can use the chi-square model as the sampling distribution as long as the sample size is large enough. This is checked by calculating that the expected cell counts (not the observed cell counts!) are at least 5 in each cell.\nThe following infer pipeline will run a hypothesis test using the theoretical chi-squared distribution with 2 degrees of freedom.\n\ncyl_test &lt;- mtcars2 %&gt;%\n  specify(response = cyl_fct) %&gt;%\n  assume(distribution = \"chisq\")\ncyl_test\n\nA Chi-squared distribution with 2 degrees of freedom.\n\n\nHere is the theoretical distribution:\n\ncyl_test %&gt;%\n  visualize()\n\n\n\n\nAnd here it is will our test statistic (the chi-squared value for our observed data) marked:\n\ncyl_test %&gt;%\n  visualize() +\n  shade_p_value(obs_chisq, direction = \"greater\")\n\n\n\n\nFinally, here is the P-value associated with the shaded area to the right of the test statistic:\n\ncyl_test %&gt;%\n  get_p_value(obs_chisq, direction = \"greater\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1  0.0235\n\n\nNote that this P-value is quite similar to the P-value derived from the simulation earlier.\nWe’ll walk through the engine cylinder example from top to bottom using the rubric. Most of this is just repeating work we’ve already done, but showing this work in the context of the rubric will help you as you take over in the “Your Turn” section later."
  },
  {
    "objectID": "17-chi_square_goodness_of_fit-web.html#exploratory-data-analysis",
    "href": "17-chi_square_goodness_of_fit-web.html#exploratory-data-analysis",
    "title": "17  Chi-square goodness-of-fit test",
    "section": "17.8 Exploratory data analysis",
    "text": "17.8 Exploratory data analysis\n\n17.8.1 Use data documentation (help files, code books, Google, etc.) to determine as much as possible about the data provenance and structure.\nType ?mtcars at the Console to read the help file. Motor Trend is a reputable publication and, therefore, we do not doubt the accuracy of the data. It’s not clear, however, why these specific 32 cars were chosen and if they reflect a representative sample of cars on the road in 1974.\n\nmtcars\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2\n\n\n\nglimpse(mtcars)\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\n\n\n17.8.2 Prepare the data for analysis.\n\n# Although we've already done this above, \n# we include it here again for completeness.\nmtcars2 &lt;- mtcars %&gt;%\n  mutate(cyl_fct = factor(cyl))\nmtcars2\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb cyl_fct\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4       6\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4       6\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1       4\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1       6\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2       8\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1       6\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4       8\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2       4\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2       4\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4       6\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4       6\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3       8\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3       8\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3       8\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4       8\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4       8\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4       8\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1       4\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2       4\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1       4\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1       4\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2       8\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2       8\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4       8\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2       8\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1       4\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2       4\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2       4\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4       8\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6       6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8       8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2       4\n\n\n\nglimpse(mtcars2)\n\nRows: 32\nColumns: 12\n$ mpg     &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17…\n$ cyl     &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4,…\n$ disp    &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8,…\n$ hp      &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, …\n$ drat    &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.…\n$ wt      &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150,…\n$ qsec    &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90,…\n$ vs      &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,…\n$ am      &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,…\n$ gear    &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3,…\n$ carb    &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1,…\n$ cyl_fct &lt;fct&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4,…\n\n\n\n\n17.8.3 Make tables or plots to explore the data visually.\n\ntabyl(mtcars2, cyl_fct) %&gt;%\n    adorn_totals() %&gt;%\n    adorn_pct_formatting()\n\n cyl_fct  n percent\n       4 11   34.4%\n       6  7   21.9%\n       8 14   43.8%\n   Total 32  100.0%"
  },
  {
    "objectID": "17-chi_square_goodness_of_fit-web.html#hypotheses",
    "href": "17-chi_square_goodness_of_fit-web.html#hypotheses",
    "title": "17  Chi-square goodness-of-fit test",
    "section": "17.9 Hypotheses",
    "text": "17.9 Hypotheses\n\n17.9.1 Identify the sample (or samples) and a reasonable population (or populations) of interest.\nThe sample is a set of 32 cars from a 1974 Motor Trends magazine. The population is all cars from 1974.\n\n\n17.9.2 Express the null and alternative hypotheses as contextually meaningful full sentences.\n\\(H_{0}:\\) In 1974, the proportion of cars with 4, 6, and 8 cylinders was 38%, 38%, and 24%, respectively.\n\\(H_{A}:\\) In 1974, the proportion of cars with 4, 6, and 8 cylinders was not 38%, 38%, and 24%.\n\n\n17.9.3 Express the null and alternative hypotheses in symbols (when possible).\n\\(H_{0}: p_{4} = 0.38, p_{6} = 0.38, p_{8} = 0.24\\)\nThere is no easy way to express the alternate hypothesis in symbols because any deviation in any of the categories can lead to rejection of the null. You can’t just say \\(p_{4} \\neq 0.38, p_{6} \\neq 0.38, p_{8} \\neq 0.24\\) because one of these categories might have the correct proportion with the other two different and that would still be consistent with the alternative hypothesis.\nSo the only requirement here is to express the null in symbols."
  },
  {
    "objectID": "17-chi_square_goodness_of_fit-web.html#model",
    "href": "17-chi_square_goodness_of_fit-web.html#model",
    "title": "17  Chi-square goodness-of-fit test",
    "section": "17.10 Model",
    "text": "17.10 Model\n\n17.10.1 Identify the sampling distribution model.\nWe use a \\(\\chi^{2}\\) model with 2 degrees of freedom.\nCommentary: Unlike the normal model, there are infinitely many different \\(\\chi^{2}\\) models, so you have to specify the degrees of freedom when you identify it as the sampling distribution model.\n\n\n17.10.2 Check the relevant conditions to ensure that model assumptions are met.\n\nRandom\n\nWe do not know how Motor Trends magazine sampled these 32 cars, so we’re not sure if this list is random or representative of all cars from 1974. We should be cautious in our conclusions.\n\n10%\n\nAs long as there are at least 320 different car models, we are okay. This sounds like a lot, so this condition might not quite be met. Again, we need to be careful. (Also note that the population is not all automobiles manufactured in 1974. It is all types of automobile manufactured in 1974. There’s a big difference.)\n\nExpected cell counts\n\nThis condition says that under the null, we should see at least 5 cars in each category. The expected counts are \\(32(0.38) = 12.2\\), \\(32(0.38) = 12.2\\), and \\(32(0.24) = 7.7\\). So this condition is met.\n\n\nCommentary: The expected counts condition is necessary for using the theoretical chi-squared distribution. If we were using simulation instead, we would not need this condition."
  },
  {
    "objectID": "17-chi_square_goodness_of_fit-web.html#mechanics",
    "href": "17-chi_square_goodness_of_fit-web.html#mechanics",
    "title": "17  Chi-square goodness-of-fit test",
    "section": "17.11 Mechanics",
    "text": "17.11 Mechanics\n\n17.11.1 Compute the test statistic.\n\nobs_chisq &lt;- mtcars2 %&gt;%\n  specify(response = cyl_fct) %&gt;%\n  hypothesize(null = \"point\",\n              p = c(\"4\" = 0.38,\n                    \"6\" = 0.38,\n                    \"8\" = 0.24)) %&gt;%\n  calculate(stat = \"chisq\")\nobs_chisq\n\nResponse: cyl_fct (factor)\nNull Hypothesis: point\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  7.50\n\n\n\n\n17.11.2 Report the test statistic in context (when possible).\nThe value of \\(\\chi^{2}\\) is 7.5010965.\nCommentary: The \\(\\chi^{2}\\) test statistic is, of course, the same value we computed manually by hand earlier. Also, the formula for \\(\\chi^{2}\\) is a complicated function of observed and expected values, making it difficult to say anything about this number in the context of cars and engine cylinders. So even though the requirement is to “report the test statistic in context,” there’s not much one can say here other than just to report the test statistic.\n\n\n17.11.3 Plot the null distribution.\n\ncyl_test &lt;- mtcars2 %&gt;%\n  specify(response = cyl_fct) %&gt;%\n  assume(distribution = \"chisq\")\ncyl_test\n\nA Chi-squared distribution with 2 degrees of freedom.\n\n\n\ncyl_test %&gt;%\n  visualize() +\n  shade_p_value(obs_chisq, direction = \"greater\")\n\n\n\n\nCommentary: We will use the theoretical distribution\n\n\n17.11.4 Calculate the P-value.\n\ncyl_test_p &lt;- cyl_test %&gt;%\n  get_p_value(obs_chisq, direction = \"greater\")\ncyl_test_p\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1  0.0235\n\n\n\n\n17.11.5 Interpret the P-value as a probability given the null.\nThe P-value is 0.0235049. If the true distribution of cars in 1974 were 38% 4-cylinder, 38% 6-cylinder, and 24% 8-cylinder, there would be a 2.3504856% chance of seeing data at least as extreme as what we saw."
  },
  {
    "objectID": "17-chi_square_goodness_of_fit-web.html#conclusion",
    "href": "17-chi_square_goodness_of_fit-web.html#conclusion",
    "title": "17  Chi-square goodness-of-fit test",
    "section": "17.12 Conclusion",
    "text": "17.12 Conclusion\n\n17.12.1 State the statistical conclusion.\nWe reject the null.\n\n\n17.12.2 State (but do not overstate) a contextually meaningful conclusion.\nThere is sufficient evidence that in 1974, the distribution of cars was not 38% 4-cylinder, 38% 6-cylinder, and 24% 8-cylinder.\n\n\n17.12.3 Express reservations or uncertainty about the generalizability of the conclusion.\nAs long as we restrict our attention to cars in 1974, we are pretty safe, although we are still uncertain if the sample we had was representative of all cars in 1974.\n\n\n17.12.4 Identify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.\nIf we made a Type I error, that would mean the true distribution of cars in 1974 was 38% 4-cylinder, 38% 6-cylinder, and 24% 8-cylinder, but our sample showed otherwise."
  },
  {
    "objectID": "17-chi_square_goodness_of_fit-web.html#confidence-interval",
    "href": "17-chi_square_goodness_of_fit-web.html#confidence-interval",
    "title": "17  Chi-square goodness-of-fit test",
    "section": "17.13 Confidence interval",
    "text": "17.13 Confidence interval\nThere is no confidence interval for a chi-square test. Since our test is not about measuring some parameter of interest (like \\(p\\) or \\(p_{1} - p_{2}\\)), there is no interval to produce."
  },
  {
    "objectID": "17-chi_square_goodness_of_fit-web.html#your-turn",
    "href": "17-chi_square_goodness_of_fit-web.html#your-turn",
    "title": "17  Chi-square goodness-of-fit test",
    "section": "17.14 Your turn",
    "text": "17.14 Your turn\nUse the hsb2 data and determine if the proportion of high school students who attend general programs, academic programs, and vocational programs is 15%, 60%, and 25% respectively.\nThe rubric outline is reproduced below. You may refer to the worked example above and modify it accordingly. Remember to strip out all the commentary. That is just exposition for your benefit in understanding the steps, but is not meant to form part of the formal inference process.\nAnother word of warning: the copy/paste process is not a substitute for your brain. You will often need to modify more than just the names of the data frames and variables to adapt the worked examples to your own work. Do not blindly copy and paste code without understanding what it does. And you should never copy and paste text. All the sentences and paragraphs you write are expressions of your own analysis. They must reflect your own understanding of the inferential process.\nAlso, so that your answers here don’t mess up the code chunks above, use new variable names everywhere.\n\nExploratory data analysis\n\nUse data documentation (help files, code books, Google, etc.) to determine as much as possible about the data provenance and structure.\n\nPlease write up your answer here\n\n# Add code here to print the data\n\n\n# Add code here to glimpse the variables\n\n\n\n\nPrepare the data for analysis. [Not always necessary.]\n\n\n# Add code here to prepare the data for analysis.\n\n\n\n\nMake tables or plots to explore the data visually.\n\n\n# Add code here to make tables or plots.\n\n\n\n\n\nHypotheses\n\nIdentify the sample (or samples) and a reasonable population (or populations) of interest.\n\nPlease write up your answer here.\n\n\n\nExpress the null and alternative hypotheses as contextually meaningful full sentences.\n\n\\(H_{0}:\\) Null hypothesis goes here.\n\\(H_{A}:\\) Alternative hypothesis goes here.\n\n\n\nExpress the null and alternative hypotheses in symbols (when possible).\n\n\\(H_{0}: math\\)\n\\(H_{A}: math\\)\n\n\n\n\nModel\n\nIdentify the sampling distribution model.\n\nPlease write up your answer here.\n\n\n\nCheck the relevant conditions to ensure that model assumptions are met.\n\nPlease write up your answer here. (Some conditions may require R code as well.)\n\n\n\n\nMechanics\n\nCompute the test statistic.\n\n\n# Add code here to compute the test statistic.\n\n\n\n\nReport the test statistic in context (when possible).\n\nPlease write up your answer here.\n\n\n\nPlot the null distribution.\n\n\n# Add code here to plot the null distribution.\n\n\n\n\nCalculate the P-value.\n\n\n# Add code here to calculate the P-value.\n\n\n\n\nInterpret the P-value as a probability given the null.\n\nPlease write up your answer here.\n\n\n\n\nConclusion\n\nState the statistical conclusion.\n\nPlease write up your answer here.\n\n\n\nState (but do not overstate) a contextually meaningful conclusion.\n\nPlease write up your answer here.\n\n\n\nExpress reservations or uncertainty about the generalizability of the conclusion.\n\nPlease write up your answer here.\n\n\n\nIdentify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.\n\nPlease write up your answer here."
  },
  {
    "objectID": "17-chi_square_goodness_of_fit-web.html#bonus-section-residuals",
    "href": "17-chi_square_goodness_of_fit-web.html#bonus-section-residuals",
    "title": "17  Chi-square goodness-of-fit test",
    "section": "17.15 Bonus section: residuals",
    "text": "17.15 Bonus section: residuals\nThe chi-square test can tell us if there is some difference from the expected distribution of counts across the categories, but it doesn’t tell us which category has a higher or lower count than expected. For that, we’ll need to turn to another tool: residuals.\nFor technical reasons, the infer package doesn’t provide residuals, so we’ll have to turn to slightly different tools. Here’s how this works; we’ll return to the example of distribution of cars across the different categories of number of cylinders.\nThe function we’ll use is called chisq.test. It requires us to give it input in the form of a table of counts, together with the proportions we wish to compare to:\n\ntable(mtcars2$cyl_fct) %&gt;%\n  chisq.test(p = c(.38, .38, .24)) -&gt; cyl_chisq.test\ncyl_chisq.test\n\n\n    Chi-squared test for given probabilities\n\ndata:  .\nX-squared = 7.5011, df = 2, p-value = 0.0235\n\n\nNotice that the chi-squared value 7.5011 and the p-value 0.0235 are the same as those we calculated using infer tools above.\nHere’s how to obtain the table of residuals:\n\ncyl_chisq.test$residuals\n\n\n         4          6          8 \n-0.3326528 -1.4797315  2.2805336 \n\n\nWhat do these numbers mean in the real world? Not much. (Essentially, they are the values that were squared to become the individual cell contributions to the overall chi-squared score of the table.)\nWhat we’ll do with them is look for the most positive and most negative values. - We see that the 8-cylinder column has the most positive value: this means that the number of 8-cylinder cars in 1974 was substantially higher than we expected. - We see that the 6-cylinder column has the most negative value: this means that the number of 6-cylinder cars in 1974 was substantially lower than we expected.\n\n17.15.1 Your turn\nDetermine which of the high school program types is the most substantially overrepresented and the most substantially underrepresented, according to our hypothesized distribution.\n\n\n# Add code here to produce the chisq.test result.\n\n# Add code here to examine the residuals.\n\nPlease write your answer here."
  },
  {
    "objectID": "17-chi_square_goodness_of_fit-web.html#conclusion-2",
    "href": "17-chi_square_goodness_of_fit-web.html#conclusion-2",
    "title": "17  Chi-square goodness-of-fit test",
    "section": "17.16 Conclusion",
    "text": "17.16 Conclusion\nWhen a categorical variable has three or more categories, we can run a chi-square goodness-of-fit test to determine if the distribution of counts across those categories matches some pre-specified null hypothesis. The key new mathematical tool we need is the chi-square distribution, a way of measuring the deviation between observed counts and expected counts according to the null.\n\n17.16.1 Preparing and submitting your assignment\n\nFrom the “Run” menu, select “Restart R and Run All Chunks”.\nDeal with any code errors that crop up. Repeat steps 1–-2 until there are no more code errors.\nSpell check your document by clicking the icon with “ABC” and a check mark.\nHit the “Preview” button one last time to generate the final draft of the .nb.html file.\nProofread the HTML file carefully. If there are errors, go back and fix them, then repeat steps 1–5 again.\n\nIf you have completed this chapter as part of a statistics course, follow the directions you receive from your professor to submit your assignment."
  },
  {
    "objectID": "17-chi_square_goodness_of_fit-web.html#footnotes",
    "href": "17-chi_square_goodness_of_fit-web.html#footnotes",
    "title": "17  Chi-square goodness-of-fit test",
    "section": "",
    "text": "Rhyming is fun!↩︎"
  },
  {
    "objectID": "18-chi_square_test_for_independence-web.html#introduction",
    "href": "18-chi_square_test_for_independence-web.html#introduction",
    "title": "18  Chi-square test for independence",
    "section": "18.1 Introduction",
    "text": "18.1 Introduction\nIn this chapter we will learn how to run the chi-square test for independence.\nA chi-square test for independence tests the relationship between two categorical variables. This is an extension of the test for two proportions, except now applied in situations where either the predictor or response variables (or both) have three or more categories.\n\n18.1.1 Install new packages\nThere are no new packages used in this chapter.\n\n\n18.1.2 Download the R notebook file\nCheck the upper-right corner in RStudio to make sure you’re in your intro_stats project. Then click on the following link to download this chapter as an R notebook file (.Rmd).\nhttps://vectorposse.github.io/intro_stats/chapter_downloads/18-chi_square_test_for_independence.Rmd\nOnce the file is downloaded, move it to your project folder in RStudio and open it there.\n\n\n18.1.3 Restart R and run all chunks\nIn RStudio, select “Restart R and Run All Chunks” from the “Run” menu."
  },
  {
    "objectID": "18-chi_square_test_for_independence-web.html#load-packages",
    "href": "18-chi_square_test_for_independence-web.html#load-packages",
    "title": "18  Chi-square test for independence",
    "section": "18.2 Load packages",
    "text": "18.2 Load packages\nWe load the standard tideverse, janitor, and infer packages. We also use the MASS package for the birthwt data, and the openintro package for the smoking data.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(infer)\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nlibrary(openintro)\n\nLoading required package: airports\nLoading required package: cherryblossom\nLoading required package: usdata\n\nAttaching package: 'openintro'\n\nThe following objects are masked from 'package:MASS':\n\n    housing, mammals"
  },
  {
    "objectID": "18-chi_square_test_for_independence-web.html#research-question",
    "href": "18-chi_square_test_for_independence-web.html#research-question",
    "title": "18  Chi-square test for independence",
    "section": "18.3 Research question",
    "text": "18.3 Research question\nAre mothers from certain racial groups more or less likely to have low birth weight babies? In other words, are low birth weight and race associated?\nLet’s look at the data. The birthwt data was collected at Baystate Medical Center, Springfield, Mass during 1986. In terms of addressing the research question, we are, of course, limited to conclusions about women in that area of the country in the mid-1980s.\n\nbirthwt\n\n    low age lwt race smoke ptl ht ui ftv  bwt\n85    0  19 182    2     0   0  0  1   0 2523\n86    0  33 155    3     0   0  0  0   3 2551\n87    0  20 105    1     1   0  0  0   1 2557\n88    0  21 108    1     1   0  0  1   2 2594\n89    0  18 107    1     1   0  0  1   0 2600\n91    0  21 124    3     0   0  0  0   0 2622\n92    0  22 118    1     0   0  0  0   1 2637\n93    0  17 103    3     0   0  0  0   1 2637\n94    0  29 123    1     1   0  0  0   1 2663\n95    0  26 113    1     1   0  0  0   0 2665\n96    0  19  95    3     0   0  0  0   0 2722\n97    0  19 150    3     0   0  0  0   1 2733\n98    0  22  95    3     0   0  1  0   0 2751\n99    0  30 107    3     0   1  0  1   2 2750\n100   0  18 100    1     1   0  0  0   0 2769\n101   0  18 100    1     1   0  0  0   0 2769\n102   0  15  98    2     0   0  0  0   0 2778\n103   0  25 118    1     1   0  0  0   3 2782\n104   0  20 120    3     0   0  0  1   0 2807\n105   0  28 120    1     1   0  0  0   1 2821\n106   0  32 121    3     0   0  0  0   2 2835\n107   0  31 100    1     0   0  0  1   3 2835\n108   0  36 202    1     0   0  0  0   1 2836\n109   0  28 120    3     0   0  0  0   0 2863\n111   0  25 120    3     0   0  0  1   2 2877\n112   0  28 167    1     0   0  0  0   0 2877\n113   0  17 122    1     1   0  0  0   0 2906\n114   0  29 150    1     0   0  0  0   2 2920\n115   0  26 168    2     1   0  0  0   0 2920\n116   0  17 113    2     0   0  0  0   1 2920\n117   0  17 113    2     0   0  0  0   1 2920\n118   0  24  90    1     1   1  0  0   1 2948\n119   0  35 121    2     1   1  0  0   1 2948\n120   0  25 155    1     0   0  0  0   1 2977\n121   0  25 125    2     0   0  0  0   0 2977\n123   0  29 140    1     1   0  0  0   2 2977\n124   0  19 138    1     1   0  0  0   2 2977\n125   0  27 124    1     1   0  0  0   0 2922\n126   0  31 215    1     1   0  0  0   2 3005\n127   0  33 109    1     1   0  0  0   1 3033\n128   0  21 185    2     1   0  0  0   2 3042\n129   0  19 189    1     0   0  0  0   2 3062\n130   0  23 130    2     0   0  0  0   1 3062\n131   0  21 160    1     0   0  0  0   0 3062\n132   0  18  90    1     1   0  0  1   0 3062\n133   0  18  90    1     1   0  0  1   0 3062\n134   0  32 132    1     0   0  0  0   4 3080\n135   0  19 132    3     0   0  0  0   0 3090\n136   0  24 115    1     0   0  0  0   2 3090\n137   0  22  85    3     1   0  0  0   0 3090\n138   0  22 120    1     0   0  1  0   1 3100\n139   0  23 128    3     0   0  0  0   0 3104\n140   0  22 130    1     1   0  0  0   0 3132\n141   0  30  95    1     1   0  0  0   2 3147\n142   0  19 115    3     0   0  0  0   0 3175\n143   0  16 110    3     0   0  0  0   0 3175\n144   0  21 110    3     1   0  0  1   0 3203\n145   0  30 153    3     0   0  0  0   0 3203\n146   0  20 103    3     0   0  0  0   0 3203\n147   0  17 119    3     0   0  0  0   0 3225\n148   0  17 119    3     0   0  0  0   0 3225\n149   0  23 119    3     0   0  0  0   2 3232\n150   0  24 110    3     0   0  0  0   0 3232\n151   0  28 140    1     0   0  0  0   0 3234\n154   0  26 133    3     1   2  0  0   0 3260\n155   0  20 169    3     0   1  0  1   1 3274\n156   0  24 115    3     0   0  0  0   2 3274\n159   0  28 250    3     1   0  0  0   6 3303\n160   0  20 141    1     0   2  0  1   1 3317\n161   0  22 158    2     0   1  0  0   2 3317\n162   0  22 112    1     1   2  0  0   0 3317\n163   0  31 150    3     1   0  0  0   2 3321\n164   0  23 115    3     1   0  0  0   1 3331\n166   0  16 112    2     0   0  0  0   0 3374\n167   0  16 135    1     1   0  0  0   0 3374\n168   0  18 229    2     0   0  0  0   0 3402\n169   0  25 140    1     0   0  0  0   1 3416\n170   0  32 134    1     1   1  0  0   4 3430\n172   0  20 121    2     1   0  0  0   0 3444\n173   0  23 190    1     0   0  0  0   0 3459\n174   0  22 131    1     0   0  0  0   1 3460\n175   0  32 170    1     0   0  0  0   0 3473\n176   0  30 110    3     0   0  0  0   0 3544\n177   0  20 127    3     0   0  0  0   0 3487\n179   0  23 123    3     0   0  0  0   0 3544\n180   0  17 120    3     1   0  0  0   0 3572\n181   0  19 105    3     0   0  0  0   0 3572\n182   0  23 130    1     0   0  0  0   0 3586\n183   0  36 175    1     0   0  0  0   0 3600\n184   0  22 125    1     0   0  0  0   1 3614\n185   0  24 133    1     0   0  0  0   0 3614\n186   0  21 134    3     0   0  0  0   2 3629\n187   0  19 235    1     1   0  1  0   0 3629\n188   0  25  95    1     1   3  0  1   0 3637\n189   0  16 135    1     1   0  0  0   0 3643\n190   0  29 135    1     0   0  0  0   1 3651\n191   0  29 154    1     0   0  0  0   1 3651\n192   0  19 147    1     1   0  0  0   0 3651\n193   0  19 147    1     1   0  0  0   0 3651\n195   0  30 137    1     0   0  0  0   1 3699\n196   0  24 110    1     0   0  0  0   1 3728\n197   0  19 184    1     1   0  1  0   0 3756\n199   0  24 110    3     0   1  0  0   0 3770\n200   0  23 110    1     0   0  0  0   1 3770\n201   0  20 120    3     0   0  0  0   0 3770\n202   0  25 241    2     0   0  1  0   0 3790\n203   0  30 112    1     0   0  0  0   1 3799\n204   0  22 169    1     0   0  0  0   0 3827\n205   0  18 120    1     1   0  0  0   2 3856\n206   0  16 170    2     0   0  0  0   4 3860\n207   0  32 186    1     0   0  0  0   2 3860\n208   0  18 120    3     0   0  0  0   1 3884\n209   0  29 130    1     1   0  0  0   2 3884\n210   0  33 117    1     0   0  0  1   1 3912\n211   0  20 170    1     1   0  0  0   0 3940\n212   0  28 134    3     0   0  0  0   1 3941\n213   0  14 135    1     0   0  0  0   0 3941\n214   0  28 130    3     0   0  0  0   0 3969\n215   0  25 120    1     0   0  0  0   2 3983\n216   0  16  95    3     0   0  0  0   1 3997\n217   0  20 158    1     0   0  0  0   1 3997\n218   0  26 160    3     0   0  0  0   0 4054\n219   0  21 115    1     0   0  0  0   1 4054\n220   0  22 129    1     0   0  0  0   0 4111\n221   0  25 130    1     0   0  0  0   2 4153\n222   0  31 120    1     0   0  0  0   2 4167\n223   0  35 170    1     0   1  0  0   1 4174\n224   0  19 120    1     1   0  0  0   0 4238\n225   0  24 116    1     0   0  0  0   1 4593\n226   0  45 123    1     0   0  0  0   1 4990\n4     1  28 120    3     1   1  0  1   0  709\n10    1  29 130    1     0   0  0  1   2 1021\n11    1  34 187    2     1   0  1  0   0 1135\n13    1  25 105    3     0   1  1  0   0 1330\n15    1  25  85    3     0   0  0  1   0 1474\n16    1  27 150    3     0   0  0  0   0 1588\n17    1  23  97    3     0   0  0  1   1 1588\n18    1  24 128    2     0   1  0  0   1 1701\n19    1  24 132    3     0   0  1  0   0 1729\n20    1  21 165    1     1   0  1  0   1 1790\n22    1  32 105    1     1   0  0  0   0 1818\n23    1  19  91    1     1   2  0  1   0 1885\n24    1  25 115    3     0   0  0  0   0 1893\n25    1  16 130    3     0   0  0  0   1 1899\n26    1  25  92    1     1   0  0  0   0 1928\n27    1  20 150    1     1   0  0  0   2 1928\n28    1  21 200    2     0   0  0  1   2 1928\n29    1  24 155    1     1   1  0  0   0 1936\n30    1  21 103    3     0   0  0  0   0 1970\n31    1  20 125    3     0   0  0  1   0 2055\n32    1  25  89    3     0   2  0  0   1 2055\n33    1  19 102    1     0   0  0  0   2 2082\n34    1  19 112    1     1   0  0  1   0 2084\n35    1  26 117    1     1   1  0  0   0 2084\n36    1  24 138    1     0   0  0  0   0 2100\n37    1  17 130    3     1   1  0  1   0 2125\n40    1  20 120    2     1   0  0  0   3 2126\n42    1  22 130    1     1   1  0  1   1 2187\n43    1  27 130    2     0   0  0  1   0 2187\n44    1  20  80    3     1   0  0  1   0 2211\n45    1  17 110    1     1   0  0  0   0 2225\n46    1  25 105    3     0   1  0  0   1 2240\n47    1  20 109    3     0   0  0  0   0 2240\n49    1  18 148    3     0   0  0  0   0 2282\n50    1  18 110    2     1   1  0  0   0 2296\n51    1  20 121    1     1   1  0  1   0 2296\n52    1  21 100    3     0   1  0  0   4 2301\n54    1  26  96    3     0   0  0  0   0 2325\n56    1  31 102    1     1   1  0  0   1 2353\n57    1  15 110    1     0   0  0  0   0 2353\n59    1  23 187    2     1   0  0  0   1 2367\n60    1  20 122    2     1   0  0  0   0 2381\n61    1  24 105    2     1   0  0  0   0 2381\n62    1  15 115    3     0   0  0  1   0 2381\n63    1  23 120    3     0   0  0  0   0 2410\n65    1  30 142    1     1   1  0  0   0 2410\n67    1  22 130    1     1   0  0  0   1 2410\n68    1  17 120    1     1   0  0  0   3 2414\n69    1  23 110    1     1   1  0  0   0 2424\n71    1  17 120    2     0   0  0  0   2 2438\n75    1  26 154    3     0   1  1  0   1 2442\n76    1  20 105    3     0   0  0  0   3 2450\n77    1  26 190    1     1   0  0  0   0 2466\n78    1  14 101    3     1   1  0  0   0 2466\n79    1  28  95    1     1   0  0  0   2 2466\n81    1  14 100    3     0   0  0  0   2 2495\n82    1  23  94    3     1   0  0  0   0 2495\n83    1  17 142    2     0   0  1  0   0 2495\n84    1  21 130    1     1   0  1  0   3 2495\n\n\n\nglimpse(birthwt)\n\nRows: 189\nColumns: 10\n$ low   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ age   &lt;int&gt; 19, 33, 20, 21, 18, 21, 22, 17, 29, 26, 19, 19, 22, 30, 18, 18, …\n$ lwt   &lt;int&gt; 182, 155, 105, 108, 107, 124, 118, 103, 123, 113, 95, 150, 95, 1…\n$ race  &lt;int&gt; 2, 3, 1, 1, 1, 3, 1, 3, 1, 1, 3, 3, 3, 3, 1, 1, 2, 1, 3, 1, 3, 1…\n$ smoke &lt;int&gt; 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0…\n$ ptl   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ht    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ui    &lt;int&gt; 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1…\n$ ftv   &lt;int&gt; 0, 3, 1, 2, 0, 0, 1, 1, 1, 0, 0, 1, 0, 2, 0, 0, 0, 3, 0, 1, 2, 3…\n$ bwt   &lt;int&gt; 2523, 2551, 2557, 2594, 2600, 2622, 2637, 2637, 2663, 2665, 2722…\n\n\nThe low variable is an indicator of birth weight less than 2.5 kg. So even though birth weight is numerical, we have a convenient categorical variable that serves as a marker of low birth weight, gathering all low birth weight babies into a single group. The race variable is categorical, coded as 1 = white, 2 = black, 3 = other.\nNeither variable appears in the data frame as a factor variable, so we will need to change that. The new tibble will be called birthwt2.\n\nbirthwt2 &lt;- birthwt %&gt;%\n  mutate(low_fct = factor(low, levels = c(0, 1),\n                          labels = c(\"no\", \"yes\")),\n         race_fct = factor(race, levels = c(1, 2, 3),\n                           labels = c(\"white\", \"black\", \"other\")))\nbirthwt2\n\n    low age lwt race smoke ptl ht ui ftv  bwt low_fct race_fct\n85    0  19 182    2     0   0  0  1   0 2523      no    black\n86    0  33 155    3     0   0  0  0   3 2551      no    other\n87    0  20 105    1     1   0  0  0   1 2557      no    white\n88    0  21 108    1     1   0  0  1   2 2594      no    white\n89    0  18 107    1     1   0  0  1   0 2600      no    white\n91    0  21 124    3     0   0  0  0   0 2622      no    other\n92    0  22 118    1     0   0  0  0   1 2637      no    white\n93    0  17 103    3     0   0  0  0   1 2637      no    other\n94    0  29 123    1     1   0  0  0   1 2663      no    white\n95    0  26 113    1     1   0  0  0   0 2665      no    white\n96    0  19  95    3     0   0  0  0   0 2722      no    other\n97    0  19 150    3     0   0  0  0   1 2733      no    other\n98    0  22  95    3     0   0  1  0   0 2751      no    other\n99    0  30 107    3     0   1  0  1   2 2750      no    other\n100   0  18 100    1     1   0  0  0   0 2769      no    white\n101   0  18 100    1     1   0  0  0   0 2769      no    white\n102   0  15  98    2     0   0  0  0   0 2778      no    black\n103   0  25 118    1     1   0  0  0   3 2782      no    white\n104   0  20 120    3     0   0  0  1   0 2807      no    other\n105   0  28 120    1     1   0  0  0   1 2821      no    white\n106   0  32 121    3     0   0  0  0   2 2835      no    other\n107   0  31 100    1     0   0  0  1   3 2835      no    white\n108   0  36 202    1     0   0  0  0   1 2836      no    white\n109   0  28 120    3     0   0  0  0   0 2863      no    other\n111   0  25 120    3     0   0  0  1   2 2877      no    other\n112   0  28 167    1     0   0  0  0   0 2877      no    white\n113   0  17 122    1     1   0  0  0   0 2906      no    white\n114   0  29 150    1     0   0  0  0   2 2920      no    white\n115   0  26 168    2     1   0  0  0   0 2920      no    black\n116   0  17 113    2     0   0  0  0   1 2920      no    black\n117   0  17 113    2     0   0  0  0   1 2920      no    black\n118   0  24  90    1     1   1  0  0   1 2948      no    white\n119   0  35 121    2     1   1  0  0   1 2948      no    black\n120   0  25 155    1     0   0  0  0   1 2977      no    white\n121   0  25 125    2     0   0  0  0   0 2977      no    black\n123   0  29 140    1     1   0  0  0   2 2977      no    white\n124   0  19 138    1     1   0  0  0   2 2977      no    white\n125   0  27 124    1     1   0  0  0   0 2922      no    white\n126   0  31 215    1     1   0  0  0   2 3005      no    white\n127   0  33 109    1     1   0  0  0   1 3033      no    white\n128   0  21 185    2     1   0  0  0   2 3042      no    black\n129   0  19 189    1     0   0  0  0   2 3062      no    white\n130   0  23 130    2     0   0  0  0   1 3062      no    black\n131   0  21 160    1     0   0  0  0   0 3062      no    white\n132   0  18  90    1     1   0  0  1   0 3062      no    white\n133   0  18  90    1     1   0  0  1   0 3062      no    white\n134   0  32 132    1     0   0  0  0   4 3080      no    white\n135   0  19 132    3     0   0  0  0   0 3090      no    other\n136   0  24 115    1     0   0  0  0   2 3090      no    white\n137   0  22  85    3     1   0  0  0   0 3090      no    other\n138   0  22 120    1     0   0  1  0   1 3100      no    white\n139   0  23 128    3     0   0  0  0   0 3104      no    other\n140   0  22 130    1     1   0  0  0   0 3132      no    white\n141   0  30  95    1     1   0  0  0   2 3147      no    white\n142   0  19 115    3     0   0  0  0   0 3175      no    other\n143   0  16 110    3     0   0  0  0   0 3175      no    other\n144   0  21 110    3     1   0  0  1   0 3203      no    other\n145   0  30 153    3     0   0  0  0   0 3203      no    other\n146   0  20 103    3     0   0  0  0   0 3203      no    other\n147   0  17 119    3     0   0  0  0   0 3225      no    other\n148   0  17 119    3     0   0  0  0   0 3225      no    other\n149   0  23 119    3     0   0  0  0   2 3232      no    other\n150   0  24 110    3     0   0  0  0   0 3232      no    other\n151   0  28 140    1     0   0  0  0   0 3234      no    white\n154   0  26 133    3     1   2  0  0   0 3260      no    other\n155   0  20 169    3     0   1  0  1   1 3274      no    other\n156   0  24 115    3     0   0  0  0   2 3274      no    other\n159   0  28 250    3     1   0  0  0   6 3303      no    other\n160   0  20 141    1     0   2  0  1   1 3317      no    white\n161   0  22 158    2     0   1  0  0   2 3317      no    black\n162   0  22 112    1     1   2  0  0   0 3317      no    white\n163   0  31 150    3     1   0  0  0   2 3321      no    other\n164   0  23 115    3     1   0  0  0   1 3331      no    other\n166   0  16 112    2     0   0  0  0   0 3374      no    black\n167   0  16 135    1     1   0  0  0   0 3374      no    white\n168   0  18 229    2     0   0  0  0   0 3402      no    black\n169   0  25 140    1     0   0  0  0   1 3416      no    white\n170   0  32 134    1     1   1  0  0   4 3430      no    white\n172   0  20 121    2     1   0  0  0   0 3444      no    black\n173   0  23 190    1     0   0  0  0   0 3459      no    white\n174   0  22 131    1     0   0  0  0   1 3460      no    white\n175   0  32 170    1     0   0  0  0   0 3473      no    white\n176   0  30 110    3     0   0  0  0   0 3544      no    other\n177   0  20 127    3     0   0  0  0   0 3487      no    other\n179   0  23 123    3     0   0  0  0   0 3544      no    other\n180   0  17 120    3     1   0  0  0   0 3572      no    other\n181   0  19 105    3     0   0  0  0   0 3572      no    other\n182   0  23 130    1     0   0  0  0   0 3586      no    white\n183   0  36 175    1     0   0  0  0   0 3600      no    white\n184   0  22 125    1     0   0  0  0   1 3614      no    white\n185   0  24 133    1     0   0  0  0   0 3614      no    white\n186   0  21 134    3     0   0  0  0   2 3629      no    other\n187   0  19 235    1     1   0  1  0   0 3629      no    white\n188   0  25  95    1     1   3  0  1   0 3637      no    white\n189   0  16 135    1     1   0  0  0   0 3643      no    white\n190   0  29 135    1     0   0  0  0   1 3651      no    white\n191   0  29 154    1     0   0  0  0   1 3651      no    white\n192   0  19 147    1     1   0  0  0   0 3651      no    white\n193   0  19 147    1     1   0  0  0   0 3651      no    white\n195   0  30 137    1     0   0  0  0   1 3699      no    white\n196   0  24 110    1     0   0  0  0   1 3728      no    white\n197   0  19 184    1     1   0  1  0   0 3756      no    white\n199   0  24 110    3     0   1  0  0   0 3770      no    other\n200   0  23 110    1     0   0  0  0   1 3770      no    white\n201   0  20 120    3     0   0  0  0   0 3770      no    other\n202   0  25 241    2     0   0  1  0   0 3790      no    black\n203   0  30 112    1     0   0  0  0   1 3799      no    white\n204   0  22 169    1     0   0  0  0   0 3827      no    white\n205   0  18 120    1     1   0  0  0   2 3856      no    white\n206   0  16 170    2     0   0  0  0   4 3860      no    black\n207   0  32 186    1     0   0  0  0   2 3860      no    white\n208   0  18 120    3     0   0  0  0   1 3884      no    other\n209   0  29 130    1     1   0  0  0   2 3884      no    white\n210   0  33 117    1     0   0  0  1   1 3912      no    white\n211   0  20 170    1     1   0  0  0   0 3940      no    white\n212   0  28 134    3     0   0  0  0   1 3941      no    other\n213   0  14 135    1     0   0  0  0   0 3941      no    white\n214   0  28 130    3     0   0  0  0   0 3969      no    other\n215   0  25 120    1     0   0  0  0   2 3983      no    white\n216   0  16  95    3     0   0  0  0   1 3997      no    other\n217   0  20 158    1     0   0  0  0   1 3997      no    white\n218   0  26 160    3     0   0  0  0   0 4054      no    other\n219   0  21 115    1     0   0  0  0   1 4054      no    white\n220   0  22 129    1     0   0  0  0   0 4111      no    white\n221   0  25 130    1     0   0  0  0   2 4153      no    white\n222   0  31 120    1     0   0  0  0   2 4167      no    white\n223   0  35 170    1     0   1  0  0   1 4174      no    white\n224   0  19 120    1     1   0  0  0   0 4238      no    white\n225   0  24 116    1     0   0  0  0   1 4593      no    white\n226   0  45 123    1     0   0  0  0   1 4990      no    white\n4     1  28 120    3     1   1  0  1   0  709     yes    other\n10    1  29 130    1     0   0  0  1   2 1021     yes    white\n11    1  34 187    2     1   0  1  0   0 1135     yes    black\n13    1  25 105    3     0   1  1  0   0 1330     yes    other\n15    1  25  85    3     0   0  0  1   0 1474     yes    other\n16    1  27 150    3     0   0  0  0   0 1588     yes    other\n17    1  23  97    3     0   0  0  1   1 1588     yes    other\n18    1  24 128    2     0   1  0  0   1 1701     yes    black\n19    1  24 132    3     0   0  1  0   0 1729     yes    other\n20    1  21 165    1     1   0  1  0   1 1790     yes    white\n22    1  32 105    1     1   0  0  0   0 1818     yes    white\n23    1  19  91    1     1   2  0  1   0 1885     yes    white\n24    1  25 115    3     0   0  0  0   0 1893     yes    other\n25    1  16 130    3     0   0  0  0   1 1899     yes    other\n26    1  25  92    1     1   0  0  0   0 1928     yes    white\n27    1  20 150    1     1   0  0  0   2 1928     yes    white\n28    1  21 200    2     0   0  0  1   2 1928     yes    black\n29    1  24 155    1     1   1  0  0   0 1936     yes    white\n30    1  21 103    3     0   0  0  0   0 1970     yes    other\n31    1  20 125    3     0   0  0  1   0 2055     yes    other\n32    1  25  89    3     0   2  0  0   1 2055     yes    other\n33    1  19 102    1     0   0  0  0   2 2082     yes    white\n34    1  19 112    1     1   0  0  1   0 2084     yes    white\n35    1  26 117    1     1   1  0  0   0 2084     yes    white\n36    1  24 138    1     0   0  0  0   0 2100     yes    white\n37    1  17 130    3     1   1  0  1   0 2125     yes    other\n40    1  20 120    2     1   0  0  0   3 2126     yes    black\n42    1  22 130    1     1   1  0  1   1 2187     yes    white\n43    1  27 130    2     0   0  0  1   0 2187     yes    black\n44    1  20  80    3     1   0  0  1   0 2211     yes    other\n45    1  17 110    1     1   0  0  0   0 2225     yes    white\n46    1  25 105    3     0   1  0  0   1 2240     yes    other\n47    1  20 109    3     0   0  0  0   0 2240     yes    other\n49    1  18 148    3     0   0  0  0   0 2282     yes    other\n50    1  18 110    2     1   1  0  0   0 2296     yes    black\n51    1  20 121    1     1   1  0  1   0 2296     yes    white\n52    1  21 100    3     0   1  0  0   4 2301     yes    other\n54    1  26  96    3     0   0  0  0   0 2325     yes    other\n56    1  31 102    1     1   1  0  0   1 2353     yes    white\n57    1  15 110    1     0   0  0  0   0 2353     yes    white\n59    1  23 187    2     1   0  0  0   1 2367     yes    black\n60    1  20 122    2     1   0  0  0   0 2381     yes    black\n61    1  24 105    2     1   0  0  0   0 2381     yes    black\n62    1  15 115    3     0   0  0  1   0 2381     yes    other\n63    1  23 120    3     0   0  0  0   0 2410     yes    other\n65    1  30 142    1     1   1  0  0   0 2410     yes    white\n67    1  22 130    1     1   0  0  0   1 2410     yes    white\n68    1  17 120    1     1   0  0  0   3 2414     yes    white\n69    1  23 110    1     1   1  0  0   0 2424     yes    white\n71    1  17 120    2     0   0  0  0   2 2438     yes    black\n75    1  26 154    3     0   1  1  0   1 2442     yes    other\n76    1  20 105    3     0   0  0  0   3 2450     yes    other\n77    1  26 190    1     1   0  0  0   0 2466     yes    white\n78    1  14 101    3     1   1  0  0   0 2466     yes    other\n79    1  28  95    1     1   0  0  0   2 2466     yes    white\n81    1  14 100    3     0   0  0  0   2 2495     yes    other\n82    1  23  94    3     1   0  0  0   0 2495     yes    other\n83    1  17 142    2     0   0  1  0   0 2495     yes    black\n84    1  21 130    1     1   0  1  0   3 2495     yes    white\n\n\n\nglimpse(birthwt2)\n\nRows: 189\nColumns: 12\n$ low      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ age      &lt;int&gt; 19, 33, 20, 21, 18, 21, 22, 17, 29, 26, 19, 19, 22, 30, 18, 1…\n$ lwt      &lt;int&gt; 182, 155, 105, 108, 107, 124, 118, 103, 123, 113, 95, 150, 95…\n$ race     &lt;int&gt; 2, 3, 1, 1, 1, 3, 1, 3, 1, 1, 3, 3, 3, 3, 1, 1, 2, 1, 3, 1, 3…\n$ smoke    &lt;int&gt; 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0…\n$ ptl      &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…\n$ ht       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ui       &lt;int&gt; 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0…\n$ ftv      &lt;int&gt; 0, 3, 1, 2, 0, 0, 1, 1, 1, 0, 0, 1, 0, 2, 0, 0, 0, 3, 0, 1, 2…\n$ bwt      &lt;int&gt; 2523, 2551, 2557, 2594, 2600, 2622, 2637, 2637, 2663, 2665, 2…\n$ low_fct  &lt;fct&gt; no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, n…\n$ race_fct &lt;fct&gt; black, other, white, white, white, other, white, other, white…"
  },
  {
    "objectID": "18-chi_square_test_for_independence-web.html#chi-square-test-for-independence",
    "href": "18-chi_square_test_for_independence-web.html#chi-square-test-for-independence",
    "title": "18  Chi-square test for independence",
    "section": "18.4 Chi-square test for independence",
    "text": "18.4 Chi-square test for independence\nIn a previous chapter, we learned about the chi-square goodness-of-fit test. With a single categorical variable, we summarized data in a frequency table. Each cell of the table had an observed count from the data that we compared to an expected count from the assumption of a null hypothesis. The chi-square statistic measured the discrepancy between observed and expected.\nWith two categorical variables, we use a contingency table instead of a frequency table. But the principle of the chi-square statistic is the same: each cell in the contingency table has an observed count and an expected count. This forms the basis of a chi-square test for independence.\nBelow is the contingency table for these two variables. Normally, we only care about column totals because we care how the response variable (here, low_fct) is distributed in each group of the predictor variable (i.e., each racial group). But for the calculation of chi-squared, we will need both row and column totals.\n\ntabyl(birthwt2, low_fct, race_fct) %&gt;%\n    adorn_totals(where = c(\"row\", \"col\"))\n\n low_fct white black other Total\n      no    73    15    42   130\n     yes    23    11    25    59\n   Total    96    26    67   189\n\n\nA test for independence has a simple null hypothesis: the two variables are independent. This gives us a way to compute expected counts. To see how, look at the sum of all the normal weight babies (\\(73 + 15 + 42 = 130\\)) and all the low birth weight babies (\\(23 + 11 + 25 = 59\\)). In other words, if race is ignored, there were 130 normal weight babies and 59 low birth weight babies out of 189 total babies. 59 of 189 is 0.31217 or 31.217%, and 130 of 189 is 0.68783 or 68.783%.\nNow, if low birth weight and race are truly independent, it shouldn’t matter if the mothers were white, black, or some other race. In other words, of 96 white mothers, we should still expect 68.783% of them to have normal weight babies and 31.217% of them to have low birth weight babies. 68.783% of 96 is 66.032. This is the expected cell count for normal birth weight babies of white women. 31.217% of 96 is 29.968. This is the expected cell count for low birth weight babies of white women. The same analysis can be done for the next two columns as well.\n\nExercise 1\nComplete the list of expected cell counts in the table above. In other words, apply the percentages 68.783% and 31.217% to the totals of the “black” and “other” columns. Put them in the table below:\n\n\n\n\n\nwhite\nblack\nother\n\n\n\n\nno\n66.032\n?\n?\n\n\nyes\n29.968\n?\n?\n\n\n\n\n\nUnlike the goodness-of-fit test that requires one to specify expected counts for each cell, the test for independence uses only the data to determine the expected counts. For any given cell, if \\(R\\) is the row total, \\(C\\) is the column total, and \\(n\\) is the grand total (the sample size), the expected count in any cell is simply\n\\[\nE = \\frac{R C}{n}.\n\\]\nThis is equivalent to the explanation in the previous paragraph. Using low birth weight babies among white mothers as an example, \\(R/n\\) is \\(59/189\\) which is 0.31217. Then we multiply this by the column total \\(C = 96\\) to get\n\\[\n\\left(\\frac{R}{n}\\right) C = \\frac{R C}{n} = \\frac{59 \\times 96}{189} =  29.96825.\n\\]\nEverything else works almost the same as it did for a chi-square goodness-of-fit test. We still compute \\(\\chi^{2}\\) by adding up deviations across all cells:\n\\[\n\\chi^{2} = \\sum \\frac{(O - E)^{2}}{E}.\n\\]\nEven under the assumption of the null, there will still be some sampling variability. Like any hypothesis test, our job is to determine whether the deviations we see are possible due to pure chance alone. The random values of \\(\\chi^{2}\\) that result from sampling variability will follow a chi-square model. But how many degrees of freedom are there? This is a little different from the goodness-of-fit test. Instead of the number of cells minus one, we use the following formula:\n\\[\ndf = (\\#rows - 1)(\\#columns - 1).\n\\]\nIn our example we have 2 rows (“yes”, “no”) and 3 columns (“white”, “black”, “other”); therefore,\n\\[\ndf = (2 - 1)(3 - 1) = 1 \\times 2 = 2\n\\]\nand we have 2 degrees of freedom (even though there are 6 cells).\nLet’s run through the rubric in its entirety."
  },
  {
    "objectID": "18-chi_square_test_for_independence-web.html#exploratory-data-analysis",
    "href": "18-chi_square_test_for_independence-web.html#exploratory-data-analysis",
    "title": "18  Chi-square test for independence",
    "section": "18.5 Exploratory data analysis",
    "text": "18.5 Exploratory data analysis\n\n18.5.1 Use data documentation (help files, code books, Google, etc.) to determine as much as possible about the data provenance and structure.\nYou should type ?birthwt at the Console to read the help file. We don’t have any information about how these mothers were selected. The “Source” at the end of the help file is a statistics textbook, so we’d have to track down that book to see where they got the data and if traced back to a primary source.\n\nbirthwt\n\n    low age lwt race smoke ptl ht ui ftv  bwt\n85    0  19 182    2     0   0  0  1   0 2523\n86    0  33 155    3     0   0  0  0   3 2551\n87    0  20 105    1     1   0  0  0   1 2557\n88    0  21 108    1     1   0  0  1   2 2594\n89    0  18 107    1     1   0  0  1   0 2600\n91    0  21 124    3     0   0  0  0   0 2622\n92    0  22 118    1     0   0  0  0   1 2637\n93    0  17 103    3     0   0  0  0   1 2637\n94    0  29 123    1     1   0  0  0   1 2663\n95    0  26 113    1     1   0  0  0   0 2665\n96    0  19  95    3     0   0  0  0   0 2722\n97    0  19 150    3     0   0  0  0   1 2733\n98    0  22  95    3     0   0  1  0   0 2751\n99    0  30 107    3     0   1  0  1   2 2750\n100   0  18 100    1     1   0  0  0   0 2769\n101   0  18 100    1     1   0  0  0   0 2769\n102   0  15  98    2     0   0  0  0   0 2778\n103   0  25 118    1     1   0  0  0   3 2782\n104   0  20 120    3     0   0  0  1   0 2807\n105   0  28 120    1     1   0  0  0   1 2821\n106   0  32 121    3     0   0  0  0   2 2835\n107   0  31 100    1     0   0  0  1   3 2835\n108   0  36 202    1     0   0  0  0   1 2836\n109   0  28 120    3     0   0  0  0   0 2863\n111   0  25 120    3     0   0  0  1   2 2877\n112   0  28 167    1     0   0  0  0   0 2877\n113   0  17 122    1     1   0  0  0   0 2906\n114   0  29 150    1     0   0  0  0   2 2920\n115   0  26 168    2     1   0  0  0   0 2920\n116   0  17 113    2     0   0  0  0   1 2920\n117   0  17 113    2     0   0  0  0   1 2920\n118   0  24  90    1     1   1  0  0   1 2948\n119   0  35 121    2     1   1  0  0   1 2948\n120   0  25 155    1     0   0  0  0   1 2977\n121   0  25 125    2     0   0  0  0   0 2977\n123   0  29 140    1     1   0  0  0   2 2977\n124   0  19 138    1     1   0  0  0   2 2977\n125   0  27 124    1     1   0  0  0   0 2922\n126   0  31 215    1     1   0  0  0   2 3005\n127   0  33 109    1     1   0  0  0   1 3033\n128   0  21 185    2     1   0  0  0   2 3042\n129   0  19 189    1     0   0  0  0   2 3062\n130   0  23 130    2     0   0  0  0   1 3062\n131   0  21 160    1     0   0  0  0   0 3062\n132   0  18  90    1     1   0  0  1   0 3062\n133   0  18  90    1     1   0  0  1   0 3062\n134   0  32 132    1     0   0  0  0   4 3080\n135   0  19 132    3     0   0  0  0   0 3090\n136   0  24 115    1     0   0  0  0   2 3090\n137   0  22  85    3     1   0  0  0   0 3090\n138   0  22 120    1     0   0  1  0   1 3100\n139   0  23 128    3     0   0  0  0   0 3104\n140   0  22 130    1     1   0  0  0   0 3132\n141   0  30  95    1     1   0  0  0   2 3147\n142   0  19 115    3     0   0  0  0   0 3175\n143   0  16 110    3     0   0  0  0   0 3175\n144   0  21 110    3     1   0  0  1   0 3203\n145   0  30 153    3     0   0  0  0   0 3203\n146   0  20 103    3     0   0  0  0   0 3203\n147   0  17 119    3     0   0  0  0   0 3225\n148   0  17 119    3     0   0  0  0   0 3225\n149   0  23 119    3     0   0  0  0   2 3232\n150   0  24 110    3     0   0  0  0   0 3232\n151   0  28 140    1     0   0  0  0   0 3234\n154   0  26 133    3     1   2  0  0   0 3260\n155   0  20 169    3     0   1  0  1   1 3274\n156   0  24 115    3     0   0  0  0   2 3274\n159   0  28 250    3     1   0  0  0   6 3303\n160   0  20 141    1     0   2  0  1   1 3317\n161   0  22 158    2     0   1  0  0   2 3317\n162   0  22 112    1     1   2  0  0   0 3317\n163   0  31 150    3     1   0  0  0   2 3321\n164   0  23 115    3     1   0  0  0   1 3331\n166   0  16 112    2     0   0  0  0   0 3374\n167   0  16 135    1     1   0  0  0   0 3374\n168   0  18 229    2     0   0  0  0   0 3402\n169   0  25 140    1     0   0  0  0   1 3416\n170   0  32 134    1     1   1  0  0   4 3430\n172   0  20 121    2     1   0  0  0   0 3444\n173   0  23 190    1     0   0  0  0   0 3459\n174   0  22 131    1     0   0  0  0   1 3460\n175   0  32 170    1     0   0  0  0   0 3473\n176   0  30 110    3     0   0  0  0   0 3544\n177   0  20 127    3     0   0  0  0   0 3487\n179   0  23 123    3     0   0  0  0   0 3544\n180   0  17 120    3     1   0  0  0   0 3572\n181   0  19 105    3     0   0  0  0   0 3572\n182   0  23 130    1     0   0  0  0   0 3586\n183   0  36 175    1     0   0  0  0   0 3600\n184   0  22 125    1     0   0  0  0   1 3614\n185   0  24 133    1     0   0  0  0   0 3614\n186   0  21 134    3     0   0  0  0   2 3629\n187   0  19 235    1     1   0  1  0   0 3629\n188   0  25  95    1     1   3  0  1   0 3637\n189   0  16 135    1     1   0  0  0   0 3643\n190   0  29 135    1     0   0  0  0   1 3651\n191   0  29 154    1     0   0  0  0   1 3651\n192   0  19 147    1     1   0  0  0   0 3651\n193   0  19 147    1     1   0  0  0   0 3651\n195   0  30 137    1     0   0  0  0   1 3699\n196   0  24 110    1     0   0  0  0   1 3728\n197   0  19 184    1     1   0  1  0   0 3756\n199   0  24 110    3     0   1  0  0   0 3770\n200   0  23 110    1     0   0  0  0   1 3770\n201   0  20 120    3     0   0  0  0   0 3770\n202   0  25 241    2     0   0  1  0   0 3790\n203   0  30 112    1     0   0  0  0   1 3799\n204   0  22 169    1     0   0  0  0   0 3827\n205   0  18 120    1     1   0  0  0   2 3856\n206   0  16 170    2     0   0  0  0   4 3860\n207   0  32 186    1     0   0  0  0   2 3860\n208   0  18 120    3     0   0  0  0   1 3884\n209   0  29 130    1     1   0  0  0   2 3884\n210   0  33 117    1     0   0  0  1   1 3912\n211   0  20 170    1     1   0  0  0   0 3940\n212   0  28 134    3     0   0  0  0   1 3941\n213   0  14 135    1     0   0  0  0   0 3941\n214   0  28 130    3     0   0  0  0   0 3969\n215   0  25 120    1     0   0  0  0   2 3983\n216   0  16  95    3     0   0  0  0   1 3997\n217   0  20 158    1     0   0  0  0   1 3997\n218   0  26 160    3     0   0  0  0   0 4054\n219   0  21 115    1     0   0  0  0   1 4054\n220   0  22 129    1     0   0  0  0   0 4111\n221   0  25 130    1     0   0  0  0   2 4153\n222   0  31 120    1     0   0  0  0   2 4167\n223   0  35 170    1     0   1  0  0   1 4174\n224   0  19 120    1     1   0  0  0   0 4238\n225   0  24 116    1     0   0  0  0   1 4593\n226   0  45 123    1     0   0  0  0   1 4990\n4     1  28 120    3     1   1  0  1   0  709\n10    1  29 130    1     0   0  0  1   2 1021\n11    1  34 187    2     1   0  1  0   0 1135\n13    1  25 105    3     0   1  1  0   0 1330\n15    1  25  85    3     0   0  0  1   0 1474\n16    1  27 150    3     0   0  0  0   0 1588\n17    1  23  97    3     0   0  0  1   1 1588\n18    1  24 128    2     0   1  0  0   1 1701\n19    1  24 132    3     0   0  1  0   0 1729\n20    1  21 165    1     1   0  1  0   1 1790\n22    1  32 105    1     1   0  0  0   0 1818\n23    1  19  91    1     1   2  0  1   0 1885\n24    1  25 115    3     0   0  0  0   0 1893\n25    1  16 130    3     0   0  0  0   1 1899\n26    1  25  92    1     1   0  0  0   0 1928\n27    1  20 150    1     1   0  0  0   2 1928\n28    1  21 200    2     0   0  0  1   2 1928\n29    1  24 155    1     1   1  0  0   0 1936\n30    1  21 103    3     0   0  0  0   0 1970\n31    1  20 125    3     0   0  0  1   0 2055\n32    1  25  89    3     0   2  0  0   1 2055\n33    1  19 102    1     0   0  0  0   2 2082\n34    1  19 112    1     1   0  0  1   0 2084\n35    1  26 117    1     1   1  0  0   0 2084\n36    1  24 138    1     0   0  0  0   0 2100\n37    1  17 130    3     1   1  0  1   0 2125\n40    1  20 120    2     1   0  0  0   3 2126\n42    1  22 130    1     1   1  0  1   1 2187\n43    1  27 130    2     0   0  0  1   0 2187\n44    1  20  80    3     1   0  0  1   0 2211\n45    1  17 110    1     1   0  0  0   0 2225\n46    1  25 105    3     0   1  0  0   1 2240\n47    1  20 109    3     0   0  0  0   0 2240\n49    1  18 148    3     0   0  0  0   0 2282\n50    1  18 110    2     1   1  0  0   0 2296\n51    1  20 121    1     1   1  0  1   0 2296\n52    1  21 100    3     0   1  0  0   4 2301\n54    1  26  96    3     0   0  0  0   0 2325\n56    1  31 102    1     1   1  0  0   1 2353\n57    1  15 110    1     0   0  0  0   0 2353\n59    1  23 187    2     1   0  0  0   1 2367\n60    1  20 122    2     1   0  0  0   0 2381\n61    1  24 105    2     1   0  0  0   0 2381\n62    1  15 115    3     0   0  0  1   0 2381\n63    1  23 120    3     0   0  0  0   0 2410\n65    1  30 142    1     1   1  0  0   0 2410\n67    1  22 130    1     1   0  0  0   1 2410\n68    1  17 120    1     1   0  0  0   3 2414\n69    1  23 110    1     1   1  0  0   0 2424\n71    1  17 120    2     0   0  0  0   2 2438\n75    1  26 154    3     0   1  1  0   1 2442\n76    1  20 105    3     0   0  0  0   3 2450\n77    1  26 190    1     1   0  0  0   0 2466\n78    1  14 101    3     1   1  0  0   0 2466\n79    1  28  95    1     1   0  0  0   2 2466\n81    1  14 100    3     0   0  0  0   2 2495\n82    1  23  94    3     1   0  0  0   0 2495\n83    1  17 142    2     0   0  1  0   0 2495\n84    1  21 130    1     1   0  1  0   3 2495\n\n\n\nglimpse(birthwt)\n\nRows: 189\nColumns: 10\n$ low   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ age   &lt;int&gt; 19, 33, 20, 21, 18, 21, 22, 17, 29, 26, 19, 19, 22, 30, 18, 18, …\n$ lwt   &lt;int&gt; 182, 155, 105, 108, 107, 124, 118, 103, 123, 113, 95, 150, 95, 1…\n$ race  &lt;int&gt; 2, 3, 1, 1, 1, 3, 1, 3, 1, 1, 3, 3, 3, 3, 1, 1, 2, 1, 3, 1, 3, 1…\n$ smoke &lt;int&gt; 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0…\n$ ptl   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ht    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ui    &lt;int&gt; 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1…\n$ ftv   &lt;int&gt; 0, 3, 1, 2, 0, 0, 1, 1, 1, 0, 0, 1, 0, 2, 0, 0, 0, 3, 0, 1, 2, 3…\n$ bwt   &lt;int&gt; 2523, 2551, 2557, 2594, 2600, 2622, 2637, 2637, 2663, 2665, 2722…\n\n\n\n\n18.5.2 Prepare the data for analysis.\n\n# Although we've already done this above, \n# we include it here again for completeness.\nbirthwt2 &lt;- birthwt %&gt;%\n  mutate(low_fct = factor(low, levels = c(0, 1),\n                          labels = c(\"no\", \"yes\")),\n         race_fct = factor(race, levels = c(1, 2, 3),\n                           labels = c(\"white\", \"black\", \"other\")))\nbirthwt2\n\n    low age lwt race smoke ptl ht ui ftv  bwt low_fct race_fct\n85    0  19 182    2     0   0  0  1   0 2523      no    black\n86    0  33 155    3     0   0  0  0   3 2551      no    other\n87    0  20 105    1     1   0  0  0   1 2557      no    white\n88    0  21 108    1     1   0  0  1   2 2594      no    white\n89    0  18 107    1     1   0  0  1   0 2600      no    white\n91    0  21 124    3     0   0  0  0   0 2622      no    other\n92    0  22 118    1     0   0  0  0   1 2637      no    white\n93    0  17 103    3     0   0  0  0   1 2637      no    other\n94    0  29 123    1     1   0  0  0   1 2663      no    white\n95    0  26 113    1     1   0  0  0   0 2665      no    white\n96    0  19  95    3     0   0  0  0   0 2722      no    other\n97    0  19 150    3     0   0  0  0   1 2733      no    other\n98    0  22  95    3     0   0  1  0   0 2751      no    other\n99    0  30 107    3     0   1  0  1   2 2750      no    other\n100   0  18 100    1     1   0  0  0   0 2769      no    white\n101   0  18 100    1     1   0  0  0   0 2769      no    white\n102   0  15  98    2     0   0  0  0   0 2778      no    black\n103   0  25 118    1     1   0  0  0   3 2782      no    white\n104   0  20 120    3     0   0  0  1   0 2807      no    other\n105   0  28 120    1     1   0  0  0   1 2821      no    white\n106   0  32 121    3     0   0  0  0   2 2835      no    other\n107   0  31 100    1     0   0  0  1   3 2835      no    white\n108   0  36 202    1     0   0  0  0   1 2836      no    white\n109   0  28 120    3     0   0  0  0   0 2863      no    other\n111   0  25 120    3     0   0  0  1   2 2877      no    other\n112   0  28 167    1     0   0  0  0   0 2877      no    white\n113   0  17 122    1     1   0  0  0   0 2906      no    white\n114   0  29 150    1     0   0  0  0   2 2920      no    white\n115   0  26 168    2     1   0  0  0   0 2920      no    black\n116   0  17 113    2     0   0  0  0   1 2920      no    black\n117   0  17 113    2     0   0  0  0   1 2920      no    black\n118   0  24  90    1     1   1  0  0   1 2948      no    white\n119   0  35 121    2     1   1  0  0   1 2948      no    black\n120   0  25 155    1     0   0  0  0   1 2977      no    white\n121   0  25 125    2     0   0  0  0   0 2977      no    black\n123   0  29 140    1     1   0  0  0   2 2977      no    white\n124   0  19 138    1     1   0  0  0   2 2977      no    white\n125   0  27 124    1     1   0  0  0   0 2922      no    white\n126   0  31 215    1     1   0  0  0   2 3005      no    white\n127   0  33 109    1     1   0  0  0   1 3033      no    white\n128   0  21 185    2     1   0  0  0   2 3042      no    black\n129   0  19 189    1     0   0  0  0   2 3062      no    white\n130   0  23 130    2     0   0  0  0   1 3062      no    black\n131   0  21 160    1     0   0  0  0   0 3062      no    white\n132   0  18  90    1     1   0  0  1   0 3062      no    white\n133   0  18  90    1     1   0  0  1   0 3062      no    white\n134   0  32 132    1     0   0  0  0   4 3080      no    white\n135   0  19 132    3     0   0  0  0   0 3090      no    other\n136   0  24 115    1     0   0  0  0   2 3090      no    white\n137   0  22  85    3     1   0  0  0   0 3090      no    other\n138   0  22 120    1     0   0  1  0   1 3100      no    white\n139   0  23 128    3     0   0  0  0   0 3104      no    other\n140   0  22 130    1     1   0  0  0   0 3132      no    white\n141   0  30  95    1     1   0  0  0   2 3147      no    white\n142   0  19 115    3     0   0  0  0   0 3175      no    other\n143   0  16 110    3     0   0  0  0   0 3175      no    other\n144   0  21 110    3     1   0  0  1   0 3203      no    other\n145   0  30 153    3     0   0  0  0   0 3203      no    other\n146   0  20 103    3     0   0  0  0   0 3203      no    other\n147   0  17 119    3     0   0  0  0   0 3225      no    other\n148   0  17 119    3     0   0  0  0   0 3225      no    other\n149   0  23 119    3     0   0  0  0   2 3232      no    other\n150   0  24 110    3     0   0  0  0   0 3232      no    other\n151   0  28 140    1     0   0  0  0   0 3234      no    white\n154   0  26 133    3     1   2  0  0   0 3260      no    other\n155   0  20 169    3     0   1  0  1   1 3274      no    other\n156   0  24 115    3     0   0  0  0   2 3274      no    other\n159   0  28 250    3     1   0  0  0   6 3303      no    other\n160   0  20 141    1     0   2  0  1   1 3317      no    white\n161   0  22 158    2     0   1  0  0   2 3317      no    black\n162   0  22 112    1     1   2  0  0   0 3317      no    white\n163   0  31 150    3     1   0  0  0   2 3321      no    other\n164   0  23 115    3     1   0  0  0   1 3331      no    other\n166   0  16 112    2     0   0  0  0   0 3374      no    black\n167   0  16 135    1     1   0  0  0   0 3374      no    white\n168   0  18 229    2     0   0  0  0   0 3402      no    black\n169   0  25 140    1     0   0  0  0   1 3416      no    white\n170   0  32 134    1     1   1  0  0   4 3430      no    white\n172   0  20 121    2     1   0  0  0   0 3444      no    black\n173   0  23 190    1     0   0  0  0   0 3459      no    white\n174   0  22 131    1     0   0  0  0   1 3460      no    white\n175   0  32 170    1     0   0  0  0   0 3473      no    white\n176   0  30 110    3     0   0  0  0   0 3544      no    other\n177   0  20 127    3     0   0  0  0   0 3487      no    other\n179   0  23 123    3     0   0  0  0   0 3544      no    other\n180   0  17 120    3     1   0  0  0   0 3572      no    other\n181   0  19 105    3     0   0  0  0   0 3572      no    other\n182   0  23 130    1     0   0  0  0   0 3586      no    white\n183   0  36 175    1     0   0  0  0   0 3600      no    white\n184   0  22 125    1     0   0  0  0   1 3614      no    white\n185   0  24 133    1     0   0  0  0   0 3614      no    white\n186   0  21 134    3     0   0  0  0   2 3629      no    other\n187   0  19 235    1     1   0  1  0   0 3629      no    white\n188   0  25  95    1     1   3  0  1   0 3637      no    white\n189   0  16 135    1     1   0  0  0   0 3643      no    white\n190   0  29 135    1     0   0  0  0   1 3651      no    white\n191   0  29 154    1     0   0  0  0   1 3651      no    white\n192   0  19 147    1     1   0  0  0   0 3651      no    white\n193   0  19 147    1     1   0  0  0   0 3651      no    white\n195   0  30 137    1     0   0  0  0   1 3699      no    white\n196   0  24 110    1     0   0  0  0   1 3728      no    white\n197   0  19 184    1     1   0  1  0   0 3756      no    white\n199   0  24 110    3     0   1  0  0   0 3770      no    other\n200   0  23 110    1     0   0  0  0   1 3770      no    white\n201   0  20 120    3     0   0  0  0   0 3770      no    other\n202   0  25 241    2     0   0  1  0   0 3790      no    black\n203   0  30 112    1     0   0  0  0   1 3799      no    white\n204   0  22 169    1     0   0  0  0   0 3827      no    white\n205   0  18 120    1     1   0  0  0   2 3856      no    white\n206   0  16 170    2     0   0  0  0   4 3860      no    black\n207   0  32 186    1     0   0  0  0   2 3860      no    white\n208   0  18 120    3     0   0  0  0   1 3884      no    other\n209   0  29 130    1     1   0  0  0   2 3884      no    white\n210   0  33 117    1     0   0  0  1   1 3912      no    white\n211   0  20 170    1     1   0  0  0   0 3940      no    white\n212   0  28 134    3     0   0  0  0   1 3941      no    other\n213   0  14 135    1     0   0  0  0   0 3941      no    white\n214   0  28 130    3     0   0  0  0   0 3969      no    other\n215   0  25 120    1     0   0  0  0   2 3983      no    white\n216   0  16  95    3     0   0  0  0   1 3997      no    other\n217   0  20 158    1     0   0  0  0   1 3997      no    white\n218   0  26 160    3     0   0  0  0   0 4054      no    other\n219   0  21 115    1     0   0  0  0   1 4054      no    white\n220   0  22 129    1     0   0  0  0   0 4111      no    white\n221   0  25 130    1     0   0  0  0   2 4153      no    white\n222   0  31 120    1     0   0  0  0   2 4167      no    white\n223   0  35 170    1     0   1  0  0   1 4174      no    white\n224   0  19 120    1     1   0  0  0   0 4238      no    white\n225   0  24 116    1     0   0  0  0   1 4593      no    white\n226   0  45 123    1     0   0  0  0   1 4990      no    white\n4     1  28 120    3     1   1  0  1   0  709     yes    other\n10    1  29 130    1     0   0  0  1   2 1021     yes    white\n11    1  34 187    2     1   0  1  0   0 1135     yes    black\n13    1  25 105    3     0   1  1  0   0 1330     yes    other\n15    1  25  85    3     0   0  0  1   0 1474     yes    other\n16    1  27 150    3     0   0  0  0   0 1588     yes    other\n17    1  23  97    3     0   0  0  1   1 1588     yes    other\n18    1  24 128    2     0   1  0  0   1 1701     yes    black\n19    1  24 132    3     0   0  1  0   0 1729     yes    other\n20    1  21 165    1     1   0  1  0   1 1790     yes    white\n22    1  32 105    1     1   0  0  0   0 1818     yes    white\n23    1  19  91    1     1   2  0  1   0 1885     yes    white\n24    1  25 115    3     0   0  0  0   0 1893     yes    other\n25    1  16 130    3     0   0  0  0   1 1899     yes    other\n26    1  25  92    1     1   0  0  0   0 1928     yes    white\n27    1  20 150    1     1   0  0  0   2 1928     yes    white\n28    1  21 200    2     0   0  0  1   2 1928     yes    black\n29    1  24 155    1     1   1  0  0   0 1936     yes    white\n30    1  21 103    3     0   0  0  0   0 1970     yes    other\n31    1  20 125    3     0   0  0  1   0 2055     yes    other\n32    1  25  89    3     0   2  0  0   1 2055     yes    other\n33    1  19 102    1     0   0  0  0   2 2082     yes    white\n34    1  19 112    1     1   0  0  1   0 2084     yes    white\n35    1  26 117    1     1   1  0  0   0 2084     yes    white\n36    1  24 138    1     0   0  0  0   0 2100     yes    white\n37    1  17 130    3     1   1  0  1   0 2125     yes    other\n40    1  20 120    2     1   0  0  0   3 2126     yes    black\n42    1  22 130    1     1   1  0  1   1 2187     yes    white\n43    1  27 130    2     0   0  0  1   0 2187     yes    black\n44    1  20  80    3     1   0  0  1   0 2211     yes    other\n45    1  17 110    1     1   0  0  0   0 2225     yes    white\n46    1  25 105    3     0   1  0  0   1 2240     yes    other\n47    1  20 109    3     0   0  0  0   0 2240     yes    other\n49    1  18 148    3     0   0  0  0   0 2282     yes    other\n50    1  18 110    2     1   1  0  0   0 2296     yes    black\n51    1  20 121    1     1   1  0  1   0 2296     yes    white\n52    1  21 100    3     0   1  0  0   4 2301     yes    other\n54    1  26  96    3     0   0  0  0   0 2325     yes    other\n56    1  31 102    1     1   1  0  0   1 2353     yes    white\n57    1  15 110    1     0   0  0  0   0 2353     yes    white\n59    1  23 187    2     1   0  0  0   1 2367     yes    black\n60    1  20 122    2     1   0  0  0   0 2381     yes    black\n61    1  24 105    2     1   0  0  0   0 2381     yes    black\n62    1  15 115    3     0   0  0  1   0 2381     yes    other\n63    1  23 120    3     0   0  0  0   0 2410     yes    other\n65    1  30 142    1     1   1  0  0   0 2410     yes    white\n67    1  22 130    1     1   0  0  0   1 2410     yes    white\n68    1  17 120    1     1   0  0  0   3 2414     yes    white\n69    1  23 110    1     1   1  0  0   0 2424     yes    white\n71    1  17 120    2     0   0  0  0   2 2438     yes    black\n75    1  26 154    3     0   1  1  0   1 2442     yes    other\n76    1  20 105    3     0   0  0  0   3 2450     yes    other\n77    1  26 190    1     1   0  0  0   0 2466     yes    white\n78    1  14 101    3     1   1  0  0   0 2466     yes    other\n79    1  28  95    1     1   0  0  0   2 2466     yes    white\n81    1  14 100    3     0   0  0  0   2 2495     yes    other\n82    1  23  94    3     1   0  0  0   0 2495     yes    other\n83    1  17 142    2     0   0  1  0   0 2495     yes    black\n84    1  21 130    1     1   0  1  0   3 2495     yes    white\n\n\n\n\n18.5.3 Make tables or plots to explore the data visually.\n\ntabyl(birthwt2, low_fct, race_fct) %&gt;%\n    adorn_totals()\n\n low_fct white black other\n      no    73    15    42\n     yes    23    11    25\n   Total    96    26    67\n\n\n\ntabyl(birthwt2, low_fct, race_fct) %&gt;%\n    adorn_totals() %&gt;%\n    adorn_percentages(\"col\") %&gt;%\n    adorn_pct_formatting()\n\n low_fct  white  black  other\n      no  76.0%  57.7%  62.7%\n     yes  24.0%  42.3%  37.3%\n   Total 100.0% 100.0% 100.0%\n\n\nCommentary: Earlier we used row and column total to explain how expected cell counts arise. Here, however, we will revert back to our previous standard practice of generating one contingency table with counts and another with column percentages."
  },
  {
    "objectID": "18-chi_square_test_for_independence-web.html#hypotheses",
    "href": "18-chi_square_test_for_independence-web.html#hypotheses",
    "title": "18  Chi-square test for independence",
    "section": "18.6 Hypotheses",
    "text": "18.6 Hypotheses\n\n18.6.1 Identify the sample (or samples) and a reasonable population (or populations) of interest.\nThe sample consists of 189 mothers who gave birth at the Baystate Medical Center in Springfield, Massachusetts in 1986. The population is presumably all mothers, although it’s safest to conclude only about mothers who gave birth at this hospital.\n\n\n18.6.2 Express the null and alternative hypotheses as contextually meaningful full sentences.\n\\(H_{0}:\\) Low birth weight and race are independent.\n\\(H_{A}:\\) Low birth weight and race are associated.\n\n\n18.6.3 Express the null and alternative hypotheses in symbols (when possible).\nFor a chi-square test for independence, this section is not applicable. With multiple categories in the response and predictor variables, there are no specific parameters of interest to express symbolically."
  },
  {
    "objectID": "18-chi_square_test_for_independence-web.html#model",
    "href": "18-chi_square_test_for_independence-web.html#model",
    "title": "18  Chi-square test for independence",
    "section": "18.7 Model",
    "text": "18.7 Model\n\n18.7.1 Identify the sampling distribution model.\nWe will use a chi-square model with 2 degrees of freedom.\n\n\n18.7.2 Check the relevant conditions to ensure that model assumptions are met.\n\nRandom\n\nWe hope that these 189 women are representative of all women who gave birth in this hospital (or, at best, in that region) around that time.\n\n10%\n\nWe don’t know how many women gave birth at this hospital, but perhaps over many years we might have more than 1890 women.\n\nExpected cell counts\n\nYou checked the cell counts as a part of Exercise 1. Note that all expected cell counts are larger than 5, so the condition is met."
  },
  {
    "objectID": "18-chi_square_test_for_independence-web.html#mechanics",
    "href": "18-chi_square_test_for_independence-web.html#mechanics",
    "title": "18  Chi-square test for independence",
    "section": "18.8 Mechanics",
    "text": "18.8 Mechanics\n\n18.8.1 Compute the test statistic.\n\nobs_chisq &lt;- birthwt2 %&gt;%\n  specify(response = low_fct, explanatory = race_fct) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  calculate(stat = \"chisq\")\nobs_chisq\n\nResponse: low_fct (factor)\nExplanatory: race_fct (factor)\nNull Hypothesis: independence\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  5.00\n\n\n\n\n18.8.2 Report the test statistic in context (when possible).\nThe value of \\(\\chi^{2}\\) is 5.004813.\nCommentary: As in the last chapter, there’s not much context to report with a value of \\(\\chi^{2}\\), so the most we can do here is just report it in a full sentence.\n\n\n18.8.3 Plot the null distribution.\n\nlow_race_test &lt;- birthwt2 %&gt;%\n  specify(response = low_fct, explanatory = race_fct) %&gt;%\n  assume(distribution = \"chisq\")\nlow_race_test\n\nA Chi-squared distribution with 2 degrees of freedom.\n\n\n\nlow_race_test %&gt;%\n  visualize() +\n  shade_p_value(obs_chisq, direction = \"greater\")\n\n\n\n\n\n\n18.8.4 Calculate the P-value.\n\nlow_race_test_p &lt;- low_race_test %&gt;%\n  get_p_value(obs_chisq, direction = \"greater\")\nlow_race_test_p\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1  0.0819\n\n\n\n\n18.8.5 Interpret the P-value as a probability given the null.\nThe P-value is 0.0818877. If low birth weight and race were independent, there would be a 8.1887698% chance of seeing results at least as extreme as we saw in the data."
  },
  {
    "objectID": "18-chi_square_test_for_independence-web.html#conclusion",
    "href": "18-chi_square_test_for_independence-web.html#conclusion",
    "title": "18  Chi-square test for independence",
    "section": "18.9 Conclusion",
    "text": "18.9 Conclusion\n\n18.9.1 State the statistical conclusion.\nWe fail to reject the null hypothesis.\n\n\n18.9.2 State (but do not overstate) a contextually meaningful conclusion.\nThere is insufficient evidence that low birth weight and race are associated.\n\n\n18.9.3 Express reservations or uncertainty about the generalizability of the conclusion.\nGiven our uncertainly about how the data was collected, it’s not clear what our conclusion means. Also, failing to reject the null is really a “non-conclusion” in that it leaves us basically knowing nothing. We don’t have evidence of such an association (and there are good reasons to believe there may not be one), but failing to reject the null does not prove anything.\n\n\n18.9.4 Identify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.\nIt’s possible that we have made a Type II error. It may be that low birth weight and race are associated, but our sample has not given enough evidence of such an association."
  },
  {
    "objectID": "18-chi_square_test_for_independence-web.html#confidence-interval",
    "href": "18-chi_square_test_for_independence-web.html#confidence-interval",
    "title": "18  Chi-square test for independence",
    "section": "18.10 Confidence interval",
    "text": "18.10 Confidence interval\nThere are no parameters of interest in a chi-square test, so there is no confidence interval to report."
  },
  {
    "objectID": "18-chi_square_test_for_independence-web.html#your-turn",
    "href": "18-chi_square_test_for_independence-web.html#your-turn",
    "title": "18  Chi-square test for independence",
    "section": "18.11 Your turn",
    "text": "18.11 Your turn\nUse the smoking data set from the openintro package. Run a chi-square test for independence to determine if smoking status is associated with marital status.\nThe rubric outline is reproduced below. You may refer to the worked example above and modify it accordingly. Remember to strip out all the commentary. That is just exposition for your benefit in understanding the steps, but is not meant to form part of the formal inference process.\nAnother word of warning: the copy/paste process is not a substitute for your brain. You will often need to modify more than just the names of the data frames and variables to adapt the worked examples to your own work. Do not blindly copy and paste code without understanding what it does. And you should never copy and paste text. All the sentences and paragraphs you write are expressions of your own analysis. They must reflect your own understanding of the inferential process.\nAlso, so that your answers here don’t mess up the code chunks above, use new variable names everywhere.\n\nExploratory data analysis\n\nUse data documentation (help files, code books, Google, etc.) to determine as much as possible about the data provenance and structure.\n\nPlease write up your answer here\n\n# Add code here to print the data\n\n\n# Add code here to glimpse the variables\n\n\n\n\nPrepare the data for analysis. [Not always necessary.]\n\n\n# Add code here to prepare the data for analysis.\n\n\n\n\nMake tables or plots to explore the data visually.\n\n\n# Add code here to make tables or plots.\n\n\n\n\n\nHypotheses\n\nIdentify the sample (or samples) and a reasonable population (or populations) of interest.\n\nPlease write up your answer here.\n\n\n\nExpress the null and alternative hypotheses as contextually meaningful full sentences.\n\n\\(H_{0}:\\) Null hypothesis goes here.\n\\(H_{A}:\\) Alternative hypothesis goes here.\n\n\n\nExpress the null and alternative hypotheses in symbols (when possible).\n\n\\(H_{0}: math\\)\n\\(H_{A}: math\\)\n\n\n\n\nModel\n\nIdentify the sampling distribution model.\n\nPlease write up your answer here.\n\n\n\nCheck the relevant conditions to ensure that model assumptions are met.\n\nPlease write up your answer here. (Some conditions may require R code as well.)\n\n\n\n\nMechanics\n\nCompute the test statistic.\n\n\n# Add code here to compute the test statistic.\n\n\n\n\nReport the test statistic in context (when possible).\n\nPlease write up your answer here.\n\n\n\nPlot the null distribution.\n\n\n# Add code here to plot the null distribution.\n\n\n\n\nCalculate the P-value.\n\n\n# Add code here to calculate the P-value.\n\n\n\n\nInterpret the P-value as a probability given the null.\n\nPlease write up your answer here.\n\n\n\n\nConclusion\n\nState the statistical conclusion.\n\nPlease write up your answer here.\n\n\n\nState (but do not overstate) a contextually meaningful conclusion.\n\nPlease write up your answer here.\n\n\n\nExpress reservations or uncertainty about the generalizability of the conclusion.\n\nPlease write up your answer here.\n\n\n\nIdentify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.\n\nPlease write up your answer here."
  },
  {
    "objectID": "18-chi_square_test_for_independence-web.html#bonus-section-residuals",
    "href": "18-chi_square_test_for_independence-web.html#bonus-section-residuals",
    "title": "18  Chi-square test for independence",
    "section": "18.12 Bonus section: Residuals",
    "text": "18.12 Bonus section: Residuals\nJust like with the chi-square test for goodness of fit, rejecting the null hypothesis using the chi-square test for independence informs us that two variables are associated, but it doesn’t tell us the useful information about which combinations of variables have higher and lower counts than expected. And just like the chi-square test for goodness of fit, we can examine the residuals table to find that information.\nA word of caution: You should only examine the residuals if your test was statistically significant! The residuals table for tests in which we fail to reject the null hypothesis can be misleading.\nBecause we failed to reject the null hypothesis in the low_race_test, it would be unwise for us to examine the residuals table in that test. Instead, we’ll use a different example.\nThe diabetes2 dataset in the openintro package contains information about an experiment evaluating three treatments for Type 2 diabetes in patients aged 10-17 who were being treated with metformin. The three treatments summarized in the treatment variable were: continued treatment with metformin (met), treatment with metformin combined with rosiglitazone (rosi), or a lifestyle intervention program (lifestyle). Each patient had a primary outcome, which was either “lacked glycemic control” (failure) or did not lack that control (success). Here is the summary of the results of the experiment:\n\ntabyl(diabetes2, treatment, outcome) \n\n treatment failure success\n lifestyle     109     125\n       met     120     112\n      rosi      90     143\n\n\nFor the sake of a streamlined presentation, we’ll omit the usual details of condition-checking, hypothesis-writing, etc., and skip right to the conclusion.\n\ntabyl(diabetes2, treatment, outcome) %&gt;%\n  chisq.test() -&gt; outcome_treatment_chisq.test\noutcome_treatment_chisq.test\n\n\n    Pearson's Chi-squared test\n\ndata:  .\nX-squared = 8.1645, df = 2, p-value = 0.01687\n\n\nNotice that the p-value obtained from the test is below our usual significance level (= 0.05), so it makes sense for us to examine the residuals.\n\noutcome_treatment_chisq.test$residuals\n\n treatment    failure    success\n lifestyle  0.2138881 -0.1959703\n       met  1.3725470 -1.2575659\n      rosi -1.5839451  1.4512548\n\n\nAgain, these values don’t mean much in the real world; our job is to look at the most positive and most negative values.\n\nSince the rosi and failure cell has the most negative value, the count of people who failed to achieve glycemic control with rosiglitazone is the most below expected. (That’s a good result!)\nSince the rosi and success cell has the most positive value, the count of people who succeeded in achieving glycemic control with rosiglitazone is the most above expected. (That’s also a good result!)\n\nOverall, we can conclude that the rosiglitazone treatment was quite successful in helping people achieve their glycemic control goals.\n\n18.12.1 Your turn\nExamine the residuals table to determine which marital statuses are most associated with smoking or not smoking.\n\n\n# Add code here to produce the chisq.test result.\n\n# Add code here to examine the residuals table.\n\nPlease write your answer here."
  },
  {
    "objectID": "18-chi_square_test_for_independence-web.html#conclusion-2",
    "href": "18-chi_square_test_for_independence-web.html#conclusion-2",
    "title": "18  Chi-square test for independence",
    "section": "18.13 Conclusion",
    "text": "18.13 Conclusion\nWith two categorical variables, we can run a chi-square test for independence to test the null hypothesis that the two variables are independent. While technically we can run this test for any two categorical variables, if both variables have only two levels, we would usually choose to run a test for two proportions. The chi-square test for independence is useful when one or both of the response and predictor variables have three or more levels. The expected cell counts are derived from the data and then the chi-squared statistic is computed as usual. Using the correct degrees of freedom, we can test how much the observed cell counts deviate from the expected cell counts and derive a P-value.\n\n18.13.1 Preparing and submitting your assignment\n\nFrom the “Run” menu, select “Restart R and Run All Chunks”.\nDeal with any code errors that crop up. Repeat steps 1–-2 until there are no more code errors.\nSpell check your document by clicking the icon with “ABC” and a check mark.\nHit the “Preview” button one last time to generate the final draft of the .nb.html file.\nProofread the HTML file carefully. If there are errors, go back and fix them, then repeat steps 1–5 again.\n\nIf you have completed this chapter as part of a statistics course, follow the directions you receive from your professor to submit your assignment."
  },
  {
    "objectID": "19-inference_for_one_mean-web.html#introduction",
    "href": "19-inference_for_one_mean-web.html#introduction",
    "title": "19  Inference for one mean",
    "section": "19.1 Introduction",
    "text": "19.1 Introduction\nIn this chapter, we’ll learn about the Student t distribution and use it to perform a t test for a single mean.\n\n19.1.1 Install new packages\nThere are no new packages used in this chapter.\n\n\n19.1.2 Download the R notebook file\nCheck the upper-right corner in RStudio to make sure you’re in your intro_stats project. Then click on the following link to download this chapter as an R notebook file (.Rmd).\nhttps://vectorposse.github.io/intro_stats/chapter_downloads/19-inference_for_one_mean.Rmd\nOnce the file is downloaded, move it to your project folder in RStudio and open it there.\n\n\n19.1.3 Restart R and run all chunks\nIn RStudio, select “Restart R and Run All Chunks” from the “Run” menu."
  },
  {
    "objectID": "19-inference_for_one_mean-web.html#load-packages",
    "href": "19-inference_for_one_mean-web.html#load-packages",
    "title": "19  Inference for one mean",
    "section": "19.2 Load packages",
    "text": "19.2 Load packages\nWe load the standard tidyverse and infer packages as well as the mosaic package to run some simulation. The openintro package contains the teacher data and the hsb2 data.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(infer)\nlibrary(mosaic)\n\nWarning: package 'mosaic' was built under R version 4.3.1\n\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:infer':\n\n    prop_test, t_test\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(openintro)\n\nLoading required package: airports\nLoading required package: cherryblossom\nLoading required package: usdata\n\nAttaching package: 'openintro'\n\nThe following object is masked from 'package:mosaic':\n\n    dotPlot\n\nThe following objects are masked from 'package:lattice':\n\n    ethanol, lsegments"
  },
  {
    "objectID": "19-inference_for_one_mean-web.html#simulating-means",
    "href": "19-inference_for_one_mean-web.html#simulating-means",
    "title": "19  Inference for one mean",
    "section": "19.3 Simulating means",
    "text": "19.3 Simulating means\nSystolic blood pressure (SBP) for women in the U.S. and Canada follows a normal distribution with a mean of 114 and a standard deviation of 14.\nSuppose we gather a random sample of 4 women and measure their SBP. We can simulate doing that with the rnorm command:\n\nset.seed(5151977)\nSBP_sample &lt;- rnorm(4, mean = 114, sd = 14)\nSBP_sample\n\n[1]  99.75130 126.47739  99.53632 115.05247\n\n\nWe summarize our sample by taking the mean and standard deviation:\n\nmean(SBP_sample)\n\n[1] 110.2044\n\n\n\nsd(SBP_sample)\n\n[1] 13.05615\n\n\nThe sample mean \\(\\bar{y}\\) = 110.2043696 is somewhat close to the true population mean \\(\\mu = 114\\) and the sample standard deviation \\(s\\) = 13.0561519 is somewhat close to the true population standard deviation \\(\\sigma = 14\\). (\\(\\mu\\) is the Greek letter “mu” and \\(\\sigma\\) is the Greek letter “sigma”.)\nLet’s simulate lots of samples of size 4. For each sample, we calculate the sample mean.\n\nset.seed(5151977)\nsims &lt;- do(2000) * mean(rnorm(4, mean = 114, sd = 14))\nsims\n\n          mean\n1    110.95524\n2    111.06853\n3    109.91266\n4    113.51487\n5    114.84292\n6    124.12671\n7    110.52277\n8    122.91483\n9    113.79958\n10   121.52306\n11   119.45527\n12   130.95196\n13   106.25140\n14   119.48189\n15   122.95412\n16   111.36293\n17   115.26561\n18   120.00887\n19   111.12422\n20   125.11449\n21   112.54356\n22   121.05007\n23   111.92577\n24   112.37685\n25   108.60242\n26   112.14135\n27   121.03786\n28   102.21504\n29   131.42457\n30   115.75208\n31   118.57539\n32   107.75367\n33   113.73938\n34   107.48598\n35   104.02251\n36   110.26283\n37   114.03591\n38   105.89310\n39   112.81019\n40   123.99549\n41   102.07213\n42   102.65507\n43   119.93490\n44   123.99603\n45   119.72605\n46   122.57296\n47   112.79102\n48   108.88674\n49   109.46094\n50   111.52494\n51   106.51913\n52   118.92374\n53   122.65041\n54   106.33611\n55   114.84009\n56   119.94925\n57    87.48567\n58   107.67256\n59   112.29705\n60   114.49032\n61   106.00521\n62   103.61574\n63   114.44472\n64   124.40115\n65   107.25545\n66   106.18013\n67   107.38138\n68   115.50453\n69   118.83450\n70   109.98443\n71   133.63093\n72   118.93599\n73   112.55365\n74   122.22781\n75   119.94346\n76   120.08051\n77   115.73125\n78    99.12175\n79   110.20178\n80    97.50553\n81   126.13150\n82   110.10237\n83   116.45862\n84   118.18392\n85   120.15207\n86   107.32720\n87   117.33775\n88    96.64247\n89   109.86058\n90   124.84727\n91   109.67761\n92   117.45921\n93   110.36776\n94   118.71447\n95   122.94817\n96   113.04646\n97   116.69160\n98   113.14800\n99   117.60656\n100  116.98939\n101  113.87627\n102  117.60049\n103  119.06600\n104  126.74302\n105  116.53015\n106  121.92932\n107  107.90235\n108  118.06294\n109  116.88269\n110  119.81950\n111  127.56483\n112  109.67205\n113  113.93574\n114  110.89664\n115  115.59765\n116   98.08517\n117  108.69878\n118  114.50676\n119  109.82143\n120  118.93792\n121  121.50253\n122  101.73570\n123  117.77834\n124  103.81164\n125  101.48508\n126  127.18256\n127  119.56654\n128  120.47221\n129  123.70693\n130  125.67436\n131  124.50634\n132   99.11626\n133  113.36051\n134  107.59688\n135  119.69572\n136  113.57789\n137  114.00803\n138  114.95061\n139  117.94756\n140  106.20955\n141  112.69388\n142  115.82052\n143  124.41148\n144  119.49821\n145  114.44646\n146  101.22920\n147  109.58204\n148  109.16187\n149  105.36936\n150  111.49145\n151  118.48739\n152  101.84622\n153  115.05308\n154  121.74454\n155  115.84609\n156  114.60402\n157  121.84957\n158  118.38499\n159  117.98274\n160  121.94268\n161  112.60397\n162  106.21758\n163  121.90313\n164  122.05917\n165  128.85365\n166  106.67919\n167  120.88093\n168  105.27210\n169  133.73894\n170  112.95960\n171  114.62501\n172  118.79292\n173  114.05784\n174  106.07207\n175  122.25110\n176  124.99923\n177  111.32837\n178  112.67882\n179  118.10980\n180  113.55150\n181  109.94996\n182  130.17665\n183  117.41869\n184  112.29039\n185  115.18728\n186  119.10711\n187  121.18710\n188  116.40250\n189  123.58668\n190  117.05543\n191  114.30052\n192  120.59040\n193  108.93992\n194  116.69512\n195  123.65056\n196  120.25289\n197  119.10736\n198  121.35013\n199  108.22576\n200  123.96013\n201  120.50076\n202  109.45569\n203  124.60173\n204  109.20374\n205  109.14185\n206  111.64284\n207  127.80637\n208   97.05353\n209  104.42525\n210  108.70502\n211  123.53495\n212  111.92085\n213  103.79728\n214  109.04242\n215  101.15528\n216  108.99493\n217  115.66033\n218  104.27866\n219  127.74945\n220  119.18990\n221   99.37513\n222  119.24557\n223  107.03566\n224  118.83983\n225  118.84264\n226  124.91099\n227  103.66402\n228  109.91857\n229  116.49506\n230  112.01135\n231  110.40098\n232  100.23115\n233  115.89741\n234  120.00895\n235  110.26257\n236  104.91429\n237  121.20485\n238  127.85001\n239  121.99891\n240  116.34753\n241  113.57648\n242  113.91281\n243  117.83396\n244  117.19323\n245  123.04011\n246  111.43295\n247  108.88549\n248  101.10892\n249  108.54658\n250  128.54127\n251  132.02932\n252  117.36163\n253  100.19385\n254  113.30224\n255  120.65156\n256  104.76686\n257  118.55390\n258  118.08333\n259  118.85312\n260  116.92587\n261  125.34601\n262  113.04661\n263  127.01136\n264  116.97079\n265  115.09776\n266  120.77965\n267  112.78021\n268  120.98030\n269   96.97945\n270  109.06035\n271  113.31895\n272  118.24567\n273  128.56256\n274  122.71663\n275  122.79106\n276  107.69711\n277  122.51593\n278  121.62137\n279  115.44487\n280  114.65932\n281  100.87231\n282  118.26446\n283  108.46425\n284  115.83714\n285  121.39197\n286  110.09557\n287  113.85471\n288  117.69545\n289  116.22425\n290  120.78184\n291  126.43991\n292  103.53681\n293  116.32864\n294  108.06495\n295  106.65624\n296  120.69772\n297  119.37433\n298  100.02332\n299  118.59332\n300  119.53438\n301  107.11014\n302  111.97493\n303  103.47491\n304  111.99805\n305  118.71416\n306  116.33954\n307  125.49563\n308  107.78016\n309  102.12925\n310  112.12212\n311  117.51136\n312  110.08975\n313  114.72259\n314  120.56031\n315  122.04100\n316  111.17129\n317  116.39056\n318  111.50435\n319  104.30895\n320  101.31131\n321  114.53301\n322  113.94972\n323  116.04217\n324  112.54460\n325  113.52116\n326  110.60055\n327  117.48808\n328  116.50048\n329  119.46474\n330  123.91257\n331  111.94294\n332  102.98073\n333  109.80824\n334  106.57737\n335  113.14494\n336  100.74728\n337  100.16375\n338  115.02875\n339  110.51485\n340  110.32509\n341  120.91380\n342  118.33534\n343  111.63758\n344  110.58353\n345  118.32547\n346  106.03945\n347  114.78878\n348   95.12731\n349  115.50274\n350  123.32999\n351  104.88001\n352  127.10250\n353  127.14507\n354  108.64777\n355  112.02036\n356  120.33362\n357  120.23128\n358  111.15694\n359  123.51130\n360  116.82204\n361  104.68623\n362  114.13924\n363  111.40374\n364  109.04713\n365  118.19404\n366  126.41994\n367  119.38439\n368  112.72901\n369  106.14565\n370  115.27480\n371  112.79306\n372  111.38774\n373  115.34948\n374  105.88397\n375  127.93875\n376  106.13218\n377  103.12044\n378  117.84138\n379  117.41520\n380  125.36306\n381  105.82215\n382  127.51360\n383  103.99779\n384  113.93482\n385  104.04683\n386  106.93355\n387  107.05414\n388  104.54855\n389  125.37328\n390  112.21401\n391  113.13934\n392  125.71206\n393  105.71941\n394  112.40308\n395  108.61642\n396  107.48780\n397  118.09707\n398  125.35679\n399   97.45444\n400  100.10943\n401  116.58694\n402  106.78057\n403  111.91079\n404  116.75726\n405  108.17398\n406   99.58362\n407  114.57293\n408  109.85168\n409  121.84334\n410  107.86493\n411  127.12080\n412  124.86587\n413   99.53627\n414  116.46358\n415  124.81236\n416  111.73796\n417  108.87264\n418  117.94757\n419  115.56643\n420  123.96318\n421  113.77360\n422  119.94670\n423  108.32990\n424  124.58518\n425  114.06451\n426  110.54113\n427  114.85524\n428  117.35423\n429  125.28117\n430  114.69364\n431  106.83007\n432  110.89630\n433  115.50097\n434  121.92301\n435  118.78799\n436  113.84525\n437  120.64767\n438  109.36883\n439  121.13011\n440  113.52213\n441  115.16573\n442  123.03323\n443  111.16598\n444  110.23718\n445  121.01684\n446  104.57516\n447  114.23794\n448  116.48334\n449  112.93738\n450  116.97262\n451  123.01939\n452  103.86612\n453  108.16585\n454  117.46619\n455  102.80921\n456  111.45025\n457  113.71313\n458  115.76154\n459  107.26893\n460  122.26012\n461  136.24026\n462  123.72361\n463  110.92298\n464  100.08531\n465  112.24392\n466  110.54597\n467  111.99873\n468  112.89430\n469  112.26102\n470  117.39683\n471  117.50764\n472  106.53525\n473  105.80527\n474  115.67630\n475  100.35041\n476  113.07986\n477  114.39667\n478  118.53729\n479  125.13422\n480  116.61993\n481  113.62256\n482  117.60229\n483  121.42464\n484  123.01585\n485  110.59016\n486  118.49153\n487  116.60030\n488  114.53784\n489  126.91723\n490   96.27709\n491  103.54786\n492  105.34090\n493  113.60563\n494  119.49589\n495  120.85729\n496  111.34998\n497  108.19074\n498  105.44374\n499  111.48404\n500  115.11209\n501  113.31679\n502  107.93316\n503  121.78264\n504  110.72774\n505  108.02673\n506  113.40761\n507  121.72887\n508  112.27018\n509  105.09043\n510  121.76014\n511  116.73332\n512  121.76908\n513  117.22549\n514  108.76471\n515  107.87862\n516  117.75028\n517  110.29232\n518  116.54346\n519  109.25235\n520  113.03419\n521  117.64512\n522  121.47154\n523  114.08779\n524  106.18617\n525  119.07393\n526  106.86533\n527  115.46940\n528  101.70763\n529   97.54206\n530  110.64455\n531  107.06610\n532  112.42027\n533  118.73436\n534  111.71727\n535  104.05510\n536  129.54831\n537  117.09248\n538  124.54404\n539  116.33296\n540  123.40549\n541  102.44298\n542  111.77241\n543  120.33461\n544  117.91417\n545  108.52797\n546  126.19165\n547  113.31332\n548  107.87823\n549  119.66154\n550  111.16370\n551  109.34024\n552  117.21184\n553  119.04455\n554  117.19549\n555  107.11711\n556  106.42320\n557  121.77364\n558  119.82572\n559  113.46557\n560  115.69528\n561  110.62206\n562  122.38456\n563  122.98836\n564  108.48447\n565  106.09706\n566  116.32697\n567  116.35801\n568  124.40857\n569  116.82206\n570  114.09462\n571  115.40778\n572  116.96016\n573   98.85140\n574  135.20693\n575  119.75133\n576  114.80116\n577  108.22753\n578  117.61364\n579  116.36189\n580  109.92622\n581  111.91415\n582  116.15821\n583  105.58993\n584  108.08802\n585  117.67850\n586  111.13633\n587  132.24823\n588  110.85715\n589   87.83593\n590  125.64538\n591  109.59319\n592  101.12824\n593  113.94740\n594  124.31554\n595  118.67357\n596  111.03314\n597  121.03873\n598  110.29637\n599  112.24814\n600  119.22314\n601  124.26302\n602  112.70908\n603   97.54202\n604  112.54098\n605  117.30295\n606  113.61166\n607  126.07466\n608  108.19994\n609  117.06018\n610  117.99884\n611  124.48122\n612  120.04676\n613  120.79039\n614  113.56916\n615  106.28474\n616  121.85101\n617  121.80984\n618  107.49041\n619  110.51965\n620  122.22094\n621  112.96608\n622  107.79417\n623  109.04927\n624  100.50307\n625  117.33123\n626  125.95204\n627  122.03779\n628  116.83302\n629  110.13387\n630  118.26938\n631  123.07836\n632  106.96144\n633  119.32938\n634  114.60838\n635  104.26998\n636  117.78356\n637  112.10798\n638  116.92210\n639  122.20747\n640  103.41158\n641  104.35021\n642  111.00875\n643  126.15944\n644  120.43646\n645  103.26239\n646  121.87818\n647  109.79967\n648  111.64820\n649  116.67954\n650  105.66557\n651  112.75183\n652  121.22979\n653  114.24457\n654  103.54787\n655  101.95563\n656  103.88058\n657  124.59750\n658  113.34938\n659  104.30297\n660  124.46201\n661  114.08120\n662  126.73495\n663  117.66581\n664   99.67641\n665  107.33070\n666  107.93766\n667  113.07169\n668  114.49677\n669  109.61490\n670  102.14626\n671  118.50619\n672  109.63734\n673  125.07082\n674  106.13135\n675  120.89767\n676  118.49616\n677  121.94440\n678  116.67561\n679  110.53741\n680  109.26362\n681  121.35528\n682  120.08566\n683  106.30738\n684  105.02832\n685  116.33245\n686  113.73313\n687  121.30509\n688  127.22500\n689  115.56041\n690  121.46557\n691  118.54388\n692  113.01171\n693  130.12382\n694  120.11217\n695  105.06264\n696  107.70540\n697  116.29044\n698  107.87553\n699   99.27654\n700  111.77306\n701  112.65223\n702  109.55930\n703  116.77807\n704  109.78229\n705  119.13192\n706  113.67539\n707  118.85713\n708  121.56431\n709  116.28196\n710  119.04540\n711  109.45345\n712  114.95872\n713  115.29909\n714  112.15066\n715  116.73322\n716  114.44525\n717  111.12546\n718  112.27558\n719  113.56506\n720  114.10238\n721  100.49031\n722  113.25783\n723  111.85214\n724  116.96490\n725  108.83318\n726  114.62116\n727  106.61273\n728  109.46670\n729  123.27669\n730  120.57396\n731  103.87767\n732  106.94421\n733  108.34143\n734  116.92814\n735  110.42256\n736  109.48496\n737  116.48718\n738  120.68135\n739  111.55352\n740   93.88022\n741  107.22182\n742  124.23818\n743  113.48573\n744  114.27485\n745  111.79580\n746  113.71912\n747  110.32422\n748  122.13764\n749  111.87946\n750  127.66771\n751  117.10136\n752  115.48153\n753  110.11040\n754  112.85943\n755  105.63839\n756  108.13891\n757  120.85112\n758  117.88342\n759  111.69815\n760  119.76180\n761  134.35632\n762  109.77925\n763  119.67662\n764  120.93808\n765  109.29167\n766  122.32388\n767  109.15243\n768  117.27312\n769  108.50841\n770  111.76736\n771  124.59931\n772  112.06909\n773  112.19180\n774  114.38893\n775  120.83596\n776  107.44710\n777  121.63091\n778  114.90195\n779  101.89752\n780  111.35287\n781  117.87474\n782  101.78017\n783  110.58340\n784  125.94421\n785  123.96811\n786  113.46274\n787  121.76359\n788  110.06839\n789  102.44855\n790  111.36805\n791  112.06821\n792  107.07052\n793  109.29914\n794  123.65203\n795  105.85683\n796  111.35574\n797  125.17185\n798  100.63606\n799  104.69494\n800  116.48918\n801   97.65872\n802  110.70257\n803   99.96854\n804  118.34047\n805   98.87707\n806  106.96261\n807  121.66617\n808  120.60981\n809  113.29107\n810  111.57254\n811  108.33329\n812  122.84750\n813  116.70816\n814  123.37593\n815  105.93103\n816  120.38120\n817  117.05266\n818  117.38626\n819  111.90372\n820  124.06628\n821  108.95796\n822  119.86165\n823  117.27993\n824  120.37133\n825  128.86851\n826  109.71630\n827  111.71660\n828  110.05162\n829  113.51702\n830  108.89157\n831  107.63479\n832  108.94371\n833  118.58841\n834  114.21696\n835  111.22482\n836  122.48018\n837  115.61993\n838  109.40633\n839  104.44660\n840  111.94576\n841  127.03510\n842  119.93454\n843  111.68510\n844  120.58653\n845  108.03814\n846  113.34691\n847  106.62631\n848  110.40374\n849  122.61251\n850  114.44325\n851  104.69718\n852  106.56099\n853  127.06369\n854  125.45967\n855  114.71837\n856  117.62471\n857  120.52498\n858  116.44214\n859  107.40783\n860  114.50855\n861  115.58185\n862  115.97269\n863  114.63601\n864   99.79335\n865  109.73196\n866  108.74116\n867  102.28575\n868  107.32777\n869  117.85405\n870  105.90642\n871  112.89515\n872  134.41702\n873  112.23719\n874  104.71251\n875  118.32708\n876  104.18355\n877  128.33015\n878  116.90507\n879  120.39067\n880  126.98088\n881  116.34320\n882  109.82074\n883  123.30521\n884  106.80564\n885  118.13258\n886   97.52207\n887  112.72367\n888  115.18713\n889  117.35420\n890  118.52945\n891  112.23963\n892  112.33860\n893  117.71835\n894  113.51003\n895  102.75577\n896  120.41303\n897  113.45623\n898  106.96468\n899  118.39375\n900  112.78840\n901  111.73239\n902  100.42487\n903  117.71950\n904  111.69543\n905  102.44391\n906  110.05755\n907  116.58030\n908  116.50860\n909  120.90876\n910  120.61065\n911  114.09941\n912  108.47591\n913  114.89356\n914  111.55837\n915  125.64014\n916  120.40303\n917  115.25511\n918  113.53279\n919  108.45547\n920  104.94686\n921  113.27691\n922  113.20703\n923  108.27743\n924  118.50170\n925  116.89015\n926  111.98375\n927  116.81695\n928  122.73135\n929  103.39012\n930  117.62376\n931  112.30233\n932  113.45888\n933  116.66527\n934  118.67719\n935  114.26432\n936  122.97697\n937  125.19933\n938  118.29743\n939  110.50635\n940  115.82745\n941  121.11219\n942  113.55447\n943  117.57714\n944  112.28155\n945  122.27081\n946  106.57600\n947  109.08308\n948  117.50010\n949  122.11137\n950  122.16193\n951  121.44458\n952  117.33063\n953  123.51882\n954  121.94215\n955  118.96786\n956  114.31738\n957  107.41746\n958  113.77130\n959  111.35407\n960  107.59777\n961  109.19277\n962  127.13358\n963  100.17054\n964  120.18044\n965  117.28016\n966  109.87820\n967  118.51631\n968  108.74053\n969  107.78547\n970   95.13907\n971  108.20715\n972  118.80471\n973  118.20027\n974  113.92949\n975  130.31542\n976  114.68755\n977  103.50685\n978  109.87666\n979  117.27346\n980  113.62313\n981  106.39070\n982  113.30711\n983  110.87394\n984  125.48873\n985  110.72711\n986  112.07703\n987  106.68431\n988  105.44745\n989  110.80564\n990  109.69366\n991  113.20748\n992  114.57158\n993  110.00366\n994  106.38230\n995  113.29721\n996  121.58053\n997  116.29353\n998  118.41607\n999  100.96017\n1000 108.10720\n1001 130.32826\n1002 120.17653\n1003 115.76080\n1004 104.11123\n1005 111.41403\n1006 110.55287\n1007 109.45958\n1008 117.71342\n1009 106.94909\n1010 119.46146\n1011 111.08046\n1012 117.20810\n1013 121.10426\n1014 113.24686\n1015 116.21945\n1016 103.16162\n1017 109.84911\n1018 119.90426\n1019 116.16652\n1020 115.20789\n1021 115.37497\n1022 109.81569\n1023 123.91835\n1024 118.28780\n1025 117.16872\n1026 114.28380\n1027 117.47676\n1028 127.92206\n1029 120.61338\n1030 119.82510\n1031 117.25111\n1032 109.81241\n1033 110.86057\n1034  99.17878\n1035 105.74768\n1036 124.20102\n1037 125.06881\n1038 113.94185\n1039 119.46429\n1040 110.56440\n1041 103.70747\n1042 114.42114\n1043 119.36101\n1044 114.96361\n1045 127.03302\n1046 110.93612\n1047 121.32774\n1048 125.58299\n1049 113.66107\n1050 127.45563\n1051 121.29938\n1052 115.91205\n1053 125.66449\n1054 117.38157\n1055 113.01597\n1056 113.25878\n1057 127.29828\n1058 125.32686\n1059 109.75475\n1060 112.37593\n1061 107.38527\n1062 115.14333\n1063 111.45853\n1064 120.82785\n1065 105.20941\n1066 108.54900\n1067 114.02939\n1068 118.37864\n1069 102.11114\n1070 116.64180\n1071 108.40744\n1072 117.18136\n1073 108.19509\n1074 107.14360\n1075 116.90222\n1076 104.15030\n1077 100.26139\n1078 105.81597\n1079 113.34212\n1080 111.94739\n1081 121.14570\n1082 118.44696\n1083 107.34237\n1084 117.24360\n1085 107.60404\n1086 118.85538\n1087 106.40600\n1088 122.85663\n1089 104.07504\n1090 113.22320\n1091 114.97140\n1092 118.09961\n1093 117.03136\n1094 107.14066\n1095 123.18202\n1096 112.09900\n1097 107.97797\n1098 111.92963\n1099 111.93445\n1100 128.90915\n1101 115.24124\n1102 111.91907\n1103  99.85996\n1104 113.54477\n1105 108.94829\n1106 128.90917\n1107 107.32140\n1108 120.24379\n1109 105.11357\n1110 111.05352\n1111 120.79707\n1112 115.00555\n1113 111.01941\n1114 111.84065\n1115 114.34084\n1116 105.92397\n1117 111.70965\n1118 119.16662\n1119 111.73266\n1120 105.54516\n1121 113.79357\n1122 117.76457\n1123 102.44144\n1124 103.29840\n1125 115.05315\n1126 111.03007\n1127 116.53265\n1128 107.45888\n1129 115.61642\n1130 118.95059\n1131 111.51774\n1132 117.40121\n1133 112.47827\n1134 125.66763\n1135 119.79011\n1136 111.11484\n1137 114.83657\n1138 105.88437\n1139 108.63627\n1140 116.41257\n1141 110.12795\n1142 123.79206\n1143 118.45650\n1144 113.94787\n1145 102.62388\n1146 111.09719\n1147 118.36165\n1148 112.94732\n1149 108.43720\n1150 115.28137\n1151 119.46534\n1152 106.99961\n1153 119.62218\n1154 118.95339\n1155 117.88381\n1156 119.81714\n1157 100.78069\n1158 131.23269\n1159 107.50770\n1160 111.55628\n1161 108.06604\n1162 106.77945\n1163 113.80202\n1164 105.77542\n1165 116.13634\n1166 119.07567\n1167 111.49404\n1168 100.29369\n1169 116.34176\n1170 113.23696\n1171 115.80735\n1172 116.01929\n1173 117.67437\n1174 115.35731\n1175 117.38773\n1176 109.38466\n1177 118.08699\n1178 114.07595\n1179 111.67969\n1180 111.36346\n1181 111.30338\n1182 112.80356\n1183 113.69834\n1184 124.86923\n1185 108.54999\n1186 113.09888\n1187 104.47500\n1188 116.31281\n1189 122.09594\n1190 112.86609\n1191  98.80012\n1192 106.70147\n1193 117.85387\n1194 112.45122\n1195 116.41153\n1196 117.87522\n1197 109.68763\n1198 111.52220\n1199 107.07082\n1200 111.32972\n1201 112.26528\n1202 111.10109\n1203 114.52308\n1204 112.31476\n1205 117.47502\n1206 116.52666\n1207 120.81598\n1208 116.33793\n1209 112.74801\n1210 127.52139\n1211 121.52660\n1212 105.79790\n1213 117.85491\n1214  97.34274\n1215 111.33687\n1216 107.07140\n1217 126.33510\n1218 110.39163\n1219 113.56320\n1220 115.64189\n1221 113.85816\n1222 104.43309\n1223 111.79075\n1224 110.78703\n1225 110.67750\n1226 116.06451\n1227 108.96969\n1228 108.15498\n1229 112.92344\n1230 107.14717\n1231 115.92812\n1232 113.04762\n1233 109.43041\n1234 114.23104\n1235 119.86467\n1236 126.11721\n1237 128.23638\n1238 113.65702\n1239 108.32141\n1240 114.21378\n1241 107.63545\n1242 117.78865\n1243 129.03578\n1244 115.47835\n1245 123.17433\n1246 115.30096\n1247 112.59428\n1248 105.57202\n1249 111.88252\n1250 110.88433\n1251 114.84263\n1252 107.40725\n1253 111.47207\n1254 120.21071\n1255 111.89527\n1256 118.35163\n1257 117.88052\n1258 116.34688\n1259 112.34029\n1260 103.99297\n1261 123.56539\n1262 111.97952\n1263 116.69397\n1264 116.34907\n1265 116.79808\n1266 101.95297\n1267 117.86954\n1268 108.23262\n1269 132.30578\n1270 120.46958\n1271 114.94029\n1272 120.20149\n1273 100.30723\n1274 129.85113\n1275 115.81816\n1276 117.02384\n1277 120.27815\n1278 121.85546\n1279 103.90078\n1280 122.19051\n1281 116.57745\n1282 109.95009\n1283 105.23317\n1284 116.93435\n1285 111.57148\n1286 119.13922\n1287 125.54779\n1288 109.10373\n1289 110.70050\n1290 105.11765\n1291 109.87981\n1292 118.19105\n1293 114.29103\n1294 106.15203\n1295 102.50338\n1296 109.96188\n1297 108.15569\n1298 113.50295\n1299 111.48597\n1300 121.54557\n1301 110.93386\n1302 103.80765\n1303 119.33408\n1304 102.02712\n1305 118.55922\n1306 107.06847\n1307 108.79602\n1308 116.50134\n1309 118.38453\n1310 118.79259\n1311 106.82584\n1312 102.34545\n1313 130.01962\n1314 109.81604\n1315 119.12788\n1316 110.37244\n1317  95.99251\n1318 119.60626\n1319 119.44452\n1320 123.12385\n1321 119.36772\n1322 116.31835\n1323 111.28271\n1324 122.10482\n1325 114.14807\n1326 127.06285\n1327 118.53668\n1328 115.02598\n1329 108.22127\n1330 116.81348\n1331 112.81088\n1332 118.34949\n1333 114.29864\n1334 123.59628\n1335 119.70373\n1336 117.51460\n1337 127.34626\n1338 113.07955\n1339 110.45866\n1340 119.92736\n1341 117.28564\n1342 108.09077\n1343 104.62263\n1344 105.07416\n1345 104.88394\n1346 110.06062\n1347 108.95208\n1348 132.09090\n1349 107.30942\n1350 112.99771\n1351 117.46157\n1352 117.16070\n1353 109.21347\n1354 112.24620\n1355 112.61793\n1356 121.18151\n1357 103.94874\n1358 113.37763\n1359 122.73741\n1360 115.32113\n1361 111.77167\n1362 110.99553\n1363 121.94203\n1364 110.34328\n1365 105.30604\n1366 116.09629\n1367 102.05262\n1368 109.95499\n1369 119.25235\n1370 118.15268\n1371 112.32892\n1372 105.47613\n1373 111.76131\n1374 100.76358\n1375 109.93715\n1376 100.96184\n1377 125.21202\n1378 110.37468\n1379 124.80175\n1380 109.96470\n1381 115.14373\n1382 121.15185\n1383 115.30872\n1384 118.92581\n1385 110.46122\n1386 111.08989\n1387 105.44359\n1388 117.45240\n1389 109.14479\n1390 115.13759\n1391 110.39389\n1392 115.01801\n1393 108.04009\n1394 120.60450\n1395 118.56538\n1396 117.14946\n1397 120.93808\n1398 114.59867\n1399 118.94317\n1400 117.58117\n1401 110.22179\n1402 106.15499\n1403 113.10225\n1404 110.11491\n1405 102.21913\n1406 105.08358\n1407 103.49084\n1408 100.73561\n1409 107.55905\n1410 106.16039\n1411 129.10527\n1412 108.82824\n1413 121.21923\n1414 106.53840\n1415 102.58445\n1416 118.46176\n1417 127.49937\n1418 106.71160\n1419 122.65682\n1420 114.51373\n1421 121.56658\n1422 113.11742\n1423 122.15894\n1424 115.77924\n1425 111.80496\n1426 112.46076\n1427 109.06221\n1428 113.28135\n1429 110.47385\n1430 122.36573\n1431 118.63585\n1432 111.77054\n1433 110.29033\n1434 114.62444\n1435 108.00727\n1436 114.92012\n1437 108.95962\n1438 102.97774\n1439 119.22310\n1440 120.43765\n1441 108.49537\n1442 110.92757\n1443 107.97688\n1444 116.12105\n1445 101.42974\n1446 108.56945\n1447 123.53596\n1448 113.23547\n1449 111.06023\n1450 123.54516\n1451 116.48961\n1452 114.14523\n1453 122.20655\n1454 103.98278\n1455 121.67445\n1456  99.28229\n1457 114.72245\n1458 123.91351\n1459 125.14108\n1460 113.67279\n1461 111.81406\n1462 113.03427\n1463 112.31009\n1464 123.09269\n1465 118.35434\n1466 114.97969\n1467 116.46900\n1468 110.49186\n1469 106.00737\n1470 111.91734\n1471 123.08918\n1472 120.01327\n1473 108.06641\n1474 109.38992\n1475 109.96492\n1476 123.97694\n1477 109.80171\n1478 108.36984\n1479 120.85546\n1480 118.34128\n1481 117.86559\n1482 116.26975\n1483 108.53301\n1484 118.59358\n1485 116.85862\n1486 116.77585\n1487 120.74770\n1488 107.73169\n1489 119.16405\n1490 108.31084\n1491 116.06004\n1492 112.46070\n1493 116.88191\n1494 111.17441\n1495 104.23124\n1496 103.50059\n1497 101.43330\n1498 113.01985\n1499 110.29478\n1500 115.67790\n1501 118.72363\n1502 120.64310\n1503 109.50663\n1504 129.39962\n1505 110.08841\n1506 125.67869\n1507 112.94456\n1508 130.30593\n1509 110.96469\n1510 105.34348\n1511 113.96547\n1512 108.46737\n1513 122.71343\n1514 102.94200\n1515 124.77048\n1516 116.56960\n1517 122.55535\n1518 116.02211\n1519 114.43897\n1520 121.18077\n1521 119.74168\n1522 118.83429\n1523 121.15772\n1524 114.06548\n1525 106.46666\n1526 119.30676\n1527 133.72091\n1528 123.13389\n1529 104.38520\n1530 115.42896\n1531 117.66344\n1532 119.83591\n1533 123.98602\n1534 102.37509\n1535 114.07874\n1536 106.97540\n1537 110.35280\n1538 116.97690\n1539 111.18541\n1540 105.69587\n1541 104.59612\n1542 115.75452\n1543 120.29783\n1544 123.31126\n1545 131.44892\n1546 122.69949\n1547 113.97907\n1548 110.09498\n1549 115.16476\n1550 100.29843\n1551 113.91254\n1552 114.66159\n1553 103.84541\n1554 110.39713\n1555 105.25746\n1556 102.13013\n1557 124.34440\n1558 118.88271\n1559 109.24473\n1560 103.34364\n1561 120.40739\n1562 116.99994\n1563 123.02828\n1564 108.33611\n1565 119.99752\n1566 121.03319\n1567  99.99384\n1568 116.67556\n1569 107.83469\n1570 108.69174\n1571 105.66309\n1572 119.96092\n1573 117.85073\n1574 117.72457\n1575 108.18494\n1576 105.57252\n1577 109.74319\n1578 103.37063\n1579 120.24026\n1580 117.46247\n1581 118.56704\n1582 113.29901\n1583 126.00136\n1584 118.37080\n1585 114.59037\n1586 123.25053\n1587 118.19777\n1588 105.07041\n1589 110.12177\n1590 108.81398\n1591 118.84109\n1592 114.03587\n1593 118.08697\n1594 114.20147\n1595 115.63426\n1596 115.48063\n1597 119.33707\n1598 113.05393\n1599 110.55678\n1600 121.74840\n1601 123.84287\n1602 103.31903\n1603 107.83778\n1604 117.31981\n1605 100.17249\n1606 123.31731\n1607 113.20546\n1608 110.73020\n1609 126.64609\n1610 102.29858\n1611 109.91586\n1612 108.10079\n1613 111.44146\n1614 122.73071\n1615 115.16343\n1616 109.03564\n1617 119.87047\n1618 106.47498\n1619 109.46235\n1620 101.66624\n1621 118.03451\n1622 116.10188\n1623 108.65630\n1624 113.09310\n1625 104.06154\n1626 101.97438\n1627 116.78661\n1628 110.29435\n1629 123.46893\n1630 116.59034\n1631 102.98659\n1632 110.18205\n1633 109.23006\n1634 120.82451\n1635 121.75575\n1636 112.22303\n1637 105.56302\n1638 121.47504\n1639 110.05367\n1640 104.12317\n1641 113.38777\n1642 111.67334\n1643 114.42323\n1644 106.43060\n1645 135.61538\n1646 115.60082\n1647 117.35365\n1648 113.11384\n1649 117.95815\n1650 112.93363\n1651 113.47685\n1652 105.97033\n1653 114.98154\n1654 107.74614\n1655 116.06752\n1656 122.10031\n1657 112.99138\n1658 114.29996\n1659 106.96841\n1660 108.60291\n1661 121.43145\n1662  92.95222\n1663 113.02822\n1664 111.52353\n1665 105.84010\n1666 129.57170\n1667 116.08035\n1668 117.68832\n1669 116.61900\n1670 122.81734\n1671 115.13204\n1672 114.64356\n1673 116.38536\n1674 109.62207\n1675 110.92888\n1676 114.36763\n1677 105.78363\n1678 116.03262\n1679 101.17360\n1680 118.69656\n1681 114.24099\n1682 113.32326\n1683 103.40502\n1684 112.39048\n1685 113.95558\n1686 115.26227\n1687 124.42804\n1688 118.27150\n1689 123.43397\n1690 124.46988\n1691 110.85936\n1692 114.07836\n1693 112.95339\n1694 120.96305\n1695 122.99706\n1696 115.55654\n1697 115.87079\n1698 115.29694\n1699 125.24085\n1700 128.62481\n1701 108.18660\n1702 105.87661\n1703 104.39514\n1704 113.98015\n1705 119.94162\n1706 120.89582\n1707 115.58448\n1708 124.34802\n1709 119.27451\n1710 125.94165\n1711 118.74313\n1712 118.66193\n1713 117.66496\n1714 107.85271\n1715 116.25929\n1716 118.91040\n1717 112.54798\n1718 108.35888\n1719 106.78994\n1720 122.41502\n1721 112.78309\n1722 111.49801\n1723 110.25142\n1724 100.29204\n1725 109.43258\n1726 109.33677\n1727 108.22743\n1728 121.09013\n1729 126.03049\n1730 114.12910\n1731 114.46407\n1732 101.81167\n1733 109.25527\n1734 117.17836\n1735 114.15739\n1736 106.89497\n1737 113.97003\n1738 109.90265\n1739 120.60851\n1740 126.19142\n1741 121.11972\n1742 111.88430\n1743 105.94220\n1744 113.77476\n1745 111.42531\n1746 114.39007\n1747 115.78462\n1748 112.93819\n1749 118.81692\n1750 118.76391\n1751 123.01901\n1752 111.04410\n1753 118.35484\n1754 110.54607\n1755 110.85959\n1756 105.96548\n1757 116.78229\n1758 108.15793\n1759 110.14765\n1760 109.63972\n1761 112.02199\n1762 114.85539\n1763 117.21206\n1764 115.58728\n1765  99.67584\n1766 116.18988\n1767 106.56255\n1768 110.93185\n1769 120.20929\n1770 110.24173\n1771 115.38537\n1772 123.69769\n1773 115.34699\n1774 111.34985\n1775 109.82229\n1776 115.89685\n1777 118.99048\n1778 118.77597\n1779 111.15591\n1780 116.88276\n1781 116.84949\n1782 107.54415\n1783 115.28064\n1784 113.47038\n1785 110.72918\n1786 111.94738\n1787 107.27141\n1788 115.04275\n1789  96.72293\n1790 122.32240\n1791 104.26958\n1792 123.25807\n1793 115.92358\n1794 117.70162\n1795 118.16755\n1796 118.03596\n1797 120.34519\n1798 104.31188\n1799 132.04806\n1800 117.71137\n1801 113.05951\n1802 110.26341\n1803 127.21428\n1804 117.25141\n1805 108.35096\n1806 110.27506\n1807 111.23149\n1808 124.83066\n1809 123.39050\n1810 106.58225\n1811 109.74921\n1812 109.04106\n1813 125.43409\n1814 110.93092\n1815 111.74767\n1816 101.40743\n1817 116.73829\n1818 102.78626\n1819 112.74032\n1820 105.15150\n1821 106.97115\n1822 120.82963\n1823 115.17882\n1824 118.71154\n1825 124.19609\n1826 109.75987\n1827 120.38832\n1828 121.82306\n1829 106.27523\n1830 128.54055\n1831 117.93971\n1832 106.59459\n1833 119.75123\n1834 117.02807\n1835 117.46441\n1836 117.25068\n1837 112.56719\n1838 108.33113\n1839 107.22700\n1840 114.48208\n1841 110.34761\n1842 117.18823\n1843 124.86804\n1844 115.99743\n1845 118.54041\n1846 114.31177\n1847 122.35911\n1848 115.61515\n1849 111.68315\n1850 119.04893\n1851 105.15279\n1852 104.46286\n1853 108.21831\n1854 120.25840\n1855 113.72293\n1856 116.31275\n1857 110.21878\n1858 104.04796\n1859 116.13271\n1860  99.73447\n1861 114.76161\n1862 123.04099\n1863 114.48397\n1864 119.41272\n1865 114.43066\n1866 116.57754\n1867 104.68885\n1868 102.26670\n1869 111.31379\n1870 107.89620\n1871 107.26937\n1872 128.56182\n1873 112.31984\n1874 117.48175\n1875 111.82601\n1876 121.53766\n1877 108.59204\n1878 114.00073\n1879 109.15453\n1880 115.40349\n1881 120.02438\n1882 120.00529\n1883 114.15522\n1884  97.21296\n1885 118.74600\n1886 110.07800\n1887 105.74195\n1888 109.99513\n1889 115.58094\n1890  98.49195\n1891 119.22469\n1892 108.36079\n1893 123.17149\n1894 122.71776\n1895 119.61528\n1896 113.61297\n1897 104.29065\n1898 119.35944\n1899 114.59634\n1900 114.87640\n1901 114.83493\n1902 120.75232\n1903 116.33686\n1904 112.85593\n1905 108.99668\n1906 119.80091\n1907 107.51762\n1908 117.00237\n1909 125.47799\n1910 109.23858\n1911  99.10170\n1912 113.58951\n1913 110.50543\n1914 120.26970\n1915 112.06393\n1916 101.04741\n1917 112.63951\n1918 113.25368\n1919 121.02941\n1920 120.40065\n1921 102.51873\n1922 122.20321\n1923 121.08449\n1924 119.55367\n1925 115.73619\n1926 108.47358\n1927 113.91919\n1928 115.65892\n1929 117.53470\n1930 113.44030\n1931 112.06709\n1932 106.90271\n1933 113.75108\n1934 118.57237\n1935 115.23998\n1936 108.66065\n1937 108.24943\n1938 112.21938\n1939 124.59338\n1940 113.36595\n1941 107.43284\n1942 115.07636\n1943 116.41288\n1944 114.93979\n1945 112.58356\n1946 118.89955\n1947 113.45179\n1948 109.08609\n1949 122.58892\n1950 101.93728\n1951 106.47563\n1952 120.28890\n1953 109.49638\n1954 104.30374\n1955 112.77269\n1956 124.76056\n1957 118.72269\n1958 123.78044\n1959 110.63524\n1960 109.31897\n1961 107.73594\n1962 116.17672\n1963 105.96558\n1964 119.74607\n1965 118.69882\n1966 115.85835\n1967 104.62583\n1968 113.57872\n1969 128.22431\n1970 115.12682\n1971 114.34633\n1972 106.33976\n1973 112.85725\n1974 109.54481\n1975 126.89872\n1976 106.20579\n1977 114.33387\n1978 118.06756\n1979 120.88291\n1980 112.68291\n1981 126.43337\n1982 110.43387\n1983 114.83281\n1984 116.18950\n1985 105.62630\n1986 122.38782\n1987 118.93003\n1988 113.00455\n1989 121.08291\n1990 124.71230\n1991 111.14368\n1992 111.19670\n1993 114.69397\n1994 113.91546\n1995 111.82721\n1996 112.65771\n1997 118.70725\n1998 109.79392\n1999 114.41826\n2000 114.76945\n\n\nAgain, we see that the sample means are close to 114, but there is some variability. Naturally, not every sample is going to have an average of exactly 114. So how much variability do we expect? Let’s graph and find out. We’re going to set the x-axis manually so that we can do some comparisons later.\n\nggplot(sims, aes(x = mean)) +\n    geom_histogram(binwidth = 1) +\n    scale_x_continuous(limits = c(86, 142),\n                       breaks = c(93, 100, 107, 114, 121, 128, 135))\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\nMost sample means are around 114, but there is a good range of possibilities from around 93 to 135. The population standard deviation \\(\\sigma\\) is 14, but the standard deviation in this graph is clearly much smaller than that. (A large majority of the samples are within 14 of the mean!)\nWith some fancy mathematics, one can show that the standard deviation of this sampling distribution is not \\(\\sigma\\), but rather \\(\\sigma/\\sqrt{n}\\). In other words, this sampling distribution of the mean has a standard error of\n\\[\n\\frac{\\sigma}{\\sqrt{n}} = \\frac{14}{\\sqrt{4}} = 7.\n\\]\nThis makes sense: as the sample size increases, we expect the sample mean to be more and more accurate, so the standard error should shrink with large sample sizes.\nLet’s re-scale the y-axis to use percentages instead of counts. Then we should be able to superimpose the normal model \\(N(114, 7)\\) to check visually that it’s the right fit.\n\n# Don't worry about the syntax here.\n# You won't need to know how to do this on your own.\nggplot(sims, aes(x = mean)) +\n    geom_histogram(aes(y = ..density..), binwidth = 1) +\n    scale_x_continuous(limits = c(86, 142),\n                       breaks = c(93, 100, 107, 114, 121, 128, 135)) +\n    stat_function(fun = dnorm, args = list(mean = 114, sd = 7),\n                  color = \"red\", size = 1.5)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\nLooks pretty good!\nAll we do now is convert everything to z scores. In other words, suppose we sample 4 individuals from a population distributed according to the normal model \\(N(0, 1)\\). Now the standard error of the sampling distribution is\n\\[\n\\frac{\\sigma}{\\sqrt{n}} = \\frac{1}{\\sqrt{4}} = 0.5.\n\\]\nThe following code will accomplish all of this. (Don’t worry about the messy syntax. All I’m doing here is making sure that this graph looks exactly the same as the previous graph, except now centered at \\(\\mu = 0\\) instead of \\(\\mu = 114\\).)\n\n# Don't worry about the syntax here.\n# You won't need to know how to do this on your own.\nsims_z &lt;- data.frame(mean = scale(sims$mean, center = 114, scale = 14))\nggplot(sims_z, aes(x = mean)) +\n    geom_histogram(aes(y = ..density..), binwidth = 1/14) +\n    scale_x_continuous(limits = c(-2, 2),\n                       breaks = c(-1.5, -1, -0.5, 0, 0.5, 1, 1.5)) +\n    stat_function(fun = dnorm, args = list(mean = 0, sd = 0.5),\n                  color = \"red\",  size = 1.5)\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\nRemember that this is not the standard normal model \\(N(0, 1)\\). The standard deviation in the graph above is not 1, but 0.5 because that is the standard error when using samples of size 4. (\\(1/\\sqrt{4} = 0.5\\).)"
  },
  {
    "objectID": "19-inference_for_one_mean-web.html#unknown-standard-errors",
    "href": "19-inference_for_one_mean-web.html#unknown-standard-errors",
    "title": "19  Inference for one mean",
    "section": "19.4 Unknown standard errors",
    "text": "19.4 Unknown standard errors\nIf we want to run a hypothesis test, we will have a null hypothesis about the true value of the population mean \\(\\mu\\). For example,\n\\[\nH_{0}: \\mu = 114\n\\]\nNow we gather a sample and compute the sample mean, say 110.2043696. We would like to be able to compare the sample mean \\(\\bar{y}\\) to the hypothesized value 114 using a z score:\n\\[\nz = \\frac{(\\bar{y} - \\mu)}{\\sigma/\\sqrt{n}} = \\frac{(110.2 - 114)}{\\sigma/\\sqrt{4}}.\n\\]\nHowever, we have a problem: we usually don’t know the true value of \\(\\sigma\\). In our SBP example, we do happen to know it’s 14, but we won’t know this for a general research question.\nThe best we can do with a sample is calculate this z score replacing the unknown \\(\\sigma\\) with the sample standard deviation \\(s\\), 13.0561519. We’ll call this a “t score” instead of a “z score”:\n\\[\nt = \\frac{(\\bar{y} - \\mu)}{s/\\sqrt{n}} = \\frac{(110.2 - 114)}{13.06/\\sqrt{4}} = -0.58.\n\\]\nThe problem is that \\(s\\) is not a perfect estimate of \\(\\sigma\\). We saw earlier that \\(s\\) is usually close to \\(\\sigma\\), but \\(s\\) has its own sampling variability. That means that our earlier simulation in which we assumed that \\(\\sigma\\) was known and equal to 14 was wrong for the type of situation that will arise when we run a hypothesis test. How wrong was it?"
  },
  {
    "objectID": "19-inference_for_one_mean-web.html#simulating-t-scores",
    "href": "19-inference_for_one_mean-web.html#simulating-t-scores",
    "title": "19  Inference for one mean",
    "section": "19.5 Simulating t scores",
    "text": "19.5 Simulating t scores\nLet’s run the simulation again, but this time with the added uncertainty of using \\(s\\) to estimate \\(\\sigma\\).\nThe first step is to write a little function of our own to compute simulated t scores. This function will take a sample of size \\(n\\) from the true population \\(N(\\mu, \\sigma)\\), calculate the sample mean and sample standard deviation, then compute the t score. Don’t worry: you won’t be required to do anything like this on your own.\n\n# Don't worry about the syntax here.\n# You won't need to know how to do this on your own.\nsim_t &lt;- function(n, mu, sigma) {\n    sample_values &lt;- rnorm(n, mean = mu, sd = sigma)\n    y_bar &lt;- mean(sample_values)\n    s &lt;- sd(sample_values)\n    t &lt;- (y_bar - mu)/(s / sqrt(n))\n}\n\nNow we can simulate doing this 2000 times.\n\nset.seed(5151977)\nsims_t &lt;- do(2000) * sim_t(4, mu = 114, sigma = 14) \nsims_t\n\n             sim_t\n1      1.670726734\n2     -0.975666678\n3     -0.278839393\n4      0.907808022\n5     -1.527274531\n6     -1.717671837\n7     -0.610956296\n8     -0.177107883\n9     -0.081578742\n10    -0.150764283\n11     0.105561464\n12     0.989851233\n13     0.754578374\n14    -0.221752375\n15    -0.569806798\n16     1.056144154\n17     0.709520796\n18     1.786249608\n19    -0.022957371\n20    -0.479076521\n21     2.196497891\n22    -0.057126903\n23     0.723176732\n24     0.462163070\n25     2.305842397\n26    -0.541132956\n27    -1.155518891\n28     1.893602331\n29     3.587253178\n30    -1.329845154\n31     1.786070559\n32     0.205368769\n33    -0.617185683\n34     1.408927566\n35     0.174600728\n36    -0.585585461\n37     0.975358819\n38     0.867186495\n39    -0.509037457\n40     0.463270308\n41     2.961196587\n42     0.250917786\n43    -0.151400364\n44     2.379911294\n45     0.965542692\n46    -1.639114331\n47    -0.187393864\n48     0.702999822\n49    -1.649486008\n50     0.642256403\n51    -0.445978914\n52    -0.870684799\n53    -0.506327234\n54     0.515425890\n55     1.188525622\n56     1.173749591\n57    -3.089680034\n58     1.479209494\n59    10.039858675\n60    -1.865677247\n61     0.208720956\n62     1.698415163\n63     0.874459927\n64    -0.414113539\n65    -2.079229096\n66    -0.641514036\n67    -0.046016401\n68    -2.051611648\n69    -1.116638893\n70    -2.568290582\n71    -3.634987999\n72     0.131241299\n73    -0.317803823\n74    -1.063949859\n75     0.004811193\n76     4.439627383\n77    -1.364313839\n78     1.645804106\n79    -0.201914744\n80     0.504043393\n81     1.440774874\n82    -3.291032994\n83    -1.551130801\n84    -0.802710562\n85    -4.861382113\n86     1.016265268\n87     1.080518333\n88     2.980709799\n89     4.326429336\n90     0.458414619\n91     2.037994906\n92    -1.820144738\n93     1.040068322\n94     2.555396424\n95    -0.478768875\n96    -0.929751963\n97    -0.508981112\n98    -0.569059363\n99    -1.094024179\n100    0.110893966\n101   -0.923379631\n102    0.408635917\n103   -0.521992962\n104    2.636311764\n105    0.636091866\n106    0.859720275\n107    1.253116033\n108    0.874350704\n109   -0.867757352\n110   -1.337827858\n111    0.156515269\n112   -2.023417372\n113    0.789119890\n114    0.664206505\n115   -5.013338827\n116    1.080852724\n117   -0.468189050\n118   -0.592941304\n119   -0.224440854\n120    1.566295593\n121    0.104289555\n122   -1.197675728\n123   -1.007030300\n124    0.407430926\n125   -1.942399658\n126   -3.000766684\n127    0.061485310\n128   -1.592080649\n129    1.051971725\n130    3.007244391\n131   -0.926063447\n132    0.360010372\n133   -1.154431763\n134    0.837885024\n135   -0.865787271\n136   -1.185354554\n137    0.295746913\n138   -0.396571358\n139    0.887971205\n140   -1.027778834\n141   -1.056473957\n142   -0.790085592\n143    2.166777070\n144    0.009600946\n145    0.761096684\n146   -0.445841081\n147   -0.513983827\n148    0.831912239\n149    0.716585444\n150   -0.341729523\n151    1.959676409\n152    0.501861848\n153    1.419772119\n154   -1.145028443\n155    0.404685855\n156    0.572805957\n157   -1.261116341\n158   -1.077860929\n159   -0.340670950\n160    3.191331484\n161   -2.919014184\n162    1.362479919\n163    1.326437044\n164   -0.619316503\n165   -1.330164481\n166    0.114571544\n167    0.275918212\n168   -1.609972483\n169    0.746043178\n170    0.571191844\n171    1.155595866\n172    0.134574629\n173   -1.218916492\n174   -1.492947751\n175    1.012713541\n176   -0.651309215\n177   -2.690012483\n178    0.381110576\n179   -0.709852732\n180    1.127924885\n181    2.690832381\n182    1.716396925\n183   -0.697362354\n184   -0.961945375\n185    0.746108381\n186   -1.524226171\n187   -0.458618707\n188   -0.055254402\n189    1.020115666\n190    0.018051809\n191    0.979239006\n192    0.785251827\n193   -0.178483558\n194   -1.244265037\n195    0.744906482\n196   -0.491305065\n197   -0.345225608\n198   -0.857919408\n199    0.767931118\n200    0.567650649\n201    0.285171950\n202   -0.912431467\n203   -0.016306668\n204   -0.018041076\n205    0.864570995\n206    1.856671982\n207    0.481038270\n208   -1.469329052\n209    2.623871232\n210   -0.712124175\n211    0.392677868\n212   -0.960771180\n213    1.503009840\n214   -1.308729342\n215   -0.714134598\n216    0.910092338\n217    0.687880279\n218   -0.706690653\n219    1.039393080\n220    1.285188816\n221    2.082287808\n222    0.065838057\n223    1.905921689\n224    1.228140674\n225   -0.765591982\n226    0.605332968\n227   -0.017615429\n228   -0.220003147\n229   -0.921723662\n230   -1.408301607\n231    0.307375781\n232   -0.384728667\n233   -4.815204952\n234    0.153630251\n235   -0.544127519\n236   -0.012780210\n237    0.143751438\n238    1.320877365\n239   -1.291725993\n240   -0.482246881\n241    0.752661778\n242    0.393190471\n243    1.179327701\n244    0.393345460\n245   -3.793928233\n246    5.181415482\n247    0.564651863\n248   -1.295222322\n249   -1.416412176\n250    0.491626455\n251   -3.145790254\n252    0.254944191\n253    2.515832119\n254    0.820769536\n255    0.645464631\n256   -0.270108112\n257    1.810842034\n258    1.074959231\n259    2.627121628\n260    1.387446754\n261    1.645532448\n262   -0.384565059\n263    5.407605220\n264   -0.037234681\n265   -3.045039779\n266    0.226437021\n267   -0.146152727\n268    1.122665692\n269   -0.757175673\n270    0.183402023\n271    0.696221348\n272    1.020714292\n273   -0.042622579\n274    2.912200674\n275    0.002357622\n276    0.699894074\n277    0.228627097\n278    0.104690123\n279    0.661475603\n280   -0.506233167\n281   -1.170819473\n282    0.225067302\n283   -0.286442271\n284    1.034292157\n285    0.968956715\n286    0.269954196\n287    1.606642913\n288   -3.655783532\n289    1.138644184\n290   -0.593614901\n291    0.089351830\n292    0.583687533\n293   -3.131934208\n294    4.141194148\n295   -0.538553813\n296   -0.195671796\n297   -0.952154129\n298   -0.412867470\n299   -2.633934189\n300    2.676456838\n301   -0.365352128\n302   -1.524525321\n303    0.691961595\n304    0.117792930\n305   -1.966522333\n306    2.396111764\n307    0.158270827\n308    0.089115221\n309    1.095316968\n310   -0.304480598\n311    0.405375406\n312   -0.525285654\n313    0.077370056\n314    0.322573677\n315    0.550125365\n316   -0.836923161\n317    0.853458742\n318   -0.153190888\n319    0.426522118\n320    0.416588871\n321    1.665861614\n322    0.245350802\n323   -0.425537399\n324   -1.399886864\n325   -1.101151020\n326   -0.195676630\n327    1.374298361\n328    0.896422001\n329    2.034473123\n330    1.160952652\n331    3.155376516\n332   -2.194758925\n333   -1.342957830\n334   -4.302821158\n335    1.520409119\n336    0.161026761\n337   -0.858873653\n338   -2.234242006\n339    2.664978720\n340   -0.325694033\n341   -0.162072513\n342    0.419374037\n343    0.040149235\n344    0.753124668\n345    0.629287085\n346    1.405714938\n347    0.026077230\n348   -2.930378187\n349   -1.963771968\n350   -0.275931005\n351    1.492102994\n352    0.422755335\n353    1.364728012\n354    1.755187258\n355   -0.805715021\n356   -3.759095166\n357   -0.089061286\n358    0.315457365\n359    0.422526784\n360   -0.066293002\n361   -0.082625911\n362    0.030700304\n363   -0.572736076\n364    0.609248931\n365    2.237477557\n366   -1.101976715\n367    0.852254060\n368    0.565323495\n369   -0.409330460\n370   -2.525449990\n371    0.258198977\n372   -0.155976375\n373    1.713712143\n374   -0.117440894\n375    0.978363477\n376   -0.295776559\n377    0.413207781\n378   -0.113175493\n379    0.990093200\n380   -0.022918883\n381    0.549205857\n382   -0.052790585\n383    0.040575930\n384   -0.292532738\n385    0.639195715\n386   -0.013228408\n387   -1.881623593\n388    1.637375851\n389    0.774513263\n390    0.027607716\n391    1.527196670\n392    1.624357378\n393    0.931386941\n394   -0.291767122\n395    0.535967556\n396    1.179312447\n397    1.537035187\n398  -13.448053979\n399   -0.790771070\n400    2.083921975\n401    1.067028943\n402   -0.929967278\n403    1.547377203\n404   -1.006231606\n405   -0.480039478\n406   -0.226170119\n407    2.171631036\n408    1.209164065\n409   -0.634197264\n410    1.168913920\n411   -1.209455505\n412    0.236386507\n413   -0.343579491\n414    0.561363444\n415    1.655111860\n416    0.133171203\n417   -3.087070219\n418    0.360239166\n419   -1.218840158\n420   -0.597036378\n421   -1.018712950\n422   -0.570737036\n423    1.406809822\n424    0.519374240\n425   -0.480235004\n426   -0.403953907\n427   -0.631731646\n428    0.186698413\n429   -1.183039695\n430   -0.262268243\n431   -3.287276247\n432    0.359065901\n433   -0.505551442\n434   -1.320142014\n435    0.364654330\n436   -1.885659342\n437   -1.455481065\n438    1.226269594\n439    2.578741242\n440    3.846835949\n441    0.873998739\n442    1.506630849\n443   -2.988994581\n444   -0.279364518\n445    0.781926119\n446   -0.403122067\n447   -0.844081180\n448   -1.042618412\n449    0.457285503\n450    1.431224917\n451    1.209652423\n452   -3.683650911\n453    1.393770996\n454    1.720084469\n455   -2.230431231\n456    0.134609859\n457   -0.408620761\n458    0.999314450\n459    0.314023571\n460   -0.372848530\n461   -0.296119292\n462   -0.150450959\n463   -0.356862667\n464    1.383127233\n465   -1.860842022\n466    0.605805125\n467    0.152247462\n468    1.007301713\n469    0.765607632\n470   -0.871449843\n471   -0.648254493\n472   -0.930334676\n473   -1.349523909\n474    0.905013805\n475    1.388240794\n476   -3.438014952\n477    1.819725450\n478   -0.294196927\n479    0.986265047\n480    0.187133472\n481    0.552328349\n482    2.113986298\n483   -0.043963581\n484    3.590154410\n485   -0.006183080\n486    0.106542240\n487    0.657637300\n488    1.235365257\n489    0.314752210\n490   -1.739762948\n491    1.682474392\n492   -1.504560768\n493   -0.328829005\n494   -0.301441343\n495   -0.900253920\n496   -0.042854272\n497   -1.494956777\n498    3.144871165\n499   -0.720509064\n500    1.813776977\n501    1.896355460\n502   -1.871342764\n503    3.374841664\n504    0.178730593\n505    1.015395706\n506   -0.379659796\n507   -2.371334183\n508    0.939599149\n509   -0.200982845\n510    2.343383750\n511    1.106325676\n512   -1.144706599\n513   -0.916929140\n514    1.128801935\n515    0.641931894\n516    0.297937489\n517    0.406864789\n518   -2.774211121\n519    0.888483995\n520   -0.629204839\n521    1.418468601\n522    2.036061086\n523    1.439590335\n524   -0.271836839\n525   -0.175824831\n526   -0.338271232\n527    1.927815452\n528    1.512879557\n529    0.378511022\n530    2.845399324\n531    0.109042091\n532   -0.083921454\n533    0.886072470\n534   -0.726462152\n535   -0.558078587\n536    0.680400472\n537    1.802017133\n538   -1.176004753\n539   -1.916491222\n540    3.333289221\n541   -0.789699279\n542    0.547902167\n543   -0.088759086\n544   -2.534317259\n545    1.260407314\n546    0.703405451\n547    2.334909385\n548   -0.457216745\n549    0.789376258\n550    0.455350445\n551    0.721712170\n552   -0.182200217\n553   -1.515374135\n554   -0.480620772\n555    1.767572267\n556    1.187207823\n557    1.193733236\n558    2.411566680\n559    0.364429766\n560   -2.219328757\n561    0.085287694\n562    0.531591789\n563   -9.341273275\n564    0.094853504\n565    0.785084721\n566   -0.634924243\n567   -0.858426461\n568    1.733052640\n569    1.242191829\n570    1.569673781\n571    1.069168621\n572   -1.521836188\n573   -0.645073812\n574    0.111012855\n575   -0.040402131\n576   -0.197406483\n577    0.617917659\n578    1.993147674\n579    0.346510921\n580    0.780109907\n581    2.090928794\n582   -0.004185166\n583    1.349686189\n584   -1.421752348\n585   -1.601158478\n586   -0.106531520\n587    0.209839990\n588   -2.045089991\n589   -1.234780588\n590   -0.461004820\n591   -0.726951479\n592   -0.423468783\n593    0.817807644\n594   -1.188983170\n595    1.204874973\n596   -0.133536565\n597   -2.592167903\n598   -0.699481674\n599    0.703027125\n600   -1.079842721\n601    1.023587812\n602   -0.433562412\n603   -0.988467936\n604   -2.670492513\n605   -0.405054168\n606    1.138635723\n607   -4.547017979\n608   -2.661674486\n609   -0.202076484\n610    0.708493361\n611    2.718968071\n612   -2.128790696\n613    0.397993079\n614   -0.376750125\n615    2.464988702\n616   -0.240840568\n617   -0.926389805\n618    0.722448449\n619   -2.863359383\n620   -0.718307594\n621   -0.158636810\n622   -1.000882017\n623    0.503105050\n624   -1.641816283\n625   -0.391703819\n626    1.988374553\n627    0.373060429\n628   -0.911117546\n629    0.727572449\n630   -0.906238623\n631    2.047456061\n632    0.260991694\n633   -0.602544898\n634    0.030703231\n635    0.269998976\n636    1.217862010\n637   -0.747867807\n638   -0.971587187\n639   -0.911399652\n640   -0.190915752\n641   -1.106996675\n642   -1.122937663\n643    0.046394561\n644   -0.121906856\n645   -0.007749496\n646   -1.469233577\n647    0.246686114\n648    0.624422073\n649   -0.345384370\n650   -0.910899695\n651   -0.141657072\n652   -0.382851158\n653   -0.539948064\n654   -2.496415504\n655    0.448029935\n656    0.551416084\n657    0.399083932\n658   -0.663320517\n659    1.175334007\n660    2.863997683\n661    3.155675712\n662   -2.225264098\n663   -0.258376140\n664    0.628880493\n665   -1.963660373\n666   -0.291929352\n667   -0.535754083\n668   -0.583840122\n669   -1.802510943\n670   -3.854886130\n671   -0.225790532\n672    0.650160540\n673   -1.510854956\n674   -0.602191297\n675   -2.250936994\n676   -2.176366039\n677    0.199527708\n678    0.596295642\n679   -0.610092497\n680    0.826319844\n681   -0.406057365\n682   -2.791436051\n683    1.016551228\n684   -3.832118970\n685    0.474703675\n686   -0.392337439\n687   -0.414976635\n688   -1.766244742\n689   -1.252073689\n690   -3.751861386\n691    1.022733152\n692    0.882560368\n693   -1.521596800\n694   -0.612430392\n695    0.103893932\n696   -2.056366948\n697   -3.682288537\n698   -0.770294858\n699    0.263251202\n700    0.698337535\n701    0.986237494\n702   -0.260951421\n703   -2.285881307\n704   -1.182122288\n705    1.972595161\n706   -1.750006324\n707    2.675074586\n708    1.974046390\n709   -0.609375213\n710   -0.254129786\n711   -0.523115828\n712   -0.072300521\n713    0.611214547\n714    1.596620666\n715    2.306383754\n716   -1.419869458\n717   -0.376853558\n718   -0.117070894\n719    0.951879840\n720   -0.790275047\n721    0.310070760\n722   -2.824664332\n723   -1.379521650\n724    1.668106523\n725    0.022702649\n726   -0.635325983\n727   -0.359415998\n728   -0.933730278\n729   -1.140490968\n730    0.333124364\n731    2.425355154\n732   -0.507101338\n733    2.119591235\n734   -0.232517000\n735    0.712292633\n736   -0.654089022\n737   -0.223122214\n738   -0.585805638\n739   -0.918021780\n740    0.700829615\n741    0.160607319\n742   -2.099742294\n743   -0.200215140\n744    0.303891449\n745   -0.257338792\n746   -0.328260599\n747    0.243176094\n748   -0.085191687\n749   -0.871863259\n750   -0.856766501\n751    2.480765033\n752   -0.278128581\n753   -2.142664872\n754   -0.889514335\n755   -1.762439222\n756    0.832724710\n757    0.686305106\n758   -2.104581727\n759    0.567277023\n760   -0.511621161\n761    3.663699867\n762    0.302672919\n763   -1.140676457\n764    0.620449123\n765    6.421933234\n766    0.368526892\n767   -1.075629491\n768    0.199039023\n769   -0.007360514\n770    0.462910912\n771   -1.425572785\n772   -0.345610941\n773   -3.098008791\n774   -1.958626339\n775   -1.004602181\n776   -0.500937913\n777   -1.665725321\n778   -1.090476929\n779    2.400728753\n780   -0.817570219\n781   -0.660999236\n782    0.904997966\n783   -0.266663748\n784    0.318056265\n785    1.661822423\n786   -1.640156345\n787   -4.756981266\n788    0.577606743\n789   -0.308861651\n790   -1.150271004\n791   -1.627229938\n792   -0.980164694\n793   -1.066120071\n794   -1.457905137\n795    0.299263089\n796   -0.349031501\n797    1.534238168\n798    4.097141405\n799   -3.631181562\n800    0.471849634\n801   -0.988695064\n802    0.038049817\n803    0.396302397\n804    0.322771451\n805   -3.158854812\n806   -0.986408328\n807   -0.268281111\n808   -2.860154110\n809    0.362559601\n810    0.552265488\n811   -0.861090613\n812    2.144060801\n813   -2.050856369\n814    2.955034571\n815    0.098469162\n816    3.093684330\n817   -0.363663950\n818   -0.323551241\n819    1.680685212\n820    0.340180512\n821   -0.578391528\n822    1.329548200\n823    1.809529276\n824    0.480853786\n825    1.430165094\n826    0.836765941\n827   -2.707082948\n828   -1.758176032\n829    0.277666166\n830   -0.845274445\n831    1.891522820\n832    0.263956829\n833   -0.305065811\n834   -0.444100542\n835   -0.832133502\n836    1.584719736\n837    0.662723604\n838    1.018975319\n839    0.133071965\n840    1.503813337\n841    1.660804214\n842    0.253183799\n843   -0.101678251\n844    0.521611568\n845    0.729517569\n846    2.881727329\n847   -1.599790182\n848   -1.095986176\n849    0.763666941\n850    3.268114443\n851    0.006155721\n852    1.841689702\n853   -0.848697008\n854   -0.723285225\n855    0.141026496\n856    1.034208339\n857    0.570545240\n858   -1.059584931\n859    2.256888490\n860    0.218106644\n861    0.119763833\n862    0.515775210\n863   -2.424967874\n864    0.434591838\n865   -0.307744759\n866   -2.178715876\n867    0.323150371\n868    1.072889144\n869    1.362182109\n870    0.891800388\n871    1.255617487\n872   -0.398858495\n873   -0.024776420\n874   -0.053741887\n875    0.927007657\n876   -0.052900194\n877   -0.654057127\n878    0.012066258\n879    1.071104781\n880    0.607243092\n881   -0.032708359\n882    1.006930173\n883    0.596201330\n884   -0.043870537\n885    1.364728823\n886    0.359146350\n887   -0.798584856\n888   -1.388090992\n889   -0.411679156\n890   -2.534136571\n891    0.677893153\n892   -2.303311561\n893   -1.274039074\n894  -12.876024629\n895    0.034091110\n896    0.870246811\n897    0.440710160\n898   -0.440934112\n899   -0.204777576\n900   -0.413712686\n901    0.303877859\n902    2.330154376\n903    0.837433166\n904    0.075834877\n905   -1.728999374\n906   -0.433398626\n907   -1.237728779\n908   -1.556073749\n909    0.541534085\n910   -0.412478800\n911   -1.234088662\n912    2.159294673\n913   -0.300622547\n914   -0.277136722\n915   -0.048932774\n916   -1.651987115\n917    0.411460155\n918    0.357884786\n919   -0.959020471\n920    0.652461567\n921    0.869394728\n922   -1.052548303\n923    7.735766381\n924   -1.858632914\n925    1.113097838\n926   -0.653838040\n927   -0.363131151\n928    2.757841945\n929   -1.050550646\n930    0.333704168\n931   -0.519988076\n932    2.784028955\n933   -0.336139186\n934    0.328824510\n935   -0.793858728\n936    1.142554991\n937   -0.251327219\n938   -1.782530638\n939   -0.432279847\n940   -0.667963498\n941    0.453203165\n942   -1.027829292\n943    0.510713083\n944    0.302223440\n945   -0.682919997\n946    1.267671677\n947   -4.361014643\n948   -1.980776525\n949    0.389335928\n950   -0.024079309\n951   -7.178334583\n952    1.422483253\n953   -0.178124970\n954   -0.892870249\n955    0.053239863\n956   -4.218448310\n957    0.582503371\n958    0.858599622\n959    3.859424705\n960    1.273544431\n961    0.218920339\n962   -3.964303194\n963   -0.067487123\n964    3.040461061\n965    0.414046231\n966   -1.731130480\n967    0.595039185\n968   -0.789370576\n969    0.760666649\n970    0.603495502\n971   -0.647671226\n972   -0.406906433\n973    0.504575989\n974   -0.441102622\n975    0.204804348\n976    1.256872540\n977    0.073100559\n978   -0.152361811\n979    1.216515068\n980   -0.566578552\n981   -2.572576504\n982   -1.871178048\n983   -1.315115063\n984   -0.464006373\n985   -0.657201966\n986    0.513818033\n987   -0.038699190\n988    0.049218763\n989    0.666306475\n990    1.710078219\n991    1.308764161\n992    2.092958839\n993   -0.297879988\n994    1.699824920\n995   -0.561901059\n996   -0.569983374\n997   -2.062624183\n998    0.116585032\n999   -0.691662280\n1000   0.454501578\n1001   0.115681607\n1002  -0.088101861\n1003  -0.179487282\n1004  -0.048374434\n1005  -0.887888492\n1006   0.635667878\n1007   1.109505293\n1008   0.915217647\n1009  -0.484481384\n1010  -0.059942457\n1011  -0.851350746\n1012  -3.352807055\n1013  -0.062506323\n1014  -3.077742291\n1015   2.038985316\n1016   1.714390486\n1017  -0.365361959\n1018   0.821890973\n1019  -0.892618890\n1020  -1.165390718\n1021   0.949877146\n1022   2.778657780\n1023   0.443728775\n1024  -1.987553453\n1025   1.617540382\n1026   3.012009259\n1027   0.329400717\n1028   3.192548011\n1029  -0.601935849\n1030   0.207863082\n1031  -0.402755736\n1032   0.975270853\n1033   0.590699124\n1034   0.590344288\n1035  -0.694925060\n1036   1.280512240\n1037   0.320842610\n1038   0.879190555\n1039  -0.421247403\n1040   0.482409584\n1041  -0.173461502\n1042  -0.762309013\n1043   0.640210578\n1044   2.921763772\n1045   2.465518280\n1046  -0.394633962\n1047   0.013767253\n1048  -0.227148899\n1049  -1.437343875\n1050   0.854553718\n1051   1.444743214\n1052   0.352313934\n1053  -1.418960956\n1054  -0.433563044\n1055   0.213926802\n1056  -2.762004219\n1057  -1.970564368\n1058   0.784245562\n1059  -2.668064591\n1060  -1.839751324\n1061  -0.372832627\n1062   1.577134085\n1063  -1.534273992\n1064   1.384169832\n1065  -0.203847011\n1066  -0.160122769\n1067   0.412128639\n1068   1.194348530\n1069  -0.336802653\n1070   0.521225688\n1071  -1.209735063\n1072  -4.336767111\n1073   3.558754438\n1074   0.288635772\n1075   0.265339029\n1076  -0.771790420\n1077   1.870272455\n1078  -0.968482516\n1079   0.399774383\n1080  -1.595623724\n1081   2.107980908\n1082   0.509871763\n1083  -1.715073906\n1084  -4.236678577\n1085  -0.810134926\n1086  -0.174073493\n1087   0.483745461\n1088  -0.715191969\n1089   0.302479914\n1090   1.873513177\n1091  -0.762798444\n1092  -0.512772225\n1093   1.104466345\n1094  -1.177130801\n1095  -0.059396575\n1096   2.819139356\n1097  -0.466195794\n1098  -1.161766919\n1099   0.541721723\n1100   0.551086355\n1101  -0.410143789\n1102   0.142285532\n1103  -1.409158800\n1104  -1.999603948\n1105   0.383202262\n1106   0.252158976\n1107   0.848396573\n1108   5.290096585\n1109   0.268312814\n1110   4.203285976\n1111  -0.786918453\n1112   0.028217665\n1113  -0.824632477\n1114  -0.445547860\n1115   1.194786610\n1116  -0.877747130\n1117   1.531146314\n1118  -1.405574675\n1119  -0.095432631\n1120   0.471860656\n1121   0.175268260\n1122   1.099012154\n1123  -0.279112608\n1124   0.842098136\n1125   1.379772263\n1126  -1.226010809\n1127   1.264588931\n1128   0.688864301\n1129  -0.881439374\n1130  -2.218421802\n1131  -1.172365209\n1132  -1.773827177\n1133   0.649231874\n1134   1.912460841\n1135   0.839105311\n1136   1.976246914\n1137   0.798928381\n1138  -0.541835471\n1139   0.618860671\n1140  -0.072465710\n1141   0.103287755\n1142  -2.341294296\n1143   0.157198323\n1144  -0.394060017\n1145   0.261624806\n1146   1.170755719\n1147  -1.229135173\n1148   1.407054275\n1149   2.452136702\n1150  -0.934792613\n1151   1.110353751\n1152  -2.171629061\n1153   0.108007080\n1154  -0.410686302\n1155  -1.523732276\n1156   0.252399008\n1157   0.431287929\n1158   0.547242335\n1159   0.546819981\n1160  -0.095161123\n1161  -0.726619195\n1162  -0.860841670\n1163  -0.431344431\n1164   3.221572848\n1165   0.249040297\n1166  -0.081927285\n1167  -0.624354664\n1168  -0.394809412\n1169  -0.557736656\n1170   0.100340864\n1171   3.021686043\n1172   1.260570229\n1173  -0.410989305\n1174  -0.146871045\n1175   1.964353831\n1176  -2.500153444\n1177  -0.447536875\n1178  -2.574574866\n1179  -4.287548129\n1180   1.153457810\n1181   2.173666410\n1182  -0.519943099\n1183  -0.473815823\n1184   0.432745124\n1185  -2.281937336\n1186  -0.056261091\n1187  -0.007256448\n1188  -2.612554921\n1189   2.942839329\n1190   0.008701550\n1191   0.675950427\n1192  -0.324858423\n1193  -0.687838364\n1194   0.269435765\n1195   2.062511161\n1196   0.916646877\n1197  -0.421622496\n1198  -1.474024780\n1199  -0.299467592\n1200   0.551409461\n1201   1.084585807\n1202  -1.037964724\n1203   1.887821041\n1204   0.244311617\n1205  -0.342557943\n1206   0.076218510\n1207  -2.141643929\n1208   0.011344198\n1209  -0.208091283\n1210   0.499466700\n1211   0.352609206\n1212  -1.971065657\n1213  -0.118231244\n1214  -0.737973540\n1215   1.306761700\n1216  -1.060298655\n1217  -1.109264984\n1218   1.848097802\n1219   1.341300964\n1220  -0.327415139\n1221   0.711614165\n1222  -0.964588141\n1223   1.747049360\n1224  -0.684578675\n1225   0.606712182\n1226  -0.396094186\n1227   2.094981879\n1228   3.738627328\n1229  -0.048426414\n1230   0.978287949\n1231  -1.738942614\n1232  -2.678693719\n1233   1.991243173\n1234  -0.075896678\n1235   1.861303762\n1236   0.279789378\n1237  -0.704633114\n1238  -2.245840330\n1239  -0.491596345\n1240   1.350821063\n1241   1.159268941\n1242   2.563835474\n1243  -0.316994459\n1244   0.131036611\n1245  -0.816719847\n1246  -0.519524394\n1247  -1.123900063\n1248   1.796256766\n1249  -2.398445781\n1250   2.224808670\n1251   0.668688472\n1252   0.133588247\n1253   2.321659262\n1254  -0.833244563\n1255   6.647704218\n1256  -0.081147508\n1257   0.309002663\n1258  -2.555130980\n1259  -0.633583294\n1260  -0.330585206\n1261   0.493718836\n1262  -0.552787196\n1263   0.741720135\n1264   0.196605577\n1265  -2.125804693\n1266  -1.779726127\n1267   0.579019979\n1268   0.291791195\n1269   0.161875521\n1270   0.212720644\n1271  -1.134643593\n1272   0.681981061\n1273   0.498504138\n1274  -1.386284271\n1275  -0.220590580\n1276   1.487585710\n1277   0.537019055\n1278   4.451643014\n1279   0.770233782\n1280  -0.758778647\n1281  -1.786389883\n1282   0.417687649\n1283  -1.664440526\n1284   1.122732640\n1285  -0.452907306\n1286   0.152293053\n1287   1.933638283\n1288  -0.097661837\n1289   0.809181211\n1290   0.051716281\n1291  -1.233689147\n1292  -0.049879862\n1293   1.028282129\n1294  -1.294527592\n1295  -0.469395574\n1296  -7.721252513\n1297  -0.330432885\n1298  -1.106866776\n1299   0.399146461\n1300   0.477407917\n1301  -2.745928602\n1302   2.059424546\n1303  -0.235986960\n1304  -1.394616728\n1305   0.853551350\n1306  -0.719213021\n1307   0.036203143\n1308   0.592916761\n1309  -2.768588911\n1310  -1.313387893\n1311  -2.952215023\n1312   0.830775706\n1313  -1.441512502\n1314  -0.663934636\n1315   0.927809448\n1316  -0.383536835\n1317  -0.129197527\n1318  -0.033924310\n1319  -7.169829889\n1320   0.007148680\n1321   2.638155643\n1322   1.860135094\n1323   0.021305769\n1324   1.521029847\n1325   1.999452646\n1326  -1.157030579\n1327   0.756898977\n1328   1.065187461\n1329  -2.470330068\n1330  -0.698741193\n1331  -0.801165260\n1332  -0.798597179\n1333  -0.629974599\n1334  -1.143161002\n1335  -0.182075853\n1336  -2.338966459\n1337   0.066622219\n1338  -0.690948538\n1339  -0.470581019\n1340  -0.347169990\n1341  -1.957197143\n1342  -1.233320257\n1343   0.801232172\n1344   1.200831630\n1345  -0.390554845\n1346   0.469616780\n1347   1.782332491\n1348  -2.149798084\n1349  -2.613294156\n1350   0.438782481\n1351   1.139382762\n1352  -0.308855219\n1353   0.999896372\n1354   0.314012020\n1355   2.463681804\n1356   2.215526503\n1357   0.386637491\n1358  -0.358231248\n1359  -0.325300248\n1360  -2.022475852\n1361  -1.001495535\n1362  -0.816259532\n1363   0.521460410\n1364  -0.297710762\n1365   1.576904130\n1366   0.534457372\n1367  -0.720381551\n1368  -0.101406070\n1369  -1.039553163\n1370  -1.173355442\n1371   0.369268619\n1372  -1.301283563\n1373  -0.050649282\n1374  -0.687560101\n1375   1.527027773\n1376  -1.194595115\n1377   1.150533620\n1378   0.287574264\n1379   0.626507651\n1380   0.968699197\n1381   1.572480545\n1382  -0.728840817\n1383   2.159037325\n1384  -0.667439741\n1385  -0.602737372\n1386   0.952528504\n1387   1.936817690\n1388  -0.987760178\n1389   1.178225379\n1390   3.077060534\n1391  -1.053587017\n1392   0.807477552\n1393  -0.890167424\n1394  -0.811802927\n1395   0.417211818\n1396  -1.407006337\n1397  -0.780232333\n1398  -0.381211875\n1399  -3.201664166\n1400  -1.108139876\n1401  -0.325111693\n1402   0.759960002\n1403  -0.327380083\n1404   5.274185714\n1405  -0.833327398\n1406   2.503631589\n1407   0.460560479\n1408  -0.935272631\n1409  -0.345666893\n1410   0.430683949\n1411   0.696055383\n1412   3.843824227\n1413  -0.121868072\n1414  -0.490721075\n1415   0.926346776\n1416  -0.461519136\n1417   0.708146691\n1418   0.503874891\n1419   0.422430471\n1420  -1.231466554\n1421   1.218929365\n1422   4.288494018\n1423  -0.516789511\n1424  -0.032704246\n1425   0.180499676\n1426   0.080421906\n1427   0.318356439\n1428   0.174124621\n1429  -0.324119009\n1430   0.321239852\n1431   0.770467073\n1432  -3.011207623\n1433   0.090665245\n1434   0.300468577\n1435  -0.604616867\n1436  -0.707905275\n1437  -0.261749622\n1438   0.406102311\n1439  -0.302461886\n1440   1.382221602\n1441   0.017695113\n1442   3.555920752\n1443  -1.868913101\n1444   0.500642572\n1445  -0.076810316\n1446  -1.360614021\n1447  -0.702102041\n1448  -0.017093657\n1449   0.483282102\n1450  -0.334988697\n1451  -0.927678205\n1452  -0.066942973\n1453   0.057608258\n1454   1.190272598\n1455  -1.461951961\n1456   4.419592157\n1457  -1.042666916\n1458  -1.371622876\n1459   2.079175996\n1460  -0.873932770\n1461   0.871692904\n1462   0.209582761\n1463  -0.911768871\n1464  -0.118634663\n1465  -1.193339533\n1466  -1.545605258\n1467  -0.468149352\n1468  -1.697889179\n1469   0.661741562\n1470   0.612425714\n1471  -0.594900022\n1472  -0.641563664\n1473  -0.851446174\n1474  -0.183969459\n1475   0.824904247\n1476  -0.554708352\n1477   0.720919778\n1478  -1.168043785\n1479  -0.328803749\n1480  -0.197667699\n1481  -2.414323067\n1482   0.462409501\n1483  -0.962574080\n1484  -0.020550655\n1485   2.367209356\n1486   0.158580545\n1487  -0.091190936\n1488  -1.076725631\n1489   0.032262636\n1490  -0.711142844\n1491  -0.455510585\n1492   1.098242092\n1493  -0.059830299\n1494  -0.611522224\n1495  -0.626424025\n1496   1.486783900\n1497   1.595967258\n1498  -0.888434140\n1499  -0.266378633\n1500  -0.939822603\n1501   2.589398642\n1502  -0.218828040\n1503   0.563434027\n1504   0.434432006\n1505   0.262213953\n1506  -3.118741247\n1507  -0.617500114\n1508   0.581591939\n1509  -0.276759620\n1510   0.288052321\n1511  -1.598773475\n1512   0.021832099\n1513  -6.974078864\n1514   0.486744176\n1515   0.636557801\n1516   0.392121118\n1517   0.517205996\n1518  -0.538942525\n1519  -0.215029092\n1520   1.416198851\n1521   1.626127373\n1522   0.158949634\n1523   4.549209452\n1524  -0.902323383\n1525  -0.601068188\n1526  -1.388538512\n1527   1.554620950\n1528   0.364235521\n1529   1.002223331\n1530  -1.030499393\n1531  -1.006627222\n1532   2.089119117\n1533   1.178268951\n1534   2.602879637\n1535  -0.419353359\n1536   1.181689843\n1537   0.348529141\n1538   4.254783630\n1539  -1.137243337\n1540   0.408030834\n1541  -0.583707352\n1542  -1.151355186\n1543   1.358954598\n1544  -1.147339306\n1545  -0.472154839\n1546   0.725269370\n1547  -0.794886721\n1548  -0.447723960\n1549   0.109899936\n1550   0.709707248\n1551   1.138930354\n1552  -0.507806136\n1553  -2.214779536\n1554   1.288584567\n1555   0.721578976\n1556  -0.367826188\n1557   0.139879213\n1558   4.781695259\n1559  -1.016720590\n1560  -0.432739357\n1561  -1.077164801\n1562   2.540890638\n1563   0.689251719\n1564  -1.013459415\n1565  -2.515843294\n1566  -0.673855328\n1567  -0.375476789\n1568  -0.916219044\n1569   1.549304588\n1570   1.360792750\n1571   0.843166673\n1572  -0.558579907\n1573  -0.084642378\n1574   0.439714247\n1575   1.523576748\n1576   0.145536798\n1577  -0.875930356\n1578   0.842339344\n1579  -3.171521827\n1580  -3.692743737\n1581  -0.400794562\n1582   1.911938625\n1583  -0.566976032\n1584  -0.968506736\n1585  -1.115103942\n1586   0.145175659\n1587  -0.984834947\n1588  -1.305448618\n1589   3.295349848\n1590  -1.165658689\n1591  -1.845432609\n1592   0.170522717\n1593  -0.363562190\n1594  -0.168452528\n1595   1.698956155\n1596  -1.386215391\n1597  -1.489997078\n1598  -0.814450078\n1599  -1.014306255\n1600   1.013378952\n1601   0.351210846\n1602  -1.469309772\n1603  -2.843906663\n1604  -0.451553048\n1605  -0.437467998\n1606  -0.661090971\n1607  -2.364554960\n1608  -3.947712307\n1609   0.372874967\n1610  -0.817729561\n1611  -2.444505852\n1612   1.831984089\n1613  -0.644249182\n1614   0.787011605\n1615   1.959075243\n1616   1.686181224\n1617   1.278091026\n1618  -0.566425596\n1619  -0.101294954\n1620   0.349554990\n1621  -0.272791347\n1622   1.763222216\n1623  -1.297241599\n1624  -0.282142273\n1625   3.369303210\n1626   0.038739340\n1627   0.372240615\n1628   2.176687667\n1629   0.966583562\n1630   0.294144531\n1631  -0.924339801\n1632  -0.805942341\n1633   0.721619147\n1634  -0.355998391\n1635   0.818389503\n1636  -0.699578508\n1637   0.387726348\n1638   1.463883367\n1639  -0.245300158\n1640  -0.218009542\n1641   3.244028578\n1642  -0.680401009\n1643  -0.936290709\n1644  -0.512382706\n1645   1.086573712\n1646  -1.093709977\n1647   0.729652289\n1648   0.548847371\n1649   1.037099580\n1650  -0.396714115\n1651   2.791648679\n1652  -0.805443037\n1653  -0.584678755\n1654  -0.356144843\n1655  -0.404034530\n1656   1.359927361\n1657  -0.495495218\n1658  -1.240287121\n1659  -0.082211339\n1660  -1.188018749\n1661  -2.223184727\n1662   0.705587014\n1663  -0.848632473\n1664  -2.613258924\n1665  -0.863908222\n1666  -2.107749753\n1667   2.082153516\n1668   1.496670703\n1669   0.016416946\n1670   1.014578005\n1671  -0.361644011\n1672   0.247235364\n1673   1.144823453\n1674  -0.047697451\n1675   0.455343948\n1676   0.994593364\n1677  -0.822444222\n1678  -0.244816328\n1679  -1.082771869\n1680   0.747409305\n1681  -0.428650753\n1682  -0.169425334\n1683   1.605816199\n1684   0.449971184\n1685   0.730435284\n1686   1.847506343\n1687  -0.206396757\n1688   0.380880583\n1689   0.818313605\n1690  -0.408848628\n1691  -0.515786900\n1692   0.974370595\n1693  -0.133150873\n1694   1.398333843\n1695  -1.361151145\n1696   0.433309662\n1697  -0.946376931\n1698  -0.670063632\n1699   1.676048959\n1700  -0.140611177\n1701   0.053654636\n1702   1.259689693\n1703  -5.174206131\n1704   0.788702296\n1705  -1.993087093\n1706   0.397864475\n1707   2.134884681\n1708  -0.710201299\n1709   2.996060042\n1710   0.510889890\n1711   0.054068572\n1712   0.605433933\n1713   0.347134535\n1714   1.103668504\n1715   1.103076166\n1716   1.332205225\n1717   0.423082535\n1718  -1.625596444\n1719  -1.554203022\n1720   0.006527303\n1721   0.053696296\n1722  -1.561823405\n1723   0.207694829\n1724   1.301721385\n1725   0.603758316\n1726  -2.775142964\n1727   0.063536743\n1728   2.740397766\n1729  -2.752915518\n1730   0.822732164\n1731  -0.980567935\n1732   3.973534763\n1733  -0.740899772\n1734   1.420636878\n1735  -1.333517659\n1736  -0.706797886\n1737  -0.709147617\n1738   3.371441854\n1739   1.005492756\n1740  -3.541571056\n1741  -1.439834921\n1742   3.286784985\n1743  -0.122735530\n1744  -0.437715190\n1745   3.251385190\n1746  -0.593354656\n1747  -1.079917550\n1748   0.606761232\n1749  -1.127159142\n1750   2.358211611\n1751   0.763686667\n1752   1.110251032\n1753  -1.492509083\n1754  -1.241463822\n1755   4.439832289\n1756   2.554971740\n1757   0.660895643\n1758   0.123687788\n1759   1.333725257\n1760   4.152832797\n1761   1.217302777\n1762   1.656895371\n1763   0.353317077\n1764  -0.657602012\n1765  -0.381770876\n1766   0.187400308\n1767   1.939343087\n1768   0.210374661\n1769  -2.345500420\n1770  -0.874157596\n1771   0.540670356\n1772   0.112802661\n1773  -2.648256979\n1774   0.597568786\n1775  -0.137426550\n1776   3.516064434\n1777   0.102408252\n1778   0.776033821\n1779   0.930709076\n1780   5.220574704\n1781   0.736020501\n1782  -0.990894962\n1783  -0.274559644\n1784  -1.016884505\n1785  -0.221887192\n1786  -0.445992770\n1787   0.475688115\n1788   0.785786694\n1789  -0.130032635\n1790  -0.394688042\n1791  -2.323386527\n1792   0.514375139\n1793   1.492241939\n1794   0.327791984\n1795  -0.075720368\n1796   0.514881334\n1797  -1.119208961\n1798  -0.180152878\n1799   0.637308878\n1800   3.964044307\n1801  -0.144384160\n1802   1.487932212\n1803  -0.566635527\n1804  -1.139370142\n1805  -3.086612508\n1806   0.862030400\n1807   0.474449333\n1808   0.961474292\n1809  -0.538548656\n1810   0.017726335\n1811  -1.138437401\n1812   0.121364311\n1813  -0.978068660\n1814   0.283660468\n1815   0.242053638\n1816  -0.117018330\n1817   0.540950519\n1818   1.580644887\n1819   1.028931010\n1820   1.015550193\n1821   1.196996138\n1822   0.230669296\n1823   0.031274355\n1824  -0.707303604\n1825  -1.142676757\n1826   1.804785405\n1827   0.112926949\n1828   0.477232896\n1829  -0.476903681\n1830  -0.692818107\n1831   1.332466553\n1832   2.318784256\n1833   1.184052989\n1834   1.141068630\n1835   0.167916703\n1836  -1.116243275\n1837  -0.045689694\n1838   0.596004263\n1839  -0.748392267\n1840  -0.060920315\n1841  -1.444313228\n1842  -0.044715427\n1843  -0.056960004\n1844   1.151901771\n1845  -0.174865186\n1846   0.545593634\n1847  -0.692471122\n1848  -0.734818390\n1849   1.457787809\n1850   0.875233226\n1851   0.391506603\n1852   1.740417860\n1853  -0.388065238\n1854  -0.877747675\n1855   0.284135482\n1856   0.111826285\n1857   0.815318224\n1858  -0.140032745\n1859  -1.361405539\n1860  -0.758963912\n1861   0.360491065\n1862  -0.205572385\n1863  -0.363727621\n1864   1.604171479\n1865  -0.120997962\n1866  -0.766683547\n1867   0.468191113\n1868  -1.837601301\n1869   1.415300784\n1870  -1.098654854\n1871   0.035359762\n1872   0.156320433\n1873   1.539551984\n1874   0.266961864\n1875   1.352917387\n1876  -0.404440536\n1877   1.808759952\n1878  -1.881284209\n1879  -0.549492991\n1880   2.526688917\n1881   0.228924017\n1882   0.513811303\n1883   1.017006255\n1884  -0.742499144\n1885  -0.140586012\n1886  -0.053718530\n1887   0.803828055\n1888   0.048360449\n1889  -0.215828947\n1890  -0.058291264\n1891   0.864983841\n1892  -1.356170107\n1893  -0.617262864\n1894  -1.402265309\n1895  -0.523441459\n1896   0.830853039\n1897   0.317281478\n1898   0.084830762\n1899   2.121363127\n1900   0.121462979\n1901   0.834729191\n1902   0.040652843\n1903   0.722788277\n1904  -0.747640271\n1905   0.387297140\n1906  -0.812770956\n1907   1.454741416\n1908   0.620606741\n1909   0.833451137\n1910  -1.683346033\n1911   0.804701034\n1912  -0.229263120\n1913   0.046194911\n1914  -0.166435185\n1915  -1.112652661\n1916  -1.073200728\n1917  -0.046565310\n1918   5.696117906\n1919  -0.290236383\n1920   1.207304711\n1921  -0.762685782\n1922  -1.497926637\n1923  -0.822479149\n1924   1.052504492\n1925   1.198638323\n1926  -0.126984105\n1927  -2.196066627\n1928   2.821882676\n1929   0.888531536\n1930   1.030936408\n1931   0.557465035\n1932   0.289026047\n1933  -0.709589288\n1934   1.018615649\n1935   1.014718518\n1936   0.118878497\n1937  -2.353307515\n1938  -0.463709158\n1939  -3.089588325\n1940  -2.124134818\n1941  -3.397232314\n1942   0.910430585\n1943  -1.056790935\n1944  -0.262030547\n1945   0.607755870\n1946   0.403856235\n1947   1.412269952\n1948   0.422769816\n1949  -0.005290671\n1950  -0.361825063\n1951   2.228995584\n1952  -0.093855139\n1953   0.088769126\n1954   2.985708776\n1955  -1.616981175\n1956  -0.814294262\n1957  -0.579969780\n1958  -0.532228413\n1959   0.475891817\n1960  -0.028348796\n1961  -0.097690038\n1962  -1.338162601\n1963  -1.294586067\n1964   0.687677162\n1965  -0.201650989\n1966  -0.658662267\n1967  -0.364505858\n1968  -0.822221317\n1969   3.268173150\n1970  -4.967636498\n1971  -0.584376271\n1972  -1.161526012\n1973  -0.244878422\n1974   3.032321344\n1975  -1.812160139\n1976  -1.261326720\n1977  -2.309825696\n1978   0.131785814\n1979  -0.512137299\n1980  -2.212688313\n1981  -0.833872274\n1982   0.185610652\n1983  -0.141494928\n1984   0.109487405\n1985   0.089989645\n1986   0.668121661\n1987  -0.430441702\n1988   0.792453656\n1989  -1.400129839\n1990  -0.215107105\n1991  -0.085294745\n1992   0.437635054\n1993   1.414558604\n1994  -1.470842044\n1995   0.204152049\n1996  -0.603812902\n1997   0.788499060\n1998   0.489937346\n1999  -1.605398619\n2000   0.409543307\n\n\nLet’s plot our simulated t scores alongside a normal distribution.\n\n# Don't worry about the syntax here.\n# You won't need to know how to do this on your own.\nggplot(sims_t, aes(x = sim_t)) +\n    geom_histogram(aes(y = ..density..), binwidth = 0.25) +\n    scale_x_continuous(limits = c(-5, 5),\n                       breaks = c(-4, -3, -2, -1, 0, 1, 2, 3, 4)) +\n    stat_function(fun = dnorm, args = list(mean = 0, sd = 1),\n                  color = \"red\", size = 1.5)\n\nWarning: Removed 19 rows containing non-finite values (`stat_bin()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\nThese t scores are somewhat close to the normal model we had when we knew \\(\\sigma\\), but the fit doesn’t look quite right. The peak of the simulated values isn’t quite high enough, and the tails seem to spill out over the much thinner tails of the normal model.\nWilliam Gosset figured this all out in the early 20th century. While working for the Guinness brewery in Dublin, Ireland, he started noticing that his quality control tests (using very small sample sizes) didn’t yield statistical results consistent with the normal models that were universally used at the time. At the encouragement of the company, which saw his work as a potential source of cost savings, he took some time off to study and consult with other statisticians. As a result, he found a new function that is similar to a normal distribution but is more spread out. This new function accounts for the extra variability one gets when using the sample standard deviation \\(s\\) as an estimate for the true population standard deviation \\(\\sigma\\). Guinness considered the result a “trade secret”, so they wouldn’t allow Gosset to publish under his own name. But they did permit him to publish his findings under the pseudonym “Student”. He used data sets unrelated to brewing and submitted his work to the top statistical journal of the time.\nThe new function Gosset discovered became known as the Student t distribution. He realized that the spread of the t distribution depends on the sample size. This makes sense: the accuracy of \\(s\\) will be greater when we have a larger sample. In fact, for large enough samples, the t distribution is very close to a normal model.\nGosset used the term degrees of freedom to describe how the sample size influences the spread of the t distribution. It’s somewhat mathematical and technical, so suffice it to say here that the number of degrees of freedom is simply the sample size minus 1:\n\\[\ndf = n - 1.\n\\]\nSo is the t model correct for our simulated t scores? Our sample size was 4, so we should use a t model with 3 degrees of freedom. Let’s plot it in green on top of our previous graph and see:\n\n# Don't worry about the syntax here.\n# You won't need to know how to do this on your own.\nggplot(sims_t, aes(x = sim_t)) +\n    geom_histogram(aes(y = ..density..), binwidth = 0.25) +\n    scale_x_continuous(limits = c(-5, 5),\n                       breaks = c(-4, -3, -2, -1, 0, 1, 2, 3, 4)) +\n    stat_function(fun = dnorm, args = list(mean = 0, sd = 1),\n                  color = \"red\", size = 1.5) +\n    stat_function(fun = dt, args = list(df = 3),\n                  color = \"green\", size = 1.5)\n\nWarning: Removed 19 rows containing non-finite values (`stat_bin()`).\n\n\nWarning: Removed 2 rows containing missing values (`geom_bar()`).\n\n\n\n\n\nThe green curve fits the simulated values much better."
  },
  {
    "objectID": "19-inference_for_one_mean-web.html#inference-for-one-mean",
    "href": "19-inference_for_one_mean-web.html#inference-for-one-mean",
    "title": "19  Inference for one mean",
    "section": "19.6 Inference for one mean",
    "text": "19.6 Inference for one mean\nWhen we have a single numerical variable, we can ask if the sample mean is consistent or not with a null hypothesis. We will use a t model for our sampling distribution model as long as certain conditions are met.\nOne of the assumptions we made in the simulation above was that the true population was normally distributed. In general, we have no way of knowing if this is true. So instead we check the nearly normal condition: if a histogram or QQ plot of our data shows that the data is nearly normal, then there is a reasonable assumption that the whole population is shaped the same way.\nIf our sample size is large enough, the central limit theorem tells us that the sampling distribution gets closer and closer to a normal model. Therefore, we’ll use a rule of thumb that says that if the sample size is greater than 30, we won’t worry too much about any deviations from normality in the data.\nThe number 30 is somewhat arbitrary. If the sample size is 25 and a histogram shows only a little skewness, we’re probably okay. But if the sample size is 10, we need for the data to be very normal to justify using the t model. The irony, of course, is that small sample sizes are the hardest to check for normality. We’ll have to use our best judgment."
  },
  {
    "objectID": "19-inference_for_one_mean-web.html#outliers",
    "href": "19-inference_for_one_mean-web.html#outliers",
    "title": "19  Inference for one mean",
    "section": "19.7 Outliers",
    "text": "19.7 Outliers\nWe also need to be on the lookout for outliers. We’ve seen before that outliers can have a huge effect on means and standard deviations, especially when sample sizes are small. Whenever we find an outlier, we need to investigate.\nSome outliers are mistakes. Perhaps someone entered data incorrectly into the computer. When it’s clear that outliers are data entry errors, we are free to either correct them (if we know what error was made) or delete them from our data completely.\nSome outliers are not necessarily mistakes, but should be excluded for other reasons. For example, if we are studying the weight of birds and we have sampled a bunch of hummingbirds and one emu, the emu’s weight will appear as an outlier. It’s not that its weight is “wrong”, but it clearly doesn’t belong in the analysis.\nIn general, though, outliers are real data that just happen to be unusual. It’s not ethical simply to throw away such data points because they are inconvenient. (We only do so in very narrow and well-justified circumstances like the emu.) The best policy to follow when faced with such outliers is to run inference twice—once with the outlier included, and once with the outlier excluded. If, when running a hypothesis test, the conclusion is the same either way, then the outlier wasn’t all that influential, so we leave it in. If, when computing a confidence interval, the endpoints don’t change a lot either way, then we leave the outlier in. However, when conclusions or intervals are dramatically different depending on whether the outlier was in or out, then we have no choice but to state that honestly."
  },
  {
    "objectID": "19-inference_for_one_mean-web.html#research-question",
    "href": "19-inference_for_one_mean-web.html#research-question",
    "title": "19  Inference for one mean",
    "section": "19.8 Research question",
    "text": "19.8 Research question\nThe teacher data from the openintro package contains information on 71 teachers employed by the St. Louis Public School in Michigan. According to Google, the average teacher salary in Michigan was $63,024 in 2010. So does this data suggest that the teachers in the St. Louis region of Michigan are paid differently than teachers in other parts of Michigan?\nLet’s walk through the rubric."
  },
  {
    "objectID": "19-inference_for_one_mean-web.html#exploratory-data-analysis",
    "href": "19-inference_for_one_mean-web.html#exploratory-data-analysis",
    "title": "19  Inference for one mean",
    "section": "19.9 Exploratory data analysis",
    "text": "19.9 Exploratory data analysis\n\n19.9.1 Use data documentation (help files, code books, Google, etc.) to determine as much as possible about the data provenance and structure.\nYou should type ?teacher at the Console to read the help file. Unfortunately, the help file does not give us a lot of information about how the data was collected. The only source listed is a website that no longer contains this data set. Besides, that website is just an open repository for data, so it’s not clear that the site would have contained any additional information about the provenance of the data. We will have to assume that the data was collected accurately.\nHere is the data set:\n\nteacher\n\n# A tibble: 71 × 8\n   id    degree fte   years  base  fica retirement  total\n * &lt;fct&gt; &lt;fct&gt;  &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1 01    BA     1       5   45388 3472.      7689. 56549.\n 2 02    MA     1      15   60649 4640.     10274. 75563.\n 3 03    MA     1      16   60649 4640.     10274. 75563.\n 4 04    BA     1      10   54466 4167.      9227. 67859.\n 5 05    BA     1      26   65360 5000.     11072. 81432.\n 6 06    BA     1      28.5 65360 5000.     11072. 81432.\n 7 07    BA     1      12   58097 4444.      9842. 72383.\n 8 08    MA     1      32   68230 5220.     11558. 85008.\n 9 09    BA     1      25   65360 5000.     11072. 81432.\n10 11    BA     1      12   58097 4444.      9842. 72383.\n# ℹ 61 more rows\n\n\n\nglimpse(teacher)\n\nRows: 71\nColumns: 8\n$ id         &lt;fct&gt; 01, 02, 03, 04, 05, 06, 07, 08, 09, 11, 12, 13, 14, 15, 16,…\n$ degree     &lt;fct&gt; BA, MA, MA, BA, BA, BA, BA, MA, BA, BA, BA, BA, BA, BA, MA,…\n$ fte        &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ years      &lt;dbl&gt; 5.0, 15.0, 16.0, 10.0, 26.0, 28.5, 12.0, 32.0, 25.0, 12.0, …\n$ base       &lt;int&gt; 45388, 60649, 60649, 54466, 65360, 65360, 58097, 68230, 653…\n$ fica       &lt;dbl&gt; 3472.18, 4639.65, 4639.65, 4166.65, 5000.04, 5000.04, 4444.…\n$ retirement &lt;dbl&gt; 7688.73, 10273.94, 10273.94, 9226.54, 11071.98, 11071.98, 9…\n$ total      &lt;dbl&gt; 56548.91, 75562.59, 75562.59, 67859.19, 81432.02, 81432.02,…\n\n\nSince total is a numerical variable, we can use the summary function to produce the five-number summary. (The function also reports the mean.)\n\nsummary(teacher$total)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24793   63758   74647   70289   81432   85008 \n\n\n\n\n19.9.2 Prepare the data for analysis.\nNot necessary here, but see the next section to find out what we do when we discover an outlier.\n\n\n19.9.3 Make tables or plots to explore the data visually.\nHere is a histogram.\n\nggplot(teacher, aes(x = total)) +\n    geom_histogram(binwidth = 5000, boundary = 60000)\n\n\n\n\nAnd here is a QQ plot.\n\nggplot(teacher, aes(sample = total)) +\n    geom_qq() +\n    geom_qq_line()\n\n\n\n\nThis distribution is quite skewed to the left. Of even more concern is the extreme outlier on the left.\nWith any outlier, we need to investigate.\n\nExercise 1\nLet’s sort the data by total (ascending) using the arrange command.\n\nteacher %&gt;%\n    arrange(total)\n\n# A tibble: 71 × 8\n   id    degree fte   years  base  fica retirement  total\n   &lt;fct&gt; &lt;fct&gt;  &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;\n 1 37    MA     0.5       1 19900 1522.      3371. 24793.\n 2 12    BA     1         0 35427 2710.      6001. 44138.\n 3 57    BA     1         0 35427 2710.      6001. 44138.\n 4 41    BA     1         1 37199 2846.      6302. 46346.\n 5 69    BA     1         2 38968 2981.      6601. 48550.\n 6 48    BA     1         3 40739 3117.      6901. 50757.\n 7 54    BA     1         3 40739 3117.      6901. 50757.\n 8 38    MA     1         2 41695 3190.      7063. 51948.\n 9 15    BA     1         4 43575 3333.      7382. 54290.\n10 39    MA     1         3 43593 3335.      7385. 54313.\n# ℹ 61 more rows\n\n\nCan you figure out why the person with the lowest total salary is different from all the other teachers?\n\nPlease write up your answer here.\n\n\nBased on your answer to the above exercise, hopefully it’s clear that this is an outlier for which we can easily justify exclusion. We can use the filter command to get only the rows we want. There are lots of ways to do this, but it’s easy enough to grab only salaries above $30,000. (There’s only one salary below $30,000, so that outlier will be excluded.)\nCAUTION: If you are copying and pasting from this example to use for another research question, the following code chuck is specific to this research question and not applicable in other contexts.\n\nteacher2 &lt;- teacher %&gt;%\n    filter(total &gt; 30000)\n\nCheck to make sure this had the desired effect:\n\nsummary(teacher2$total)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  44139   63758   74647   70939   81432   85008 \n\n\nNotice how the min is no longer $24,793.41.\nHere are the new plots:\n\nggplot(teacher2, aes(x = total)) +\n    geom_histogram(binwidth = 5000, boundary = 60000)\n\n\n\n\n\nggplot(teacher2, aes(sample = total)) +\n    geom_qq() +\n    geom_qq_line()\n\n\n\n\nThe left skew is still present, but we have removed the outlier."
  },
  {
    "objectID": "19-inference_for_one_mean-web.html#hypotheses",
    "href": "19-inference_for_one_mean-web.html#hypotheses",
    "title": "19  Inference for one mean",
    "section": "19.10 Hypotheses",
    "text": "19.10 Hypotheses\n\n19.10.1 Identify the sample (or samples) and a reasonable population (or populations) of interest.\nThe sample consists of 70 teachers employed by the St. Louis Public School in Michigan. We are using these 70 teachers as a hopefully representative sample of all teachers in that region of Michigan.\n\n\n19.10.2 Express the null and alternative hypotheses as contextually meaningful full sentences.\n\\(H_{0}:\\) Teachers in the St. Louis region earn $63,024 on average. (In other words, these teachers are the same as the teachers anywhere else in Michigan.)\n\\(H_{A}:\\) Teachers in the St. Louis region do not earn $63,024 on average. (In other words, these teachers are not the same as the teachers anywhere else in Michigan.)\n\n\n19.10.3 Express the null and alternative hypotheses in symbols (when possible).\n\\(H_0: \\mu = 63024\\)\n\\(H_A: \\mu \\neq 63024\\)"
  },
  {
    "objectID": "19-inference_for_one_mean-web.html#model",
    "href": "19-inference_for_one_mean-web.html#model",
    "title": "19  Inference for one mean",
    "section": "19.11 Model",
    "text": "19.11 Model\n\n19.11.1 Identify the sampling distribution model.\nWe will use a t model with 69 degrees of freedom.\nCommentary: The original teacher data had 71 observations. The teacher2 data has only 70 observations because we removed an outlier. Therefore \\(n = 70\\) and thus \\(df = n - 1 = 69\\).\n\n\n19.11.2 Check the relevant conditions to ensure that model assumptions are met.\n\nRandom\n\nWe know this isn’t a random sample. We’re not sure if this school is representative of other schools in the region, so we’ll proceed with caution.\n\n10%\n\nThis is also suspect, as it’s not clear that there are 700 teachers in the region. One way to look at it is this: if there are 10 or more schools in the region, and all the school are about the size of the St. Louis Public School under consideration, then we should be okay.\n\nNearly Normal\n\nFor this, we note that the sample size is much larger than 30, so we should be okay, even with the skewness in the data."
  },
  {
    "objectID": "19-inference_for_one_mean-web.html#mechanics",
    "href": "19-inference_for_one_mean-web.html#mechanics",
    "title": "19  Inference for one mean",
    "section": "19.12 Mechanics",
    "text": "19.12 Mechanics\n\n19.12.1 Compute the test statistic.\n\ntotal_mean &lt;- teacher2 %&gt;%\n  specify(response = total) %&gt;%\n  calculate(stat = \"mean\")\ntotal_mean\n\nResponse: total (numeric)\n# A tibble: 1 × 1\n    stat\n   &lt;dbl&gt;\n1 70939.\n\n\n\ntotal_t &lt;- teacher2 %&gt;%\n  specify(response = total) %&gt;%\n  hypothesize(null = \"point\", mu = 63024) %&gt;%\n  calculate(stat = \"t\")\ntotal_t\n\nResponse: total (numeric)\nNull Hypothesis: point\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  5.89\n\n\n\n\n19.12.2 Report the test statistic in context (when possible).\nThe sample mean is $70938.5725714.\nThe t score is 5.886253. The mean teacher salary in our sample is almost 6 standard errors to the right of the null value.\n\n\n19.12.3 Plot the null distribution.\n\ntotal_test &lt;- teacher2 %&gt;%\n  specify(response = total) %&gt;%\n  assume(\"t\")\ntotal_test\n\nA T distribution with 69 degrees of freedom.\n\n\n\ntotal_test %&gt;%\n  visualize() +\n  shade_p_value(obs_stat = total_t, direction = \"two-sided\")\n\n\n\n\nCommentary: Although we are conducting a two-sided test, the area in the tails is so small that it can’t really be seen in the picture above.\n\n\n19.12.4 Calculate the P-value.\n\ntotal_test_p &lt;- total_test %&gt;%\n  get_p_value(obs_stat = total_t, direction = \"two-sided\")\ntotal_test_p\n\n# A tibble: 1 × 1\n      p_value\n        &lt;dbl&gt;\n1 0.000000129\n\n\n\n\n19.12.5 Interpret the P-value as a probability given the null.\n\\(P &lt; 0.001\\). If teachers in the St. Louis region truly earned $63,024 on average, there would be only a 0.0000129% chance of seeing data at least as extreme as what we saw.\nCommentary: When the P-value is this small, remember that it is traditional to report simply \\(P &lt; 0.001\\)."
  },
  {
    "objectID": "19-inference_for_one_mean-web.html#conclusion",
    "href": "19-inference_for_one_mean-web.html#conclusion",
    "title": "19  Inference for one mean",
    "section": "19.13 Conclusion",
    "text": "19.13 Conclusion\n\n19.13.1 State the statistical conclusion.\nWe reject the null hypothesis.\n\n\n19.13.2 State (but do not overstate) a contextually meaningful conclusion.\nThere is sufficient evidence that teachers in the St. Louis region do not earn $63,024 on average.\n\n\n19.13.3 Express reservations or uncertainty about the generalizability of the conclusion.\nBecause we do not know how this data was collected (was it every teacher in this region? was it a sample of some of the teachers? was it a representative sample?), we do not know if we can generalize it to all teachers in the region. Also, the data set was from 2010, so we know that this data cannot be applied to teachers in St. Louis, Michigan now.\n\n\n19.13.4 Identify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.\nIf we’ve made a Type I error, then the truth is that teachers in this region do make around $63,024 on average, but our sample was way off."
  },
  {
    "objectID": "19-inference_for_one_mean-web.html#confidence-interval",
    "href": "19-inference_for_one_mean-web.html#confidence-interval",
    "title": "19  Inference for one mean",
    "section": "19.14 Confidence interval",
    "text": "19.14 Confidence interval\n\n19.14.1 Check the relevant conditions to ensure that model assumptions are met.\nAll the conditions have been checked already.\n\n\n19.14.2 Calculate and graph the confidence interval.\n\ntotal_ci &lt;- total_test %&gt;%\n  get_confidence_interval(point_estimate = total_mean, level = 0.95)\ntotal_ci\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1   68256.   73621.\n\n\n\ntotal_test %&gt;%\n  visualize() +\n  shade_confidence_interval(endpoints = total_ci)\n\n\n\n\n\n\n19.14.3 State (but do not overstate) a contextually meaningful interpretation.\nWe are 95% confident that the true mean salary for teachers in the St. Louis region is captured in the interval (68256.2, 73620.95).\nCommentary: As these are dollar amounts, it makes sense to round them to two decimal places. Even then, R is finicky and sometimes it will not respect your wishes.)\n\n\n19.14.4 If running a two-sided test, explain how the confidence interval reinforces the conclusion of the hypothesis test.\nSince $63,024 is not contained in the confidence interval, it is not a plausible value for the mean teacher salary in the St Louis region of Michigan.\n\n\n19.14.5 When comparing two groups, comment on the effect size and the practical significance of the result.\nWe are not comparing two groups."
  },
  {
    "objectID": "19-inference_for_one_mean-web.html#your-turn",
    "href": "19-inference_for_one_mean-web.html#your-turn",
    "title": "19  Inference for one mean",
    "section": "19.15 Your turn",
    "text": "19.15 Your turn\nIn the High School and Beyond survey (the hsb2 data set from the openintro package), among the many scores that are recorded are standardized math scores. Suppose that these scores are normalized so that a score of 50 represents some kind of international average. (This is not really true. I had to make something up here to give you a baseline number with which to work.) The question is, then, are American students different from this international baseline?\nThe rubric outline is reproduced below. You may refer to the worked example above and modify it accordingly. Remember to strip out all the commentary. That is just exposition for your benefit in understanding the steps, but is not meant to form part of the formal inference process.\nAnother word of warning: the copy/paste process is not a substitute for your brain. You will often need to modify more than just the names of the data frames and variables to adapt the worked examples to your own work. Do not blindly copy and paste code without understanding what it does. And you should never copy and paste text. All the sentences and paragraphs you write are expressions of your own analysis. They must reflect your own understanding of the inferential process.\nAlso, so that your answers here don’t mess up the code chunks above, use new variable names everywhere.\n\nExploratory data analysis\n\nUse data documentation (help files, code books, Google, etc.) to determine as much as possible about the data provenance and structure.\n\nPlease write up your answer here\n\n# Add code here to print the data\n\n\n# Add code here to glimpse the variables\n\n\n\n\nPrepare the data for analysis. [Not always necessary.]\n\n\n# Add code here to prepare the data for analysis.\n\n\n\n\nMake tables or plots to explore the data visually.\n\n\n# Add code here to make tables or plots.\n\n\n\n\n\nHypotheses\n\nIdentify the sample (or samples) and a reasonable population (or populations) of interest.\n\nPlease write up your answer here.\n\n\n\nExpress the null and alternative hypotheses as contextually meaningful full sentences.\n\n\\(H_{0}:\\) Null hypothesis goes here.\n\\(H_{A}:\\) Alternative hypothesis goes here.\n\n\n\nExpress the null and alternative hypotheses in symbols (when possible).\n\n\\(H_{0}: math\\)\n\\(H_{A}: math\\)\n\n\n\n\nModel\n\nIdentify the sampling distribution model.\n\nPlease write up your answer here.\n\n\n\nCheck the relevant conditions to ensure that model assumptions are met.\n\nPlease write up your answer here. (Some conditions may require R code as well.)\n\n\n\n\nMechanics\n\nCompute the test statistic.\n\n\n# Add code here to compute the test statistic.\n\n\n\n\nReport the test statistic in context (when possible).\n\nPlease write up your answer here.\n\n\n\nPlot the null distribution.\n\n\n# IF CONDUCTING A SIMULATION...\nset.seed(1)\n# Add code here to simulate the null distribution.\n\n\n# Add code here to plot the null distribution.\n\n\n\n\nCalculate the P-value.\n\n\n# Add code here to calculate the P-value.\n\n\n\n\nInterpret the P-value as a probability given the null.\n\nPlease write up your answer here.\n\n\n\n\nConclusion\n\nState the statistical conclusion.\n\nPlease write up your answer here. {-}\n\n\n\nState (but do not overstate) a contextually meaningful conclusion.\n\nPlease write up your answer here.\n\n\n\nExpress reservations or uncertainty about the generalizability of the conclusion.\n\nPlease write up your answer here.\n\n\n\nIdentify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.\n\nPlease write up your answer here.\n\n\n\n\nConfidence interval\n\nCheck the relevant conditions to ensure that model assumptions are met.\n\nPlease write up your answer here. (Some conditions may require R code as well.)\n\n\n\nCalculate and graph the confidence interval.\n\n\n# Add code here to calculate the confidence interval.\n\n\n# Add code here to graph the confidence interval.\n\n\n\n\nState (but do not overstate) a contextually meaningful interpretation.\n\nPlease write up your answer here.\n\n\n\nIf running a two-sided test, explain how the confidence interval reinforces the conclusion of the hypothesis test. [Not always applicable.]\n\nPlease write up your answer here.\n\n\n\nWhen comparing two groups, comment on the effect size and the practical significance of the result. [Not always applicable.]\n\nPlease write up your answer here."
  },
  {
    "objectID": "19-inference_for_one_mean-web.html#additional-exercises",
    "href": "19-inference_for_one_mean-web.html#additional-exercises",
    "title": "19  Inference for one mean",
    "section": "19.16 Additional exercises",
    "text": "19.16 Additional exercises\nAfter running inference above, answer the following questions:\n\nExercise 2\nEven though the result was statistically significant, do you think the result is practically significant? By this, I mean, are scores for American students so vastly different than 50? Do we have a lot of reason to brag about American scores based on your analysis?\n\nPlease write up your answer here.\n\n\n\nExercise 3\nWhat makes it possible for a small effect like this to be statistically significant even if it’s not practically very different from 50? In other words, what has to be true of data to detect small but statistically significant effects?\n\nPlease write up your answer here."
  },
  {
    "objectID": "19-inference_for_one_mean-web.html#conclusion-2",
    "href": "19-inference_for_one_mean-web.html#conclusion-2",
    "title": "19  Inference for one mean",
    "section": "19.17 Conclusion",
    "text": "19.17 Conclusion\nWhen working with numerical data, we have to estimate a mean and a standard deviation. The extra variability in estimating both gives rise to a sampling distribution model with thicker tails called the Student t distribution. Using this distribution gives us a way to calculate P-values and confidence intervals that take this variation into account.\n\n19.17.1 Preparing and submitting your assignment\n\nFrom the “Run” menu, select “Restart R and Run All Chunks”.\nDeal with any code errors that crop up. Repeat steps 1–-2 until there are no more code errors.\nSpell check your document by clicking the icon with “ABC” and a check mark.\nHit the “Preview” button one last time to generate the final draft of the .nb.html file.\nProofread the HTML file carefully. If there are errors, go back and fix them, then repeat steps 1–5 again.\n\nIf you have completed this chapter as part of a statistics course, follow the directions you receive from your professor to submit your assignment."
  },
  {
    "objectID": "20-inference_for_paired_data-web.html#introduction",
    "href": "20-inference_for_paired_data-web.html#introduction",
    "title": "20  Inference for paired data",
    "section": "20.1 Introduction",
    "text": "20.1 Introduction\nIn this chapter we will learn how to run inference for two paired numerical variables.\n\n20.1.1 Install new packages\nThere are no new packages used in this chapter.\n\n\n20.1.2 Download the R notebook file\nCheck the upper-right corner in RStudio to make sure you’re in your intro_stats project. Then click on the following link to download this chapter as an R notebook file (.Rmd).\nhttps://vectorposse.github.io/intro_stats/chapter_downloads/20-inference_for_paired_data.Rmd\nOnce the file is downloaded, move it to your project folder in RStudio and open it there.\n\n\n20.1.3 Restart R and run all chunks\nIn RStudio, select “Restart R and Run All Chunks” from the “Run” menu."
  },
  {
    "objectID": "20-inference_for_paired_data-web.html#load-packages",
    "href": "20-inference_for_paired_data-web.html#load-packages",
    "title": "20  Inference for paired data",
    "section": "20.2 Load packages",
    "text": "20.2 Load packages\nWe load the standard tidyverse and infer packages. The openintro package will give access to the textbooks data and the hsb2 data.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(infer)\nlibrary(openintro)\n\nLoading required package: airports\nLoading required package: cherryblossom\nLoading required package: usdata"
  },
  {
    "objectID": "20-inference_for_paired_data-web.html#paired-data",
    "href": "20-inference_for_paired_data-web.html#paired-data",
    "title": "20  Inference for paired data",
    "section": "20.3 Paired data",
    "text": "20.3 Paired data\nSometimes data sets have two numerical variables that are related to each other. For example, a diet study might include a pre-weight and a post-weight. The research question is not about either of these variables directly, but rather the difference between the variables, for example how much weight was lost during the diet.\nWhen this is the case, we run inference for paired data. The procedure involves calculating a new variable d that represents the difference of the two paired variables. The null hypothesis is almost always that there is no difference between the paired variables, and that translates into the statement that the average value of d is zero."
  },
  {
    "objectID": "20-inference_for_paired_data-web.html#research-question",
    "href": "20-inference_for_paired_data-web.html#research-question",
    "title": "20  Inference for paired data",
    "section": "20.4 Research question",
    "text": "20.4 Research question\nThe textbooks data frame (from the openintro package) has data on the price of books at the UCLA bookstore versus Amazon.com. The question of interest here is whether the campus bookstore charges more than Amazon."
  },
  {
    "objectID": "20-inference_for_paired_data-web.html#inference-for-paired-data",
    "href": "20-inference_for_paired_data-web.html#inference-for-paired-data",
    "title": "20  Inference for paired data",
    "section": "20.5 Inference for paired data",
    "text": "20.5 Inference for paired data\nThe key idea is that we don’t actually care about the book prices themselves. All we care about is if there is a difference between the prices for each book. These are not two independent variables because each row represents a single book. Therefore, the two measurements are “paired” and should be treated as a single numerical variable of interest, representing the difference between ucla_new and amaz_new.\nSince we’re only interested in analyzing the one numerical variable d, this process is nothing more than a one-sample t test. Therefore, there is really nothing new in this chapter.\nLet’s go through the rubric."
  },
  {
    "objectID": "20-inference_for_paired_data-web.html#exploratory-data-analysis",
    "href": "20-inference_for_paired_data-web.html#exploratory-data-analysis",
    "title": "20  Inference for paired data",
    "section": "20.6 Exploratory data analysis",
    "text": "20.6 Exploratory data analysis\n\n20.6.1 Use data documentation (help files, code books, Google, etc.) to determine as much as possible about the data provenance and structure.\nYou should type textbooks at the Console to read the help file. The data was collected by a person, David Diez. A quick Google search reveals that he is a statistician who graduated from UCLA. We presume he had access to accurate information about the prices of books at the UCLA bookstore and from Amazon.com at the time the data was collected.\nHere is the data set:\n\ntextbooks\n\n# A tibble: 73 × 7\n   dept_abbr course  isbn           ucla_new amaz_new more   diff\n   &lt;fct&gt;     &lt;fct&gt;   &lt;fct&gt;             &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;\n 1 Am Ind    \" C170\" 978-0803272620     27.7     28.0 Y     -0.28\n 2 Anthro    \"9\"     978-0030119194     40.6     31.1 Y      9.45\n 3 Anthro    \"135T\"  978-0300080643     31.7     32   Y     -0.32\n 4 Anthro    \"191HB\" 978-0226206813     16       11.5 Y      4.48\n 5 Art His   \"M102K\" 978-0892365999     19.0     14.2 Y      4.74\n 6 Art His   \"118E\"  978-0394723693     15.0     10.2 Y      4.78\n 7 Asia Am   \"187B\"  978-0822338437     24.7     20.1 Y      4.64\n 8 Asia Am   \"191E\"  978-0816646135     19.5     16.7 N      2.84\n 9 Ch Engr   \"C125\"  978-0195123401    124.     106.  N     17.6 \n10 Chicano   \"M145B\" 978-0896086265     17       13.3 Y      3.74\n# ℹ 63 more rows\n\n\n\nglimpse(textbooks)\n\nRows: 73\nColumns: 7\n$ dept_abbr &lt;fct&gt; Am Ind, Anthro, Anthro, Anthro, Art His, Art His, Asia Am, A…\n$ course    &lt;fct&gt;  C170, 9, 135T, 191HB, M102K, 118E, 187B, 191E, C125, M145B,…\n$ isbn      &lt;fct&gt; 978-0803272620, 978-0030119194, 978-0300080643, 978-02262068…\n$ ucla_new  &lt;dbl&gt; 27.67, 40.59, 31.68, 16.00, 18.95, 14.95, 24.70, 19.50, 123.…\n$ amaz_new  &lt;dbl&gt; 27.95, 31.14, 32.00, 11.52, 14.21, 10.17, 20.06, 16.66, 106.…\n$ more      &lt;fct&gt; Y, Y, Y, Y, Y, Y, Y, N, N, Y, Y, N, Y, Y, N, N, N, N, N, N, …\n$ diff      &lt;dbl&gt; -0.28, 9.45, -0.32, 4.48, 4.74, 4.78, 4.64, 2.84, 17.59, 3.7…\n\n\nThe two paired variables are ucla_new and amaz_new.\n\n\n20.6.2 Prepare the data for analysis.\nGenerally, we will need to create a new variable d that represents the difference between the two paired variables of interest. This uses the mutate command that adds an extra column to our data frame. The order of subtraction usually does not matter, but we will want to keep track of that order so that we can interpret our test statistic correctly. In the case of a one-sided test (which this is), it is especially important to keep track of the order of subtraction. Since we suspect the bookstore will charge more than Amazon, let’s subtract in that order. Our hunch is that it will be a positive number, on average.\n\ntextbooks_d &lt;- textbooks %&gt;%\n    mutate(d = ucla_new - amaz_new)\ntextbooks_d\n\n# A tibble: 73 × 8\n   dept_abbr course  isbn           ucla_new amaz_new more   diff      d\n   &lt;fct&gt;     &lt;fct&gt;   &lt;fct&gt;             &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Am Ind    \" C170\" 978-0803272620     27.7     28.0 Y     -0.28 -0.280\n 2 Anthro    \"9\"     978-0030119194     40.6     31.1 Y      9.45  9.45 \n 3 Anthro    \"135T\"  978-0300080643     31.7     32   Y     -0.32 -0.320\n 4 Anthro    \"191HB\" 978-0226206813     16       11.5 Y      4.48  4.48 \n 5 Art His   \"M102K\" 978-0892365999     19.0     14.2 Y      4.74  4.74 \n 6 Art His   \"118E\"  978-0394723693     15.0     10.2 Y      4.78  4.78 \n 7 Asia Am   \"187B\"  978-0822338437     24.7     20.1 Y      4.64  4.64 \n 8 Asia Am   \"191E\"  978-0816646135     19.5     16.7 N      2.84  2.84 \n 9 Ch Engr   \"C125\"  978-0195123401    124.     106.  N     17.6  17.6  \n10 Chicano   \"M145B\" 978-0896086265     17       13.3 Y      3.74  3.74 \n# ℹ 63 more rows\n\n\nIf you look closely at the tibble above, you will see that there is a column already in our data called diff. It is the same as the column d we just created. So in this case, we didn’t really need to create a new difference variable. However, since most data sets do not come pre-prepared with such a difference variable, it is good to know how to make one if needed.\n\n\n20.6.3 Make tables or plots to explore the data visually.\nHere are summary statistics, a histogram, and a QQ plot for d.\n\nsummary(textbooks_d$d)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  -9.53    3.80    8.23   12.76   17.59   66.00 \n\n\n\nggplot(textbooks_d, aes(x = d)) +\n    geom_histogram(binwidth = 10, boundary = 0)\n\n\n\n\n\nggplot(textbooks_d, aes(sample = d)) +\n    geom_qq() +\n    geom_qq_line()\n\n\n\n\nThe data is somewhat skewed to the right with one observation that might be a bit of an outlier. If the sample size were much smaller, we might be concerned about this point However, it’s not much higher than other points in that right tail, and it doesn’t appear that its inclusion or exclusion will change the overall conclusion much. If you are concerned that the point might alter the conclusion, run the hypothesis test twice, once with and once without the outlier present to see if the main conclusion changes."
  },
  {
    "objectID": "20-inference_for_paired_data-web.html#hypotheses",
    "href": "20-inference_for_paired_data-web.html#hypotheses",
    "title": "20  Inference for paired data",
    "section": "20.7 Hypotheses",
    "text": "20.7 Hypotheses\n\n20.7.1 Identify the sample (or samples) and a reasonable population (or populations) of interest.\nThe sample consists of 73 textbooks. The population is all textbooks that might be sold both at the UCLA bookstore and on Amazon.\n\n\n20.7.2 Express the null and alternative hypotheses as contextually meaningful full sentences.\n\\(H_{0}:\\) There is no difference in textbooks prices between the UCLA bookstore and Amazon.\n\\(H_{A}:\\) Textbook prices at the UCLA bookstore are higher on average than on Amazon.\nCommentary: Note we are performing a one-sided test. If we are conducting our own research with our own data, we can decide whether we want to run a two-sided or one-sided test. Remember that we only do the latter when we have a strong hypothesis in advance that the difference should be clearly in one direction and not the other. In this case, it’s not up to us. We have to respect the research question as it was given to us: “The question of interest here is whether the campus bookstore charges more than Amazon.”\n\nExercise 1\nWhat would the research question say if we were supposed to run a two-sided test instead? In other words, write down a slightly different research question about textbook prices that would prompt us to run a two-sided test.\n\nPlease write up your answer here.\n\n\n\n\n20.7.3 Express the null and alternative hypotheses in symbols (when possible).\n\\(H_{0}: \\mu_{d} = 0\\)\n\\(H_{A}: \\mu_{d} &gt; 0\\)\nCommentary: Since we’re really just doing a one-sample t test, we could just call this parameter \\(\\mu\\), but the subscript \\(d\\) is a good reminder that it’s the mean of the difference variable we care about (as opposed to the mean price of all the books at the UCLA bookstore or the mean price of all the same books on Amazon)."
  },
  {
    "objectID": "20-inference_for_paired_data-web.html#model",
    "href": "20-inference_for_paired_data-web.html#model",
    "title": "20  Inference for paired data",
    "section": "20.8 Model",
    "text": "20.8 Model\n\n20.8.1 Identify the sampling distribution model.\nWe use a t model with 72 degrees of freedom.\n\nExercise 2\nExplain how we got 72 degrees of freedom.\n\nPlease write up your answer here.\n\n\n\n\n20.8.2 Check the relevant conditions to ensure that model assumptions are met.\n\nRandom\n\nWe do not know how exactly how David Diez obtained this sample, but the help file claims it is a random sample.\n\n10%\n\nWe do not know how many total textbooks were available at the UCLA bookstore at the time the sample was taken, so we do not know if this condition is met. As long as there were at least 730 books, we are okay. We suspect that, based on the size of UCLA and the number of course offerings there, this is a reasonable assumption.\n\nNearly normal\n\nAlthough the sample distribution is skewed (with a possible mild outlier), the sample size is more than 30."
  },
  {
    "objectID": "20-inference_for_paired_data-web.html#mechanics",
    "href": "20-inference_for_paired_data-web.html#mechanics",
    "title": "20  Inference for paired data",
    "section": "20.9 Mechanics",
    "text": "20.9 Mechanics\n\n20.9.1 Compute the test statistic.\n\nd_mean &lt;- textbooks_d %&gt;%\n  specify(response = d) %&gt;%\n  calculate(stat = \"mean\")\nd_mean\n\nResponse: d (numeric)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  12.8\n\n\n\nd_t &lt;- textbooks_d %&gt;%\n  specify(response = d) %&gt;%\n  hypothesize(null = \"point\", mu = 0) %&gt;%\n  calculate(stat = \"t\")\nd_t\n\nResponse: d (numeric)\nNull Hypothesis: point\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  7.65\n\n\n\n\n20.9.2 Report the test statistic in context (when possible).\nThe mean difference in textbook prices is 12.7616438.\nThe value of t is 7.6487711. The mean difference in textbook prices is more than 7 standard errors above a difference of zero.\n\n\n20.9.3 Plot the null distribution.\n\nprice_test &lt;- textbooks_d %&gt;%\n  specify(response = d) %&gt;%\n  assume(\"t\")\nprice_test\n\nA T distribution with 72 degrees of freedom.\n\n\n\nprice_test %&gt;%\n  visualize() +\n  shade_p_value(obs_stat = d_t, direction = \"greater\")\n\n\n\n\n\n\n20.9.4 Calculate the P-value.\n\nprice_test_p &lt;- price_test %&gt;%\n  get_p_value(obs_stat = d_t, direction = \"greater\")\nprice_test_p\n\n# A tibble: 1 × 1\n   p_value\n     &lt;dbl&gt;\n1 3.46e-11\n\n\n\n\n20.9.5 Interpret the P-value as a probability given the null.\n\\(P &lt; 0.001\\). If there were no difference in textbook prices between the UCLA bookstore and Amazon, there is only a 0% chance of seeing data at least as extreme as what we saw. (Note that the number is so small that it rounds to zero in the inline code above. That zero is technically incorrect. The P-value is never exactly zero. That’s why why also are clear to state \\(P &lt; 0.001\\).)"
  },
  {
    "objectID": "20-inference_for_paired_data-web.html#conclusion",
    "href": "20-inference_for_paired_data-web.html#conclusion",
    "title": "20  Inference for paired data",
    "section": "20.10 Conclusion",
    "text": "20.10 Conclusion\n\n20.10.1 State the statistical conclusion.\nWe reject the null hypothesis.\n\n\n20.10.2 State (but do not overstate) a contextually meaningful conclusion.\nWe have sufficient evidence that UCLA prices are higher than Amazon prices.\nCommentary: Note that because we performed a one-sided test, our conclusion is also one-sided in the hypothesized direction.\n\n\n20.10.3 Express reservations or uncertainty about the generalizability of the conclusion.\nWe can be confident about the validity of this data, and therefore the conclusion drawn. We should be careful to limit our conclusion to the UCLA bookstore (and not extrapolate the findings, say, to other campus bookstores.) Depending on when this data was collected, we may not be able to say anything about current prices at the UCLA bookstore either.\n\n\n20.10.4 Identify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.\nIf we made a Type I error, that would mean there was actually no difference in textbook prices, but that we got an unusual sample that detected a difference."
  },
  {
    "objectID": "20-inference_for_paired_data-web.html#confidence-interval",
    "href": "20-inference_for_paired_data-web.html#confidence-interval",
    "title": "20  Inference for paired data",
    "section": "20.11 Confidence interval",
    "text": "20.11 Confidence interval\n\n20.11.1 Check the relevant conditions to ensure that model assumptions are met.\nAll necessary conditions have already been checked.\n\n\n20.11.2 Calculate and graph the confidence interval.\n\nprice_ci &lt;- price_test %&gt;%\n  get_confidence_interval(point_estimate = d_mean, level = 0.95)\nprice_ci\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     9.44     16.1\n\n\n\nprice_test %&gt;%\n  visualize() +\n  shade_confidence_interval(endpoints = price_ci)\n\n\n\n\n\n\n20.11.3 State (but do not overstate) a contextually meaningful interpretation.\nWe are 95% confident that the true difference in textbook prices between the UCLA bookstore and Amazon is captured in the interval (9.4356361, 16.0876516). This was obtained by subtracting the Amazon price minus the UCLA bookstore. (In other words, since all differences in the confidence interval are positive, all plausible differences indicate that the UCLA prices are higher than the Amazon prices.)\nCommentary: Don’t forget that any time we find a number that represents a difference, we have to be clear in the conclusion about the direction of subtraction. Otherwise, we have no idea how to interpret positive and negative values.\n\n\n20.11.4 If running a two-sided test, explain how the confidence interval reinforces the conclusion of the hypothesis test.\nThe confidence interval does not contain zero, which means that zero is not a plausible value for the difference textbook prices.\n\n\n20.11.5 When comparing two groups, comment on the effect size and the practical significance of the result.\nTo think about the practical significance, imagine that you were a student at UCLA and that every textbook you needed was (on average) $10 to $15 more expensive in the bookstore than purchasing on Amazon. Multiplied across the number of textbooks you need, that could amount to a significant increase in expenses. In other words, that dollar figure is not likely a trivial amount of money for many students who require multiple textbooks each semester."
  },
  {
    "objectID": "20-inference_for_paired_data-web.html#your-turn",
    "href": "20-inference_for_paired_data-web.html#your-turn",
    "title": "20  Inference for paired data",
    "section": "20.12 Your turn",
    "text": "20.12 Your turn\nThe hsb2 data set contains data from a random sample of 200 high school seniors from the “High School and Beyond” survey conducted by the National Center of Education Statistics. It contains, among other things, students’ scores on standardized tests in math, reading, writing, science, and social studies. We want to know if students do better on the math test or on the reading test.\nRun inference to determine if there is a difference between math scores and reading scores.\nThe rubric outline is reproduced below. You may refer to the worked example above and modify it accordingly. Remember to strip out all the commentary. That is just exposition for your benefit in understanding the steps, but is not meant to form part of the formal inference process.\nAnother word of warning: the copy/paste process is not a substitute for your brain. You will often need to modify more than just the names of the data frames and variables to adapt the worked examples to your own work. Do not blindly copy and paste code without understanding what it does. And you should never copy and paste text. All the sentences and paragraphs you write are expressions of your own analysis. They must reflect your own understanding of the inferential process.\nAlso, so that your answers here don’t mess up the code chunks above, use new variable names everywhere.\n\nExploratory data analysis\n\nUse data documentation (help files, code books, Google, etc.) to determine as much as possible about the data provenance and structure.\n\nPlease write up your answer here\n\n# Add code here to print the data\n\n\n# Add code here to glimpse the variables\n\n\n\n\nPrepare the data for analysis. [Not always necessary.]\n\n\n# Add code here to prepare the data for analysis.\n\n\n\n\nMake tables or plots to explore the data visually.\n\n\n# Add code here to make tables or plots.\n\n\n\n\n\nHypotheses\n\nIdentify the sample (or samples) and a reasonable population (or populations) of interest.\n\nPlease write up your answer here.\n\n\n\nExpress the null and alternative hypotheses as contextually meaningful full sentences.\n\n\\(H_{0}:\\) Null hypothesis goes here.\n\\(H_{A}:\\) Alternative hypothesis goes here.\n\n\n\nExpress the null and alternative hypotheses in symbols (when possible).\n\n\\(H_{0}: math\\)\n\\(H_{A}: math\\)\n\n\n\n\nModel\n\nIdentify the sampling distribution model.\n\nPlease write up your answer here.\n\n\n\nCheck the relevant conditions to ensure that model assumptions are met.\n\nPlease write up your answer here. (Some conditions may require R code as well.)\n\n\n\n\nMechanics\n\nCompute the test statistic.\n\n\n# Add code here to compute the test statistic.\n\n\n\n\nReport the test statistic in context (when possible).\n\nPlease write up your answer here.\n\n\n\nPlot the null distribution.\n\n\n# IF CONDUCTING A SIMULATION...\nset.seed(1)\n# Add code here to simulate the null distribution.\n\n\n# Add code here to plot the null distribution.\n\n\n\n\nCalculate the P-value.\n\n\n# Add code here to calculate the P-value.\n\n\n\n\nInterpret the P-value as a probability given the null.\n\nPlease write up your answer here.\n\n\n\n\nConclusion\n\nState the statistical conclusion.\n\nPlease write up your answer here.\n\n\n\nState (but do not overstate) a contextually meaningful conclusion.\n\nPlease write up your answer here.\n\n\n\nExpress reservations or uncertainty about the generalizability of the conclusion.\n\nPlease write up your answer here.\n\n\n\nIdentify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.\n\nPlease write up your answer here.\n\n\n\n\nConfidence interval\n\nCheck the relevant conditions to ensure that model assumptions are met.\n\nPlease write up your answer here. (Some conditions may require R code as well.)\n\n\n\nCalculate and graph the confidence interval.\n\n\n# Add code here to calculate the confidence interval.\n\n\n# Add code here to graph the confidence interval.\n\n\n\n\nState (but do not overstate) a contextually meaningful interpretation.\n\nPlease write up your answer here.\n\n\n\nIf running a two-sided test, explain how the confidence interval reinforces the conclusion of the hypothesis test. [Not always applicable.]\n\nPlease write up your answer here.\n\n\n\nWhen comparing two groups, comment on the effect size and the practical significance of the result. [Not always applicable.]\n\nPlease write up your answer here."
  },
  {
    "objectID": "20-inference_for_paired_data-web.html#conclusion-2",
    "href": "20-inference_for_paired_data-web.html#conclusion-2",
    "title": "20  Inference for paired data",
    "section": "20.13 Conclusion",
    "text": "20.13 Conclusion\nPaired data occurs whenever we have two numerical measurements that are related to each other, whether because they come from the same observational units or from closely related ones. When our data is structured as pairs of measurements in this way, we can subtract the two columns and obtain a difference. That difference variable is the object of our study, and now that it is represented as a single numerical variable, we can apply the one-sample t test from the last chapter.\n\n20.13.1 Preparing and submitting your assignment\n\nFrom the “Run” menu, select “Restart R and Run All Chunks”.\nDeal with any code errors that crop up. Repeat steps 1–-2 until there are no more code errors.\nSpell check your document by clicking the icon with “ABC” and a check mark.\nHit the “Preview” button one last time to generate the final draft of the .nb.html file.\nProofread the HTML file carefully. If there are errors, go back and fix them, then repeat steps 1–5 again.\n\nIf you have completed this chapter as part of a statistics course, follow the directions you receive from your professor to submit your assignment."
  },
  {
    "objectID": "21-inference_for_two_independent_means-web.html#introduction",
    "href": "21-inference_for_two_independent_means-web.html#introduction",
    "title": "21  Inference for two independent means",
    "section": "21.1 Introduction",
    "text": "21.1 Introduction\nIf we have a numerical variable and a categorical variable with two categories, we can think of the numerical variable as response and the categorical variable as predictor. The idea is that the two categories sort your numerical data into two groups which can be compared. Assuming the two groups are independent of each other, we can use them as samples of two larger populations. This leads to inference to decide if the difference between the means of the two groups is statistically significant and then estimate the difference between the means of the two populations represented. The relevant hypothesis test is called a two-sample t test (or Welch’s t test, to be specific).\n\n21.1.1 Install new packages\nThere are no new packages used in this chapter.\n\n\n21.1.2 Download the R notebook file\nCheck the upper-right corner in RStudio to make sure you’re in your intro_stats project. Then click on the following link to download this chapter as an R notebook file (.Rmd).\nhttps://vectorposse.github.io/intro_stats/chapter_downloads/21-inference_for_two_independent_means.Rmd\nOnce the file is downloaded, move it to your project folder in RStudio and open it there.\n\n\n21.1.3 Restart R and run all chunks\nIn RStudio, select “Restart R and Run All Chunks” from the “Run” menu."
  },
  {
    "objectID": "21-inference_for_two_independent_means-web.html#load-packages",
    "href": "21-inference_for_two_independent_means-web.html#load-packages",
    "title": "21  Inference for two independent means",
    "section": "21.2 Load packages",
    "text": "21.2 Load packages\nWe load the standard tidyverse, janitor, and infer packages. We also use the MASS package for the birthwt data.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(infer)\nlibrary(MASS)\n\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select"
  },
  {
    "objectID": "21-inference_for_two_independent_means-web.html#research-question",
    "href": "21-inference_for_two_independent_means-web.html#research-question",
    "title": "21  Inference for two independent means",
    "section": "21.3 Research question",
    "text": "21.3 Research question\nRecall the birthwt data that was collected at Baystate Medical Center, Springfield, Mass during 1986. In a previous chapter, we measured low birth weight babies using a categorical variable that served as an indicator for low birth weight.\n\nExercise 1\nHow was it determined if a baby was considered “low birth weight” for purposes of constructing the variable low? Use the help file to find out.\n\nPlease write up your answer here.\n\n\nWe have the actual birth weight of the babies in this data. So, rather than using a coarse classification into a binary “yes or no” variable, why not use the full precision of the birth weight measured in grams? This is a very precisely measured numerical variable.\nWe’d like to compare mean birth weights among two groups: women who smoked during pregnancy, and women who didn’t."
  },
  {
    "objectID": "21-inference_for_two_independent_means-web.html#data-preparation",
    "href": "21-inference_for_two_independent_means-web.html#data-preparation",
    "title": "21  Inference for two independent means",
    "section": "21.4 Data preparation",
    "text": "21.4 Data preparation\nThe actual mean weights in each sample (the smoking women and the nonsmoking women) can be found using a group_by and summarise pipeline:\n\nbirthwt %&gt;%\n  group_by(smoke) %&gt;%\n  summarise(mean(bwt))\n\n# A tibble: 2 × 2\n  smoke `mean(bwt)`\n  &lt;int&gt;       &lt;dbl&gt;\n1     0       3056.\n2     1       2772.\n\n\nNote that 0 means “nonsmoker” and 1 means “smoker”. Looks like We need to address the fact the smoke variable is recorded as a numerical variable instead of a categorical variable. Here is birthwt2 that we will use from here on out:\n\nbirthwt2 &lt;- birthwt %&gt;%\n    mutate(smoke_fct = factor(smoke, levels = c(0, 1), labels = c(\"Nonsmoker\", \"Smoker\")))\nbirthwt2\n\n    low age lwt race smoke ptl ht ui ftv  bwt smoke_fct\n85    0  19 182    2     0   0  0  1   0 2523 Nonsmoker\n86    0  33 155    3     0   0  0  0   3 2551 Nonsmoker\n87    0  20 105    1     1   0  0  0   1 2557    Smoker\n88    0  21 108    1     1   0  0  1   2 2594    Smoker\n89    0  18 107    1     1   0  0  1   0 2600    Smoker\n91    0  21 124    3     0   0  0  0   0 2622 Nonsmoker\n92    0  22 118    1     0   0  0  0   1 2637 Nonsmoker\n93    0  17 103    3     0   0  0  0   1 2637 Nonsmoker\n94    0  29 123    1     1   0  0  0   1 2663    Smoker\n95    0  26 113    1     1   0  0  0   0 2665    Smoker\n96    0  19  95    3     0   0  0  0   0 2722 Nonsmoker\n97    0  19 150    3     0   0  0  0   1 2733 Nonsmoker\n98    0  22  95    3     0   0  1  0   0 2751 Nonsmoker\n99    0  30 107    3     0   1  0  1   2 2750 Nonsmoker\n100   0  18 100    1     1   0  0  0   0 2769    Smoker\n101   0  18 100    1     1   0  0  0   0 2769    Smoker\n102   0  15  98    2     0   0  0  0   0 2778 Nonsmoker\n103   0  25 118    1     1   0  0  0   3 2782    Smoker\n104   0  20 120    3     0   0  0  1   0 2807 Nonsmoker\n105   0  28 120    1     1   0  0  0   1 2821    Smoker\n106   0  32 121    3     0   0  0  0   2 2835 Nonsmoker\n107   0  31 100    1     0   0  0  1   3 2835 Nonsmoker\n108   0  36 202    1     0   0  0  0   1 2836 Nonsmoker\n109   0  28 120    3     0   0  0  0   0 2863 Nonsmoker\n111   0  25 120    3     0   0  0  1   2 2877 Nonsmoker\n112   0  28 167    1     0   0  0  0   0 2877 Nonsmoker\n113   0  17 122    1     1   0  0  0   0 2906    Smoker\n114   0  29 150    1     0   0  0  0   2 2920 Nonsmoker\n115   0  26 168    2     1   0  0  0   0 2920    Smoker\n116   0  17 113    2     0   0  0  0   1 2920 Nonsmoker\n117   0  17 113    2     0   0  0  0   1 2920 Nonsmoker\n118   0  24  90    1     1   1  0  0   1 2948    Smoker\n119   0  35 121    2     1   1  0  0   1 2948    Smoker\n120   0  25 155    1     0   0  0  0   1 2977 Nonsmoker\n121   0  25 125    2     0   0  0  0   0 2977 Nonsmoker\n123   0  29 140    1     1   0  0  0   2 2977    Smoker\n124   0  19 138    1     1   0  0  0   2 2977    Smoker\n125   0  27 124    1     1   0  0  0   0 2922    Smoker\n126   0  31 215    1     1   0  0  0   2 3005    Smoker\n127   0  33 109    1     1   0  0  0   1 3033    Smoker\n128   0  21 185    2     1   0  0  0   2 3042    Smoker\n129   0  19 189    1     0   0  0  0   2 3062 Nonsmoker\n130   0  23 130    2     0   0  0  0   1 3062 Nonsmoker\n131   0  21 160    1     0   0  0  0   0 3062 Nonsmoker\n132   0  18  90    1     1   0  0  1   0 3062    Smoker\n133   0  18  90    1     1   0  0  1   0 3062    Smoker\n134   0  32 132    1     0   0  0  0   4 3080 Nonsmoker\n135   0  19 132    3     0   0  0  0   0 3090 Nonsmoker\n136   0  24 115    1     0   0  0  0   2 3090 Nonsmoker\n137   0  22  85    3     1   0  0  0   0 3090    Smoker\n138   0  22 120    1     0   0  1  0   1 3100 Nonsmoker\n139   0  23 128    3     0   0  0  0   0 3104 Nonsmoker\n140   0  22 130    1     1   0  0  0   0 3132    Smoker\n141   0  30  95    1     1   0  0  0   2 3147    Smoker\n142   0  19 115    3     0   0  0  0   0 3175 Nonsmoker\n143   0  16 110    3     0   0  0  0   0 3175 Nonsmoker\n144   0  21 110    3     1   0  0  1   0 3203    Smoker\n145   0  30 153    3     0   0  0  0   0 3203 Nonsmoker\n146   0  20 103    3     0   0  0  0   0 3203 Nonsmoker\n147   0  17 119    3     0   0  0  0   0 3225 Nonsmoker\n148   0  17 119    3     0   0  0  0   0 3225 Nonsmoker\n149   0  23 119    3     0   0  0  0   2 3232 Nonsmoker\n150   0  24 110    3     0   0  0  0   0 3232 Nonsmoker\n151   0  28 140    1     0   0  0  0   0 3234 Nonsmoker\n154   0  26 133    3     1   2  0  0   0 3260    Smoker\n155   0  20 169    3     0   1  0  1   1 3274 Nonsmoker\n156   0  24 115    3     0   0  0  0   2 3274 Nonsmoker\n159   0  28 250    3     1   0  0  0   6 3303    Smoker\n160   0  20 141    1     0   2  0  1   1 3317 Nonsmoker\n161   0  22 158    2     0   1  0  0   2 3317 Nonsmoker\n162   0  22 112    1     1   2  0  0   0 3317    Smoker\n163   0  31 150    3     1   0  0  0   2 3321    Smoker\n164   0  23 115    3     1   0  0  0   1 3331    Smoker\n166   0  16 112    2     0   0  0  0   0 3374 Nonsmoker\n167   0  16 135    1     1   0  0  0   0 3374    Smoker\n168   0  18 229    2     0   0  0  0   0 3402 Nonsmoker\n169   0  25 140    1     0   0  0  0   1 3416 Nonsmoker\n170   0  32 134    1     1   1  0  0   4 3430    Smoker\n172   0  20 121    2     1   0  0  0   0 3444    Smoker\n173   0  23 190    1     0   0  0  0   0 3459 Nonsmoker\n174   0  22 131    1     0   0  0  0   1 3460 Nonsmoker\n175   0  32 170    1     0   0  0  0   0 3473 Nonsmoker\n176   0  30 110    3     0   0  0  0   0 3544 Nonsmoker\n177   0  20 127    3     0   0  0  0   0 3487 Nonsmoker\n179   0  23 123    3     0   0  0  0   0 3544 Nonsmoker\n180   0  17 120    3     1   0  0  0   0 3572    Smoker\n181   0  19 105    3     0   0  0  0   0 3572 Nonsmoker\n182   0  23 130    1     0   0  0  0   0 3586 Nonsmoker\n183   0  36 175    1     0   0  0  0   0 3600 Nonsmoker\n184   0  22 125    1     0   0  0  0   1 3614 Nonsmoker\n185   0  24 133    1     0   0  0  0   0 3614 Nonsmoker\n186   0  21 134    3     0   0  0  0   2 3629 Nonsmoker\n187   0  19 235    1     1   0  1  0   0 3629    Smoker\n188   0  25  95    1     1   3  0  1   0 3637    Smoker\n189   0  16 135    1     1   0  0  0   0 3643    Smoker\n190   0  29 135    1     0   0  0  0   1 3651 Nonsmoker\n191   0  29 154    1     0   0  0  0   1 3651 Nonsmoker\n192   0  19 147    1     1   0  0  0   0 3651    Smoker\n193   0  19 147    1     1   0  0  0   0 3651    Smoker\n195   0  30 137    1     0   0  0  0   1 3699 Nonsmoker\n196   0  24 110    1     0   0  0  0   1 3728 Nonsmoker\n197   0  19 184    1     1   0  1  0   0 3756    Smoker\n199   0  24 110    3     0   1  0  0   0 3770 Nonsmoker\n200   0  23 110    1     0   0  0  0   1 3770 Nonsmoker\n201   0  20 120    3     0   0  0  0   0 3770 Nonsmoker\n202   0  25 241    2     0   0  1  0   0 3790 Nonsmoker\n203   0  30 112    1     0   0  0  0   1 3799 Nonsmoker\n204   0  22 169    1     0   0  0  0   0 3827 Nonsmoker\n205   0  18 120    1     1   0  0  0   2 3856    Smoker\n206   0  16 170    2     0   0  0  0   4 3860 Nonsmoker\n207   0  32 186    1     0   0  0  0   2 3860 Nonsmoker\n208   0  18 120    3     0   0  0  0   1 3884 Nonsmoker\n209   0  29 130    1     1   0  0  0   2 3884    Smoker\n210   0  33 117    1     0   0  0  1   1 3912 Nonsmoker\n211   0  20 170    1     1   0  0  0   0 3940    Smoker\n212   0  28 134    3     0   0  0  0   1 3941 Nonsmoker\n213   0  14 135    1     0   0  0  0   0 3941 Nonsmoker\n214   0  28 130    3     0   0  0  0   0 3969 Nonsmoker\n215   0  25 120    1     0   0  0  0   2 3983 Nonsmoker\n216   0  16  95    3     0   0  0  0   1 3997 Nonsmoker\n217   0  20 158    1     0   0  0  0   1 3997 Nonsmoker\n218   0  26 160    3     0   0  0  0   0 4054 Nonsmoker\n219   0  21 115    1     0   0  0  0   1 4054 Nonsmoker\n220   0  22 129    1     0   0  0  0   0 4111 Nonsmoker\n221   0  25 130    1     0   0  0  0   2 4153 Nonsmoker\n222   0  31 120    1     0   0  0  0   2 4167 Nonsmoker\n223   0  35 170    1     0   1  0  0   1 4174 Nonsmoker\n224   0  19 120    1     1   0  0  0   0 4238    Smoker\n225   0  24 116    1     0   0  0  0   1 4593 Nonsmoker\n226   0  45 123    1     0   0  0  0   1 4990 Nonsmoker\n4     1  28 120    3     1   1  0  1   0  709    Smoker\n10    1  29 130    1     0   0  0  1   2 1021 Nonsmoker\n11    1  34 187    2     1   0  1  0   0 1135    Smoker\n13    1  25 105    3     0   1  1  0   0 1330 Nonsmoker\n15    1  25  85    3     0   0  0  1   0 1474 Nonsmoker\n16    1  27 150    3     0   0  0  0   0 1588 Nonsmoker\n17    1  23  97    3     0   0  0  1   1 1588 Nonsmoker\n18    1  24 128    2     0   1  0  0   1 1701 Nonsmoker\n19    1  24 132    3     0   0  1  0   0 1729 Nonsmoker\n20    1  21 165    1     1   0  1  0   1 1790    Smoker\n22    1  32 105    1     1   0  0  0   0 1818    Smoker\n23    1  19  91    1     1   2  0  1   0 1885    Smoker\n24    1  25 115    3     0   0  0  0   0 1893 Nonsmoker\n25    1  16 130    3     0   0  0  0   1 1899 Nonsmoker\n26    1  25  92    1     1   0  0  0   0 1928    Smoker\n27    1  20 150    1     1   0  0  0   2 1928    Smoker\n28    1  21 200    2     0   0  0  1   2 1928 Nonsmoker\n29    1  24 155    1     1   1  0  0   0 1936    Smoker\n30    1  21 103    3     0   0  0  0   0 1970 Nonsmoker\n31    1  20 125    3     0   0  0  1   0 2055 Nonsmoker\n32    1  25  89    3     0   2  0  0   1 2055 Nonsmoker\n33    1  19 102    1     0   0  0  0   2 2082 Nonsmoker\n34    1  19 112    1     1   0  0  1   0 2084    Smoker\n35    1  26 117    1     1   1  0  0   0 2084    Smoker\n36    1  24 138    1     0   0  0  0   0 2100 Nonsmoker\n37    1  17 130    3     1   1  0  1   0 2125    Smoker\n40    1  20 120    2     1   0  0  0   3 2126    Smoker\n42    1  22 130    1     1   1  0  1   1 2187    Smoker\n43    1  27 130    2     0   0  0  1   0 2187 Nonsmoker\n44    1  20  80    3     1   0  0  1   0 2211    Smoker\n45    1  17 110    1     1   0  0  0   0 2225    Smoker\n46    1  25 105    3     0   1  0  0   1 2240 Nonsmoker\n47    1  20 109    3     0   0  0  0   0 2240 Nonsmoker\n49    1  18 148    3     0   0  0  0   0 2282 Nonsmoker\n50    1  18 110    2     1   1  0  0   0 2296    Smoker\n51    1  20 121    1     1   1  0  1   0 2296    Smoker\n52    1  21 100    3     0   1  0  0   4 2301 Nonsmoker\n54    1  26  96    3     0   0  0  0   0 2325 Nonsmoker\n56    1  31 102    1     1   1  0  0   1 2353    Smoker\n57    1  15 110    1     0   0  0  0   0 2353 Nonsmoker\n59    1  23 187    2     1   0  0  0   1 2367    Smoker\n60    1  20 122    2     1   0  0  0   0 2381    Smoker\n61    1  24 105    2     1   0  0  0   0 2381    Smoker\n62    1  15 115    3     0   0  0  1   0 2381 Nonsmoker\n63    1  23 120    3     0   0  0  0   0 2410 Nonsmoker\n65    1  30 142    1     1   1  0  0   0 2410    Smoker\n67    1  22 130    1     1   0  0  0   1 2410    Smoker\n68    1  17 120    1     1   0  0  0   3 2414    Smoker\n69    1  23 110    1     1   1  0  0   0 2424    Smoker\n71    1  17 120    2     0   0  0  0   2 2438 Nonsmoker\n75    1  26 154    3     0   1  1  0   1 2442 Nonsmoker\n76    1  20 105    3     0   0  0  0   3 2450 Nonsmoker\n77    1  26 190    1     1   0  0  0   0 2466    Smoker\n78    1  14 101    3     1   1  0  0   0 2466    Smoker\n79    1  28  95    1     1   0  0  0   2 2466    Smoker\n81    1  14 100    3     0   0  0  0   2 2495 Nonsmoker\n82    1  23  94    3     1   0  0  0   0 2495    Smoker\n83    1  17 142    2     0   0  1  0   0 2495 Nonsmoker\n84    1  21 130    1     1   0  1  0   3 2495    Smoker\n\n\n\nglimpse(birthwt2)\n\nRows: 189\nColumns: 11\n$ low       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ age       &lt;int&gt; 19, 33, 20, 21, 18, 21, 22, 17, 29, 26, 19, 19, 22, 30, 18, …\n$ lwt       &lt;int&gt; 182, 155, 105, 108, 107, 124, 118, 103, 123, 113, 95, 150, 9…\n$ race      &lt;int&gt; 2, 3, 1, 1, 1, 3, 1, 3, 1, 1, 3, 3, 3, 3, 1, 1, 2, 1, 3, 1, …\n$ smoke     &lt;int&gt; 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, …\n$ ptl       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, …\n$ ht        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …\n$ ui        &lt;int&gt; 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, …\n$ ftv       &lt;int&gt; 0, 3, 1, 2, 0, 0, 1, 1, 1, 0, 0, 1, 0, 2, 0, 0, 0, 3, 0, 1, …\n$ bwt       &lt;int&gt; 2523, 2551, 2557, 2594, 2600, 2622, 2637, 2637, 2663, 2665, …\n$ smoke_fct &lt;fct&gt; Nonsmoker, Nonsmoker, Smoker, Smoker, Smoker, Nonsmoker, Non…\n\n\nThe difference between the means is now calculated using infer tools. We will store the result as obs_diff for “observed difference”.\n\nobs_diff &lt;- birthwt2 %&gt;%\n  specify(response = bwt, explanatory = smoke_fct) %&gt;% \n  calculate(stat = \"diff in means\", order = c(\"Nonsmoker\", \"Smoker\"))\nobs_diff\n\nResponse: bwt (numeric)\nExplanatory: smoke_fct (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  284.\n\n\n\nExercise 2\nWhat would happen if we used order = c(\"Smoker\", \"Nonsmoker\") instead? Why might we have a slight preference for order = c(\"Nonsmoker\", \"Smoker\")?\n\nPlease write up your answer here.\n\n\nNote that it will not actually make a difference to the inferential process in which order we subtract. However, we do have to be consistent to use the same order throughout. When interpreting the test statistic, effect size, and confidence interval, we will need to pay attention to the order of subtraction to make sure we are interpreting our results correctly."
  },
  {
    "objectID": "21-inference_for_two_independent_means-web.html#every-day-im-shuffling",
    "href": "21-inference_for_two_independent_means-web.html#every-day-im-shuffling",
    "title": "21  Inference for two independent means",
    "section": "21.5 Every day I’m shuffling",
    "text": "21.5 Every day I’m shuffling\nWhenever there are two groups, the obvious null hypothesis is that there is no difference between them.\nConsider the smoke variable. If there were truly no difference in mean birth weights between women who smoked and women who didn’t, then it shouldn’t matter if we know the smoking status or not. It becomes irrelevant under the assumption of the null.\nWe can simulate this assumption by shuffling the list of smoking status. More concretely, we can randomly assign a smoking status label to each mother and then calculate the average birth weight in each group. Since the smoking labels are random, there’s no reason to expect a difference between the two average weights other than random fluctuations due to sampling variability.\nFor example, here is the actual smoking status of the women:\n\nbirthwt2$smoke_fct\n\n  [1] Nonsmoker Nonsmoker Smoker    Smoker    Smoker    Nonsmoker Nonsmoker\n  [8] Nonsmoker Smoker    Smoker    Nonsmoker Nonsmoker Nonsmoker Nonsmoker\n [15] Smoker    Smoker    Nonsmoker Smoker    Nonsmoker Smoker    Nonsmoker\n [22] Nonsmoker Nonsmoker Nonsmoker Nonsmoker Nonsmoker Smoker    Nonsmoker\n [29] Smoker    Nonsmoker Nonsmoker Smoker    Smoker    Nonsmoker Nonsmoker\n [36] Smoker    Smoker    Smoker    Smoker    Smoker    Smoker    Nonsmoker\n [43] Nonsmoker Nonsmoker Smoker    Smoker    Nonsmoker Nonsmoker Nonsmoker\n [50] Smoker    Nonsmoker Nonsmoker Smoker    Smoker    Nonsmoker Nonsmoker\n [57] Smoker    Nonsmoker Nonsmoker Nonsmoker Nonsmoker Nonsmoker Nonsmoker\n [64] Nonsmoker Smoker    Nonsmoker Nonsmoker Smoker    Nonsmoker Nonsmoker\n [71] Smoker    Smoker    Smoker    Nonsmoker Smoker    Nonsmoker Nonsmoker\n [78] Smoker    Smoker    Nonsmoker Nonsmoker Nonsmoker Nonsmoker Nonsmoker\n [85] Nonsmoker Smoker    Nonsmoker Nonsmoker Nonsmoker Nonsmoker Nonsmoker\n [92] Nonsmoker Smoker    Smoker    Smoker    Nonsmoker Nonsmoker Smoker   \n [99] Smoker    Nonsmoker Nonsmoker Smoker    Nonsmoker Nonsmoker Nonsmoker\n[106] Nonsmoker Nonsmoker Nonsmoker Smoker    Nonsmoker Nonsmoker Nonsmoker\n[113] Smoker    Nonsmoker Smoker    Nonsmoker Nonsmoker Nonsmoker Nonsmoker\n[120] Nonsmoker Nonsmoker Nonsmoker Nonsmoker Nonsmoker Nonsmoker Nonsmoker\n[127] Nonsmoker Smoker    Nonsmoker Nonsmoker Smoker    Nonsmoker Smoker   \n[134] Nonsmoker Nonsmoker Nonsmoker Nonsmoker Nonsmoker Nonsmoker Smoker   \n[141] Smoker    Smoker    Nonsmoker Nonsmoker Smoker    Smoker    Nonsmoker\n[148] Smoker    Nonsmoker Nonsmoker Nonsmoker Nonsmoker Smoker    Smoker   \n[155] Nonsmoker Smoker    Smoker    Smoker    Nonsmoker Smoker    Smoker   \n[162] Nonsmoker Nonsmoker Nonsmoker Smoker    Smoker    Nonsmoker Nonsmoker\n[169] Smoker    Nonsmoker Smoker    Smoker    Smoker    Nonsmoker Nonsmoker\n[176] Smoker    Smoker    Smoker    Smoker    Nonsmoker Nonsmoker Nonsmoker\n[183] Smoker    Smoker    Smoker    Nonsmoker Smoker    Nonsmoker Smoker   \nLevels: Nonsmoker Smoker\n\n\nBut we’re going to use values that have been randomly shuffled, like this one, for example:\n\nset.seed(1729)\nsample(birthwt2$smoke_fct)\n\n  [1] Nonsmoker Smoker    Nonsmoker Nonsmoker Smoker    Nonsmoker Smoker   \n  [8] Nonsmoker Smoker    Nonsmoker Nonsmoker Smoker    Smoker    Nonsmoker\n [15] Nonsmoker Nonsmoker Nonsmoker Smoker    Smoker    Nonsmoker Nonsmoker\n [22] Nonsmoker Smoker    Nonsmoker Nonsmoker Nonsmoker Nonsmoker Smoker   \n [29] Nonsmoker Nonsmoker Nonsmoker Nonsmoker Smoker    Smoker    Nonsmoker\n [36] Smoker    Smoker    Smoker    Nonsmoker Nonsmoker Nonsmoker Nonsmoker\n [43] Nonsmoker Nonsmoker Nonsmoker Smoker    Nonsmoker Nonsmoker Nonsmoker\n [50] Smoker    Nonsmoker Nonsmoker Smoker    Nonsmoker Smoker    Nonsmoker\n [57] Nonsmoker Nonsmoker Smoker    Nonsmoker Nonsmoker Smoker    Smoker   \n [64] Nonsmoker Nonsmoker Smoker    Nonsmoker Nonsmoker Smoker    Nonsmoker\n [71] Nonsmoker Nonsmoker Nonsmoker Smoker    Nonsmoker Nonsmoker Smoker   \n [78] Smoker    Smoker    Smoker    Smoker    Smoker    Smoker    Nonsmoker\n [85] Smoker    Nonsmoker Smoker    Smoker    Smoker    Nonsmoker Nonsmoker\n [92] Nonsmoker Nonsmoker Smoker    Smoker    Nonsmoker Nonsmoker Smoker   \n [99] Smoker    Nonsmoker Nonsmoker Smoker    Nonsmoker Smoker    Nonsmoker\n[106] Nonsmoker Nonsmoker Smoker    Nonsmoker Smoker    Smoker    Smoker   \n[113] Nonsmoker Smoker    Smoker    Nonsmoker Nonsmoker Smoker    Nonsmoker\n[120] Nonsmoker Nonsmoker Nonsmoker Smoker    Smoker    Smoker    Smoker   \n[127] Nonsmoker Nonsmoker Nonsmoker Smoker    Smoker    Smoker    Nonsmoker\n[134] Nonsmoker Nonsmoker Smoker    Nonsmoker Nonsmoker Nonsmoker Smoker   \n[141] Nonsmoker Nonsmoker Smoker    Nonsmoker Nonsmoker Smoker    Nonsmoker\n[148] Smoker    Nonsmoker Nonsmoker Smoker    Nonsmoker Smoker    Smoker   \n[155] Smoker    Nonsmoker Nonsmoker Nonsmoker Smoker    Smoker    Nonsmoker\n[162] Nonsmoker Nonsmoker Nonsmoker Nonsmoker Nonsmoker Nonsmoker Nonsmoker\n[169] Nonsmoker Smoker    Smoker    Nonsmoker Smoker    Nonsmoker Nonsmoker\n[176] Nonsmoker Smoker    Smoker    Nonsmoker Nonsmoker Nonsmoker Nonsmoker\n[183] Smoker    Nonsmoker Nonsmoker Nonsmoker Smoker    Nonsmoker Nonsmoker\nLevels: Nonsmoker Smoker\n\n\nThe infer package will perform this random shuffling over and over again. Given the now arbitrary labels of “Nonsmoker” and “Smoker” (which are meaningless because each women was assigned to one of these labels randomly with no regard to her actual smoking status), infer will calculate the mean birth weights among the first group of women (labeled “Nonsmokers” but not really consisting of all nonsmokers) and the second group of women (labeled “Smokers” but not really consisting of all smokers). Finally infer will compute the difference between those two means. And it will do this process 1000 times.\n\nset.seed(1729)\nbwt_smoke_test &lt;- birthwt2 %&gt;%\n  specify(response = bwt, explanatory = smoke_fct) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 1000, type = \"permute\") %&gt;%\n  calculate(stat = \"diff in means\", order = c(\"Nonsmoker\", \"Smoker\"))\nbwt_smoke_test\n\nResponse: bwt (numeric)\nExplanatory: smoke_fct (factor)\nNull Hypothesis: independence\n# A tibble: 1,000 × 2\n   replicate   stat\n       &lt;int&gt;  &lt;dbl&gt;\n 1         1 -173. \n 2         2  -79.3\n 3         3  -95.8\n 4         4 -253. \n 5         5   31.3\n 6         6 -229. \n 7         7   63.4\n 8         8   13.8\n 9         9   22.6\n10        10 -118. \n# ℹ 990 more rows\n\n\n\nExercise 3\nBefore we graph these simulated values, what do you guess will be the mean value? Keep in mind that we have computed differences in the mean birth weights between two groups of women. But because we have shuffled the smoking labels randomly, we aren’t really calculating the difference in mean birth weights of nonsmokers vs smokers. We’re just computing the difference in mean birth weights of randomly assigned groups of women.\n\nPlease write up your answer here.\n\n\nHere’s the visualization:\n\nbwt_smoke_test %&gt;%\n    visualize()\n\n\n\n\nNo surprise that this histogram looks nearly normal, centered at zero: the simulation is working under the assumption of the null hypothesis of no difference between the groups.\nHere is the same plot but including our sample difference:\n\nbwt_smoke_test %&gt;%\n    visualize() +\n    shade_p_value(obs_stat = obs_diff, direction = \"two_sided\")\n\n\n\n\nOur observed difference (from the sampled data) is quite far out into the tail of this simulated sampling distribution, so it appears that our actual data would be somewhat unlikely due to pure chance alone if the null hypothesis were true.\nWe can even find a P-value by calculating how many of our sampled values are as extreme or more extreme than the observed data difference.\n\nbwt_smoke_test %&gt;%\n    get_p_value(obs_stat = obs_diff, direction = \"two-sided\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.016\n\n\nIndeed, this is a small P-value."
  },
  {
    "objectID": "21-inference_for_two_independent_means-web.html#the-sampling-distribution-model",
    "href": "21-inference_for_two_independent_means-web.html#the-sampling-distribution-model",
    "title": "21  Inference for two independent means",
    "section": "21.6 The sampling distribution model",
    "text": "21.6 The sampling distribution model\nIn the previous section, we simulated the sampling distribution under the assumption of a null hypothesis of no difference between the groups. It certainly looked like a normal model, but which normal model? The center is obviously zero, but what about the standard deviation?\nLet’s assume that both groups come from populations that are normally distributed with normal models \\(N(\\mu_{1}, \\sigma_{1})\\) and \\(N(\\mu_{2}, \\sigma_{2})\\). If we take samples of size \\(n_{1}\\) from group 1 and \\(n_{2}\\) from group 2, some fancy math shows that the distribution of the differences between sample means is\n\\[\nN\\left(\\mu_{1} - \\mu_{2}, \\sqrt{\\frac{\\sigma_{1}^{2}}{n_{1}} + \\frac{\\sigma_{2}^{2}}{n_{2}}}\\right).\n\\]\nUnder the assumption of the null, the difference of the means is zero (\\(\\mu_{1} - \\mu_{2} = 0\\)). Unfortunately, though, we make no assumption on the standard deviations. It should be clear that the only solution is to substitute the sample standard deviations \\(s_{1}\\) and \\(s_{2}\\) for the population standard deviations \\(\\sigma_{1}\\) and \\(\\sigma_{2}\\).1\n\\[\nSE = \\sqrt{\\frac{s_{1}^{2}}{n_{1}} + \\frac{s_{2}^{2}}{n_{2}}}.\n\\]\nHowever, \\(s_{1}\\) and \\(s_{2}\\) are not perfect estimates of \\(\\sigma_{1}\\) and \\(\\sigma_{2}\\); they are subject to sampling variability too. This extra variability means that a normal model is no longer appropriate as the sampling distribution model.\nIn the one-sample case, a Student t model with \\(df = n - 1\\) was the right choice. In the two-sample case, we don’t know the right answer. And I don’t mean that we haven’t learned it yet in our stats class. I mean, statisticians have not found a formula for the correct sampling distribution. It is a famous unsolved problem, called the Behrens-Fisher problem.\nSeveral researchers have proposed solutions that are “close” though. One compelling one is called “Welch’s t test”. Welch showed that even though it’s not quite right, a Student t model is very close as long as you pick the degrees of freedom carefully. Unfortunately, the way to compute the right degrees of freedom is crazy complicated. Fortunately, R is good at crazy complicated computations.\nLet’s go through the full rubric."
  },
  {
    "objectID": "21-inference_for_two_independent_means-web.html#exploratory-data-analysis",
    "href": "21-inference_for_two_independent_means-web.html#exploratory-data-analysis",
    "title": "21  Inference for two independent means",
    "section": "21.7 Exploratory data analysis",
    "text": "21.7 Exploratory data analysis\n\n21.7.1 Use data documentation (help files, code books, Google, etc.) to determine as much as possible about the data provenance and structure.\nType birthwt at the Console to read the help file. We have the same concerns about the lack of details as we did in Chapter 16.\n\nbirthwt\n\n    low age lwt race smoke ptl ht ui ftv  bwt\n85    0  19 182    2     0   0  0  1   0 2523\n86    0  33 155    3     0   0  0  0   3 2551\n87    0  20 105    1     1   0  0  0   1 2557\n88    0  21 108    1     1   0  0  1   2 2594\n89    0  18 107    1     1   0  0  1   0 2600\n91    0  21 124    3     0   0  0  0   0 2622\n92    0  22 118    1     0   0  0  0   1 2637\n93    0  17 103    3     0   0  0  0   1 2637\n94    0  29 123    1     1   0  0  0   1 2663\n95    0  26 113    1     1   0  0  0   0 2665\n96    0  19  95    3     0   0  0  0   0 2722\n97    0  19 150    3     0   0  0  0   1 2733\n98    0  22  95    3     0   0  1  0   0 2751\n99    0  30 107    3     0   1  0  1   2 2750\n100   0  18 100    1     1   0  0  0   0 2769\n101   0  18 100    1     1   0  0  0   0 2769\n102   0  15  98    2     0   0  0  0   0 2778\n103   0  25 118    1     1   0  0  0   3 2782\n104   0  20 120    3     0   0  0  1   0 2807\n105   0  28 120    1     1   0  0  0   1 2821\n106   0  32 121    3     0   0  0  0   2 2835\n107   0  31 100    1     0   0  0  1   3 2835\n108   0  36 202    1     0   0  0  0   1 2836\n109   0  28 120    3     0   0  0  0   0 2863\n111   0  25 120    3     0   0  0  1   2 2877\n112   0  28 167    1     0   0  0  0   0 2877\n113   0  17 122    1     1   0  0  0   0 2906\n114   0  29 150    1     0   0  0  0   2 2920\n115   0  26 168    2     1   0  0  0   0 2920\n116   0  17 113    2     0   0  0  0   1 2920\n117   0  17 113    2     0   0  0  0   1 2920\n118   0  24  90    1     1   1  0  0   1 2948\n119   0  35 121    2     1   1  0  0   1 2948\n120   0  25 155    1     0   0  0  0   1 2977\n121   0  25 125    2     0   0  0  0   0 2977\n123   0  29 140    1     1   0  0  0   2 2977\n124   0  19 138    1     1   0  0  0   2 2977\n125   0  27 124    1     1   0  0  0   0 2922\n126   0  31 215    1     1   0  0  0   2 3005\n127   0  33 109    1     1   0  0  0   1 3033\n128   0  21 185    2     1   0  0  0   2 3042\n129   0  19 189    1     0   0  0  0   2 3062\n130   0  23 130    2     0   0  0  0   1 3062\n131   0  21 160    1     0   0  0  0   0 3062\n132   0  18  90    1     1   0  0  1   0 3062\n133   0  18  90    1     1   0  0  1   0 3062\n134   0  32 132    1     0   0  0  0   4 3080\n135   0  19 132    3     0   0  0  0   0 3090\n136   0  24 115    1     0   0  0  0   2 3090\n137   0  22  85    3     1   0  0  0   0 3090\n138   0  22 120    1     0   0  1  0   1 3100\n139   0  23 128    3     0   0  0  0   0 3104\n140   0  22 130    1     1   0  0  0   0 3132\n141   0  30  95    1     1   0  0  0   2 3147\n142   0  19 115    3     0   0  0  0   0 3175\n143   0  16 110    3     0   0  0  0   0 3175\n144   0  21 110    3     1   0  0  1   0 3203\n145   0  30 153    3     0   0  0  0   0 3203\n146   0  20 103    3     0   0  0  0   0 3203\n147   0  17 119    3     0   0  0  0   0 3225\n148   0  17 119    3     0   0  0  0   0 3225\n149   0  23 119    3     0   0  0  0   2 3232\n150   0  24 110    3     0   0  0  0   0 3232\n151   0  28 140    1     0   0  0  0   0 3234\n154   0  26 133    3     1   2  0  0   0 3260\n155   0  20 169    3     0   1  0  1   1 3274\n156   0  24 115    3     0   0  0  0   2 3274\n159   0  28 250    3     1   0  0  0   6 3303\n160   0  20 141    1     0   2  0  1   1 3317\n161   0  22 158    2     0   1  0  0   2 3317\n162   0  22 112    1     1   2  0  0   0 3317\n163   0  31 150    3     1   0  0  0   2 3321\n164   0  23 115    3     1   0  0  0   1 3331\n166   0  16 112    2     0   0  0  0   0 3374\n167   0  16 135    1     1   0  0  0   0 3374\n168   0  18 229    2     0   0  0  0   0 3402\n169   0  25 140    1     0   0  0  0   1 3416\n170   0  32 134    1     1   1  0  0   4 3430\n172   0  20 121    2     1   0  0  0   0 3444\n173   0  23 190    1     0   0  0  0   0 3459\n174   0  22 131    1     0   0  0  0   1 3460\n175   0  32 170    1     0   0  0  0   0 3473\n176   0  30 110    3     0   0  0  0   0 3544\n177   0  20 127    3     0   0  0  0   0 3487\n179   0  23 123    3     0   0  0  0   0 3544\n180   0  17 120    3     1   0  0  0   0 3572\n181   0  19 105    3     0   0  0  0   0 3572\n182   0  23 130    1     0   0  0  0   0 3586\n183   0  36 175    1     0   0  0  0   0 3600\n184   0  22 125    1     0   0  0  0   1 3614\n185   0  24 133    1     0   0  0  0   0 3614\n186   0  21 134    3     0   0  0  0   2 3629\n187   0  19 235    1     1   0  1  0   0 3629\n188   0  25  95    1     1   3  0  1   0 3637\n189   0  16 135    1     1   0  0  0   0 3643\n190   0  29 135    1     0   0  0  0   1 3651\n191   0  29 154    1     0   0  0  0   1 3651\n192   0  19 147    1     1   0  0  0   0 3651\n193   0  19 147    1     1   0  0  0   0 3651\n195   0  30 137    1     0   0  0  0   1 3699\n196   0  24 110    1     0   0  0  0   1 3728\n197   0  19 184    1     1   0  1  0   0 3756\n199   0  24 110    3     0   1  0  0   0 3770\n200   0  23 110    1     0   0  0  0   1 3770\n201   0  20 120    3     0   0  0  0   0 3770\n202   0  25 241    2     0   0  1  0   0 3790\n203   0  30 112    1     0   0  0  0   1 3799\n204   0  22 169    1     0   0  0  0   0 3827\n205   0  18 120    1     1   0  0  0   2 3856\n206   0  16 170    2     0   0  0  0   4 3860\n207   0  32 186    1     0   0  0  0   2 3860\n208   0  18 120    3     0   0  0  0   1 3884\n209   0  29 130    1     1   0  0  0   2 3884\n210   0  33 117    1     0   0  0  1   1 3912\n211   0  20 170    1     1   0  0  0   0 3940\n212   0  28 134    3     0   0  0  0   1 3941\n213   0  14 135    1     0   0  0  0   0 3941\n214   0  28 130    3     0   0  0  0   0 3969\n215   0  25 120    1     0   0  0  0   2 3983\n216   0  16  95    3     0   0  0  0   1 3997\n217   0  20 158    1     0   0  0  0   1 3997\n218   0  26 160    3     0   0  0  0   0 4054\n219   0  21 115    1     0   0  0  0   1 4054\n220   0  22 129    1     0   0  0  0   0 4111\n221   0  25 130    1     0   0  0  0   2 4153\n222   0  31 120    1     0   0  0  0   2 4167\n223   0  35 170    1     0   1  0  0   1 4174\n224   0  19 120    1     1   0  0  0   0 4238\n225   0  24 116    1     0   0  0  0   1 4593\n226   0  45 123    1     0   0  0  0   1 4990\n4     1  28 120    3     1   1  0  1   0  709\n10    1  29 130    1     0   0  0  1   2 1021\n11    1  34 187    2     1   0  1  0   0 1135\n13    1  25 105    3     0   1  1  0   0 1330\n15    1  25  85    3     0   0  0  1   0 1474\n16    1  27 150    3     0   0  0  0   0 1588\n17    1  23  97    3     0   0  0  1   1 1588\n18    1  24 128    2     0   1  0  0   1 1701\n19    1  24 132    3     0   0  1  0   0 1729\n20    1  21 165    1     1   0  1  0   1 1790\n22    1  32 105    1     1   0  0  0   0 1818\n23    1  19  91    1     1   2  0  1   0 1885\n24    1  25 115    3     0   0  0  0   0 1893\n25    1  16 130    3     0   0  0  0   1 1899\n26    1  25  92    1     1   0  0  0   0 1928\n27    1  20 150    1     1   0  0  0   2 1928\n28    1  21 200    2     0   0  0  1   2 1928\n29    1  24 155    1     1   1  0  0   0 1936\n30    1  21 103    3     0   0  0  0   0 1970\n31    1  20 125    3     0   0  0  1   0 2055\n32    1  25  89    3     0   2  0  0   1 2055\n33    1  19 102    1     0   0  0  0   2 2082\n34    1  19 112    1     1   0  0  1   0 2084\n35    1  26 117    1     1   1  0  0   0 2084\n36    1  24 138    1     0   0  0  0   0 2100\n37    1  17 130    3     1   1  0  1   0 2125\n40    1  20 120    2     1   0  0  0   3 2126\n42    1  22 130    1     1   1  0  1   1 2187\n43    1  27 130    2     0   0  0  1   0 2187\n44    1  20  80    3     1   0  0  1   0 2211\n45    1  17 110    1     1   0  0  0   0 2225\n46    1  25 105    3     0   1  0  0   1 2240\n47    1  20 109    3     0   0  0  0   0 2240\n49    1  18 148    3     0   0  0  0   0 2282\n50    1  18 110    2     1   1  0  0   0 2296\n51    1  20 121    1     1   1  0  1   0 2296\n52    1  21 100    3     0   1  0  0   4 2301\n54    1  26  96    3     0   0  0  0   0 2325\n56    1  31 102    1     1   1  0  0   1 2353\n57    1  15 110    1     0   0  0  0   0 2353\n59    1  23 187    2     1   0  0  0   1 2367\n60    1  20 122    2     1   0  0  0   0 2381\n61    1  24 105    2     1   0  0  0   0 2381\n62    1  15 115    3     0   0  0  1   0 2381\n63    1  23 120    3     0   0  0  0   0 2410\n65    1  30 142    1     1   1  0  0   0 2410\n67    1  22 130    1     1   0  0  0   1 2410\n68    1  17 120    1     1   0  0  0   3 2414\n69    1  23 110    1     1   1  0  0   0 2424\n71    1  17 120    2     0   0  0  0   2 2438\n75    1  26 154    3     0   1  1  0   1 2442\n76    1  20 105    3     0   0  0  0   3 2450\n77    1  26 190    1     1   0  0  0   0 2466\n78    1  14 101    3     1   1  0  0   0 2466\n79    1  28  95    1     1   0  0  0   2 2466\n81    1  14 100    3     0   0  0  0   2 2495\n82    1  23  94    3     1   0  0  0   0 2495\n83    1  17 142    2     0   0  1  0   0 2495\n84    1  21 130    1     1   0  1  0   3 2495\n\n\n\nglimpse(birthwt)\n\nRows: 189\nColumns: 10\n$ low   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ age   &lt;int&gt; 19, 33, 20, 21, 18, 21, 22, 17, 29, 26, 19, 19, 22, 30, 18, 18, …\n$ lwt   &lt;int&gt; 182, 155, 105, 108, 107, 124, 118, 103, 123, 113, 95, 150, 95, 1…\n$ race  &lt;int&gt; 2, 3, 1, 1, 1, 3, 1, 3, 1, 1, 3, 3, 3, 3, 1, 1, 2, 1, 3, 1, 3, 1…\n$ smoke &lt;int&gt; 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0…\n$ ptl   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ht    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ ui    &lt;int&gt; 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1…\n$ ftv   &lt;int&gt; 0, 3, 1, 2, 0, 0, 1, 1, 1, 0, 0, 1, 0, 2, 0, 0, 0, 3, 0, 1, 2, 3…\n$ bwt   &lt;int&gt; 2523, 2551, 2557, 2594, 2600, 2622, 2637, 2637, 2663, 2665, 2722…\n\n\n\n\n21.7.2 Prepare the data for analysis.\nWe need to be sure smoke is a factor variable, so we create the new tibble birthwt2 with the mutated variable smoke_fct.\n\nbirthwt2 &lt;- birthwt %&gt;%\n    mutate(smoke_fct = factor(smoke, levels = c(0, 1), labels = c(\"Nonsmoker\", \"Smoker\")))\nbirthwt2\n\n    low age lwt race smoke ptl ht ui ftv  bwt smoke_fct\n85    0  19 182    2     0   0  0  1   0 2523 Nonsmoker\n86    0  33 155    3     0   0  0  0   3 2551 Nonsmoker\n87    0  20 105    1     1   0  0  0   1 2557    Smoker\n88    0  21 108    1     1   0  0  1   2 2594    Smoker\n89    0  18 107    1     1   0  0  1   0 2600    Smoker\n91    0  21 124    3     0   0  0  0   0 2622 Nonsmoker\n92    0  22 118    1     0   0  0  0   1 2637 Nonsmoker\n93    0  17 103    3     0   0  0  0   1 2637 Nonsmoker\n94    0  29 123    1     1   0  0  0   1 2663    Smoker\n95    0  26 113    1     1   0  0  0   0 2665    Smoker\n96    0  19  95    3     0   0  0  0   0 2722 Nonsmoker\n97    0  19 150    3     0   0  0  0   1 2733 Nonsmoker\n98    0  22  95    3     0   0  1  0   0 2751 Nonsmoker\n99    0  30 107    3     0   1  0  1   2 2750 Nonsmoker\n100   0  18 100    1     1   0  0  0   0 2769    Smoker\n101   0  18 100    1     1   0  0  0   0 2769    Smoker\n102   0  15  98    2     0   0  0  0   0 2778 Nonsmoker\n103   0  25 118    1     1   0  0  0   3 2782    Smoker\n104   0  20 120    3     0   0  0  1   0 2807 Nonsmoker\n105   0  28 120    1     1   0  0  0   1 2821    Smoker\n106   0  32 121    3     0   0  0  0   2 2835 Nonsmoker\n107   0  31 100    1     0   0  0  1   3 2835 Nonsmoker\n108   0  36 202    1     0   0  0  0   1 2836 Nonsmoker\n109   0  28 120    3     0   0  0  0   0 2863 Nonsmoker\n111   0  25 120    3     0   0  0  1   2 2877 Nonsmoker\n112   0  28 167    1     0   0  0  0   0 2877 Nonsmoker\n113   0  17 122    1     1   0  0  0   0 2906    Smoker\n114   0  29 150    1     0   0  0  0   2 2920 Nonsmoker\n115   0  26 168    2     1   0  0  0   0 2920    Smoker\n116   0  17 113    2     0   0  0  0   1 2920 Nonsmoker\n117   0  17 113    2     0   0  0  0   1 2920 Nonsmoker\n118   0  24  90    1     1   1  0  0   1 2948    Smoker\n119   0  35 121    2     1   1  0  0   1 2948    Smoker\n120   0  25 155    1     0   0  0  0   1 2977 Nonsmoker\n121   0  25 125    2     0   0  0  0   0 2977 Nonsmoker\n123   0  29 140    1     1   0  0  0   2 2977    Smoker\n124   0  19 138    1     1   0  0  0   2 2977    Smoker\n125   0  27 124    1     1   0  0  0   0 2922    Smoker\n126   0  31 215    1     1   0  0  0   2 3005    Smoker\n127   0  33 109    1     1   0  0  0   1 3033    Smoker\n128   0  21 185    2     1   0  0  0   2 3042    Smoker\n129   0  19 189    1     0   0  0  0   2 3062 Nonsmoker\n130   0  23 130    2     0   0  0  0   1 3062 Nonsmoker\n131   0  21 160    1     0   0  0  0   0 3062 Nonsmoker\n132   0  18  90    1     1   0  0  1   0 3062    Smoker\n133   0  18  90    1     1   0  0  1   0 3062    Smoker\n134   0  32 132    1     0   0  0  0   4 3080 Nonsmoker\n135   0  19 132    3     0   0  0  0   0 3090 Nonsmoker\n136   0  24 115    1     0   0  0  0   2 3090 Nonsmoker\n137   0  22  85    3     1   0  0  0   0 3090    Smoker\n138   0  22 120    1     0   0  1  0   1 3100 Nonsmoker\n139   0  23 128    3     0   0  0  0   0 3104 Nonsmoker\n140   0  22 130    1     1   0  0  0   0 3132    Smoker\n141   0  30  95    1     1   0  0  0   2 3147    Smoker\n142   0  19 115    3     0   0  0  0   0 3175 Nonsmoker\n143   0  16 110    3     0   0  0  0   0 3175 Nonsmoker\n144   0  21 110    3     1   0  0  1   0 3203    Smoker\n145   0  30 153    3     0   0  0  0   0 3203 Nonsmoker\n146   0  20 103    3     0   0  0  0   0 3203 Nonsmoker\n147   0  17 119    3     0   0  0  0   0 3225 Nonsmoker\n148   0  17 119    3     0   0  0  0   0 3225 Nonsmoker\n149   0  23 119    3     0   0  0  0   2 3232 Nonsmoker\n150   0  24 110    3     0   0  0  0   0 3232 Nonsmoker\n151   0  28 140    1     0   0  0  0   0 3234 Nonsmoker\n154   0  26 133    3     1   2  0  0   0 3260    Smoker\n155   0  20 169    3     0   1  0  1   1 3274 Nonsmoker\n156   0  24 115    3     0   0  0  0   2 3274 Nonsmoker\n159   0  28 250    3     1   0  0  0   6 3303    Smoker\n160   0  20 141    1     0   2  0  1   1 3317 Nonsmoker\n161   0  22 158    2     0   1  0  0   2 3317 Nonsmoker\n162   0  22 112    1     1   2  0  0   0 3317    Smoker\n163   0  31 150    3     1   0  0  0   2 3321    Smoker\n164   0  23 115    3     1   0  0  0   1 3331    Smoker\n166   0  16 112    2     0   0  0  0   0 3374 Nonsmoker\n167   0  16 135    1     1   0  0  0   0 3374    Smoker\n168   0  18 229    2     0   0  0  0   0 3402 Nonsmoker\n169   0  25 140    1     0   0  0  0   1 3416 Nonsmoker\n170   0  32 134    1     1   1  0  0   4 3430    Smoker\n172   0  20 121    2     1   0  0  0   0 3444    Smoker\n173   0  23 190    1     0   0  0  0   0 3459 Nonsmoker\n174   0  22 131    1     0   0  0  0   1 3460 Nonsmoker\n175   0  32 170    1     0   0  0  0   0 3473 Nonsmoker\n176   0  30 110    3     0   0  0  0   0 3544 Nonsmoker\n177   0  20 127    3     0   0  0  0   0 3487 Nonsmoker\n179   0  23 123    3     0   0  0  0   0 3544 Nonsmoker\n180   0  17 120    3     1   0  0  0   0 3572    Smoker\n181   0  19 105    3     0   0  0  0   0 3572 Nonsmoker\n182   0  23 130    1     0   0  0  0   0 3586 Nonsmoker\n183   0  36 175    1     0   0  0  0   0 3600 Nonsmoker\n184   0  22 125    1     0   0  0  0   1 3614 Nonsmoker\n185   0  24 133    1     0   0  0  0   0 3614 Nonsmoker\n186   0  21 134    3     0   0  0  0   2 3629 Nonsmoker\n187   0  19 235    1     1   0  1  0   0 3629    Smoker\n188   0  25  95    1     1   3  0  1   0 3637    Smoker\n189   0  16 135    1     1   0  0  0   0 3643    Smoker\n190   0  29 135    1     0   0  0  0   1 3651 Nonsmoker\n191   0  29 154    1     0   0  0  0   1 3651 Nonsmoker\n192   0  19 147    1     1   0  0  0   0 3651    Smoker\n193   0  19 147    1     1   0  0  0   0 3651    Smoker\n195   0  30 137    1     0   0  0  0   1 3699 Nonsmoker\n196   0  24 110    1     0   0  0  0   1 3728 Nonsmoker\n197   0  19 184    1     1   0  1  0   0 3756    Smoker\n199   0  24 110    3     0   1  0  0   0 3770 Nonsmoker\n200   0  23 110    1     0   0  0  0   1 3770 Nonsmoker\n201   0  20 120    3     0   0  0  0   0 3770 Nonsmoker\n202   0  25 241    2     0   0  1  0   0 3790 Nonsmoker\n203   0  30 112    1     0   0  0  0   1 3799 Nonsmoker\n204   0  22 169    1     0   0  0  0   0 3827 Nonsmoker\n205   0  18 120    1     1   0  0  0   2 3856    Smoker\n206   0  16 170    2     0   0  0  0   4 3860 Nonsmoker\n207   0  32 186    1     0   0  0  0   2 3860 Nonsmoker\n208   0  18 120    3     0   0  0  0   1 3884 Nonsmoker\n209   0  29 130    1     1   0  0  0   2 3884    Smoker\n210   0  33 117    1     0   0  0  1   1 3912 Nonsmoker\n211   0  20 170    1     1   0  0  0   0 3940    Smoker\n212   0  28 134    3     0   0  0  0   1 3941 Nonsmoker\n213   0  14 135    1     0   0  0  0   0 3941 Nonsmoker\n214   0  28 130    3     0   0  0  0   0 3969 Nonsmoker\n215   0  25 120    1     0   0  0  0   2 3983 Nonsmoker\n216   0  16  95    3     0   0  0  0   1 3997 Nonsmoker\n217   0  20 158    1     0   0  0  0   1 3997 Nonsmoker\n218   0  26 160    3     0   0  0  0   0 4054 Nonsmoker\n219   0  21 115    1     0   0  0  0   1 4054 Nonsmoker\n220   0  22 129    1     0   0  0  0   0 4111 Nonsmoker\n221   0  25 130    1     0   0  0  0   2 4153 Nonsmoker\n222   0  31 120    1     0   0  0  0   2 4167 Nonsmoker\n223   0  35 170    1     0   1  0  0   1 4174 Nonsmoker\n224   0  19 120    1     1   0  0  0   0 4238    Smoker\n225   0  24 116    1     0   0  0  0   1 4593 Nonsmoker\n226   0  45 123    1     0   0  0  0   1 4990 Nonsmoker\n4     1  28 120    3     1   1  0  1   0  709    Smoker\n10    1  29 130    1     0   0  0  1   2 1021 Nonsmoker\n11    1  34 187    2     1   0  1  0   0 1135    Smoker\n13    1  25 105    3     0   1  1  0   0 1330 Nonsmoker\n15    1  25  85    3     0   0  0  1   0 1474 Nonsmoker\n16    1  27 150    3     0   0  0  0   0 1588 Nonsmoker\n17    1  23  97    3     0   0  0  1   1 1588 Nonsmoker\n18    1  24 128    2     0   1  0  0   1 1701 Nonsmoker\n19    1  24 132    3     0   0  1  0   0 1729 Nonsmoker\n20    1  21 165    1     1   0  1  0   1 1790    Smoker\n22    1  32 105    1     1   0  0  0   0 1818    Smoker\n23    1  19  91    1     1   2  0  1   0 1885    Smoker\n24    1  25 115    3     0   0  0  0   0 1893 Nonsmoker\n25    1  16 130    3     0   0  0  0   1 1899 Nonsmoker\n26    1  25  92    1     1   0  0  0   0 1928    Smoker\n27    1  20 150    1     1   0  0  0   2 1928    Smoker\n28    1  21 200    2     0   0  0  1   2 1928 Nonsmoker\n29    1  24 155    1     1   1  0  0   0 1936    Smoker\n30    1  21 103    3     0   0  0  0   0 1970 Nonsmoker\n31    1  20 125    3     0   0  0  1   0 2055 Nonsmoker\n32    1  25  89    3     0   2  0  0   1 2055 Nonsmoker\n33    1  19 102    1     0   0  0  0   2 2082 Nonsmoker\n34    1  19 112    1     1   0  0  1   0 2084    Smoker\n35    1  26 117    1     1   1  0  0   0 2084    Smoker\n36    1  24 138    1     0   0  0  0   0 2100 Nonsmoker\n37    1  17 130    3     1   1  0  1   0 2125    Smoker\n40    1  20 120    2     1   0  0  0   3 2126    Smoker\n42    1  22 130    1     1   1  0  1   1 2187    Smoker\n43    1  27 130    2     0   0  0  1   0 2187 Nonsmoker\n44    1  20  80    3     1   0  0  1   0 2211    Smoker\n45    1  17 110    1     1   0  0  0   0 2225    Smoker\n46    1  25 105    3     0   1  0  0   1 2240 Nonsmoker\n47    1  20 109    3     0   0  0  0   0 2240 Nonsmoker\n49    1  18 148    3     0   0  0  0   0 2282 Nonsmoker\n50    1  18 110    2     1   1  0  0   0 2296    Smoker\n51    1  20 121    1     1   1  0  1   0 2296    Smoker\n52    1  21 100    3     0   1  0  0   4 2301 Nonsmoker\n54    1  26  96    3     0   0  0  0   0 2325 Nonsmoker\n56    1  31 102    1     1   1  0  0   1 2353    Smoker\n57    1  15 110    1     0   0  0  0   0 2353 Nonsmoker\n59    1  23 187    2     1   0  0  0   1 2367    Smoker\n60    1  20 122    2     1   0  0  0   0 2381    Smoker\n61    1  24 105    2     1   0  0  0   0 2381    Smoker\n62    1  15 115    3     0   0  0  1   0 2381 Nonsmoker\n63    1  23 120    3     0   0  0  0   0 2410 Nonsmoker\n65    1  30 142    1     1   1  0  0   0 2410    Smoker\n67    1  22 130    1     1   0  0  0   1 2410    Smoker\n68    1  17 120    1     1   0  0  0   3 2414    Smoker\n69    1  23 110    1     1   1  0  0   0 2424    Smoker\n71    1  17 120    2     0   0  0  0   2 2438 Nonsmoker\n75    1  26 154    3     0   1  1  0   1 2442 Nonsmoker\n76    1  20 105    3     0   0  0  0   3 2450 Nonsmoker\n77    1  26 190    1     1   0  0  0   0 2466    Smoker\n78    1  14 101    3     1   1  0  0   0 2466    Smoker\n79    1  28  95    1     1   0  0  0   2 2466    Smoker\n81    1  14 100    3     0   0  0  0   2 2495 Nonsmoker\n82    1  23  94    3     1   0  0  0   0 2495    Smoker\n83    1  17 142    2     0   0  1  0   0 2495 Nonsmoker\n84    1  21 130    1     1   0  1  0   3 2495    Smoker\n\n\n\nglimpse(birthwt2)\n\nRows: 189\nColumns: 11\n$ low       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ age       &lt;int&gt; 19, 33, 20, 21, 18, 21, 22, 17, 29, 26, 19, 19, 22, 30, 18, …\n$ lwt       &lt;int&gt; 182, 155, 105, 108, 107, 124, 118, 103, 123, 113, 95, 150, 9…\n$ race      &lt;int&gt; 2, 3, 1, 1, 1, 3, 1, 3, 1, 1, 3, 3, 3, 3, 1, 1, 2, 1, 3, 1, …\n$ smoke     &lt;int&gt; 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, …\n$ ptl       &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, …\n$ ht        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …\n$ ui        &lt;int&gt; 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, …\n$ ftv       &lt;int&gt; 0, 3, 1, 2, 0, 0, 1, 1, 1, 0, 0, 1, 0, 2, 0, 0, 0, 3, 0, 1, …\n$ bwt       &lt;int&gt; 2523, 2551, 2557, 2594, 2600, 2622, 2637, 2637, 2663, 2665, …\n$ smoke_fct &lt;fct&gt; Nonsmoker, Nonsmoker, Smoker, Smoker, Smoker, Nonsmoker, Non…\n\n\n\n\n21.7.3 Make tables or plots to explore the data visually.\nHow many women are in each group?\n\ntabyl(birthwt2, smoke_fct) %&gt;%\n  adorn_totals()\n\n smoke_fct   n   percent\n Nonsmoker 115 0.6084656\n    Smoker  74 0.3915344\n     Total 189 1.0000000\n\n\nWith a numerical response variable and a categorical predictor variable, there are two useful plots: a side-by-side boxplot and a stacked histogram.\n\nggplot(birthwt2, aes(y = bwt, x = smoke_fct)) +\n    geom_boxplot()\n\n\n\n\n\nggplot(birthwt2, aes(x = bwt)) +\n    geom_histogram(binwidth = 250, boundary = 0) +\n    facet_grid(smoke_fct ~ .)\n\n\n\n\nThe histograms for both groups look sort of normal, but the nonsmoker group may be a little left skewed and the smoker group may have some low outliers. Here are the QQ plots to give us another way to ascertain normality of the data.\n\nggplot(birthwt2, aes(sample = bwt)) +\n    geom_qq() +\n    geom_qq_line() +\n    facet_grid(smoke_fct ~ .)\n\n\n\n\nThere’s a little deviation from normality, but nothing too crazy.\nCommentary: The boxplots and histograms show why statistical inference is so important. It’s clear that there is some difference between the two groups, but it’s not obvious if that difference will turn out to be statistically significant. There appears to be a lot of variability in both groups, and both groups have a fair number of lighter and heavier babies."
  },
  {
    "objectID": "21-inference_for_two_independent_means-web.html#hypotheses",
    "href": "21-inference_for_two_independent_means-web.html#hypotheses",
    "title": "21  Inference for two independent means",
    "section": "21.8 Hypotheses",
    "text": "21.8 Hypotheses\n\n21.8.1 Identify the sample (or samples) and a reasonable population (or populations) of interest.\nThe samples consist of 115 nonsmoking mothers and 74 smoking mothers. The populations are those women who do not smoke during pregnancy and those women who do smoke during pregnancy.\n\n\n21.8.2 Express the null and alternative hypotheses as contextually meaningful full sentences.\n\\(H_{0}:\\) There is no difference in the birth weight of babies born to mothers who do not smoke versus mothers who do smoke.\n\\(H_{A}:\\) There is a difference in the birth weight of babies born to mothers who do not smoke versus mothers who do smoke.\n\n\n21.8.3 Express the null and alternative hypotheses in symbols (when possible).\n\\(H_{0}: \\mu_{Nonsmoker} - \\mu_{Smoker} = 0\\)\n\\(H_{A}: \\mu_{Nonsmoker} - \\mu_{Smoker} \\neq 0\\)\nCommentary: As mentioned before, the order in which you subtract will not change the inference, but it will affect your interpretation of the results. Also, once you’ve chosen a direction to subtract, be consistent about that choice throughout the rubric."
  },
  {
    "objectID": "21-inference_for_two_independent_means-web.html#model",
    "href": "21-inference_for_two_independent_means-web.html#model",
    "title": "21  Inference for two independent means",
    "section": "21.9 Model",
    "text": "21.9 Model\n\n21.9.1 Identify the sampling distribution model.\nWe use a t model with the number of degrees of freedom to be determined.\nCommentary: For Welch’s t test, the degrees of freedom won’t usually be a whole number. Be sure you understand that the formula is no longer \\(df = n - 1\\). That doesn’t even make any sense as there isn’t a single \\(n\\) in a two-sample test. The infer package will tell us how many degrees of freedom to use later in the Mechanics section.\n\n\n21.9.2 Check the relevant conditions to ensure that model assumptions are met.\n\nRandom (for both groups)\n\nWe have very little information about these women. We hope that the 115 nonsmoking mothers at this hospital are representative of other nonsmoking mothers, at least in that region at that time. And same for the 74 smoking mothers.\n\n10% (for both groups)\n\n115 is less than 10% of all nonsmoking mothers and 74 is less than 10% of all smoking mothers.\n\nNearly normal (for both groups)\n\nSince the sample sizes are more than 30 in each group, we meet the condition."
  },
  {
    "objectID": "21-inference_for_two_independent_means-web.html#mechanics",
    "href": "21-inference_for_two_independent_means-web.html#mechanics",
    "title": "21  Inference for two independent means",
    "section": "21.10 Mechanics",
    "text": "21.10 Mechanics\n\n21.10.1 Compute the test statistic.\n\nobs_diff &lt;- birthwt2 %&gt;%\n  specify(response = bwt, explanatory = smoke_fct) %&gt;%\n  calculate(stat = \"diff in means\", order = c(\"Nonsmoker\", \"Smoker\"))\nobs_diff\n\nResponse: bwt (numeric)\nExplanatory: smoke_fct (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  284.\n\n\n\nobs_diff_t &lt;- birthwt2 %&gt;%\n  specify(response = bwt, explanatory = smoke_fct) %&gt;%\n  calculate(stat = \"t\", order = c(\"Nonsmoker\", \"Smoker\"))\nobs_diff_t\n\nResponse: bwt (numeric)\nExplanatory: smoke_fct (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  2.73\n\n\n\n\n21.10.2 Report the test statistic in context (when possible).\nThe difference in the mean birth weight of babies born to nonsmoking mothers and smoking mothers is 283.7767333 grams. This was obtained by subtracting nonsmoking mothers minus smoking mothers. In other words, the fact that this is positive indicates that nonsmoking mothers had heavier babies, on average, than smoking mothers.\nThe t score is 2.7298857. The sample difference in birth weights is about 2.7 standard errors higher than the null value of zero.\nCommentary: Remember that whenever you are computing the difference between two quantities, you must indicate the direction of that difference you so your reader knows how to interpret the value, whether it is positive or negative.\n\n\n21.10.3 Plot the null distribution.\n\nbwt_smoke_test_t &lt;- birthwt2 %&gt;%\n  specify(response = bwt, explanatory = smoke_fct) %&gt;%\n  hypothesise(null = \"independence\") %&gt;%\n  assume(\"t\")\nbwt_smoke_test_t\n\nA T distribution with 170 degrees of freedom.\n\n\n\nbwt_smoke_test_t %&gt;%\n  visualize() +\n  shade_p_value(obs_stat = obs_diff_t, direction = \"two-sided\")\n\n\n\n\nCommentary: We use the name bwt_smoke_test_t (using the assumption of a Student t model) as a new variable name so that it doesn’t overwrite the variable bwt_smoke_test we performed earlier as a permutation test (the one with the shuffling). This results of using bwt_smoke_test versus bwt_smoke_test_t will be very similar.\nNote that the infer output tells us there are 170 degrees of freedom. (It turns out to be 170.1.) Note that this number is the result of a complicated formula, and it’s not just a simple function of the sample sizes 115 and 74.\nFinally, note that the alternative hypothesis indicated a two-sided test, so we need to specify a “two-sided” P-value in the shade_p_value command.\n\n\n21.10.4 Calculate the P-value.\n\nbwt_smoke_p &lt;- bwt_smoke_test_t %&gt;%\n  get_p_value(obs_stat = obs_diff_t, direction = \"two-sided\")\nbwt_smoke_p\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1 0.00700\n\n\n\n\n21.10.5 Interpret the P-value as a probability given the null.\nThe P-value is 0.0070025. If there were no difference in the mean birth weights between nonsmoking and smoking women, there would be a 0.7002548% chance of seeing data at least as extreme as what we saw."
  },
  {
    "objectID": "21-inference_for_two_independent_means-web.html#conclusion",
    "href": "21-inference_for_two_independent_means-web.html#conclusion",
    "title": "21  Inference for two independent means",
    "section": "21.11 Conclusion",
    "text": "21.11 Conclusion\n\n21.11.1 State the statistical conclusion.\nWe reject the null hypothesis.\n\n\n21.11.2 State (but do not overstate) a contextually meaningful conclusion.\nWe have sufficient evidence that there is a difference in the mean birth weight of babies born to mothers who do not smoke versus mothers who do smoke.\n\n\n21.11.3 Express reservations or uncertainty about the generalizability of the conclusion.\nAs when we looked at this data before, our uncertainly about the data provenance means that we don’t know if the difference observed in these samples at this one hospital at this one time are generalizable to larger populations. Also keep in mind that this data is observational, so we cannot draw any causal conclusion about the “effect” of smoking on birth weight.\n\n\n21.11.4 Identify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.\nIf we’ve made a Type I error, then that means that there might be no difference in the birth weights of babies from nonsmoking versus smoking mothers, but we got some unusual samples that showed a difference."
  },
  {
    "objectID": "21-inference_for_two_independent_means-web.html#confidence-interval",
    "href": "21-inference_for_two_independent_means-web.html#confidence-interval",
    "title": "21  Inference for two independent means",
    "section": "21.12 Confidence interval",
    "text": "21.12 Confidence interval\n\n21.12.1 Check the relevant conditions to ensure that model assumptions are met.\nThere are no additional conditions to check.\n\n\n21.12.2 Calculate the confidence interval.\n\nbwt_smoke_ci &lt;- bwt_smoke_test_t %&gt;%\n  get_confidence_interval(point_estimate = obs_diff, level = 0.95)\nbwt_smoke_ci\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     78.6     489.\n\n\nCommentary: Pay close attention to when we use obs_diff and obs_diff_t. In the hypothesis test, we assumed a t distribution for the null and so we have to use the t score obs_diff_t to shade the P-value. However, for a confidence interval, we are building the interval centered on our sample difference obs_diff.\n\n\n21.12.3 State (but do not overstate) a contextually meaningful interpretation.\nWe are 95% confident that the true difference in birth weight between nonsmoking and smoking mothers is captured in the interval (78.5748631 g, 488.9786034 g). We obtained this by subtracting nonsmokers minus smokers.\nCommentary: Again, remember to indicate the direction of the difference by indicating the order of subtraction.\n\n\n21.12.4 If running a two-sided test, explain how the confidence interval reinforces the conclusion of the hypothesis test.\nSince zero is not contained in the confidence interval, zero is not a plausible value for the true difference in birth weights between the two groups of mothers.\n\n\n21.12.5 When comparing two groups, comment on the effect size and the practical significance of the result.\nIn order to know if smoking is a risk factor for low birth weight, we would need to know what a difference of 80 g or 490 grams means for babies. Although most of us presumably don’t have any special training in obstetrics, we could do a quick internet search to see that even half a kilogram is not a large amount of weight difference between two babies. Having said that, though, any difference in birth weight that might be attributable to smoking could be a concern to doctors. In any event, our data is observational, so we cannot make causal claims here."
  },
  {
    "objectID": "21-inference_for_two_independent_means-web.html#your-turn",
    "href": "21-inference_for_two_independent_means-web.html#your-turn",
    "title": "21  Inference for two independent means",
    "section": "21.13 Your turn",
    "text": "21.13 Your turn\nContinue to use the birthwt data set. This time, see if a history of hypertension is associated with a difference in the mean birth weight of babies. In the “Prepare the data for analysis” section, you will need to create a new tibble—call it birthwt3—in which you convert the ht variable to a factor variable.\nThe rubric outline is reproduced below. You may refer to the worked example above and modify it accordingly. Remember to strip out all the commentary. That is just exposition for your benefit in understanding the steps, but is not meant to form part of the formal inference process.\nAnother word of warning: the copy/paste process is not a substitute for your brain. You will often need to modify more than just the names of the data frames and variables to adapt the worked examples to your own work. Do not blindly copy and paste code without understanding what it does. And you should never copy and paste text. All the sentences and paragraphs you write are expressions of your own analysis. They must reflect your own understanding of the inferential process.\nAlso, so that your answers here don’t mess up the code chunks above, use new variable names everywhere.\n\nExploratory data analysis\n\nUse data documentation (help files, code books, Google, etc.) to determine as much as possible about the data provenance and structure.\n\nPlease write up your answer here\n\n# Add code here to print the data\n\n\n# Add code here to glimpse the variables\n\n\n\n\nPrepare the data for analysis. [Not always necessary.]\n\n\n# Add code here to prepare the data for analysis.\n\n\n\n\nMake tables or plots to explore the data visually.\n\n\n# Add code here to make tables or plots.\n\n\n\n\n\nHypotheses\n\nIdentify the sample (or samples) and a reasonable population (or populations) of interest.\n\nPlease write up your answer here.\n\n\n\nExpress the null and alternative hypotheses as contextually meaningful full sentences.\n\n\\(H_{0}:\\) Null hypothesis goes here.\n\\(H_{A}:\\) Alternative hypothesis goes here.\n\n\n\nExpress the null and alternative hypotheses in symbols (when possible).\n\n\\(H_{0}: math\\)\n\\(H_{A}: math\\)\n\n\n\n\nModel\n\nIdentify the sampling distribution model.\n\nPlease write up your answer here.\n\n\n\nCheck the relevant conditions to ensure that model assumptions are met.\n\nPlease write up your answer here. (Some conditions may require R code as well.)\n\n\n\n\nMechanics\n\nCompute the test statistic.\n\n\n# Add code here to compute the test statistic.\n\n\n\n\nReport the test statistic in context (when possible).\n\nPlease write up your answer here.\n\n\n\nPlot the null distribution.\n\n\n# IF CONDUCTING A SIMULATION...\nset.seed(1)\n# Add code here to simulate the null distribution.\n\n\n# Add code here to plot the null distribution.\n\n\n\n\nCalculate the P-value.\n\n\n# Add code here to calculate the P-value.\n\n\n\n\nInterpret the P-value as a probability given the null.\n\nPlease write up your answer here.\n\n\n\n\nConclusion\n\nState the statistical conclusion.\n\nPlease write up your answer here.\n\n\n\nState (but do not overstate) a contextually meaningful conclusion.\n\nPlease write up your answer here.\n\n\n\nExpress reservations or uncertainty about the generalizability of the conclusion.\n\nPlease write up your answer here.\n\n\n\nIdentify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.\n\nPlease write up your answer here.\n\n\n\n\nConfidence interval\n\nCheck the relevant conditions to ensure that model assumptions are met.\n\nPlease write up your answer here. (Some conditions may require R code as well.)\n\n\n\nCalculate and graph the confidence interval.\n\n\n# Add code here to calculate the confidence interval.\n\n\n# Add code here to graph the confidence interval.\n\n\n\n\nState (but do not overstate) a contextually meaningful interpretation.\n\nPlease write up your answer here.\n\n\n\nIf running a two-sided test, explain how the confidence interval reinforces the conclusion of the hypothesis test. [Not always applicable.]\n\nPlease write up your answer here.\n\n\n\nWhen comparing two groups, comment on the effect size and the practical significance of the result. [Not always applicable.]\n\nPlease write up your answer here."
  },
  {
    "objectID": "21-inference_for_two_independent_means-web.html#conclusion-2",
    "href": "21-inference_for_two_independent_means-web.html#conclusion-2",
    "title": "21  Inference for two independent means",
    "section": "21.14 Conclusion",
    "text": "21.14 Conclusion\nA numerical variable can be split into two groups using a categorical variable. As long as the groups are independent of each other, we can use inference to determine if there is a statistically significant difference between the mean values of the response variable for each group. Such a test can be run by simulation (using a permutation test) or by meeting the conditions for and assuming a t distribution (with a complicated formula for the degrees of freedom).\n\n21.14.1 Preparing and submitting your assignment\n\nFrom the “Run” menu, select “Restart R and Run All Chunks”.\nDeal with any code errors that crop up. Repeat steps 1–-2 until there are no more code errors.\nSpell check your document by clicking the icon with “ABC” and a check mark.\nHit the “Preview” button one last time to generate the final draft of the .nb.html file.\nProofread the HTML file carefully. If there are errors, go back and fix them, then repeat steps 1–5 again.\n\nIf you have completed this chapter as part of a statistics course, follow the directions you receive from your professor to submit your assignment."
  },
  {
    "objectID": "21-inference_for_two_independent_means-web.html#footnotes",
    "href": "21-inference_for_two_independent_means-web.html#footnotes",
    "title": "21  Inference for two independent means",
    "section": "",
    "text": "When we were testing two proportions with categorical data, one option (described in an optional appendix in that chapter) was to pool the data. With numerical data, we can calculate a pooled mean, but that doesn’t help with the unknown standard deviations. Nothing in the null hypothesis suggests that the standard deviations of the two groups should be the same. In the extremely rare situation in which one can assume equal standard deviations in the two groups, then there is a way to run a pooled t test. But this “extra” assumption of equal standard deviations is typically questionable at best.↩︎"
  },
  {
    "objectID": "22-anova-web.html#introduction",
    "href": "22-anova-web.html#introduction",
    "title": "22  ANOVA",
    "section": "22.1 Introduction",
    "text": "22.1 Introduction\nANOVA stands for “Analysis of Variance”. In this chapter, we will study the most basic form of ANOVA, called “one-way ANOVA”. We’ve already considered the one-sample and two-sample t tests for means. ANOVA is what you do when you want to compare means for three or more groups.\n\n22.1.1 Install new packages\nIf you are using R and RStudio on your own machine instead of accessing RStudio Workbench through a browser, you’ll need to type the following command at the Console:\ninstall.packages(\"quantreg\")\n\n\n22.1.2 Download the R notebook file\nCheck the upper-right corner in RStudio to make sure you’re in your intro_stats project. Then click on the following link to download this chapter as an R notebook file (.Rmd).\nhttps://vectorposse.github.io/intro_stats/chapter_downloads/22-anova.Rmd\nOnce the file is downloaded, move it to your project folder in RStudio and open it there.\n\n\n22.1.3 Restart R and run all chunks\nIn RStudio, select “Restart R and Run All Chunks” from the “Run” menu."
  },
  {
    "objectID": "22-anova-web.html#load-packages",
    "href": "22-anova-web.html#load-packages",
    "title": "22  ANOVA",
    "section": "22.2 Load packages",
    "text": "22.2 Load packages\nWe load the standard tidyverse, janitor, and infer packages. The quantreg package contains the uis data (which must be explicitly loaded using the data command) and the palmerpenguins package for the penguins data.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(infer)\nlibrary(quantreg)\n\nLoading required package: SparseM\n\nAttaching package: 'SparseM'\n\nThe following object is masked from 'package:base':\n\n    backsolve\n\ndata(uis)\nlibrary(palmerpenguins)\n\nWarning: package 'palmerpenguins' was built under R version 4.3.1"
  },
  {
    "objectID": "22-anova-web.html#research-question",
    "href": "22-anova-web.html#research-question",
    "title": "22  ANOVA",
    "section": "22.3 Research question",
    "text": "22.3 Research question\nThe uis data set from the quantreg package contains data from the UIS Drug Treatment Study. Is a history of IV drug use associated with depression?\n\nExercise 1\nThe help file for the uis data is particularly uninformative. The source, like so many we see in R packages, is a statistics textbook. If you happen to have access to a copy of the textbook, it’s pretty easy to look it up and see what the authors say about it. But it’s not likely you have such access.\nSee if you can find out more about where the data came from. This is tricky and you’re going have to dig deep.\nHint #1: Your first hits will be from the University of Illinois-Springfield. That is not the correct source.\nHint #2: You may have more success finding sources that quote from the textbook and mention more detail about the data as it’s explained in the textbook. In fact, you might even stumble across actual pages from the textbook with the direct explanation, but that is much harder. You should not try to find and download PDF files of the book itself. Not only is that illegal, but it might also come along with nasty computer viruses.\n\nPlease write up your answer here."
  },
  {
    "objectID": "22-anova-web.html#data-preparation-and-exploration",
    "href": "22-anova-web.html#data-preparation-and-exploration",
    "title": "22  ANOVA",
    "section": "22.4 Data preparation and exploration",
    "text": "22.4 Data preparation and exploration\nLet’s look at the UIS data:\n\nuis\n\n     ID AGE   BECK HC IV NDT RACE TREAT SITE LEN.T TIME CENSOR        Y\n1     1  39  9.000  4  3   1    0     1    0   123  188      1 5.236442\n2     2  33 34.000  4  2   8    0     1    0    25   26      1 3.258097\n3     3  33 10.000  2  3   3    0     1    0     7  207      1 5.332719\n4     4  32 20.000  4  3   1    0     0    0    66  144      1 4.969813\n5     5  24  5.000  2  1   5    1     1    0   173  551      0 6.311735\n6     6  30 32.550  3  3   1    0     1    0    16   32      1 3.465736\n7     7  39 19.000  4  3  34    0     1    0   179  459      1 6.129050\n8     8  27 10.000  4  3   2    0     1    0    21   22      1 3.091042\n9     9  40 29.000  2  3   3    0     1    0   176  210      1 5.347108\n10   10  36 25.000  2  3   7    0     1    0   124  184      1 5.214936\n11   12  38 18.900  2  3   8    0     1    0   176  212      1 5.356586\n12   13  29 16.000  3  1   1    0     1    0    79   87      1 4.465908\n13   14  32 36.000  3  3   2    1     1    0   182  598      0 6.393591\n14   15  41 19.000  1  3   8    0     1    0   174  260      1 5.560682\n15   16  31 18.000  1  3   1    0     1    0   181  210      1 5.347108\n16   17  27 12.000  2  3   3    0     1    0    61   84      1 4.430817\n17   18  28 34.000  1  3   6    0     1    0   177  196      1 5.278115\n18   19  28 23.000  4  2   1    0     1    0    19   19      1 2.944439\n19   20  36 26.000  3  1  15    1     1    0    27  441      1 6.089045\n20   21  32 18.900  2  3   5    0     1    0   175  449      1 6.107023\n21   22  33 15.000  3  1   1    0     0    0    12  659      0 6.490724\n22   23  28 25.200  1  3   8    0     0    0    21   21      1 3.044522\n23   24  29  6.632  4  2   0    0     0    0    48   53      1 3.970292\n24   25  35  2.100  2  3   9    0     0    0    90  225      1 5.416100\n25   26  45 26.000  1  3   6    0     0    0    91  161      1 5.081404\n26   27  35 39.789  4  3   5    0     0    0    87   87      1 4.465908\n27   28  24 20.000  3  1   3    0     0    0    88   89      1 4.488636\n28   29  36 16.000  1  3   7    0     0    0     9   44      1 3.784190\n29   31  39 22.000  1  3   9    0     0    0    94  523      0 6.259581\n30   32  36  9.947  4  2  10    0     0    0    91  226      1 5.420535\n31   33  37  9.450  4  3   1    0     0    0    90  259      1 5.556828\n32   34  30 39.000  2  3   1    0     0    0    89  289      1 5.666427\n33   35  44 41.000  1  3   5    0     0    0    89  103      1 4.634729\n34   36  28 31.000  3  1   6    1     0    0   100  624      0 6.436150\n35   37  25 20.000  3  1   3    1     0    0    67   68      1 4.219508\n36   38  30  8.000  2  3   7    0     1    0    25   57      1 4.043051\n37   39  24  9.000  4  1   1    0     0    0    12   65      1 4.174387\n38   40  27 20.000  3  1   1    0     0    0    79   79      1 4.369448\n39   41  30  8.000  3  1   2    1     0    0    79  559      0 6.326149\n40   42  34  8.000  2  3   0    0     1    0    78   79      1 4.369448\n41   43  33 23.000  4  2   2    0     1    0    84   87      1 4.465908\n42   44  34 18.000  3  3   6    0     1    0    91   91      1 4.510860\n43   45  36 13.000  2  3   1    0     1    0   162  297      1 5.693732\n44   46  27 23.000  1  3   0    0     1    0    45   45      1 3.806662\n45   47  35  9.000  4  3   1    1     1    0    61  246      1 5.505332\n46   48  24 14.000  1  3   0    0     1    0    19   37      1 3.610918\n47   49  28 23.000  4  1   2    1     1    0    37   37      1 3.610918\n48   50  46 10.000  1  3   8    0     1    0    51  538      0 6.287859\n49   51  26 11.000  3  3   1    0     1    0    60  541      0 6.293419\n50   52  42 16.000  1  3  25    0     1    0   177  184      1 5.214936\n51   53  30  0.000  3  1   0    0     1    0    43  122      1 4.804021\n52   55  30 12.000  4  1   3    1     1    0    21  156      1 5.049856\n53   56  27 21.000  2  3   2    0     0    0    88  121      1 4.795791\n54   57  38  0.000  1  3   6    0     0    0    96  231      1 5.442418\n55   58  48  8.000  4  3  10    0     0    0   111  111      1 4.709530\n56   59  36 25.000  1  3  10    0     0    0    38   38      1 3.637586\n57   60  28  6.300  3  1   7    0     0    0    15   15      1 2.708050\n58   61  31 20.000  4  2   5    0     0    0    50   54      1 3.988984\n59   62  28  4.000  2  3   5    0     0    0    61  127      1 4.844187\n60   63  28 20.000  3  1   1    0     0    0    31  105      1 4.653960\n61   64  26 17.000  2  1   2    1     0    0    11   11      1 2.397895\n62   65  34  3.000  4  3   6    0     0    0    90  153      1 5.030438\n63   66  26 29.000  2  3   5    0     0    0    11   11      1 2.397895\n64   68  31 26.000  1  3   5    0     0    0    46   46      1 3.828641\n65   69  41 12.000  1  3   0    1     0    0    38  655      0 6.484635\n66   70  30 24.000  4  3   0    0     0    0    90  166      1 5.111988\n67   72  39 15.750  4  3   5    0     0    0    88   95      1 4.553877\n68   74  33  9.000  2  3  12    0     0    0    91  151      1 5.017280\n69   75  33 18.000  4  2   6    0     0    0    85  220      1 5.393628\n70   76  29 20.000  4  1   0    1     0    0    90  227      1 5.424950\n71   77  36 17.000  1  3   5    0     0    0    52  343      1 5.837730\n72   78  26  3.000  4  3   3    0     0    0    88  119      1 4.779123\n73   79  37 27.000  1  3  13    0     0    0    43   43      1 3.761200\n74   81  29 31.500  1  3   8    0     0    0    37   47      1 3.850148\n75   83  30 19.000  3  1   0    1     0    0    87  805      0 6.690842\n76   84  35 15.000  3  2   2    0     0    0    20  321      1 5.771441\n77   85  33 22.000  3  1   1    0     0    0     9  167      1 5.117994\n78   87  36 16.000  2  3   1    0     0    0    85  491      1 6.196444\n79   88  28 17.000  1  3   2    0     0    0    18   35      1 3.555348\n80   89  31 32.550  1  3  12    1     0    0    71  123      1 4.812184\n81   90  23 24.000  1  3   2    0     0    0    88  597      0 6.391917\n82   91  33 22.000  3  2   1    0     0    0    67  762      0 6.635947\n83   93  37 18.000  2  3   4    0     0    0    30   31      1 3.433987\n84   94  25 17.850  3  1   1    0     1    0    68  228      1 5.429346\n85   95  56  5.000  2  2   9    1     1    0   182  553      0 6.315358\n86   96  23 39.000  1  3   1    0     1    0   182  190      1 5.247024\n87   97  26 21.000  3  1   1    0     1    0   146  307      1 5.726848\n88   98  26 11.000  1  3   1    0     1    0    40   73      1 4.290459\n89   99  23 14.000  3  1   1    0     1    0   177  208      1 5.337538\n90  100  28 31.000  4  2   2    1     1    0   181  267      1 5.587249\n91  102  30 14.000  1  3  15    0     1    0   168  169      1 5.129899\n92  104  25  6.000  2  3   5    0     1    0    90  655      0 6.484635\n93  105  33 16.000  1  3   5    0     1    0    61   70      1 4.248495\n94  106  22  6.000  3  1   3    1     1    0    63  398      1 5.986452\n95  108  25 20.000  4  2   8    1     1    0   121  122      1 4.804021\n96  111  38  9.000  3  1   1    1     0    0    89   96      1 4.564348\n97  112  35 11.000  2  1   3    0     1    0    51 1172      0 7.066467\n98  113  35 15.000  3  1   1    0     0    0    88  734      0 6.598509\n99  114  25 13.000  3  3   1    0     0    0    25   26      1 3.258097\n100 115  33 31.000  3  1   3    1     0    0    83   84      1 4.430817\n101 116  30  5.000  3  1   2    1     0    0    89  171      1 5.141664\n102 117  45 10.000  2  3   1    0     0    0    24  159      1 5.068904\n103 119  42 23.000  2  3  20    0     0    0     7    7      1 1.945910\n104 120  29 16.000  4  1   1    1     0    0    85  763      0 6.637258\n105 121  24 37.800  3  1   0    0     0    0    89  104      1 4.644391\n106 122  33 10.000  2  3   4    0     0    0    91  162      1 5.087596\n107 123  32  9.000  3  1   0    0     0    0    89   90      1 4.499810\n108 124  26 15.000  3  1   0    0     0    0    82  373      1 5.921578\n109 125  28  2.000  1  3   3    0     0    0    84  115      1 4.744932\n110 127  37 34.000  2  3   1    0     0    0    30   30      1 3.401197\n111 128  23 11.000  4  1   6    0     0    0     7    8      1 2.079442\n112 129  40 31.000  2  3   3    1     0    0    84  168      1 5.123964\n113 130  36 36.750  3  3   0    0     0    0    70   70      1 4.248495\n114 131  23 26.000  3  2   2    0     0    0    76  130      1 4.867534\n115 132  35  5.000  4  1   1    1     0    0    89  285      1 5.652489\n116 133  25 19.000  2  3   1    0     1    0   178  569      0 6.343880\n117 134  35 21.000  2  3   6    0     1    0    87   87      1 4.465908\n118 135  46  1.000  4  2   0    0     1    0   175  310      1 5.736572\n119 136  32  6.000  4  1   3    0     1    0    87   87      1 4.465908\n120 137  35 23.000  3  1  16    1     1    0   110  544      0 6.298949\n121 138  34 38.000  3  3   1    0     1    0    21  156      1 5.049856\n122 139  43 24.000  3  1   3    0     1    0   139  658      0 6.489205\n123 140  39  3.000  4  3  15    0     1    0   181  273      1 5.609472\n124 141  27 16.800  4  3   2    1     1    0    33  168      1 5.123964\n125 142  38 35.000  1  3   1    0     1    0    39   83      1 4.418841\n126 143  37 11.000  2  3   7    0     1    0     4    4      1 1.386294\n127 144  44  2.000  1  3   4    1     1    0   184  708      0 6.562444\n128 145  25 16.000  4  1   1    1     1    0   123  137      1 4.919981\n129 146  34 15.000  3  1   1    0     1    0   176  259      1 5.556828\n130 147  34 11.000  3  3   2    1     1    0   174  560      0 6.327937\n131 148  38 11.000  1  3   1    1     1    0   181  586      0 6.373320\n132 149  24 22.000  2  3   2    1     1    0   113  190      1 5.247024\n133 151  42 18.000  2  3   3    0     1    0   164  544      0 6.298949\n134 153  34 29.000  4  3   1    1     0    0    84  494      1 6.202536\n135 154  45 27.000  1  3   8    0     0    0    80  541      0 6.293419\n136 155  40 16.000  2  3   4    0     0    0    91   94      1 4.543295\n137 156  27  9.000  4  1   3    1     0    0    97  567      0 6.340359\n138 157  24  0.000  4  1   3    0     0    0    51   55      1 4.007333\n139 158  27 15.000  1  3   3    0     0    0    91   93      1 4.532599\n140 159  34 24.000  3  1   4    0     0    0    90  276      1 5.620401\n141 160  36  3.000  2  3   6    0     0    0    46   46      1 3.828641\n142 162  31  9.000  3  1   1    0     0    0    76  250      1 5.521461\n143 163  40  5.000  2  3   2    0     0    0    75  106      1 4.663439\n144 164  40 13.000  1  3   4    1     0    0    91  552      0 6.313548\n145 165  37 29.000  2  3   5    0     0    0    90   90      1 4.499810\n146 166  25 11.000  4  3   6    0     0    0     3  203      1 5.313206\n147 167  41 22.000  2  3   3    1     1    0     8   67      1 4.204693\n148 168  22  9.000  4  1   1    0     1    0    33  559      1 6.326149\n149 169  31 18.000  2  3   8    1     1    0    31  106      1 4.663439\n150 170  29 40.000  1  1   1    1     1    0   174  374      1 5.924256\n151 171  27 25.000  3  1   2    0     1    0    34  630      0 6.445720\n152 172  22 26.000  4  2   3    0     1    0    60   61      1 4.110874\n153 174  37 11.000  1  2   5    1     1    0    78  547      0 6.304449\n154 175  36  6.000  3  1   2    1     1    0   182  568      0 6.342121\n155 176  24 20.000  3  1   1    0     1    0   182  490      1 6.194405\n156 177  28  9.000  4  1   0    1     1    0    78  222      1 5.402677\n157 178  24  6.000  4  1   1    0     1    0    55   56      1 4.025352\n158 179  28  0.000  3  1   2    0     1    0   223  282      1 5.641907\n159 180  24  5.000  3  1  20    1     1    0    25   35      1 3.555348\n160 181  24 15.000  4  1   0    0     1    0    63  603      0 6.401917\n161 183  29 14.700  3  1   1    0     1    0   133  148      1 4.997212\n162 184  37  3.000  1  3   5    1     1    0   154  354      1 5.869297\n163 185  26 31.000  1  1   2    0     1    0    70  164      1 5.099866\n164 186  29 14.000  3  2   1    0     1    0    66   94      1 4.543295\n165 187  29 28.000  2  3   4    0     1    0    40   65      1 4.174387\n166 188  33 18.000  4  1   1    0     1    0    75  567      0 6.340359\n167 189  29 12.000  4  2   2    0     1    0   187  634      0 6.452049\n168 190  32  5.000  1  1   2    1     1    0   183  633      0 6.450470\n169 192  33 11.000  4  1   8    1     1    0   182  477      1 6.167516\n170 193  26 21.000  4  2   2    0     1    0   192  436      1 6.077642\n171 195  24 23.000  2  3   4    1     1    0   162  362      1 5.891644\n172 196  46 32.000  2  3   2    0     1    0   193  552      0 6.313548\n173 197  23 26.000  4  1   2    0     1    0   111  144      1 4.969813\n174 198  40 19.950  4  3   8    0     1    0   182  242      1 5.488938\n175 199  48 17.000  3  1   4    0     1    0   180  564      0 6.335054\n176 200  33 16.000  3  1   0    0     1    0    93  299      1 5.700444\n177 201  21 26.250  4  1   7    0     1    0   167  167      1 5.117994\n178 202  38 29.000  3  1   2    0     1    0   196  380      1 5.940171\n179 203  28 23.000  4  2   4    0     1    0   106  120      1 4.787492\n180 205  39  9.000  1  3   6    0     1    0   158  218      1 5.384495\n181 206  37 26.000  1  2   1    1     0    0    91  115      1 4.744932\n182 207  32 22.000  3  1   4    1     0    0    89  224      1 5.411646\n183 208  39 23.000  3  2   2    1     0    0    89  132      1 4.882802\n184 209  28  0.000  1  3  10    0     0    0    88  148      1 4.997212\n185 210  26 30.000  3  1   0    1     0    0    95  593      0 6.385194\n186 211  31 21.000  1  3   0    0     0    0     5   26      1 3.258097\n187 213  34 19.000  4  3   8    0     0    0    32   32      1 3.465736\n188 214  26 28.000  4  2   2    1     0    0    92  292      1 5.676754\n189 215  29  8.000  4  1   3    0     0    0    66   89      1 4.488636\n190 217  25 11.000  3  1   8    0     0    0    90  364      1 5.897154\n191 218  34 15.000  3  2   3    1     0    0    93  142      1 4.955827\n192 219  32  8.000  3  1   2    0     0    0    89  188      1 5.236442\n193 221  38 14.000  4  2   0    0     0    0    91   92      1 4.521789\n194 222  32  7.000  1  3   8    0     0    0    56   56      1 4.025352\n195 223  31 13.000  2  3   7    0     0    0    90  110      1 4.700480\n196 224  40 10.000  3  1   3    0     0    0    73  555      0 6.318968\n197 225  28 17.000  4  1   5    1     0    0    85  220      1 5.393628\n198 226  40 18.000  1  3   3    0     0    0    23   23      1 3.135494\n199 227  32  5.000  2  3   3    0     0    0    85  285      1 5.652489\n200 228  29 20.000  3  3   5    0     0    0    90   90      1 4.499810\n201 229  25 31.000  3  1   4    0     0    0    53   59      1 4.077537\n202 230  32 15.000  2  3   2    0     0    0    96  156      1 5.049856\n203 232  37  4.000  2  2   2    0     0    0    83  142      1 4.955827\n204 233  38 15.000  3  3   8    0     0    0    54   57      1 4.043051\n205 234  31 14.000  3  2   9    0     0    0    79  279      1 5.631212\n206 235  30 27.000  1  3   3    1     0    0    81  118      1 4.770685\n207 236  34 30.000  4  1   4    1     0    0    18  567      0 6.340359\n208 237  33 23.000  1  3   4    0     1    0   184  562      0 6.331502\n209 238  36 13.000  3  2  10    1     1    0    39  239      1 5.476464\n210 239  32 26.000  4  1   0    0     1    0   177  578      0 6.359574\n211 240  29 10.000  2  3   2    1     1    0   122  551      0 6.311735\n212 241  32  4.000  1  1   4    1     1    0   178  313      1 5.746203\n213 242  34  0.000  3  1   7    0     1    0   173  560      0 6.327937\n214 243  26 35.000  1  3  31    0     1    0    53   54      1 3.988984\n215 244  25 32.000  1  3   5    1     1    0    94  198      1 5.288267\n216 245  30  2.000  4  1   2    1     1    0   163  164      1 5.099866\n217 246  33 15.000  3  2   6    0     1    0   160  325      1 5.783825\n218 247  40 23.000  4  2   6    0     1    0    61   62      1 4.127134\n219 248  26 13.000  3  1  12    0     1    0    41   45      1 3.806662\n220 249  26 29.000  1  3   5    1     1    0    53   53      1 3.970292\n221 250  35 22.105  4  3   4    0     1    0    53  253      1 5.533389\n222 251  26 15.000  2  2  11    0     1    0    13   51      1 3.931826\n223 252  33  7.000  4  1   3    1     1    0   183  540      0 6.291569\n224 253  27  7.000  1  3   4    0     1    0   182  317      1 5.758902\n225 254  29 33.000  3  3   3    0     1    0   183  437      1 6.079933\n226 255  29 23.000  3  3   9    0     1    0    63  136      1 4.912655\n227 256  39 21.000  2  3   7    0     1    0   111  115      1 4.744932\n228 257  43 19.000  3  2   2    1     1    0   174  175      1 5.164786\n229 258  35  8.000  3  3   3    0     1    0   173  442      1 6.091310\n230 259  26 24.000  4  1   2    1     1    0   119  122      1 4.804021\n231 260  27 28.737  4  1   3    0     1    0   180  181      1 5.198497\n232 261  28 20.000  4  1   2    1     1    0    98  180      1 5.192957\n233 262  30 14.000  3  1   4    0     1    0    50   51      1 3.931826\n234 263  31 17.000  4  2   1    1     1    0   178  541      0 6.293419\n235 264  26 19.000  2  3  16    0     1    0   100  121      1 4.795791\n236 265  36  5.000  4  2   4    0     1    0    93  328      1 5.793014\n237 267  25  8.000  2  3   3    0     1    0   165  166      1 5.111988\n238 268  26 22.000  3  1   0    1     1    0    93  556      0 6.320768\n239 269  30 11.000  2  3   5    0     0    0    44  104      1 4.644391\n240 270  28 13.000  3  1   5    0     0    0    77  102      1 4.624973\n241 272  34 11.053  3  1   0    1     0    0    91  144      1 4.969813\n242 273  31 24.000  3  1   2    0     0    0    95  545      0 6.300786\n243 274  30 19.000  4  3   1    0     0    0    82  537      0 6.285998\n244 275  35 27.000  3  2   5    1     0    0    76  625      0 6.437752\n245 276  30  4.000  4  2   3    1     0    0     5    6      1 1.791759\n246 277  37 38.000  1  3   7    0     0    0    69  307      1 5.726848\n247 278  29 11.000  4  1  12    1     0    0    90  290      1 5.669881\n248 279  23 21.000  4  1   8    0     0    0    19   20      1 2.995732\n249 280  23  1.000  1  1   4    0     0    0    60   74      1 4.304065\n250 281  44  4.000  4  1   0    0     0    0    69  100      1 4.605170\n251 282  43  7.000  4  2   8    1     0    0    85  555      0 6.318968\n252 283  38 20.000  2  3   3    0     0    0    92  152      1 5.023881\n253 284  33 17.000  3  1   3    1     0    0    55  115      1 4.744932\n254 285  36  6.300  1  3   9    0     0    0    20   92      1 4.521789\n255 286  26 12.000  1  3   2    0     0    0    87  554      0 6.317165\n256 287  30 16.000  4  1   0    0     0    0    91   92      1 4.521789\n257 288  34 31.500  4  1   0    0     0    0     9   69      1 4.234107\n258 289  32 30.000  2  3   6    0     0    0    22   25      1 3.218876\n259 290  30  1.000  3  1   1    0     0    0    87  501      0 6.216606\n260 291  37 32.000  2  3  10    1     0    0    86   86      1 4.454347\n261 292  35 29.000  2  3   7    0     0    0    85   99      1 4.595120\n262 293  30  6.000  3  1   0    0     0    0    83   87      1 4.465908\n263 294  34 17.000  4  1   6    1     0    0    83  136      1 4.912655\n264 295  40 13.000  1  2   6    0     0    0    92  106      1 4.663439\n265 296  28 15.000  4  2   3    1     0    0    85  220      1 5.393628\n266 297  32 11.000  3  1   6    0     0    0    36   36      1 3.583519\n267 298  45 17.000  1  3   2    1     0    0    87  162      1 5.087596\n268 299  24 23.000  2  1   0    0     1    0    56  116      1 4.753590\n269 300  43 23.000  1  3   5    1     1    0    94  175      1 5.164786\n270 301  38 15.000  1  3   0    1     1    0    74  209      1 5.342334\n271 302  33 19.000  2  3   1    0     1    0   186  545      0 6.300786\n272 303  26 21.000  4  2   2    1     1    0   178  245      1 5.501258\n273 304  40  8.000  4  3   3    0     1    0    84  176      1 5.170484\n274 305  27 34.000  4  2   0    0     1    0    13   14      1 2.639057\n275 306  39 21.000  2  3  12    0     1    0    85  113      1 4.727388\n276 308  29 27.000  4  2   3    1     1    0     9  354      1 5.869297\n277 309  28 32.000  4  2   4    0     1    0   162  174      1 5.159055\n278 310  37 29.000  1  3  20    0     0    0    23   23      1 3.135494\n279 311  37 22.000  2  3  20    0     0    0    26   26      1 3.258097\n280 312  40 12.000  4  2   9    0     0    0    84   98      1 4.584967\n281 313  25 36.000  1  3   5    0     0    0    23   23      1 3.135494\n282 314  40 15.000  1  1   2    0     0    0    86  555      0 6.318968\n283 315  40  3.000  1  3   4    1     0    0    90  290      1 5.669881\n284 316  34 24.000  2  3   8    0     0    0    73  543      0 6.297109\n285 317  41 18.000  2  3   7    0     0    0    76  274      1 5.613128\n286 321  23  2.000  4  1   1    0     1    0    18  119      1 4.779123\n287 322  36 14.000  3  1   3    0     1    0    94  164      1 5.099866\n288 323  28 19.000  4  1   2    1     1    0    76  548      0 6.306275\n289 324  23  7.000  3  1   3    0     1    0    40  175      1 5.164786\n290 325  27  8.000  3  1   3    0     1    0   176  539      0 6.289716\n291 326  32 27.000  4  2   0    0     1    0   104  155      1 5.043425\n292 327  38 25.000  4  3  15    0     1    0     5   14      1 2.639057\n293 328  38 28.000  4  1   6    1     1    0   179  187      1 5.231109\n294 329  45 39.000  1  3   8    0     1    0    35   65      1 4.174387\n295 330  26 18.000  2  2   1    0     1    0    24  159      1 5.068904\n296 331  29  8.000  1  3  35    0     1    0    82   96      1 4.564348\n297 332  33 31.000  4  1   3    0     1    0    28  243      1 5.493061\n298 333  25  6.000  3  1   0    1     1    0    81   85      1 4.442651\n299 334  36 19.000  4  1   2    0     1    0     4    4      1 1.386294\n300 335  37 19.000  2  3   4    0     1    0    97  121      1 4.795791\n301 336  29 16.000  4  1   0    1     1    0    78  659      1 6.490724\n302 337  29 15.000  4  1   3    1     1    0   181  260      1 5.560682\n303 338  35 54.000  4  2   1    0     1    0    29  621      0 6.431331\n304 339  33 19.000  4  1   1    0     1    0   139  199      1 5.293305\n305 340  31 12.000  4  3   2    0     1    0   152  565      0 6.336826\n306 341  37 24.000  3  2   5    1     1    0    90  183      1 5.209486\n307 342  32 37.000  3  3   4    0     1    0    62  122      1 4.804021\n308 343  33  9.000  3  2  13    0     1    0   110  170      1 5.135798\n309 344  36 18.000  3  1  14    1     1    0    15   15      1 2.708050\n310 345  26  4.000  1  1   5    0     1    0    68  268      1 5.590987\n311 346  35 15.000  3  1   0    1     1    0    19   79      1 4.369448\n312 347  25 19.000  1  3   6    1     0    0    23   23      1 3.135494\n313 348  33 26.000  1  3  30    0     0    0    92  100      1 4.605170\n314 349  36 28.000  2  3   8    0     0    0    94   98      1 4.584967\n315 350  38 14.000  3  3   6    0     0    0    31   81      1 4.394449\n316 351  36 15.000  3  2   3    1     0    0    28  546      0 6.302619\n317 352  36 18.000  2  3  10    0     0    0    58   58      1 4.060443\n318 353  35 29.000  3  3   6    0     0    0   113  569      0 6.343880\n319 354  35 10.000  3  1   3    1     0    0    70  575      0 6.354370\n320 356  39 16.000  2  3   4    0     0    0    90   91      1 4.510860\n321 357  37  0.000  4  3   6    0     0    0    55   57      1 4.043051\n322 358  30 31.000  2  3   5    0     0    0    89  499      1 6.212606\n323 359  26 33.000  1  3   7    1     0    0    71  123      1 4.812184\n324 360  39 21.000  4  1   5    0     0    0    84  143      1 4.962845\n325 362  32 18.000  3  1   4    0     0    0    78  471      1 6.154858\n326 363  26 37.800  3  1   4    1     0    0    60   74      1 4.304065\n327 364  33 20.000  2  3   6    0     0    0    82   85      1 4.442651\n328 365  36 11.000  4  2   5    0     0    0    81   95      1 4.553877\n329 366  42 26.000  2  3   3    0     1    0    35   36      1 3.583519\n330 367  37 43.000  1  3  22    0     1    0    16   19      1 2.944439\n331 368  37 12.000  2  2   1    1     1    0     7   38      1 3.637586\n332 369  32 22.000  3  1   4    1     1    0    30  539      0 6.289716\n333 370  23 36.000  4  1   3    1     1    0   106  567      0 6.340359\n334 371  21 16.000  4  1  10    0     1    0   174  186      1 5.225747\n335 372  23 41.000  3  1   1    0     1    0   144  546      0 6.302619\n336 373  34 16.000  4  2   1    0     1    0    24   24      1 3.178054\n337 374  33  8.000  4  2   3    0     1    0    17  540      0 6.291569\n338 375  33 10.000  3  1   4    1     1    0    97  157      1 5.056246\n339 376  26 18.000  3  3   0    0     1    0    26   86      1 4.454347\n340 377  28 27.000  4  1   2    1     1    0    31  231      1 5.442418\n341 379  27 28.000  1  3   3    0     0    0    14   14      1 2.639057\n342 380  22 23.000  1  3   2    0     0    0    75   75      1 4.317488\n343 381  31 32.000  3  3   6    1     0    0    20  147      1 4.990433\n344 382  29 23.100  3  1   4    0     0    0   104  105      1 4.653960\n345 383  44 11.000  4  3  12    0     0    0    85  324      1 5.780744\n346 384  26  7.000  3  1   0    1     0    0   110  538      0 6.287859\n347 385  44 24.000  2  3  16    0     0    0   100  300      1 5.703782\n348 386  34 12.000  1  3   1    0     0    0    73   73      1 4.290459\n349 387  36 25.000  2  3   6    0     0    0    65   65      1 4.174387\n350 388  43  4.000  2  3  20    0     0    0    75  568      1 6.342121\n351 389  37  5.000  3  1   1    0     0    0    83   84      1 4.430817\n352 390  44 13.000  4  2  17    0     1    0    15   22      1 3.091042\n353 391  31 17.000  1  3  30    1     1    0    44   44      1 3.784190\n354 392  24 24.000  2  1   3    0     1    0     7    7      1 1.945910\n355 394  37 32.000  3  3   4    0     1    0    20   21      1 3.044522\n356 395  41 19.000  1  3  12    1     1    0   175  537      0 6.285998\n357 396  32  9.000  3  1   3    1     1    0    71  186      1 5.225747\n358 397  23  6.000  3  1   2    0     1    0    26   40      1 3.688879\n359 398  33 10.000  2  3   3    0     1    0   161  287      1 5.659482\n360 399  43 11.000  4  1   9    0     1    0    36  538      0 6.287859\n361 400  33 16.000  4  3   8    0     1    0    30   30      1 3.401197\n362 401  41 25.000  4  2   3    0     1    0   179  516      1 6.246107\n363 402  41 17.000  2  3   2    0     1    0   199  268      1 5.590987\n364 403  37 24.000  2  3   3    0     1    0   182  568      0 6.342121\n365 404  26 27.000  1  1   3    0     0    0   112  131      1 4.875197\n366 405  33 24.000  1  3   6    0     0    0     8  399      1 5.988961\n367 406  30 26.000  3  1   2    0     0    0    18   78      1 4.356709\n368 407  33 17.000  4  1   6    1     0    0    20   80      1 4.382027\n369 408  33 26.000  2  3   3    0     0    0    88  102      1 4.624973\n370 410  37 13.000  3  1   6    0     0    0    88  124      1 4.820282\n371 411  44 11.000  2  3  20    0     0    0    76   80      1 4.382027\n372 412  20  8.000  4  1   1    0     0    0    22   23      1 3.135494\n373 413  33 12.000  1  3   4    0     0    0   110  274      1 5.613128\n374 415  36 31.000  2  3   3    0     0    0    85  459      1 6.129050\n375 416  34  8.400  2  3   3    0     0    0    10   10      1 2.302585\n376 417  35 10.000  1  3  17    0     1    0   157  176      1 5.170484\n377 418  38 16.000  2  3  26    0     1    0   133  332      1 5.805135\n378 419  24 13.000  3  1   3    0     1    0    83  119      1 4.779123\n379 420  24 18.000  3  1   4    0     1    0   152  217      1 5.379897\n380 421  32 13.000  3  1   4    0     1    0   169  285      1 5.652489\n381 422  35 11.000  4  2   3    0     1    0    89  576      0 6.356108\n382 423  33 21.000  1  3   5    0     1    0    92  106      1 4.663439\n383 424  29 37.000  2  2   4    1     1    0    21   81      1 4.394449\n384 425  42 32.000  2  3  30    0     1    0    31   47      1 3.850148\n385 426  23 33.000  4  1   1    0     1    0    31   76      1 4.330733\n386 427  28 11.000  4  3  16    0     1    0   133  348      1 5.852202\n387 429  43 29.000  2  3   4    0     1    0   153  306      1 5.723585\n388 430  33 23.000  2  1   0    0     0    0    90  192      1 5.257495\n389 431  37 15.000  1  3  20    0     0    0   102  216      1 5.375278\n390 432  49 22.000  2  3   7    0     0    0    85  189      1 5.241747\n391 434  36 25.000  3  1   1    1     0    0    89  193      1 5.262690\n392 435  27 30.000  1  3  13    0     0    0    28   28      1 3.332205\n393 436  35 23.000  1  3   1    0     0    0    90  150      1 5.010635\n394 437  25 10.000  3  2   3    0     0    0    84   99      1 4.595120\n395 438  33  8.000  1  3   3    0     0    0    85  510      0 6.234411\n396 439  34 16.000  1  3   7    0     0    0    36  306      1 5.723585\n397 440  38  9.000  1  3  10    1     0    0    74  101      1 4.615121\n398 441  36 12.158  2  3   0    1     0    0    42  102      1 4.624973\n399 442  27  5.000  1  3   1    0     0    0    90  510      0 6.234411\n400 444  40 19.000  1  3   0    1     0    0   108  503      0 6.220590\n401 445  32 23.000  3  3   3    0     0    1    49   52      1 3.951244\n402 446  38 28.000  3  3   1    1     0    1   219  547      0 6.304449\n403 447  38 16.000  1  3   6    0     0    1   108  168      1 5.123964\n404 448  23 25.000  4  1   0    0     0    1   178  461      1 6.133398\n405 449  26 22.000  4  2   2    0     0    1    42  538      0 6.287859\n406 450  36 28.000  2  3   7    0     0    1   182  349      1 5.855072\n407 451  30 28.000  4  1   5    0     0    1     6   44      1 3.784190\n408 452  31 18.000  4  2   3    0     1    1   351  548      0 6.306275\n409 453  23 15.000  3  1   1    0     1    1    12   12      1 2.484907\n410 454  43  9.000  1  3   0    1     1    1     6    6      1 1.791759\n411 455  24 26.000  4  1   1    0     1    1    91  575      0 6.354370\n412 456  42 19.000  4  1   1    0     1    1   245  589      0 6.378426\n413 457  35 26.000  4  2   1    0     1    1   372  408      1 6.011267\n414 458  21 10.000  4  1   0    0     1    1   218  232      1 5.446737\n415 459  45  1.000  4  2   0    1     1    1    46  143      1 4.962845\n416 460  43 30.000  2  3   6    0     1    1   363  582      0 6.366470\n417 461  24  7.000  4  1   0    1     1    1   133  134      1 4.897840\n418 462  37 11.000  3  3   1    0     1    1     7    7      1 1.945910\n419 463  40 10.000  4  2   0    0     1    1   112  548      0 6.306275\n420 464  27 11.000  3  2   2    0     0    1    21   81      1 4.394449\n421 465  29 11.000  2  3   1    0     0    1   169  170      1 5.135798\n422 466  34 12.000  4  3   6    0     0    1    28   29      1 3.367296\n423 467  29 29.000  3  3  20    0     0    1    47   78      1 4.356709\n424 468  35 27.000  1  3   5    0     0    1    20   81      1 4.394449\n425 469  39 20.000  1  3   4    0     1    1   352  369      1 5.910797\n426 470  41  9.000  4  2   0    0     1    1    66   69      1 4.234107\n427 471  37 18.000  4  1   6    1     1    1    55  115      1 4.744932\n428 472  30 10.000  3  2   7    0     1    1   344  361      1 5.888878\n429 473  31  1.000  4  1   0    0     1    1   153  245      1 5.501258\n430 474  40  5.000  4  2   8    0     0    1   184  233      1 5.451038\n431 475  32 20.000  4  1   0    0     0    1   183  227      1 5.424950\n432 476  32  7.000  4  2   3    1     0    1    22   97      1 4.574711\n433 477  27  7.000  4  1   0    0     0    1   183  547      0 6.304449\n434 478  23 26.000  3  1   0    0     0    1   140  224      1 5.411646\n435 479  23  4.000  4  1   2    0     0    1    19  211      1 5.351858\n436 480  43 11.000  2  3  12    0     0    1   184  220      1 5.393628\n437 481  24 20.000  4  1   0    0     0    1    50   54      1 3.988984\n438 482  36 11.000  4  1   2    1     0    1   132  192      1 5.257495\n439 483  29 31.000  1  3   1    0     0    1   128  138      1 4.927254\n440 484  39 13.000  4  2   1    0     1    1   107  107      1 4.672829\n441 485  23  6.000  4  1   0    0     1    1   368  597      0 6.391917\n442 486  27 17.000  3  3   4    0     1    1   219  226      1 5.420535\n443 487  26  5.000  4  2   5    0     1    1   374  434      1 6.073045\n444 488  26 27.000  3  1   1    1     1    1    92  106      1 4.663439\n445 489  25  9.000  4  1   0    0     1    1    45  180      1 5.192957\n446 490  34 10.000  3  1   0    0     1    1   366  557      0 6.322565\n447 491  45  5.000  4  3   2    0     1    1   368  556      0 6.320768\n448 492  23 17.000  4  1   1    0     0    1    78  619      0 6.428105\n449 493  26  7.000  4  1   0    0     0    1   184  546      0 6.302619\n450 495  24 27.000  1  2   2    0     0    1   187  233      1 5.451038\n451 496  30 23.000  2  3   2    1     0    1   101  102      1 4.624973\n452 497  22 26.000  3  1   0    0     0    1   141  548      0 6.306275\n453 498  25 10.000  3  1   1    0     0    1    24   99      1 4.595120\n454 499  30  8.400  3  2  40    0     0    1    36   36      1 3.583519\n455 501  33 23.000  4  1   0    1     1    1    56   78      1 4.356709\n456 502  34 15.000  3  2   8    0     1    1   367  502      1 6.218600\n457 503  29 24.000  3  1   2    0     1    1    70   71      1 4.262680\n458 504  39 33.000  4  2   6    0     1    1    58   59      1 4.077537\n459 506  26 21.000  3  1   4    0     1    1   366  533      0 6.278521\n460 507  32 23.000  2  3   6    0     1    1    10   10      1 2.302585\n461 508  42 23.100  1  3   2    0     0    1   214  274      1 5.613128\n462 509  39 25.000  1  2   8    0     0    1   197  255      1 5.541264\n463 510  36  2.000  4  1   0    1     0    1    89  503      0 6.220590\n464 511  22 20.000  3  1   1    0     0    1    56  256      1 5.545177\n465 512  27 23.000  4  1   1    0     0    1     9    9      1 2.197225\n466 514  28  9.000  4  1   0    0     0    1   186  386      1 5.955837\n467 515  36 28.000  3  2   1    0     1    1   303  547      0 6.304449\n468 516  31 13.000  3  1   3    0     1    1    32   45      1 3.806662\n469 517  27 22.000  3  2   4    0     1    1     8   58      1 4.060443\n470 518  23 17.000  3  1   1    0     1    1    63  124      1 4.820282\n471 519  24 20.000  3  2  20    0     0    1   108  540      0 6.291569\n472 520  38  5.000  3  2   1    0     0    1   183  243      1 5.493061\n473 521  25  8.000  4  1   1    0     1    1   151  549      0 6.308098\n474 522  26 20.000  3  1   0    0     0    1     7   12      1 2.484907\n475 523  22 34.000  3  1   2    0     0    1    38   51      1 3.931826\n476 524  33 13.000  4  1   2    0     1    1   176  562      0 6.331502\n477 525  30 23.000  1  3   7    0     1    1    93   94      1 4.543295\n478 526  45  8.000  4  3   3    0     0    1   200  204      1 5.318120\n479 527  24 15.000  3  2   0    0     0    1   178  238      1 5.472271\n480 528  27 22.000  4  1   0    0     1    1    78  140      1 4.941642\n481 529  36 19.000  4  2  10    0     1    1   119  120      1 4.787492\n482 530  38 23.000  4  2   2    1     0    1   154  154      1 5.036953\n483 531  31 17.000  2  3   2    0     1    1   163  177      1 5.176150\n484 532  40 22.000  4  2   7    0     1    1   118  119      1 4.779123\n485 533  22 12.000  3  1   0    1     1    1    76   83      1 4.418841\n486 534  31 13.000  4  1   0    1     1    1   116  130      1 4.867534\n487 536  39  7.000  3  3   3    1     0    1    88  159      1 5.068904\n488 538  33 14.000  3  1   1    0     0    1    33   33      1 3.496508\n489 539  27 10.000  3  3   2    0     1    1    70   72      1 4.276666\n490 540  37  7.000  4  1   2    1     1    1    68  161      1 5.081404\n491 541  35 16.000  4  2  25    0     0    1   191  191      1 5.252273\n492 542  25 11.000  3  1   5    0     0    1    35  181      1 5.198497\n493 543  27 11.000  3  1   1    1     1    1    32  546      0 6.302619\n494 544  34 15.000  4  1   0    0     0    1    28  540      0 6.291569\n495 545  30 15.000  3  1   3    0     0    1    15   76      1 4.330733\n496 546  35 17.000  1  3   7    0     0    1     7    7      1 1.945910\n497 547  34 23.000  4  1   0    0     0    1    43   44      1 3.784190\n498 548  25 23.000  3  2   5    0     0    1    89  103      1 4.634729\n499 549  34 18.000  3  1   1    0     0    1    38   79      1 4.369448\n500 550  24 23.000  4  3   3    0     0    1   204  339      1 5.826000\n501 551  24 20.000  4  1   2    0     0    1    76   90      1 4.499810\n502 552  40 36.000  4  1   3    0     0    1   195  542      0 6.295266\n503 553  33  9.000  3  1   1    1     0    1   184  384      1 5.950643\n504 554  38 14.000  4  2   1    1     1    1   254  255      1 5.541264\n505 555  32  1.000  3  1   0    0     1    1   371  431      1 6.066108\n506 556  33  3.000  4  1   1    0     0    1   196  587      0 6.375025\n507 557  28 40.000  3  1   2    1     0    1   198  198      1 5.288267\n508 558  31 13.000  3  3   2    0     0    1   170  551      0 6.311735\n509 559  31 39.000  2  3   4    0     1    1    50  110      1 4.700480\n510 560  33 24.000  4  1   0    0     1    1   163  541      0 6.293419\n511 561  24 26.000  3  1  11    0     0    1   182  242      1 5.488938\n512 562  26 18.000  3  1   3    0     0    1   150  537      0 6.285998\n513 563  31 19.000  2  3   7    0     1    1    34   56      1 4.025352\n514 564  40 14.700  2  3   4    0     1    1    34   34      1 3.526361\n515 566  34  2.000  3  1   3    0     1    1   366  549      0 6.308098\n516 567  30 11.000  3  2   7    0     0    1   133  133      1 4.890349\n517 568  36  0.000  3  2   3    0     0    1    69  226      1 5.420535\n518 569  38 17.000  2  3   6    0     1    1   366  401      1 5.993961\n519 570  31 20.000  1  3   6    1     1    1    14   14      1 2.639057\n520 571  27 22.000  2  2   2    0     0    1   184  548      0 6.306275\n521 572  32 21.000  1  3  15    0     1    1    89  224      1 5.411646\n522 573  35 23.000  3  1   5    1     0    1   183  540      0 6.291569\n523 574  44 29.000  2  3  13    0     0    1   177  237      1 5.468060\n524 575  31  5.000  2  3  10    0     1    1   154  354      1 5.869297\n525 576  28 23.000  3  2  20    0     0    1   123  123      1 4.812184\n526 577  40  8.000  4  2   1    0     0    1   146  170      1 5.135798\n527 578  25 12.000  3  1  10    1     1    1   203  203      1 5.313206\n528 579  32 10.000  1  3   6    0     1    1   360  360      1 5.886104\n529 580  29 15.750  4  1   2    0     0    1    79  139      1 4.934474\n530 581  40  2.000  2  2   5    0     1    1   201  215      1 5.370638\n531 582  27  9.000  4  2   0    0     1    1   129  129      1 4.859812\n532 583  26  2.000  3  1   1    0     1    1   365  396      1 5.981414\n533 584  34 15.000  3  1   4    1     1    1   159  547      0 6.304449\n534 585  49  4.000  4  2   2    0     0    1   177  547      0 6.304449\n535 586  21 25.000  1  3   1    0     1    1    71   71      1 4.262680\n536 587  39 23.000  3  3   2    0     1    1   108  168      1 5.123964\n537 588  33 15.000  4  2   4    0     1    1   198  228      1 5.429346\n538 589  32  3.000  3  1   1    0     1    1   372  551      0 6.311735\n539 590  35  9.000  4  2   6    0     0    1    25  654      0 6.483107\n540 591  31 20.000  4  1   0    1     1    1    48   51      1 3.931826\n541 592  28  5.000  4  1   3    0     0    1   191  548      0 6.306275\n542 593  27 29.000  3  2   5    0     1    1   171  231      1 5.442418\n543 594  29 21.000  2  1   1    1     1    1   145  280      1 5.634790\n544 595  30  1.000  2  1  20    0     0    1   183  184      1 5.214936\n545 596  27 18.000  4  1   3    1     0    1    72   86      1 4.454347\n546 598  40 15.000  4  2   1    0     1    1    44   46      1 3.828641\n547 599  37 20.000  3  1   2    1     1    1   140  200      1 5.298317\n548 600  33 10.000  4  1   0    0     0    1   184  244      1 5.497168\n549 601  28 20.000  4  1   2    0     0    1    94  182      1 5.204007\n550 602  40 15.000  4  2   8    0     1    1   296  296      1 5.690359\n551 603  48 20.000  4  1   0    1     0    1    23   24      1 3.178054\n552 604  38 25.000  3  1   1    0     0    1   128  142      1 4.955827\n553 605  35 13.000  4  1   0    0     0    1   106  120      1 4.787492\n554 606  37 13.000  4  2   0    0     0    1    46   47      1 3.850148\n555 607  25 15.000  3  1   0    1     1    1   150  519      1 6.251904\n556 608  26  8.000  4  1   2    0     1    1    48  248      1 5.513429\n557 609  30  9.000  3  3   3    0     0    1    29   31      1 3.433987\n558 610  28 16.000  4  2   2    0     0    1   179  567      0 6.340359\n559 611  23 11.000  2  3   4    0     0    1   170  353      1 5.866468\n560 612  36 31.000  4  1   1    0     1    1   365  458      1 6.126869\n561 613  36 13.000  4  2   4    0     1    1   400  554      0 6.317165\n562 614  24  5.000  4  1   0    1     0    1    56  116      1 4.753590\n563 615  33  9.000  3  2   5    0     0    1    24   74      1 4.304065\n564 616  38 15.000  4  2   6    0     0    1    10   10      1 2.302585\n565 617  41 20.000  3  3  21    0     1    1   354  355      1 5.872118\n566 618  31 21.000  3  1   0    1     1    1   232  232      1 5.446737\n567 619  31 23.000  4  2  11    0     1    1    54   68      1 4.219508\n568 620  37  5.000  4  1   0    1     1    1    48   48      1 3.871201\n569 621  37 17.000  4  2   4    1     0    1    57   60      1 4.094345\n570 622  33 13.000  4  1   0    0     0    1    46   50      1 3.912023\n571 624  53  9.000  4  2   6    0     0    1    39  126      1 4.836282\n572 625  37 20.000  2  3   4    0     0    1    17   18      1 2.890372\n573 626  28 10.000  4  2   3    0     1    1    21   35      1 3.555348\n574 627  35 17.000  1  3   2    0     0    1   184  379      1 5.937536\n575 628  46 31.500  1  3  15    1     1    1     9  377      1 5.932245\n           ND1          ND2      LNDT       FRAC IV3\n1    5.0000000  -8.04718956 0.6931472 0.68333333   1\n2    1.1111111  -0.11706724 2.1972246 0.13888889   0\n3    2.5000000  -2.29072683 1.3862944 0.03888889   1\n4    5.0000000  -8.04718956 0.6931472 0.73333333   1\n5    1.6666667  -0.85137604 1.7917595 0.96111111   0\n6    5.0000000  -8.04718956 0.6931472 0.08888889   1\n7    0.2857143   0.35793228 3.5553481 0.99444444   1\n8    3.3333333  -4.01324268 1.0986123 0.11666667   1\n9    2.5000000  -2.29072683 1.3862944 0.97777778   1\n10   1.2500000  -0.27892944 2.0794415 0.68888889   1\n11   1.1111111  -0.11706724 2.1972246 0.97777778   1\n12   5.0000000  -8.04718956 0.6931472 0.43888889   0\n13   3.3333333  -4.01324268 1.0986123 1.01111111   1\n14   1.1111111  -0.11706724 2.1972246 0.96666667   1\n15   5.0000000  -8.04718956 0.6931472 1.00555556   1\n16   2.5000000  -2.29072683 1.3862944 0.33888889   1\n17   1.4285714  -0.50953563 1.9459101 0.98333333   1\n18   5.0000000  -8.04718956 0.6931472 0.10555556   0\n19   0.6250000   0.29375227 2.7725887 0.15000000   0\n20   1.6666667  -0.85137604 1.7917595 0.97222222   1\n21   5.0000000  -8.04718956 0.6931472 0.13333333   0\n22   1.1111111  -0.11706724 2.1972246 0.23333333   1\n23  10.0000000 -23.02585093 0.0000000 0.53333333   0\n24   1.0000000   0.00000000 2.3025851 1.00000000   1\n25   1.4285714  -0.50953563 1.9459101 1.01111111   1\n26   1.6666667  -0.85137604 1.7917595 0.96666667   1\n27   2.5000000  -2.29072683 1.3862944 0.97777778   0\n28   1.2500000  -0.27892944 2.0794415 0.10000000   1\n29   1.0000000   0.00000000 2.3025851 1.04444444   1\n30   0.9090909   0.08664562 2.3978953 1.01111111   0\n31   5.0000000  -8.04718956 0.6931472 1.00000000   1\n32   5.0000000  -8.04718956 0.6931472 0.98888889   1\n33   1.6666667  -0.85137604 1.7917595 0.98888889   1\n34   1.4285714  -0.50953563 1.9459101 1.11111111   0\n35   2.5000000  -2.29072683 1.3862944 0.74444444   0\n36   1.2500000  -0.27892944 2.0794415 0.13888889   1\n37   5.0000000  -8.04718956 0.6931472 0.13333333   0\n38   5.0000000  -8.04718956 0.6931472 0.87777778   0\n39   3.3333333  -4.01324268 1.0986123 0.87777778   0\n40  10.0000000 -23.02585093 0.0000000 0.43333333   1\n41   3.3333333  -4.01324268 1.0986123 0.46666667   0\n42   1.4285714  -0.50953563 1.9459101 0.50555556   1\n43   5.0000000  -8.04718956 0.6931472 0.90000000   1\n44  10.0000000 -23.02585093 0.0000000 0.25000000   1\n45   5.0000000  -8.04718956 0.6931472 0.33888889   1\n46  10.0000000 -23.02585093 0.0000000 0.10555556   1\n47   3.3333333  -4.01324268 1.0986123 0.20555556   0\n48   1.1111111  -0.11706724 2.1972246 0.28333333   1\n49   5.0000000  -8.04718956 0.6931472 0.33333333   1\n50   0.3846154   0.36750440 3.2580965 0.98333333   1\n51  10.0000000 -23.02585093 0.0000000 0.23888889   0\n52   2.5000000  -2.29072683 1.3862944 0.11666667   0\n53   3.3333333  -4.01324268 1.0986123 0.97777778   1\n54   1.4285714  -0.50953563 1.9459101 1.06666667   1\n55   0.9090909   0.08664562 2.3978953 1.23333333   1\n56   0.9090909   0.08664562 2.3978953 0.42222222   1\n57   1.2500000  -0.27892944 2.0794415 0.16666667   0\n58   1.6666667  -0.85137604 1.7917595 0.55555556   0\n59   1.6666667  -0.85137604 1.7917595 0.67777778   1\n60   5.0000000  -8.04718956 0.6931472 0.34444444   0\n61   3.3333333  -4.01324268 1.0986123 0.12222222   0\n62   1.4285714  -0.50953563 1.9459101 1.00000000   1\n63   1.6666667  -0.85137604 1.7917595 0.12222222   1\n64   1.6666667  -0.85137604 1.7917595 0.51111111   1\n65  10.0000000 -23.02585093 0.0000000 0.42222222   1\n66  10.0000000 -23.02585093 0.0000000 1.00000000   1\n67   1.6666667  -0.85137604 1.7917595 0.97777778   1\n68   0.7692308   0.20181866 2.5649494 1.01111111   1\n69   1.4285714  -0.50953563 1.9459101 0.94444444   0\n70  10.0000000 -23.02585093 0.0000000 1.00000000   0\n71   1.6666667  -0.85137604 1.7917595 0.57777778   1\n72   2.5000000  -2.29072683 1.3862944 0.97777778   1\n73   0.7142857   0.24033731 2.6390573 0.47777778   1\n74   1.1111111  -0.11706724 2.1972246 0.41111111   1\n75  10.0000000 -23.02585093 0.0000000 0.96666667   0\n76   3.3333333  -4.01324268 1.0986123 0.22222222   0\n77   5.0000000  -8.04718956 0.6931472 0.10000000   0\n78   5.0000000  -8.04718956 0.6931472 0.94444444   1\n79   3.3333333  -4.01324268 1.0986123 0.20000000   1\n80   0.7692308   0.20181866 2.5649494 0.78888889   1\n81   3.3333333  -4.01324268 1.0986123 0.97777778   1\n82   5.0000000  -8.04718956 0.6931472 0.74444444   0\n83   2.0000000  -1.38629436 1.6094379 0.33333333   1\n84   5.0000000  -8.04718956 0.6931472 0.37777778   0\n85   1.0000000   0.00000000 2.3025851 1.01111111   0\n86   5.0000000  -8.04718956 0.6931472 1.01111111   1\n87   5.0000000  -8.04718956 0.6931472 0.81111111   0\n88   5.0000000  -8.04718956 0.6931472 0.22222222   1\n89   5.0000000  -8.04718956 0.6931472 0.98333333   0\n90   3.3333333  -4.01324268 1.0986123 1.00555556   0\n91   0.6250000   0.29375227 2.7725887 0.93333333   1\n92   1.6666667  -0.85137604 1.7917595 0.50000000   1\n93   1.6666667  -0.85137604 1.7917595 0.33888889   1\n94   2.5000000  -2.29072683 1.3862944 0.35000000   0\n95   1.1111111  -0.11706724 2.1972246 0.67222222   0\n96   5.0000000  -8.04718956 0.6931472 0.98888889   0\n97   2.5000000  -2.29072683 1.3862944 0.28333333   0\n98   5.0000000  -8.04718956 0.6931472 0.97777778   0\n99   5.0000000  -8.04718956 0.6931472 0.27777778   1\n100  2.5000000  -2.29072683 1.3862944 0.92222222   0\n101  3.3333333  -4.01324268 1.0986123 0.98888889   0\n102  5.0000000  -8.04718956 0.6931472 0.26666667   1\n103  0.4761905   0.35330350 3.0445224 0.07777778   1\n104  5.0000000  -8.04718956 0.6931472 0.94444444   0\n105 10.0000000 -23.02585093 0.0000000 0.98888889   0\n106  2.0000000  -1.38629436 1.6094379 1.01111111   1\n107 10.0000000 -23.02585093 0.0000000 0.98888889   0\n108 10.0000000 -23.02585093 0.0000000 0.91111111   0\n109  2.5000000  -2.29072683 1.3862944 0.93333333   1\n110  5.0000000  -8.04718956 0.6931472 0.33333333   1\n111  1.4285714  -0.50953563 1.9459101 0.07777778   0\n112  2.5000000  -2.29072683 1.3862944 0.93333333   1\n113 10.0000000 -23.02585093 0.0000000 0.77777778   1\n114  3.3333333  -4.01324268 1.0986123 0.84444444   0\n115  5.0000000  -8.04718956 0.6931472 0.98888889   0\n116  5.0000000  -8.04718956 0.6931472 0.98888889   1\n117  1.4285714  -0.50953563 1.9459101 0.48333333   1\n118 10.0000000 -23.02585093 0.0000000 0.97222222   0\n119  2.5000000  -2.29072683 1.3862944 0.48333333   0\n120  0.5882353   0.31213427 2.8332133 0.61111111   0\n121  5.0000000  -8.04718956 0.6931472 0.11666667   1\n122  2.5000000  -2.29072683 1.3862944 0.77222222   0\n123  0.6250000   0.29375227 2.7725887 1.00555556   1\n124  3.3333333  -4.01324268 1.0986123 0.18333333   1\n125  5.0000000  -8.04718956 0.6931472 0.21666667   1\n126  1.2500000  -0.27892944 2.0794415 0.02222222   1\n127  2.0000000  -1.38629436 1.6094379 1.02222222   1\n128  5.0000000  -8.04718956 0.6931472 0.68333333   0\n129  5.0000000  -8.04718956 0.6931472 0.97777778   0\n130  3.3333333  -4.01324268 1.0986123 0.96666667   1\n131  5.0000000  -8.04718956 0.6931472 1.00555556   1\n132  3.3333333  -4.01324268 1.0986123 0.62777778   1\n133  2.5000000  -2.29072683 1.3862944 0.91111111   1\n134  5.0000000  -8.04718956 0.6931472 0.93333333   1\n135  1.1111111  -0.11706724 2.1972246 0.88888889   1\n136  2.0000000  -1.38629436 1.6094379 1.01111111   1\n137  2.5000000  -2.29072683 1.3862944 1.07777778   0\n138  2.5000000  -2.29072683 1.3862944 0.56666667   0\n139  2.5000000  -2.29072683 1.3862944 1.01111111   1\n140  2.0000000  -1.38629436 1.6094379 1.00000000   0\n141  1.4285714  -0.50953563 1.9459101 0.51111111   1\n142  5.0000000  -8.04718956 0.6931472 0.84444444   0\n143  3.3333333  -4.01324268 1.0986123 0.83333333   1\n144  2.0000000  -1.38629436 1.6094379 1.01111111   1\n145  1.6666667  -0.85137604 1.7917595 1.00000000   1\n146  1.4285714  -0.50953563 1.9459101 0.03333333   1\n147  2.5000000  -2.29072683 1.3862944 0.04444444   1\n148  5.0000000  -8.04718956 0.6931472 0.18333333   0\n149  1.1111111  -0.11706724 2.1972246 0.17222222   1\n150  5.0000000  -8.04718956 0.6931472 0.96666667   0\n151  3.3333333  -4.01324268 1.0986123 0.18888889   0\n152  2.5000000  -2.29072683 1.3862944 0.33333333   0\n153  1.6666667  -0.85137604 1.7917595 0.43333333   0\n154  3.3333333  -4.01324268 1.0986123 1.01111111   0\n155  5.0000000  -8.04718956 0.6931472 1.01111111   0\n156 10.0000000 -23.02585093 0.0000000 0.43333333   0\n157  5.0000000  -8.04718956 0.6931472 0.30555556   0\n158  3.3333333  -4.01324268 1.0986123 1.23888889   0\n159  0.4761905   0.35330350 3.0445224 0.13888889   0\n160 10.0000000 -23.02585093 0.0000000 0.35000000   0\n161  5.0000000  -8.04718956 0.6931472 0.73888889   0\n162  1.6666667  -0.85137604 1.7917595 0.85555556   1\n163  3.3333333  -4.01324268 1.0986123 0.38888889   0\n164  5.0000000  -8.04718956 0.6931472 0.36666667   0\n165  2.0000000  -1.38629436 1.6094379 0.22222222   1\n166  5.0000000  -8.04718956 0.6931472 0.41666667   0\n167  3.3333333  -4.01324268 1.0986123 1.03888889   0\n168  3.3333333  -4.01324268 1.0986123 1.01666667   0\n169  1.1111111  -0.11706724 2.1972246 1.01111111   0\n170  3.3333333  -4.01324268 1.0986123 1.06666667   0\n171  2.0000000  -1.38629436 1.6094379 0.90000000   1\n172  3.3333333  -4.01324268 1.0986123 1.07222222   1\n173  3.3333333  -4.01324268 1.0986123 0.61666667   0\n174  1.1111111  -0.11706724 2.1972246 1.01111111   1\n175  2.0000000  -1.38629436 1.6094379 1.00000000   0\n176 10.0000000 -23.02585093 0.0000000 0.51666667   0\n177  1.2500000  -0.27892944 2.0794415 0.92777778   0\n178  3.3333333  -4.01324268 1.0986123 1.08888889   0\n179  2.0000000  -1.38629436 1.6094379 0.58888889   0\n180  1.4285714  -0.50953563 1.9459101 0.87777778   1\n181  5.0000000  -8.04718956 0.6931472 1.01111111   0\n182  2.0000000  -1.38629436 1.6094379 0.98888889   0\n183  3.3333333  -4.01324268 1.0986123 0.98888889   0\n184  0.9090909   0.08664562 2.3978953 0.97777778   1\n185 10.0000000 -23.02585093 0.0000000 1.05555556   0\n186 10.0000000 -23.02585093 0.0000000 0.05555556   1\n187  1.1111111  -0.11706724 2.1972246 0.35555556   1\n188  3.3333333  -4.01324268 1.0986123 1.02222222   0\n189  2.5000000  -2.29072683 1.3862944 0.73333333   0\n190  1.1111111  -0.11706724 2.1972246 1.00000000   0\n191  2.5000000  -2.29072683 1.3862944 1.03333333   0\n192  3.3333333  -4.01324268 1.0986123 0.98888889   0\n193 10.0000000 -23.02585093 0.0000000 1.01111111   0\n194  1.1111111  -0.11706724 2.1972246 0.62222222   1\n195  1.2500000  -0.27892944 2.0794415 1.00000000   1\n196  2.5000000  -2.29072683 1.3862944 0.81111111   0\n197  1.6666667  -0.85137604 1.7917595 0.94444444   0\n198  2.5000000  -2.29072683 1.3862944 0.25555556   1\n199  2.5000000  -2.29072683 1.3862944 0.94444444   1\n200  1.6666667  -0.85137604 1.7917595 1.00000000   1\n201  2.0000000  -1.38629436 1.6094379 0.58888889   0\n202  3.3333333  -4.01324268 1.0986123 1.06666667   1\n203  3.3333333  -4.01324268 1.0986123 0.92222222   0\n204  1.1111111  -0.11706724 2.1972246 0.60000000   1\n205  1.0000000   0.00000000 2.3025851 0.87777778   0\n206  2.5000000  -2.29072683 1.3862944 0.90000000   1\n207  2.0000000  -1.38629436 1.6094379 0.20000000   0\n208  2.0000000  -1.38629436 1.6094379 1.02222222   1\n209  0.9090909   0.08664562 2.3978953 0.21666667   0\n210 10.0000000 -23.02585093 0.0000000 0.98333333   0\n211  3.3333333  -4.01324268 1.0986123 0.67777778   1\n212  2.0000000  -1.38629436 1.6094379 0.98888889   0\n213  1.2500000  -0.27892944 2.0794415 0.96111111   0\n214  0.3125000   0.36348463 3.4657359 0.29444444   1\n215  1.6666667  -0.85137604 1.7917595 0.52222222   1\n216  3.3333333  -4.01324268 1.0986123 0.90555556   0\n217  1.4285714  -0.50953563 1.9459101 0.88888889   0\n218  1.4285714  -0.50953563 1.9459101 0.33888889   0\n219  0.7692308   0.20181866 2.5649494 0.22777778   0\n220  1.6666667  -0.85137604 1.7917595 0.29444444   1\n221  2.0000000  -1.38629436 1.6094379 0.29444444   1\n222  0.8333333   0.15193463 2.4849066 0.07222222   0\n223  2.5000000  -2.29072683 1.3862944 1.01666667   0\n224  2.0000000  -1.38629436 1.6094379 1.01111111   1\n225  2.5000000  -2.29072683 1.3862944 1.01666667   1\n226  1.0000000   0.00000000 2.3025851 0.35000000   1\n227  1.2500000  -0.27892944 2.0794415 0.61666667   1\n228  3.3333333  -4.01324268 1.0986123 0.96666667   0\n229  2.5000000  -2.29072683 1.3862944 0.96111111   1\n230  3.3333333  -4.01324268 1.0986123 0.66111111   0\n231  2.5000000  -2.29072683 1.3862944 1.00000000   0\n232  3.3333333  -4.01324268 1.0986123 0.54444444   0\n233  2.0000000  -1.38629436 1.6094379 0.27777778   0\n234  5.0000000  -8.04718956 0.6931472 0.98888889   0\n235  0.5882353   0.31213427 2.8332133 0.55555556   1\n236  2.0000000  -1.38629436 1.6094379 0.51666667   0\n237  2.5000000  -2.29072683 1.3862944 0.91666667   1\n238 10.0000000 -23.02585093 0.0000000 0.51666667   0\n239  1.6666667  -0.85137604 1.7917595 0.48888889   1\n240  1.6666667  -0.85137604 1.7917595 0.85555556   0\n241 10.0000000 -23.02585093 0.0000000 1.01111111   0\n242  3.3333333  -4.01324268 1.0986123 1.05555556   0\n243  5.0000000  -8.04718956 0.6931472 0.91111111   1\n244  1.6666667  -0.85137604 1.7917595 0.84444444   0\n245  2.5000000  -2.29072683 1.3862944 0.05555556   0\n246  1.2500000  -0.27892944 2.0794415 0.76666667   1\n247  0.7692308   0.20181866 2.5649494 1.00000000   0\n248  1.1111111  -0.11706724 2.1972246 0.21111111   0\n249  2.0000000  -1.38629436 1.6094379 0.66666667   0\n250 10.0000000 -23.02585093 0.0000000 0.76666667   0\n251  1.1111111  -0.11706724 2.1972246 0.94444444   0\n252  2.5000000  -2.29072683 1.3862944 1.02222222   1\n253  2.5000000  -2.29072683 1.3862944 0.61111111   0\n254  1.0000000   0.00000000 2.3025851 0.22222222   1\n255  3.3333333  -4.01324268 1.0986123 0.96666667   1\n256 10.0000000 -23.02585093 0.0000000 1.01111111   0\n257 10.0000000 -23.02585093 0.0000000 0.10000000   0\n258  1.4285714  -0.50953563 1.9459101 0.24444444   1\n259  5.0000000  -8.04718956 0.6931472 0.96666667   0\n260  0.9090909   0.08664562 2.3978953 0.95555556   1\n261  1.2500000  -0.27892944 2.0794415 0.94444444   1\n262 10.0000000 -23.02585093 0.0000000 0.92222222   0\n263  1.4285714  -0.50953563 1.9459101 0.92222222   0\n264  1.4285714  -0.50953563 1.9459101 1.02222222   0\n265  2.5000000  -2.29072683 1.3862944 0.94444444   0\n266  1.4285714  -0.50953563 1.9459101 0.40000000   0\n267  3.3333333  -4.01324268 1.0986123 0.96666667   1\n268 10.0000000 -23.02585093 0.0000000 0.31111111   0\n269  1.6666667  -0.85137604 1.7917595 0.52222222   1\n270 10.0000000 -23.02585093 0.0000000 0.41111111   1\n271  5.0000000  -8.04718956 0.6931472 1.03333333   1\n272  3.3333333  -4.01324268 1.0986123 0.98888889   0\n273  2.5000000  -2.29072683 1.3862944 0.46666667   1\n274 10.0000000 -23.02585093 0.0000000 0.07222222   0\n275  0.7692308   0.20181866 2.5649494 0.47222222   1\n276  2.5000000  -2.29072683 1.3862944 0.05000000   0\n277  2.0000000  -1.38629436 1.6094379 0.90000000   0\n278  0.4761905   0.35330350 3.0445224 0.25555556   1\n279  0.4761905   0.35330350 3.0445224 0.28888889   1\n280  1.0000000   0.00000000 2.3025851 0.93333333   0\n281  1.6666667  -0.85137604 1.7917595 0.25555556   1\n282  3.3333333  -4.01324268 1.0986123 0.95555556   0\n283  2.0000000  -1.38629436 1.6094379 1.00000000   1\n284  1.1111111  -0.11706724 2.1972246 0.81111111   1\n285  1.2500000  -0.27892944 2.0794415 0.84444444   1\n286  5.0000000  -8.04718956 0.6931472 0.10000000   0\n287  2.5000000  -2.29072683 1.3862944 0.52222222   0\n288  3.3333333  -4.01324268 1.0986123 0.42222222   0\n289  2.5000000  -2.29072683 1.3862944 0.22222222   0\n290  2.5000000  -2.29072683 1.3862944 0.97777778   0\n291 10.0000000 -23.02585093 0.0000000 0.57777778   0\n292  0.6250000   0.29375227 2.7725887 0.02777778   1\n293  1.4285714  -0.50953563 1.9459101 0.99444444   0\n294  1.1111111  -0.11706724 2.1972246 0.19444444   1\n295  5.0000000  -8.04718956 0.6931472 0.13333333   0\n296  0.2777778   0.35581496 3.5835189 0.45555556   1\n297  2.5000000  -2.29072683 1.3862944 0.15555556   0\n298 10.0000000 -23.02585093 0.0000000 0.45000000   0\n299  3.3333333  -4.01324268 1.0986123 0.02222222   0\n300  2.0000000  -1.38629436 1.6094379 0.53888889   1\n301 10.0000000 -23.02585093 0.0000000 0.43333333   0\n302  2.5000000  -2.29072683 1.3862944 1.00555556   0\n303  5.0000000  -8.04718956 0.6931472 0.16111111   0\n304  5.0000000  -8.04718956 0.6931472 0.77222222   0\n305  3.3333333  -4.01324268 1.0986123 0.84444444   1\n306  1.6666667  -0.85137604 1.7917595 0.50000000   0\n307  2.0000000  -1.38629436 1.6094379 0.34444444   1\n308  0.7142857   0.24033731 2.6390573 0.61111111   0\n309  0.6666667   0.27031007 2.7080502 0.08333333   0\n310  1.6666667  -0.85137604 1.7917595 0.37777778   0\n311 10.0000000 -23.02585093 0.0000000 0.10555556   0\n312  1.4285714  -0.50953563 1.9459101 0.25555556   1\n313  0.3225806   0.36496842 3.4339872 1.02222222   1\n314  1.1111111  -0.11706724 2.1972246 1.04444444   1\n315  1.4285714  -0.50953563 1.9459101 0.34444444   1\n316  2.5000000  -2.29072683 1.3862944 0.31111111   0\n317  0.9090909   0.08664562 2.3978953 0.64444444   1\n318  1.4285714  -0.50953563 1.9459101 1.25555556   1\n319  2.5000000  -2.29072683 1.3862944 0.77777778   0\n320  2.0000000  -1.38629436 1.6094379 1.00000000   1\n321  1.4285714  -0.50953563 1.9459101 0.61111111   1\n322  1.6666667  -0.85137604 1.7917595 0.98888889   1\n323  1.2500000  -0.27892944 2.0794415 0.78888889   1\n324  1.6666667  -0.85137604 1.7917595 0.93333333   0\n325  2.0000000  -1.38629436 1.6094379 0.86666667   0\n326  2.0000000  -1.38629436 1.6094379 0.66666667   0\n327  1.4285714  -0.50953563 1.9459101 0.91111111   1\n328  1.6666667  -0.85137604 1.7917595 0.90000000   0\n329  2.5000000  -2.29072683 1.3862944 0.19444444   1\n330  0.4347826   0.36213440 3.1354942 0.08888889   1\n331  5.0000000  -8.04718956 0.6931472 0.03888889   0\n332  2.0000000  -1.38629436 1.6094379 0.16666667   0\n333  2.5000000  -2.29072683 1.3862944 0.58888889   0\n334  0.9090909   0.08664562 2.3978953 0.96666667   0\n335  5.0000000  -8.04718956 0.6931472 0.80000000   0\n336  5.0000000  -8.04718956 0.6931472 0.13333333   0\n337  2.5000000  -2.29072683 1.3862944 0.09444444   0\n338  2.0000000  -1.38629436 1.6094379 0.53888889   0\n339 10.0000000 -23.02585093 0.0000000 0.14444444   1\n340  3.3333333  -4.01324268 1.0986123 0.17222222   0\n341  2.5000000  -2.29072683 1.3862944 0.15555556   1\n342  3.3333333  -4.01324268 1.0986123 0.83333333   1\n343  1.4285714  -0.50953563 1.9459101 0.22222222   1\n344  2.0000000  -1.38629436 1.6094379 1.15555556   0\n345  0.7692308   0.20181866 2.5649494 0.94444444   1\n346 10.0000000 -23.02585093 0.0000000 1.22222222   0\n347  0.5882353   0.31213427 2.8332133 1.11111111   1\n348  5.0000000  -8.04718956 0.6931472 0.81111111   1\n349  1.4285714  -0.50953563 1.9459101 0.72222222   1\n350  0.4761905   0.35330350 3.0445224 0.83333333   1\n351  5.0000000  -8.04718956 0.6931472 0.92222222   0\n352  0.5555556   0.32654815 2.8903718 0.08333333   0\n353  0.3225806   0.36496842 3.4339872 0.24444444   1\n354  2.5000000  -2.29072683 1.3862944 0.03888889   0\n355  2.0000000  -1.38629436 1.6094379 0.11111111   1\n356  0.7692308   0.20181866 2.5649494 0.97222222   1\n357  2.5000000  -2.29072683 1.3862944 0.39444444   0\n358  3.3333333  -4.01324268 1.0986123 0.14444444   0\n359  2.5000000  -2.29072683 1.3862944 0.89444444   1\n360  1.0000000   0.00000000 2.3025851 0.20000000   0\n361  1.1111111  -0.11706724 2.1972246 0.16666667   1\n362  2.5000000  -2.29072683 1.3862944 0.99444444   0\n363  3.3333333  -4.01324268 1.0986123 1.10555556   1\n364  2.5000000  -2.29072683 1.3862944 1.01111111   1\n365  2.5000000  -2.29072683 1.3862944 1.24444444   0\n366  1.4285714  -0.50953563 1.9459101 0.08888889   1\n367  3.3333333  -4.01324268 1.0986123 0.20000000   0\n368  1.4285714  -0.50953563 1.9459101 0.22222222   0\n369  2.5000000  -2.29072683 1.3862944 0.97777778   1\n370  1.4285714  -0.50953563 1.9459101 0.97777778   0\n371  0.4761905   0.35330350 3.0445224 0.84444444   1\n372  5.0000000  -8.04718956 0.6931472 0.24444444   0\n373  2.0000000  -1.38629436 1.6094379 1.22222222   1\n374  2.5000000  -2.29072683 1.3862944 0.94444444   1\n375  2.5000000  -2.29072683 1.3862944 0.11111111   1\n376  0.5555556   0.32654815 2.8903718 0.87222222   1\n377  0.3703704   0.36787103 3.2958369 0.73888889   1\n378  2.5000000  -2.29072683 1.3862944 0.46111111   0\n379  2.0000000  -1.38629436 1.6094379 0.84444444   0\n380  2.0000000  -1.38629436 1.6094379 0.93888889   0\n381  2.5000000  -2.29072683 1.3862944 0.49444444   0\n382  1.6666667  -0.85137604 1.7917595 0.51111111   1\n383  2.0000000  -1.38629436 1.6094379 0.11666667   0\n384  0.3225806   0.36496842 3.4339872 0.17222222   1\n385  5.0000000  -8.04718956 0.6931472 0.17222222   0\n386  0.5882353   0.31213427 2.8332133 0.73888889   1\n387  2.0000000  -1.38629436 1.6094379 0.85000000   1\n388 10.0000000 -23.02585093 0.0000000 1.00000000   0\n389  0.4761905   0.35330350 3.0445224 1.13333333   1\n390  1.2500000  -0.27892944 2.0794415 0.94444444   1\n391  5.0000000  -8.04718956 0.6931472 0.98888889   0\n392  0.7142857   0.24033731 2.6390573 0.31111111   1\n393  5.0000000  -8.04718956 0.6931472 1.00000000   1\n394  2.5000000  -2.29072683 1.3862944 0.93333333   0\n395  2.5000000  -2.29072683 1.3862944 0.94444444   1\n396  1.2500000  -0.27892944 2.0794415 0.40000000   1\n397  0.9090909   0.08664562 2.3978953 0.82222222   1\n398 10.0000000 -23.02585093 0.0000000 0.46666667   1\n399  5.0000000  -8.04718956 0.6931472 1.00000000   1\n400 10.0000000 -23.02585093 0.0000000 1.20000000   1\n401  2.5000000  -2.29072683 1.3862944 0.54444444   1\n402  5.0000000  -8.04718956 0.6931472 2.43333333   1\n403  1.4285714  -0.50953563 1.9459101 1.20000000   1\n404 10.0000000 -23.02585093 0.0000000 1.97777778   0\n405  3.3333333  -4.01324268 1.0986123 0.46666667   0\n406  1.2500000  -0.27892944 2.0794415 2.02222222   1\n407  1.6666667  -0.85137604 1.7917595 0.06666667   0\n408  2.5000000  -2.29072683 1.3862944 1.95000000   0\n409  5.0000000  -8.04718956 0.6931472 0.06666667   0\n410 10.0000000 -23.02585093 0.0000000 0.03333333   1\n411  5.0000000  -8.04718956 0.6931472 0.50555556   0\n412  5.0000000  -8.04718956 0.6931472 1.36111111   0\n413  5.0000000  -8.04718956 0.6931472 2.06666667   0\n414 10.0000000 -23.02585093 0.0000000 1.21111111   0\n415 10.0000000 -23.02585093 0.0000000 0.25555556   0\n416  1.4285714  -0.50953563 1.9459101 2.01666667   1\n417 10.0000000 -23.02585093 0.0000000 0.73888889   0\n418  5.0000000  -8.04718956 0.6931472 0.03888889   1\n419 10.0000000 -23.02585093 0.0000000 0.62222222   0\n420  3.3333333  -4.01324268 1.0986123 0.23333333   0\n421  5.0000000  -8.04718956 0.6931472 1.87777778   1\n422  1.4285714  -0.50953563 1.9459101 0.31111111   1\n423  0.4761905   0.35330350 3.0445224 0.52222222   1\n424  1.6666667  -0.85137604 1.7917595 0.22222222   1\n425  2.0000000  -1.38629436 1.6094379 1.95555556   1\n426 10.0000000 -23.02585093 0.0000000 0.36666667   0\n427  1.4285714  -0.50953563 1.9459101 0.30555556   0\n428  1.2500000  -0.27892944 2.0794415 1.91111111   0\n429 10.0000000 -23.02585093 0.0000000 0.85000000   0\n430  1.1111111  -0.11706724 2.1972246 2.04444444   0\n431 10.0000000 -23.02585093 0.0000000 2.03333333   0\n432  2.5000000  -2.29072683 1.3862944 0.24444444   0\n433 10.0000000 -23.02585093 0.0000000 2.03333333   0\n434 10.0000000 -23.02585093 0.0000000 1.55555556   0\n435  3.3333333  -4.01324268 1.0986123 0.21111111   0\n436  0.7692308   0.20181866 2.5649494 2.04444444   1\n437 10.0000000 -23.02585093 0.0000000 0.55555556   0\n438  3.3333333  -4.01324268 1.0986123 1.46666667   0\n439  5.0000000  -8.04718956 0.6931472 1.42222222   1\n440  5.0000000  -8.04718956 0.6931472 0.59444444   0\n441 10.0000000 -23.02585093 0.0000000 2.04444444   0\n442  2.0000000  -1.38629436 1.6094379 1.21666667   1\n443  1.6666667  -0.85137604 1.7917595 2.07777778   0\n444  5.0000000  -8.04718956 0.6931472 0.51111111   0\n445 10.0000000 -23.02585093 0.0000000 0.25000000   0\n446 10.0000000 -23.02585093 0.0000000 2.03333333   0\n447  3.3333333  -4.01324268 1.0986123 2.04444444   1\n448  5.0000000  -8.04718956 0.6931472 0.86666667   0\n449 10.0000000 -23.02585093 0.0000000 2.04444444   0\n450  3.3333333  -4.01324268 1.0986123 2.07777778   0\n451  3.3333333  -4.01324268 1.0986123 1.12222222   1\n452 10.0000000 -23.02585093 0.0000000 1.56666667   0\n453  5.0000000  -8.04718956 0.6931472 0.26666667   0\n454  0.2439024   0.34414316 3.7135721 0.40000000   0\n455 10.0000000 -23.02585093 0.0000000 0.31111111   0\n456  1.1111111  -0.11706724 2.1972246 2.03888889   0\n457  3.3333333  -4.01324268 1.0986123 0.38888889   0\n458  1.4285714  -0.50953563 1.9459101 0.32222222   0\n459  2.0000000  -1.38629436 1.6094379 2.03333333   0\n460  1.4285714  -0.50953563 1.9459101 0.05555556   1\n461  3.3333333  -4.01324268 1.0986123 2.37777778   1\n462  1.1111111  -0.11706724 2.1972246 2.18888889   0\n463 10.0000000 -23.02585093 0.0000000 0.98888889   0\n464  5.0000000  -8.04718956 0.6931472 0.62222222   0\n465  5.0000000  -8.04718956 0.6931472 0.10000000   0\n466 10.0000000 -23.02585093 0.0000000 2.06666667   0\n467  5.0000000  -8.04718956 0.6931472 1.68333333   0\n468  2.5000000  -2.29072683 1.3862944 0.17777778   0\n469  2.0000000  -1.38629436 1.6094379 0.04444444   0\n470  5.0000000  -8.04718956 0.6931472 0.35000000   0\n471  0.4761905   0.35330350 3.0445224 1.20000000   0\n472  5.0000000  -8.04718956 0.6931472 2.03333333   0\n473  5.0000000  -8.04718956 0.6931472 0.83888889   0\n474 10.0000000 -23.02585093 0.0000000 0.07777778   0\n475  3.3333333  -4.01324268 1.0986123 0.42222222   0\n476  3.3333333  -4.01324268 1.0986123 0.97777778   0\n477  1.2500000  -0.27892944 2.0794415 0.51666667   1\n478  2.5000000  -2.29072683 1.3862944 2.22222222   1\n479 10.0000000 -23.02585093 0.0000000 1.97777778   0\n480 10.0000000 -23.02585093 0.0000000 0.43333333   0\n481  0.9090909   0.08664562 2.3978953 0.66111111   0\n482  3.3333333  -4.01324268 1.0986123 1.71111111   0\n483  3.3333333  -4.01324268 1.0986123 0.90555556   1\n484  1.2500000  -0.27892944 2.0794415 0.65555556   0\n485 10.0000000 -23.02585093 0.0000000 0.42222222   0\n486 10.0000000 -23.02585093 0.0000000 0.64444444   0\n487  2.5000000  -2.29072683 1.3862944 0.97777778   1\n488  5.0000000  -8.04718956 0.6931472 0.36666667   0\n489  3.3333333  -4.01324268 1.0986123 0.38888889   1\n490  3.3333333  -4.01324268 1.0986123 0.37777778   0\n491  0.3846154   0.36750440 3.2580965 2.12222222   0\n492  1.6666667  -0.85137604 1.7917595 0.38888889   0\n493  5.0000000  -8.04718956 0.6931472 0.17777778   0\n494 10.0000000 -23.02585093 0.0000000 0.31111111   0\n495  2.5000000  -2.29072683 1.3862944 0.16666667   0\n496  1.2500000  -0.27892944 2.0794415 0.07777778   1\n497 10.0000000 -23.02585093 0.0000000 0.47777778   0\n498  1.6666667  -0.85137604 1.7917595 0.98888889   0\n499  5.0000000  -8.04718956 0.6931472 0.42222222   0\n500  2.5000000  -2.29072683 1.3862944 2.26666667   1\n501  3.3333333  -4.01324268 1.0986123 0.84444444   0\n502  2.5000000  -2.29072683 1.3862944 2.16666667   0\n503  5.0000000  -8.04718956 0.6931472 2.04444444   0\n504  5.0000000  -8.04718956 0.6931472 1.41111111   0\n505 10.0000000 -23.02585093 0.0000000 2.06111111   0\n506  5.0000000  -8.04718956 0.6931472 2.17777778   0\n507  3.3333333  -4.01324268 1.0986123 2.20000000   0\n508  3.3333333  -4.01324268 1.0986123 1.88888889   1\n509  2.0000000  -1.38629436 1.6094379 0.27777778   1\n510 10.0000000 -23.02585093 0.0000000 0.90555556   0\n511  0.8333333   0.15193463 2.4849066 2.02222222   0\n512  2.5000000  -2.29072683 1.3862944 1.66666667   0\n513  1.2500000  -0.27892944 2.0794415 0.18888889   1\n514  2.0000000  -1.38629436 1.6094379 0.18888889   1\n515  2.5000000  -2.29072683 1.3862944 2.03333333   0\n516  1.2500000  -0.27892944 2.0794415 1.47777778   0\n517  2.5000000  -2.29072683 1.3862944 0.76666667   0\n518  1.4285714  -0.50953563 1.9459101 2.03333333   1\n519  1.4285714  -0.50953563 1.9459101 0.07777778   1\n520  3.3333333  -4.01324268 1.0986123 2.04444444   0\n521  0.6250000   0.29375227 2.7725887 0.49444444   1\n522  1.6666667  -0.85137604 1.7917595 2.03333333   0\n523  0.7142857   0.24033731 2.6390573 1.96666667   1\n524  0.9090909   0.08664562 2.3978953 0.85555556   1\n525  0.4761905   0.35330350 3.0445224 1.36666667   0\n526  5.0000000  -8.04718956 0.6931472 1.62222222   0\n527  0.9090909   0.08664562 2.3978953 1.12777778   0\n528  1.4285714  -0.50953563 1.9459101 2.00000000   1\n529  3.3333333  -4.01324268 1.0986123 0.87777778   0\n530  1.6666667  -0.85137604 1.7917595 1.11666667   0\n531 10.0000000 -23.02585093 0.0000000 0.71666667   0\n532  5.0000000  -8.04718956 0.6931472 2.02777778   0\n533  2.0000000  -1.38629436 1.6094379 0.88333333   0\n534  3.3333333  -4.01324268 1.0986123 1.96666667   0\n535  5.0000000  -8.04718956 0.6931472 0.39444444   1\n536  3.3333333  -4.01324268 1.0986123 0.60000000   1\n537  2.0000000  -1.38629436 1.6094379 1.10000000   0\n538  5.0000000  -8.04718956 0.6931472 2.06666667   0\n539  1.4285714  -0.50953563 1.9459101 0.27777778   0\n540 10.0000000 -23.02585093 0.0000000 0.26666667   0\n541  2.5000000  -2.29072683 1.3862944 2.12222222   0\n542  1.6666667  -0.85137604 1.7917595 0.95000000   0\n543  5.0000000  -8.04718956 0.6931472 0.80555556   0\n544  0.4761905   0.35330350 3.0445224 2.03333333   0\n545  2.5000000  -2.29072683 1.3862944 0.80000000   0\n546  5.0000000  -8.04718956 0.6931472 0.24444444   0\n547  3.3333333  -4.01324268 1.0986123 0.77777778   0\n548 10.0000000 -23.02585093 0.0000000 2.04444444   0\n549  3.3333333  -4.01324268 1.0986123 1.04444444   0\n550  1.1111111  -0.11706724 2.1972246 1.64444444   0\n551 10.0000000 -23.02585093 0.0000000 0.25555556   0\n552  5.0000000  -8.04718956 0.6931472 1.42222222   0\n553 10.0000000 -23.02585093 0.0000000 1.17777778   0\n554 10.0000000 -23.02585093 0.0000000 0.51111111   0\n555 10.0000000 -23.02585093 0.0000000 0.83333333   0\n556  3.3333333  -4.01324268 1.0986123 0.26666667   0\n557  2.5000000  -2.29072683 1.3862944 0.32222222   1\n558  3.3333333  -4.01324268 1.0986123 1.98888889   0\n559  2.0000000  -1.38629436 1.6094379 1.88888889   1\n560  5.0000000  -8.04718956 0.6931472 2.02777778   0\n561  2.0000000  -1.38629436 1.6094379 2.22222222   0\n562 10.0000000 -23.02585093 0.0000000 0.62222222   0\n563  1.6666667  -0.85137604 1.7917595 0.26666667   0\n564  1.4285714  -0.50953563 1.9459101 0.11111111   0\n565  0.4545455   0.35838971 3.0910425 1.96666667   1\n566 10.0000000 -23.02585093 0.0000000 1.28888889   0\n567  0.8333333   0.15193463 2.4849066 0.30000000   0\n568 10.0000000 -23.02585093 0.0000000 0.26666667   0\n569  2.0000000  -1.38629436 1.6094379 0.63333333   0\n570 10.0000000 -23.02585093 0.0000000 0.51111111   0\n571  1.4285714  -0.50953563 1.9459101 0.43333333   0\n572  2.0000000  -1.38629436 1.6094379 0.18888889   1\n573  2.5000000  -2.29072683 1.3862944 0.11666667   0\n574  3.3333333  -4.01324268 1.0986123 2.04444444   1\n575  0.6250000   0.29375227 2.7725887 0.05000000   1\n\n\n\nglimpse(uis)\n\nRows: 575\nColumns: 18\n$ ID     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, …\n$ AGE    &lt;dbl&gt; 39, 33, 33, 32, 24, 30, 39, 27, 40, 36, 38, 29, 32, 41, 31, 27,…\n$ BECK   &lt;dbl&gt; 9.000, 34.000, 10.000, 20.000, 5.000, 32.550, 19.000, 10.000, 2…\n$ HC     &lt;dbl&gt; 4, 4, 2, 4, 2, 3, 4, 4, 2, 2, 2, 3, 3, 1, 1, 2, 1, 4, 3, 2, 3, …\n$ IV     &lt;dbl&gt; 3, 2, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 2, 1, 3, 1, …\n$ NDT    &lt;dbl&gt; 1, 8, 3, 1, 5, 1, 34, 2, 3, 7, 8, 1, 2, 8, 1, 3, 6, 1, 15, 5, 1…\n$ RACE   &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ TREAT  &lt;dbl&gt; 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, …\n$ SITE   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ LEN.T  &lt;dbl&gt; 123, 25, 7, 66, 173, 16, 179, 21, 176, 124, 176, 79, 182, 174, …\n$ TIME   &lt;dbl&gt; 188, 26, 207, 144, 551, 32, 459, 22, 210, 184, 212, 87, 598, 26…\n$ CENSOR &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, …\n$ Y      &lt;dbl&gt; 5.236442, 3.258097, 5.332719, 4.969813, 6.311735, 3.465736, 6.1…\n$ ND1    &lt;dbl&gt; 5.0000000, 1.1111111, 2.5000000, 5.0000000, 1.6666667, 5.000000…\n$ ND2    &lt;dbl&gt; -8.0471896, -0.1170672, -2.2907268, -8.0471896, -0.8513760, -8.…\n$ LNDT   &lt;dbl&gt; 0.6931472, 2.1972246, 1.3862944, 0.6931472, 1.7917595, 0.693147…\n$ FRAC   &lt;dbl&gt; 0.68333333, 0.13888889, 0.03888889, 0.73333333, 0.96111111, 0.0…\n$ IV3    &lt;dbl&gt; 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, …\n\n\nTo talk about the ANOVA procedure, we’ll use the BECK and IV variables. We need to convert IV to a factor variable first (using the help file for guidance). We’ll add it to a new tibble called uis2.\n\nuis2 &lt;- uis %&gt;%\n  mutate(IV_fct = factor(IV, levels = c(1, 2, 3),\n                         labels = c(\"Never\", \"Previous\", \"Recent\")))\nuis2\n\n     ID AGE   BECK HC IV NDT RACE TREAT SITE LEN.T TIME CENSOR        Y\n1     1  39  9.000  4  3   1    0     1    0   123  188      1 5.236442\n2     2  33 34.000  4  2   8    0     1    0    25   26      1 3.258097\n3     3  33 10.000  2  3   3    0     1    0     7  207      1 5.332719\n4     4  32 20.000  4  3   1    0     0    0    66  144      1 4.969813\n5     5  24  5.000  2  1   5    1     1    0   173  551      0 6.311735\n6     6  30 32.550  3  3   1    0     1    0    16   32      1 3.465736\n7     7  39 19.000  4  3  34    0     1    0   179  459      1 6.129050\n8     8  27 10.000  4  3   2    0     1    0    21   22      1 3.091042\n9     9  40 29.000  2  3   3    0     1    0   176  210      1 5.347108\n10   10  36 25.000  2  3   7    0     1    0   124  184      1 5.214936\n11   12  38 18.900  2  3   8    0     1    0   176  212      1 5.356586\n12   13  29 16.000  3  1   1    0     1    0    79   87      1 4.465908\n13   14  32 36.000  3  3   2    1     1    0   182  598      0 6.393591\n14   15  41 19.000  1  3   8    0     1    0   174  260      1 5.560682\n15   16  31 18.000  1  3   1    0     1    0   181  210      1 5.347108\n16   17  27 12.000  2  3   3    0     1    0    61   84      1 4.430817\n17   18  28 34.000  1  3   6    0     1    0   177  196      1 5.278115\n18   19  28 23.000  4  2   1    0     1    0    19   19      1 2.944439\n19   20  36 26.000  3  1  15    1     1    0    27  441      1 6.089045\n20   21  32 18.900  2  3   5    0     1    0   175  449      1 6.107023\n21   22  33 15.000  3  1   1    0     0    0    12  659      0 6.490724\n22   23  28 25.200  1  3   8    0     0    0    21   21      1 3.044522\n23   24  29  6.632  4  2   0    0     0    0    48   53      1 3.970292\n24   25  35  2.100  2  3   9    0     0    0    90  225      1 5.416100\n25   26  45 26.000  1  3   6    0     0    0    91  161      1 5.081404\n26   27  35 39.789  4  3   5    0     0    0    87   87      1 4.465908\n27   28  24 20.000  3  1   3    0     0    0    88   89      1 4.488636\n28   29  36 16.000  1  3   7    0     0    0     9   44      1 3.784190\n29   31  39 22.000  1  3   9    0     0    0    94  523      0 6.259581\n30   32  36  9.947  4  2  10    0     0    0    91  226      1 5.420535\n31   33  37  9.450  4  3   1    0     0    0    90  259      1 5.556828\n32   34  30 39.000  2  3   1    0     0    0    89  289      1 5.666427\n33   35  44 41.000  1  3   5    0     0    0    89  103      1 4.634729\n34   36  28 31.000  3  1   6    1     0    0   100  624      0 6.436150\n35   37  25 20.000  3  1   3    1     0    0    67   68      1 4.219508\n36   38  30  8.000  2  3   7    0     1    0    25   57      1 4.043051\n37   39  24  9.000  4  1   1    0     0    0    12   65      1 4.174387\n38   40  27 20.000  3  1   1    0     0    0    79   79      1 4.369448\n39   41  30  8.000  3  1   2    1     0    0    79  559      0 6.326149\n40   42  34  8.000  2  3   0    0     1    0    78   79      1 4.369448\n41   43  33 23.000  4  2   2    0     1    0    84   87      1 4.465908\n42   44  34 18.000  3  3   6    0     1    0    91   91      1 4.510860\n43   45  36 13.000  2  3   1    0     1    0   162  297      1 5.693732\n44   46  27 23.000  1  3   0    0     1    0    45   45      1 3.806662\n45   47  35  9.000  4  3   1    1     1    0    61  246      1 5.505332\n46   48  24 14.000  1  3   0    0     1    0    19   37      1 3.610918\n47   49  28 23.000  4  1   2    1     1    0    37   37      1 3.610918\n48   50  46 10.000  1  3   8    0     1    0    51  538      0 6.287859\n49   51  26 11.000  3  3   1    0     1    0    60  541      0 6.293419\n50   52  42 16.000  1  3  25    0     1    0   177  184      1 5.214936\n51   53  30  0.000  3  1   0    0     1    0    43  122      1 4.804021\n52   55  30 12.000  4  1   3    1     1    0    21  156      1 5.049856\n53   56  27 21.000  2  3   2    0     0    0    88  121      1 4.795791\n54   57  38  0.000  1  3   6    0     0    0    96  231      1 5.442418\n55   58  48  8.000  4  3  10    0     0    0   111  111      1 4.709530\n56   59  36 25.000  1  3  10    0     0    0    38   38      1 3.637586\n57   60  28  6.300  3  1   7    0     0    0    15   15      1 2.708050\n58   61  31 20.000  4  2   5    0     0    0    50   54      1 3.988984\n59   62  28  4.000  2  3   5    0     0    0    61  127      1 4.844187\n60   63  28 20.000  3  1   1    0     0    0    31  105      1 4.653960\n61   64  26 17.000  2  1   2    1     0    0    11   11      1 2.397895\n62   65  34  3.000  4  3   6    0     0    0    90  153      1 5.030438\n63   66  26 29.000  2  3   5    0     0    0    11   11      1 2.397895\n64   68  31 26.000  1  3   5    0     0    0    46   46      1 3.828641\n65   69  41 12.000  1  3   0    1     0    0    38  655      0 6.484635\n66   70  30 24.000  4  3   0    0     0    0    90  166      1 5.111988\n67   72  39 15.750  4  3   5    0     0    0    88   95      1 4.553877\n68   74  33  9.000  2  3  12    0     0    0    91  151      1 5.017280\n69   75  33 18.000  4  2   6    0     0    0    85  220      1 5.393628\n70   76  29 20.000  4  1   0    1     0    0    90  227      1 5.424950\n71   77  36 17.000  1  3   5    0     0    0    52  343      1 5.837730\n72   78  26  3.000  4  3   3    0     0    0    88  119      1 4.779123\n73   79  37 27.000  1  3  13    0     0    0    43   43      1 3.761200\n74   81  29 31.500  1  3   8    0     0    0    37   47      1 3.850148\n75   83  30 19.000  3  1   0    1     0    0    87  805      0 6.690842\n76   84  35 15.000  3  2   2    0     0    0    20  321      1 5.771441\n77   85  33 22.000  3  1   1    0     0    0     9  167      1 5.117994\n78   87  36 16.000  2  3   1    0     0    0    85  491      1 6.196444\n79   88  28 17.000  1  3   2    0     0    0    18   35      1 3.555348\n80   89  31 32.550  1  3  12    1     0    0    71  123      1 4.812184\n81   90  23 24.000  1  3   2    0     0    0    88  597      0 6.391917\n82   91  33 22.000  3  2   1    0     0    0    67  762      0 6.635947\n83   93  37 18.000  2  3   4    0     0    0    30   31      1 3.433987\n84   94  25 17.850  3  1   1    0     1    0    68  228      1 5.429346\n85   95  56  5.000  2  2   9    1     1    0   182  553      0 6.315358\n86   96  23 39.000  1  3   1    0     1    0   182  190      1 5.247024\n87   97  26 21.000  3  1   1    0     1    0   146  307      1 5.726848\n88   98  26 11.000  1  3   1    0     1    0    40   73      1 4.290459\n89   99  23 14.000  3  1   1    0     1    0   177  208      1 5.337538\n90  100  28 31.000  4  2   2    1     1    0   181  267      1 5.587249\n91  102  30 14.000  1  3  15    0     1    0   168  169      1 5.129899\n92  104  25  6.000  2  3   5    0     1    0    90  655      0 6.484635\n93  105  33 16.000  1  3   5    0     1    0    61   70      1 4.248495\n94  106  22  6.000  3  1   3    1     1    0    63  398      1 5.986452\n95  108  25 20.000  4  2   8    1     1    0   121  122      1 4.804021\n96  111  38  9.000  3  1   1    1     0    0    89   96      1 4.564348\n97  112  35 11.000  2  1   3    0     1    0    51 1172      0 7.066467\n98  113  35 15.000  3  1   1    0     0    0    88  734      0 6.598509\n99  114  25 13.000  3  3   1    0     0    0    25   26      1 3.258097\n100 115  33 31.000  3  1   3    1     0    0    83   84      1 4.430817\n101 116  30  5.000  3  1   2    1     0    0    89  171      1 5.141664\n102 117  45 10.000  2  3   1    0     0    0    24  159      1 5.068904\n103 119  42 23.000  2  3  20    0     0    0     7    7      1 1.945910\n104 120  29 16.000  4  1   1    1     0    0    85  763      0 6.637258\n105 121  24 37.800  3  1   0    0     0    0    89  104      1 4.644391\n106 122  33 10.000  2  3   4    0     0    0    91  162      1 5.087596\n107 123  32  9.000  3  1   0    0     0    0    89   90      1 4.499810\n108 124  26 15.000  3  1   0    0     0    0    82  373      1 5.921578\n109 125  28  2.000  1  3   3    0     0    0    84  115      1 4.744932\n110 127  37 34.000  2  3   1    0     0    0    30   30      1 3.401197\n111 128  23 11.000  4  1   6    0     0    0     7    8      1 2.079442\n112 129  40 31.000  2  3   3    1     0    0    84  168      1 5.123964\n113 130  36 36.750  3  3   0    0     0    0    70   70      1 4.248495\n114 131  23 26.000  3  2   2    0     0    0    76  130      1 4.867534\n115 132  35  5.000  4  1   1    1     0    0    89  285      1 5.652489\n116 133  25 19.000  2  3   1    0     1    0   178  569      0 6.343880\n117 134  35 21.000  2  3   6    0     1    0    87   87      1 4.465908\n118 135  46  1.000  4  2   0    0     1    0   175  310      1 5.736572\n119 136  32  6.000  4  1   3    0     1    0    87   87      1 4.465908\n120 137  35 23.000  3  1  16    1     1    0   110  544      0 6.298949\n121 138  34 38.000  3  3   1    0     1    0    21  156      1 5.049856\n122 139  43 24.000  3  1   3    0     1    0   139  658      0 6.489205\n123 140  39  3.000  4  3  15    0     1    0   181  273      1 5.609472\n124 141  27 16.800  4  3   2    1     1    0    33  168      1 5.123964\n125 142  38 35.000  1  3   1    0     1    0    39   83      1 4.418841\n126 143  37 11.000  2  3   7    0     1    0     4    4      1 1.386294\n127 144  44  2.000  1  3   4    1     1    0   184  708      0 6.562444\n128 145  25 16.000  4  1   1    1     1    0   123  137      1 4.919981\n129 146  34 15.000  3  1   1    0     1    0   176  259      1 5.556828\n130 147  34 11.000  3  3   2    1     1    0   174  560      0 6.327937\n131 148  38 11.000  1  3   1    1     1    0   181  586      0 6.373320\n132 149  24 22.000  2  3   2    1     1    0   113  190      1 5.247024\n133 151  42 18.000  2  3   3    0     1    0   164  544      0 6.298949\n134 153  34 29.000  4  3   1    1     0    0    84  494      1 6.202536\n135 154  45 27.000  1  3   8    0     0    0    80  541      0 6.293419\n136 155  40 16.000  2  3   4    0     0    0    91   94      1 4.543295\n137 156  27  9.000  4  1   3    1     0    0    97  567      0 6.340359\n138 157  24  0.000  4  1   3    0     0    0    51   55      1 4.007333\n139 158  27 15.000  1  3   3    0     0    0    91   93      1 4.532599\n140 159  34 24.000  3  1   4    0     0    0    90  276      1 5.620401\n141 160  36  3.000  2  3   6    0     0    0    46   46      1 3.828641\n142 162  31  9.000  3  1   1    0     0    0    76  250      1 5.521461\n143 163  40  5.000  2  3   2    0     0    0    75  106      1 4.663439\n144 164  40 13.000  1  3   4    1     0    0    91  552      0 6.313548\n145 165  37 29.000  2  3   5    0     0    0    90   90      1 4.499810\n146 166  25 11.000  4  3   6    0     0    0     3  203      1 5.313206\n147 167  41 22.000  2  3   3    1     1    0     8   67      1 4.204693\n148 168  22  9.000  4  1   1    0     1    0    33  559      1 6.326149\n149 169  31 18.000  2  3   8    1     1    0    31  106      1 4.663439\n150 170  29 40.000  1  1   1    1     1    0   174  374      1 5.924256\n151 171  27 25.000  3  1   2    0     1    0    34  630      0 6.445720\n152 172  22 26.000  4  2   3    0     1    0    60   61      1 4.110874\n153 174  37 11.000  1  2   5    1     1    0    78  547      0 6.304449\n154 175  36  6.000  3  1   2    1     1    0   182  568      0 6.342121\n155 176  24 20.000  3  1   1    0     1    0   182  490      1 6.194405\n156 177  28  9.000  4  1   0    1     1    0    78  222      1 5.402677\n157 178  24  6.000  4  1   1    0     1    0    55   56      1 4.025352\n158 179  28  0.000  3  1   2    0     1    0   223  282      1 5.641907\n159 180  24  5.000  3  1  20    1     1    0    25   35      1 3.555348\n160 181  24 15.000  4  1   0    0     1    0    63  603      0 6.401917\n161 183  29 14.700  3  1   1    0     1    0   133  148      1 4.997212\n162 184  37  3.000  1  3   5    1     1    0   154  354      1 5.869297\n163 185  26 31.000  1  1   2    0     1    0    70  164      1 5.099866\n164 186  29 14.000  3  2   1    0     1    0    66   94      1 4.543295\n165 187  29 28.000  2  3   4    0     1    0    40   65      1 4.174387\n166 188  33 18.000  4  1   1    0     1    0    75  567      0 6.340359\n167 189  29 12.000  4  2   2    0     1    0   187  634      0 6.452049\n168 190  32  5.000  1  1   2    1     1    0   183  633      0 6.450470\n169 192  33 11.000  4  1   8    1     1    0   182  477      1 6.167516\n170 193  26 21.000  4  2   2    0     1    0   192  436      1 6.077642\n171 195  24 23.000  2  3   4    1     1    0   162  362      1 5.891644\n172 196  46 32.000  2  3   2    0     1    0   193  552      0 6.313548\n173 197  23 26.000  4  1   2    0     1    0   111  144      1 4.969813\n174 198  40 19.950  4  3   8    0     1    0   182  242      1 5.488938\n175 199  48 17.000  3  1   4    0     1    0   180  564      0 6.335054\n176 200  33 16.000  3  1   0    0     1    0    93  299      1 5.700444\n177 201  21 26.250  4  1   7    0     1    0   167  167      1 5.117994\n178 202  38 29.000  3  1   2    0     1    0   196  380      1 5.940171\n179 203  28 23.000  4  2   4    0     1    0   106  120      1 4.787492\n180 205  39  9.000  1  3   6    0     1    0   158  218      1 5.384495\n181 206  37 26.000  1  2   1    1     0    0    91  115      1 4.744932\n182 207  32 22.000  3  1   4    1     0    0    89  224      1 5.411646\n183 208  39 23.000  3  2   2    1     0    0    89  132      1 4.882802\n184 209  28  0.000  1  3  10    0     0    0    88  148      1 4.997212\n185 210  26 30.000  3  1   0    1     0    0    95  593      0 6.385194\n186 211  31 21.000  1  3   0    0     0    0     5   26      1 3.258097\n187 213  34 19.000  4  3   8    0     0    0    32   32      1 3.465736\n188 214  26 28.000  4  2   2    1     0    0    92  292      1 5.676754\n189 215  29  8.000  4  1   3    0     0    0    66   89      1 4.488636\n190 217  25 11.000  3  1   8    0     0    0    90  364      1 5.897154\n191 218  34 15.000  3  2   3    1     0    0    93  142      1 4.955827\n192 219  32  8.000  3  1   2    0     0    0    89  188      1 5.236442\n193 221  38 14.000  4  2   0    0     0    0    91   92      1 4.521789\n194 222  32  7.000  1  3   8    0     0    0    56   56      1 4.025352\n195 223  31 13.000  2  3   7    0     0    0    90  110      1 4.700480\n196 224  40 10.000  3  1   3    0     0    0    73  555      0 6.318968\n197 225  28 17.000  4  1   5    1     0    0    85  220      1 5.393628\n198 226  40 18.000  1  3   3    0     0    0    23   23      1 3.135494\n199 227  32  5.000  2  3   3    0     0    0    85  285      1 5.652489\n200 228  29 20.000  3  3   5    0     0    0    90   90      1 4.499810\n201 229  25 31.000  3  1   4    0     0    0    53   59      1 4.077537\n202 230  32 15.000  2  3   2    0     0    0    96  156      1 5.049856\n203 232  37  4.000  2  2   2    0     0    0    83  142      1 4.955827\n204 233  38 15.000  3  3   8    0     0    0    54   57      1 4.043051\n205 234  31 14.000  3  2   9    0     0    0    79  279      1 5.631212\n206 235  30 27.000  1  3   3    1     0    0    81  118      1 4.770685\n207 236  34 30.000  4  1   4    1     0    0    18  567      0 6.340359\n208 237  33 23.000  1  3   4    0     1    0   184  562      0 6.331502\n209 238  36 13.000  3  2  10    1     1    0    39  239      1 5.476464\n210 239  32 26.000  4  1   0    0     1    0   177  578      0 6.359574\n211 240  29 10.000  2  3   2    1     1    0   122  551      0 6.311735\n212 241  32  4.000  1  1   4    1     1    0   178  313      1 5.746203\n213 242  34  0.000  3  1   7    0     1    0   173  560      0 6.327937\n214 243  26 35.000  1  3  31    0     1    0    53   54      1 3.988984\n215 244  25 32.000  1  3   5    1     1    0    94  198      1 5.288267\n216 245  30  2.000  4  1   2    1     1    0   163  164      1 5.099866\n217 246  33 15.000  3  2   6    0     1    0   160  325      1 5.783825\n218 247  40 23.000  4  2   6    0     1    0    61   62      1 4.127134\n219 248  26 13.000  3  1  12    0     1    0    41   45      1 3.806662\n220 249  26 29.000  1  3   5    1     1    0    53   53      1 3.970292\n221 250  35 22.105  4  3   4    0     1    0    53  253      1 5.533389\n222 251  26 15.000  2  2  11    0     1    0    13   51      1 3.931826\n223 252  33  7.000  4  1   3    1     1    0   183  540      0 6.291569\n224 253  27  7.000  1  3   4    0     1    0   182  317      1 5.758902\n225 254  29 33.000  3  3   3    0     1    0   183  437      1 6.079933\n226 255  29 23.000  3  3   9    0     1    0    63  136      1 4.912655\n227 256  39 21.000  2  3   7    0     1    0   111  115      1 4.744932\n228 257  43 19.000  3  2   2    1     1    0   174  175      1 5.164786\n229 258  35  8.000  3  3   3    0     1    0   173  442      1 6.091310\n230 259  26 24.000  4  1   2    1     1    0   119  122      1 4.804021\n231 260  27 28.737  4  1   3    0     1    0   180  181      1 5.198497\n232 261  28 20.000  4  1   2    1     1    0    98  180      1 5.192957\n233 262  30 14.000  3  1   4    0     1    0    50   51      1 3.931826\n234 263  31 17.000  4  2   1    1     1    0   178  541      0 6.293419\n235 264  26 19.000  2  3  16    0     1    0   100  121      1 4.795791\n236 265  36  5.000  4  2   4    0     1    0    93  328      1 5.793014\n237 267  25  8.000  2  3   3    0     1    0   165  166      1 5.111988\n238 268  26 22.000  3  1   0    1     1    0    93  556      0 6.320768\n239 269  30 11.000  2  3   5    0     0    0    44  104      1 4.644391\n240 270  28 13.000  3  1   5    0     0    0    77  102      1 4.624973\n241 272  34 11.053  3  1   0    1     0    0    91  144      1 4.969813\n242 273  31 24.000  3  1   2    0     0    0    95  545      0 6.300786\n243 274  30 19.000  4  3   1    0     0    0    82  537      0 6.285998\n244 275  35 27.000  3  2   5    1     0    0    76  625      0 6.437752\n245 276  30  4.000  4  2   3    1     0    0     5    6      1 1.791759\n246 277  37 38.000  1  3   7    0     0    0    69  307      1 5.726848\n247 278  29 11.000  4  1  12    1     0    0    90  290      1 5.669881\n248 279  23 21.000  4  1   8    0     0    0    19   20      1 2.995732\n249 280  23  1.000  1  1   4    0     0    0    60   74      1 4.304065\n250 281  44  4.000  4  1   0    0     0    0    69  100      1 4.605170\n251 282  43  7.000  4  2   8    1     0    0    85  555      0 6.318968\n252 283  38 20.000  2  3   3    0     0    0    92  152      1 5.023881\n253 284  33 17.000  3  1   3    1     0    0    55  115      1 4.744932\n254 285  36  6.300  1  3   9    0     0    0    20   92      1 4.521789\n255 286  26 12.000  1  3   2    0     0    0    87  554      0 6.317165\n256 287  30 16.000  4  1   0    0     0    0    91   92      1 4.521789\n257 288  34 31.500  4  1   0    0     0    0     9   69      1 4.234107\n258 289  32 30.000  2  3   6    0     0    0    22   25      1 3.218876\n259 290  30  1.000  3  1   1    0     0    0    87  501      0 6.216606\n260 291  37 32.000  2  3  10    1     0    0    86   86      1 4.454347\n261 292  35 29.000  2  3   7    0     0    0    85   99      1 4.595120\n262 293  30  6.000  3  1   0    0     0    0    83   87      1 4.465908\n263 294  34 17.000  4  1   6    1     0    0    83  136      1 4.912655\n264 295  40 13.000  1  2   6    0     0    0    92  106      1 4.663439\n265 296  28 15.000  4  2   3    1     0    0    85  220      1 5.393628\n266 297  32 11.000  3  1   6    0     0    0    36   36      1 3.583519\n267 298  45 17.000  1  3   2    1     0    0    87  162      1 5.087596\n268 299  24 23.000  2  1   0    0     1    0    56  116      1 4.753590\n269 300  43 23.000  1  3   5    1     1    0    94  175      1 5.164786\n270 301  38 15.000  1  3   0    1     1    0    74  209      1 5.342334\n271 302  33 19.000  2  3   1    0     1    0   186  545      0 6.300786\n272 303  26 21.000  4  2   2    1     1    0   178  245      1 5.501258\n273 304  40  8.000  4  3   3    0     1    0    84  176      1 5.170484\n274 305  27 34.000  4  2   0    0     1    0    13   14      1 2.639057\n275 306  39 21.000  2  3  12    0     1    0    85  113      1 4.727388\n276 308  29 27.000  4  2   3    1     1    0     9  354      1 5.869297\n277 309  28 32.000  4  2   4    0     1    0   162  174      1 5.159055\n278 310  37 29.000  1  3  20    0     0    0    23   23      1 3.135494\n279 311  37 22.000  2  3  20    0     0    0    26   26      1 3.258097\n280 312  40 12.000  4  2   9    0     0    0    84   98      1 4.584967\n281 313  25 36.000  1  3   5    0     0    0    23   23      1 3.135494\n282 314  40 15.000  1  1   2    0     0    0    86  555      0 6.318968\n283 315  40  3.000  1  3   4    1     0    0    90  290      1 5.669881\n284 316  34 24.000  2  3   8    0     0    0    73  543      0 6.297109\n285 317  41 18.000  2  3   7    0     0    0    76  274      1 5.613128\n286 321  23  2.000  4  1   1    0     1    0    18  119      1 4.779123\n287 322  36 14.000  3  1   3    0     1    0    94  164      1 5.099866\n288 323  28 19.000  4  1   2    1     1    0    76  548      0 6.306275\n289 324  23  7.000  3  1   3    0     1    0    40  175      1 5.164786\n290 325  27  8.000  3  1   3    0     1    0   176  539      0 6.289716\n291 326  32 27.000  4  2   0    0     1    0   104  155      1 5.043425\n292 327  38 25.000  4  3  15    0     1    0     5   14      1 2.639057\n293 328  38 28.000  4  1   6    1     1    0   179  187      1 5.231109\n294 329  45 39.000  1  3   8    0     1    0    35   65      1 4.174387\n295 330  26 18.000  2  2   1    0     1    0    24  159      1 5.068904\n296 331  29  8.000  1  3  35    0     1    0    82   96      1 4.564348\n297 332  33 31.000  4  1   3    0     1    0    28  243      1 5.493061\n298 333  25  6.000  3  1   0    1     1    0    81   85      1 4.442651\n299 334  36 19.000  4  1   2    0     1    0     4    4      1 1.386294\n300 335  37 19.000  2  3   4    0     1    0    97  121      1 4.795791\n301 336  29 16.000  4  1   0    1     1    0    78  659      1 6.490724\n302 337  29 15.000  4  1   3    1     1    0   181  260      1 5.560682\n303 338  35 54.000  4  2   1    0     1    0    29  621      0 6.431331\n304 339  33 19.000  4  1   1    0     1    0   139  199      1 5.293305\n305 340  31 12.000  4  3   2    0     1    0   152  565      0 6.336826\n306 341  37 24.000  3  2   5    1     1    0    90  183      1 5.209486\n307 342  32 37.000  3  3   4    0     1    0    62  122      1 4.804021\n308 343  33  9.000  3  2  13    0     1    0   110  170      1 5.135798\n309 344  36 18.000  3  1  14    1     1    0    15   15      1 2.708050\n310 345  26  4.000  1  1   5    0     1    0    68  268      1 5.590987\n311 346  35 15.000  3  1   0    1     1    0    19   79      1 4.369448\n312 347  25 19.000  1  3   6    1     0    0    23   23      1 3.135494\n313 348  33 26.000  1  3  30    0     0    0    92  100      1 4.605170\n314 349  36 28.000  2  3   8    0     0    0    94   98      1 4.584967\n315 350  38 14.000  3  3   6    0     0    0    31   81      1 4.394449\n316 351  36 15.000  3  2   3    1     0    0    28  546      0 6.302619\n317 352  36 18.000  2  3  10    0     0    0    58   58      1 4.060443\n318 353  35 29.000  3  3   6    0     0    0   113  569      0 6.343880\n319 354  35 10.000  3  1   3    1     0    0    70  575      0 6.354370\n320 356  39 16.000  2  3   4    0     0    0    90   91      1 4.510860\n321 357  37  0.000  4  3   6    0     0    0    55   57      1 4.043051\n322 358  30 31.000  2  3   5    0     0    0    89  499      1 6.212606\n323 359  26 33.000  1  3   7    1     0    0    71  123      1 4.812184\n324 360  39 21.000  4  1   5    0     0    0    84  143      1 4.962845\n325 362  32 18.000  3  1   4    0     0    0    78  471      1 6.154858\n326 363  26 37.800  3  1   4    1     0    0    60   74      1 4.304065\n327 364  33 20.000  2  3   6    0     0    0    82   85      1 4.442651\n328 365  36 11.000  4  2   5    0     0    0    81   95      1 4.553877\n329 366  42 26.000  2  3   3    0     1    0    35   36      1 3.583519\n330 367  37 43.000  1  3  22    0     1    0    16   19      1 2.944439\n331 368  37 12.000  2  2   1    1     1    0     7   38      1 3.637586\n332 369  32 22.000  3  1   4    1     1    0    30  539      0 6.289716\n333 370  23 36.000  4  1   3    1     1    0   106  567      0 6.340359\n334 371  21 16.000  4  1  10    0     1    0   174  186      1 5.225747\n335 372  23 41.000  3  1   1    0     1    0   144  546      0 6.302619\n336 373  34 16.000  4  2   1    0     1    0    24   24      1 3.178054\n337 374  33  8.000  4  2   3    0     1    0    17  540      0 6.291569\n338 375  33 10.000  3  1   4    1     1    0    97  157      1 5.056246\n339 376  26 18.000  3  3   0    0     1    0    26   86      1 4.454347\n340 377  28 27.000  4  1   2    1     1    0    31  231      1 5.442418\n341 379  27 28.000  1  3   3    0     0    0    14   14      1 2.639057\n342 380  22 23.000  1  3   2    0     0    0    75   75      1 4.317488\n343 381  31 32.000  3  3   6    1     0    0    20  147      1 4.990433\n344 382  29 23.100  3  1   4    0     0    0   104  105      1 4.653960\n345 383  44 11.000  4  3  12    0     0    0    85  324      1 5.780744\n346 384  26  7.000  3  1   0    1     0    0   110  538      0 6.287859\n347 385  44 24.000  2  3  16    0     0    0   100  300      1 5.703782\n348 386  34 12.000  1  3   1    0     0    0    73   73      1 4.290459\n349 387  36 25.000  2  3   6    0     0    0    65   65      1 4.174387\n350 388  43  4.000  2  3  20    0     0    0    75  568      1 6.342121\n351 389  37  5.000  3  1   1    0     0    0    83   84      1 4.430817\n352 390  44 13.000  4  2  17    0     1    0    15   22      1 3.091042\n353 391  31 17.000  1  3  30    1     1    0    44   44      1 3.784190\n354 392  24 24.000  2  1   3    0     1    0     7    7      1 1.945910\n355 394  37 32.000  3  3   4    0     1    0    20   21      1 3.044522\n356 395  41 19.000  1  3  12    1     1    0   175  537      0 6.285998\n357 396  32  9.000  3  1   3    1     1    0    71  186      1 5.225747\n358 397  23  6.000  3  1   2    0     1    0    26   40      1 3.688879\n359 398  33 10.000  2  3   3    0     1    0   161  287      1 5.659482\n360 399  43 11.000  4  1   9    0     1    0    36  538      0 6.287859\n361 400  33 16.000  4  3   8    0     1    0    30   30      1 3.401197\n362 401  41 25.000  4  2   3    0     1    0   179  516      1 6.246107\n363 402  41 17.000  2  3   2    0     1    0   199  268      1 5.590987\n364 403  37 24.000  2  3   3    0     1    0   182  568      0 6.342121\n365 404  26 27.000  1  1   3    0     0    0   112  131      1 4.875197\n366 405  33 24.000  1  3   6    0     0    0     8  399      1 5.988961\n367 406  30 26.000  3  1   2    0     0    0    18   78      1 4.356709\n368 407  33 17.000  4  1   6    1     0    0    20   80      1 4.382027\n369 408  33 26.000  2  3   3    0     0    0    88  102      1 4.624973\n370 410  37 13.000  3  1   6    0     0    0    88  124      1 4.820282\n371 411  44 11.000  2  3  20    0     0    0    76   80      1 4.382027\n372 412  20  8.000  4  1   1    0     0    0    22   23      1 3.135494\n373 413  33 12.000  1  3   4    0     0    0   110  274      1 5.613128\n374 415  36 31.000  2  3   3    0     0    0    85  459      1 6.129050\n375 416  34  8.400  2  3   3    0     0    0    10   10      1 2.302585\n376 417  35 10.000  1  3  17    0     1    0   157  176      1 5.170484\n377 418  38 16.000  2  3  26    0     1    0   133  332      1 5.805135\n378 419  24 13.000  3  1   3    0     1    0    83  119      1 4.779123\n379 420  24 18.000  3  1   4    0     1    0   152  217      1 5.379897\n380 421  32 13.000  3  1   4    0     1    0   169  285      1 5.652489\n381 422  35 11.000  4  2   3    0     1    0    89  576      0 6.356108\n382 423  33 21.000  1  3   5    0     1    0    92  106      1 4.663439\n383 424  29 37.000  2  2   4    1     1    0    21   81      1 4.394449\n384 425  42 32.000  2  3  30    0     1    0    31   47      1 3.850148\n385 426  23 33.000  4  1   1    0     1    0    31   76      1 4.330733\n386 427  28 11.000  4  3  16    0     1    0   133  348      1 5.852202\n387 429  43 29.000  2  3   4    0     1    0   153  306      1 5.723585\n388 430  33 23.000  2  1   0    0     0    0    90  192      1 5.257495\n389 431  37 15.000  1  3  20    0     0    0   102  216      1 5.375278\n390 432  49 22.000  2  3   7    0     0    0    85  189      1 5.241747\n391 434  36 25.000  3  1   1    1     0    0    89  193      1 5.262690\n392 435  27 30.000  1  3  13    0     0    0    28   28      1 3.332205\n393 436  35 23.000  1  3   1    0     0    0    90  150      1 5.010635\n394 437  25 10.000  3  2   3    0     0    0    84   99      1 4.595120\n395 438  33  8.000  1  3   3    0     0    0    85  510      0 6.234411\n396 439  34 16.000  1  3   7    0     0    0    36  306      1 5.723585\n397 440  38  9.000  1  3  10    1     0    0    74  101      1 4.615121\n398 441  36 12.158  2  3   0    1     0    0    42  102      1 4.624973\n399 442  27  5.000  1  3   1    0     0    0    90  510      0 6.234411\n400 444  40 19.000  1  3   0    1     0    0   108  503      0 6.220590\n401 445  32 23.000  3  3   3    0     0    1    49   52      1 3.951244\n402 446  38 28.000  3  3   1    1     0    1   219  547      0 6.304449\n403 447  38 16.000  1  3   6    0     0    1   108  168      1 5.123964\n404 448  23 25.000  4  1   0    0     0    1   178  461      1 6.133398\n405 449  26 22.000  4  2   2    0     0    1    42  538      0 6.287859\n406 450  36 28.000  2  3   7    0     0    1   182  349      1 5.855072\n407 451  30 28.000  4  1   5    0     0    1     6   44      1 3.784190\n408 452  31 18.000  4  2   3    0     1    1   351  548      0 6.306275\n409 453  23 15.000  3  1   1    0     1    1    12   12      1 2.484907\n410 454  43  9.000  1  3   0    1     1    1     6    6      1 1.791759\n411 455  24 26.000  4  1   1    0     1    1    91  575      0 6.354370\n412 456  42 19.000  4  1   1    0     1    1   245  589      0 6.378426\n413 457  35 26.000  4  2   1    0     1    1   372  408      1 6.011267\n414 458  21 10.000  4  1   0    0     1    1   218  232      1 5.446737\n415 459  45  1.000  4  2   0    1     1    1    46  143      1 4.962845\n416 460  43 30.000  2  3   6    0     1    1   363  582      0 6.366470\n417 461  24  7.000  4  1   0    1     1    1   133  134      1 4.897840\n418 462  37 11.000  3  3   1    0     1    1     7    7      1 1.945910\n419 463  40 10.000  4  2   0    0     1    1   112  548      0 6.306275\n420 464  27 11.000  3  2   2    0     0    1    21   81      1 4.394449\n421 465  29 11.000  2  3   1    0     0    1   169  170      1 5.135798\n422 466  34 12.000  4  3   6    0     0    1    28   29      1 3.367296\n423 467  29 29.000  3  3  20    0     0    1    47   78      1 4.356709\n424 468  35 27.000  1  3   5    0     0    1    20   81      1 4.394449\n425 469  39 20.000  1  3   4    0     1    1   352  369      1 5.910797\n426 470  41  9.000  4  2   0    0     1    1    66   69      1 4.234107\n427 471  37 18.000  4  1   6    1     1    1    55  115      1 4.744932\n428 472  30 10.000  3  2   7    0     1    1   344  361      1 5.888878\n429 473  31  1.000  4  1   0    0     1    1   153  245      1 5.501258\n430 474  40  5.000  4  2   8    0     0    1   184  233      1 5.451038\n431 475  32 20.000  4  1   0    0     0    1   183  227      1 5.424950\n432 476  32  7.000  4  2   3    1     0    1    22   97      1 4.574711\n433 477  27  7.000  4  1   0    0     0    1   183  547      0 6.304449\n434 478  23 26.000  3  1   0    0     0    1   140  224      1 5.411646\n435 479  23  4.000  4  1   2    0     0    1    19  211      1 5.351858\n436 480  43 11.000  2  3  12    0     0    1   184  220      1 5.393628\n437 481  24 20.000  4  1   0    0     0    1    50   54      1 3.988984\n438 482  36 11.000  4  1   2    1     0    1   132  192      1 5.257495\n439 483  29 31.000  1  3   1    0     0    1   128  138      1 4.927254\n440 484  39 13.000  4  2   1    0     1    1   107  107      1 4.672829\n441 485  23  6.000  4  1   0    0     1    1   368  597      0 6.391917\n442 486  27 17.000  3  3   4    0     1    1   219  226      1 5.420535\n443 487  26  5.000  4  2   5    0     1    1   374  434      1 6.073045\n444 488  26 27.000  3  1   1    1     1    1    92  106      1 4.663439\n445 489  25  9.000  4  1   0    0     1    1    45  180      1 5.192957\n446 490  34 10.000  3  1   0    0     1    1   366  557      0 6.322565\n447 491  45  5.000  4  3   2    0     1    1   368  556      0 6.320768\n448 492  23 17.000  4  1   1    0     0    1    78  619      0 6.428105\n449 493  26  7.000  4  1   0    0     0    1   184  546      0 6.302619\n450 495  24 27.000  1  2   2    0     0    1   187  233      1 5.451038\n451 496  30 23.000  2  3   2    1     0    1   101  102      1 4.624973\n452 497  22 26.000  3  1   0    0     0    1   141  548      0 6.306275\n453 498  25 10.000  3  1   1    0     0    1    24   99      1 4.595120\n454 499  30  8.400  3  2  40    0     0    1    36   36      1 3.583519\n455 501  33 23.000  4  1   0    1     1    1    56   78      1 4.356709\n456 502  34 15.000  3  2   8    0     1    1   367  502      1 6.218600\n457 503  29 24.000  3  1   2    0     1    1    70   71      1 4.262680\n458 504  39 33.000  4  2   6    0     1    1    58   59      1 4.077537\n459 506  26 21.000  3  1   4    0     1    1   366  533      0 6.278521\n460 507  32 23.000  2  3   6    0     1    1    10   10      1 2.302585\n461 508  42 23.100  1  3   2    0     0    1   214  274      1 5.613128\n462 509  39 25.000  1  2   8    0     0    1   197  255      1 5.541264\n463 510  36  2.000  4  1   0    1     0    1    89  503      0 6.220590\n464 511  22 20.000  3  1   1    0     0    1    56  256      1 5.545177\n465 512  27 23.000  4  1   1    0     0    1     9    9      1 2.197225\n466 514  28  9.000  4  1   0    0     0    1   186  386      1 5.955837\n467 515  36 28.000  3  2   1    0     1    1   303  547      0 6.304449\n468 516  31 13.000  3  1   3    0     1    1    32   45      1 3.806662\n469 517  27 22.000  3  2   4    0     1    1     8   58      1 4.060443\n470 518  23 17.000  3  1   1    0     1    1    63  124      1 4.820282\n471 519  24 20.000  3  2  20    0     0    1   108  540      0 6.291569\n472 520  38  5.000  3  2   1    0     0    1   183  243      1 5.493061\n473 521  25  8.000  4  1   1    0     1    1   151  549      0 6.308098\n474 522  26 20.000  3  1   0    0     0    1     7   12      1 2.484907\n475 523  22 34.000  3  1   2    0     0    1    38   51      1 3.931826\n476 524  33 13.000  4  1   2    0     1    1   176  562      0 6.331502\n477 525  30 23.000  1  3   7    0     1    1    93   94      1 4.543295\n478 526  45  8.000  4  3   3    0     0    1   200  204      1 5.318120\n479 527  24 15.000  3  2   0    0     0    1   178  238      1 5.472271\n480 528  27 22.000  4  1   0    0     1    1    78  140      1 4.941642\n481 529  36 19.000  4  2  10    0     1    1   119  120      1 4.787492\n482 530  38 23.000  4  2   2    1     0    1   154  154      1 5.036953\n483 531  31 17.000  2  3   2    0     1    1   163  177      1 5.176150\n484 532  40 22.000  4  2   7    0     1    1   118  119      1 4.779123\n485 533  22 12.000  3  1   0    1     1    1    76   83      1 4.418841\n486 534  31 13.000  4  1   0    1     1    1   116  130      1 4.867534\n487 536  39  7.000  3  3   3    1     0    1    88  159      1 5.068904\n488 538  33 14.000  3  1   1    0     0    1    33   33      1 3.496508\n489 539  27 10.000  3  3   2    0     1    1    70   72      1 4.276666\n490 540  37  7.000  4  1   2    1     1    1    68  161      1 5.081404\n491 541  35 16.000  4  2  25    0     0    1   191  191      1 5.252273\n492 542  25 11.000  3  1   5    0     0    1    35  181      1 5.198497\n493 543  27 11.000  3  1   1    1     1    1    32  546      0 6.302619\n494 544  34 15.000  4  1   0    0     0    1    28  540      0 6.291569\n495 545  30 15.000  3  1   3    0     0    1    15   76      1 4.330733\n496 546  35 17.000  1  3   7    0     0    1     7    7      1 1.945910\n497 547  34 23.000  4  1   0    0     0    1    43   44      1 3.784190\n498 548  25 23.000  3  2   5    0     0    1    89  103      1 4.634729\n499 549  34 18.000  3  1   1    0     0    1    38   79      1 4.369448\n500 550  24 23.000  4  3   3    0     0    1   204  339      1 5.826000\n501 551  24 20.000  4  1   2    0     0    1    76   90      1 4.499810\n502 552  40 36.000  4  1   3    0     0    1   195  542      0 6.295266\n503 553  33  9.000  3  1   1    1     0    1   184  384      1 5.950643\n504 554  38 14.000  4  2   1    1     1    1   254  255      1 5.541264\n505 555  32  1.000  3  1   0    0     1    1   371  431      1 6.066108\n506 556  33  3.000  4  1   1    0     0    1   196  587      0 6.375025\n507 557  28 40.000  3  1   2    1     0    1   198  198      1 5.288267\n508 558  31 13.000  3  3   2    0     0    1   170  551      0 6.311735\n509 559  31 39.000  2  3   4    0     1    1    50  110      1 4.700480\n510 560  33 24.000  4  1   0    0     1    1   163  541      0 6.293419\n511 561  24 26.000  3  1  11    0     0    1   182  242      1 5.488938\n512 562  26 18.000  3  1   3    0     0    1   150  537      0 6.285998\n513 563  31 19.000  2  3   7    0     1    1    34   56      1 4.025352\n514 564  40 14.700  2  3   4    0     1    1    34   34      1 3.526361\n515 566  34  2.000  3  1   3    0     1    1   366  549      0 6.308098\n516 567  30 11.000  3  2   7    0     0    1   133  133      1 4.890349\n517 568  36  0.000  3  2   3    0     0    1    69  226      1 5.420535\n518 569  38 17.000  2  3   6    0     1    1   366  401      1 5.993961\n519 570  31 20.000  1  3   6    1     1    1    14   14      1 2.639057\n520 571  27 22.000  2  2   2    0     0    1   184  548      0 6.306275\n521 572  32 21.000  1  3  15    0     1    1    89  224      1 5.411646\n522 573  35 23.000  3  1   5    1     0    1   183  540      0 6.291569\n523 574  44 29.000  2  3  13    0     0    1   177  237      1 5.468060\n524 575  31  5.000  2  3  10    0     1    1   154  354      1 5.869297\n525 576  28 23.000  3  2  20    0     0    1   123  123      1 4.812184\n526 577  40  8.000  4  2   1    0     0    1   146  170      1 5.135798\n527 578  25 12.000  3  1  10    1     1    1   203  203      1 5.313206\n528 579  32 10.000  1  3   6    0     1    1   360  360      1 5.886104\n529 580  29 15.750  4  1   2    0     0    1    79  139      1 4.934474\n530 581  40  2.000  2  2   5    0     1    1   201  215      1 5.370638\n531 582  27  9.000  4  2   0    0     1    1   129  129      1 4.859812\n532 583  26  2.000  3  1   1    0     1    1   365  396      1 5.981414\n533 584  34 15.000  3  1   4    1     1    1   159  547      0 6.304449\n534 585  49  4.000  4  2   2    0     0    1   177  547      0 6.304449\n535 586  21 25.000  1  3   1    0     1    1    71   71      1 4.262680\n536 587  39 23.000  3  3   2    0     1    1   108  168      1 5.123964\n537 588  33 15.000  4  2   4    0     1    1   198  228      1 5.429346\n538 589  32  3.000  3  1   1    0     1    1   372  551      0 6.311735\n539 590  35  9.000  4  2   6    0     0    1    25  654      0 6.483107\n540 591  31 20.000  4  1   0    1     1    1    48   51      1 3.931826\n541 592  28  5.000  4  1   3    0     0    1   191  548      0 6.306275\n542 593  27 29.000  3  2   5    0     1    1   171  231      1 5.442418\n543 594  29 21.000  2  1   1    1     1    1   145  280      1 5.634790\n544 595  30  1.000  2  1  20    0     0    1   183  184      1 5.214936\n545 596  27 18.000  4  1   3    1     0    1    72   86      1 4.454347\n546 598  40 15.000  4  2   1    0     1    1    44   46      1 3.828641\n547 599  37 20.000  3  1   2    1     1    1   140  200      1 5.298317\n548 600  33 10.000  4  1   0    0     0    1   184  244      1 5.497168\n549 601  28 20.000  4  1   2    0     0    1    94  182      1 5.204007\n550 602  40 15.000  4  2   8    0     1    1   296  296      1 5.690359\n551 603  48 20.000  4  1   0    1     0    1    23   24      1 3.178054\n552 604  38 25.000  3  1   1    0     0    1   128  142      1 4.955827\n553 605  35 13.000  4  1   0    0     0    1   106  120      1 4.787492\n554 606  37 13.000  4  2   0    0     0    1    46   47      1 3.850148\n555 607  25 15.000  3  1   0    1     1    1   150  519      1 6.251904\n556 608  26  8.000  4  1   2    0     1    1    48  248      1 5.513429\n557 609  30  9.000  3  3   3    0     0    1    29   31      1 3.433987\n558 610  28 16.000  4  2   2    0     0    1   179  567      0 6.340359\n559 611  23 11.000  2  3   4    0     0    1   170  353      1 5.866468\n560 612  36 31.000  4  1   1    0     1    1   365  458      1 6.126869\n561 613  36 13.000  4  2   4    0     1    1   400  554      0 6.317165\n562 614  24  5.000  4  1   0    1     0    1    56  116      1 4.753590\n563 615  33  9.000  3  2   5    0     0    1    24   74      1 4.304065\n564 616  38 15.000  4  2   6    0     0    1    10   10      1 2.302585\n565 617  41 20.000  3  3  21    0     1    1   354  355      1 5.872118\n566 618  31 21.000  3  1   0    1     1    1   232  232      1 5.446737\n567 619  31 23.000  4  2  11    0     1    1    54   68      1 4.219508\n568 620  37  5.000  4  1   0    1     1    1    48   48      1 3.871201\n569 621  37 17.000  4  2   4    1     0    1    57   60      1 4.094345\n570 622  33 13.000  4  1   0    0     0    1    46   50      1 3.912023\n571 624  53  9.000  4  2   6    0     0    1    39  126      1 4.836282\n572 625  37 20.000  2  3   4    0     0    1    17   18      1 2.890372\n573 626  28 10.000  4  2   3    0     1    1    21   35      1 3.555348\n574 627  35 17.000  1  3   2    0     0    1   184  379      1 5.937536\n575 628  46 31.500  1  3  15    1     1    1     9  377      1 5.932245\n           ND1          ND2      LNDT       FRAC IV3   IV_fct\n1    5.0000000  -8.04718956 0.6931472 0.68333333   1   Recent\n2    1.1111111  -0.11706724 2.1972246 0.13888889   0 Previous\n3    2.5000000  -2.29072683 1.3862944 0.03888889   1   Recent\n4    5.0000000  -8.04718956 0.6931472 0.73333333   1   Recent\n5    1.6666667  -0.85137604 1.7917595 0.96111111   0    Never\n6    5.0000000  -8.04718956 0.6931472 0.08888889   1   Recent\n7    0.2857143   0.35793228 3.5553481 0.99444444   1   Recent\n8    3.3333333  -4.01324268 1.0986123 0.11666667   1   Recent\n9    2.5000000  -2.29072683 1.3862944 0.97777778   1   Recent\n10   1.2500000  -0.27892944 2.0794415 0.68888889   1   Recent\n11   1.1111111  -0.11706724 2.1972246 0.97777778   1   Recent\n12   5.0000000  -8.04718956 0.6931472 0.43888889   0    Never\n13   3.3333333  -4.01324268 1.0986123 1.01111111   1   Recent\n14   1.1111111  -0.11706724 2.1972246 0.96666667   1   Recent\n15   5.0000000  -8.04718956 0.6931472 1.00555556   1   Recent\n16   2.5000000  -2.29072683 1.3862944 0.33888889   1   Recent\n17   1.4285714  -0.50953563 1.9459101 0.98333333   1   Recent\n18   5.0000000  -8.04718956 0.6931472 0.10555556   0 Previous\n19   0.6250000   0.29375227 2.7725887 0.15000000   0    Never\n20   1.6666667  -0.85137604 1.7917595 0.97222222   1   Recent\n21   5.0000000  -8.04718956 0.6931472 0.13333333   0    Never\n22   1.1111111  -0.11706724 2.1972246 0.23333333   1   Recent\n23  10.0000000 -23.02585093 0.0000000 0.53333333   0 Previous\n24   1.0000000   0.00000000 2.3025851 1.00000000   1   Recent\n25   1.4285714  -0.50953563 1.9459101 1.01111111   1   Recent\n26   1.6666667  -0.85137604 1.7917595 0.96666667   1   Recent\n27   2.5000000  -2.29072683 1.3862944 0.97777778   0    Never\n28   1.2500000  -0.27892944 2.0794415 0.10000000   1   Recent\n29   1.0000000   0.00000000 2.3025851 1.04444444   1   Recent\n30   0.9090909   0.08664562 2.3978953 1.01111111   0 Previous\n31   5.0000000  -8.04718956 0.6931472 1.00000000   1   Recent\n32   5.0000000  -8.04718956 0.6931472 0.98888889   1   Recent\n33   1.6666667  -0.85137604 1.7917595 0.98888889   1   Recent\n34   1.4285714  -0.50953563 1.9459101 1.11111111   0    Never\n35   2.5000000  -2.29072683 1.3862944 0.74444444   0    Never\n36   1.2500000  -0.27892944 2.0794415 0.13888889   1   Recent\n37   5.0000000  -8.04718956 0.6931472 0.13333333   0    Never\n38   5.0000000  -8.04718956 0.6931472 0.87777778   0    Never\n39   3.3333333  -4.01324268 1.0986123 0.87777778   0    Never\n40  10.0000000 -23.02585093 0.0000000 0.43333333   1   Recent\n41   3.3333333  -4.01324268 1.0986123 0.46666667   0 Previous\n42   1.4285714  -0.50953563 1.9459101 0.50555556   1   Recent\n43   5.0000000  -8.04718956 0.6931472 0.90000000   1   Recent\n44  10.0000000 -23.02585093 0.0000000 0.25000000   1   Recent\n45   5.0000000  -8.04718956 0.6931472 0.33888889   1   Recent\n46  10.0000000 -23.02585093 0.0000000 0.10555556   1   Recent\n47   3.3333333  -4.01324268 1.0986123 0.20555556   0    Never\n48   1.1111111  -0.11706724 2.1972246 0.28333333   1   Recent\n49   5.0000000  -8.04718956 0.6931472 0.33333333   1   Recent\n50   0.3846154   0.36750440 3.2580965 0.98333333   1   Recent\n51  10.0000000 -23.02585093 0.0000000 0.23888889   0    Never\n52   2.5000000  -2.29072683 1.3862944 0.11666667   0    Never\n53   3.3333333  -4.01324268 1.0986123 0.97777778   1   Recent\n54   1.4285714  -0.50953563 1.9459101 1.06666667   1   Recent\n55   0.9090909   0.08664562 2.3978953 1.23333333   1   Recent\n56   0.9090909   0.08664562 2.3978953 0.42222222   1   Recent\n57   1.2500000  -0.27892944 2.0794415 0.16666667   0    Never\n58   1.6666667  -0.85137604 1.7917595 0.55555556   0 Previous\n59   1.6666667  -0.85137604 1.7917595 0.67777778   1   Recent\n60   5.0000000  -8.04718956 0.6931472 0.34444444   0    Never\n61   3.3333333  -4.01324268 1.0986123 0.12222222   0    Never\n62   1.4285714  -0.50953563 1.9459101 1.00000000   1   Recent\n63   1.6666667  -0.85137604 1.7917595 0.12222222   1   Recent\n64   1.6666667  -0.85137604 1.7917595 0.51111111   1   Recent\n65  10.0000000 -23.02585093 0.0000000 0.42222222   1   Recent\n66  10.0000000 -23.02585093 0.0000000 1.00000000   1   Recent\n67   1.6666667  -0.85137604 1.7917595 0.97777778   1   Recent\n68   0.7692308   0.20181866 2.5649494 1.01111111   1   Recent\n69   1.4285714  -0.50953563 1.9459101 0.94444444   0 Previous\n70  10.0000000 -23.02585093 0.0000000 1.00000000   0    Never\n71   1.6666667  -0.85137604 1.7917595 0.57777778   1   Recent\n72   2.5000000  -2.29072683 1.3862944 0.97777778   1   Recent\n73   0.7142857   0.24033731 2.6390573 0.47777778   1   Recent\n74   1.1111111  -0.11706724 2.1972246 0.41111111   1   Recent\n75  10.0000000 -23.02585093 0.0000000 0.96666667   0    Never\n76   3.3333333  -4.01324268 1.0986123 0.22222222   0 Previous\n77   5.0000000  -8.04718956 0.6931472 0.10000000   0    Never\n78   5.0000000  -8.04718956 0.6931472 0.94444444   1   Recent\n79   3.3333333  -4.01324268 1.0986123 0.20000000   1   Recent\n80   0.7692308   0.20181866 2.5649494 0.78888889   1   Recent\n81   3.3333333  -4.01324268 1.0986123 0.97777778   1   Recent\n82   5.0000000  -8.04718956 0.6931472 0.74444444   0 Previous\n83   2.0000000  -1.38629436 1.6094379 0.33333333   1   Recent\n84   5.0000000  -8.04718956 0.6931472 0.37777778   0    Never\n85   1.0000000   0.00000000 2.3025851 1.01111111   0 Previous\n86   5.0000000  -8.04718956 0.6931472 1.01111111   1   Recent\n87   5.0000000  -8.04718956 0.6931472 0.81111111   0    Never\n88   5.0000000  -8.04718956 0.6931472 0.22222222   1   Recent\n89   5.0000000  -8.04718956 0.6931472 0.98333333   0    Never\n90   3.3333333  -4.01324268 1.0986123 1.00555556   0 Previous\n91   0.6250000   0.29375227 2.7725887 0.93333333   1   Recent\n92   1.6666667  -0.85137604 1.7917595 0.50000000   1   Recent\n93   1.6666667  -0.85137604 1.7917595 0.33888889   1   Recent\n94   2.5000000  -2.29072683 1.3862944 0.35000000   0    Never\n95   1.1111111  -0.11706724 2.1972246 0.67222222   0 Previous\n96   5.0000000  -8.04718956 0.6931472 0.98888889   0    Never\n97   2.5000000  -2.29072683 1.3862944 0.28333333   0    Never\n98   5.0000000  -8.04718956 0.6931472 0.97777778   0    Never\n99   5.0000000  -8.04718956 0.6931472 0.27777778   1   Recent\n100  2.5000000  -2.29072683 1.3862944 0.92222222   0    Never\n101  3.3333333  -4.01324268 1.0986123 0.98888889   0    Never\n102  5.0000000  -8.04718956 0.6931472 0.26666667   1   Recent\n103  0.4761905   0.35330350 3.0445224 0.07777778   1   Recent\n104  5.0000000  -8.04718956 0.6931472 0.94444444   0    Never\n105 10.0000000 -23.02585093 0.0000000 0.98888889   0    Never\n106  2.0000000  -1.38629436 1.6094379 1.01111111   1   Recent\n107 10.0000000 -23.02585093 0.0000000 0.98888889   0    Never\n108 10.0000000 -23.02585093 0.0000000 0.91111111   0    Never\n109  2.5000000  -2.29072683 1.3862944 0.93333333   1   Recent\n110  5.0000000  -8.04718956 0.6931472 0.33333333   1   Recent\n111  1.4285714  -0.50953563 1.9459101 0.07777778   0    Never\n112  2.5000000  -2.29072683 1.3862944 0.93333333   1   Recent\n113 10.0000000 -23.02585093 0.0000000 0.77777778   1   Recent\n114  3.3333333  -4.01324268 1.0986123 0.84444444   0 Previous\n115  5.0000000  -8.04718956 0.6931472 0.98888889   0    Never\n116  5.0000000  -8.04718956 0.6931472 0.98888889   1   Recent\n117  1.4285714  -0.50953563 1.9459101 0.48333333   1   Recent\n118 10.0000000 -23.02585093 0.0000000 0.97222222   0 Previous\n119  2.5000000  -2.29072683 1.3862944 0.48333333   0    Never\n120  0.5882353   0.31213427 2.8332133 0.61111111   0    Never\n121  5.0000000  -8.04718956 0.6931472 0.11666667   1   Recent\n122  2.5000000  -2.29072683 1.3862944 0.77222222   0    Never\n123  0.6250000   0.29375227 2.7725887 1.00555556   1   Recent\n124  3.3333333  -4.01324268 1.0986123 0.18333333   1   Recent\n125  5.0000000  -8.04718956 0.6931472 0.21666667   1   Recent\n126  1.2500000  -0.27892944 2.0794415 0.02222222   1   Recent\n127  2.0000000  -1.38629436 1.6094379 1.02222222   1   Recent\n128  5.0000000  -8.04718956 0.6931472 0.68333333   0    Never\n129  5.0000000  -8.04718956 0.6931472 0.97777778   0    Never\n130  3.3333333  -4.01324268 1.0986123 0.96666667   1   Recent\n131  5.0000000  -8.04718956 0.6931472 1.00555556   1   Recent\n132  3.3333333  -4.01324268 1.0986123 0.62777778   1   Recent\n133  2.5000000  -2.29072683 1.3862944 0.91111111   1   Recent\n134  5.0000000  -8.04718956 0.6931472 0.93333333   1   Recent\n135  1.1111111  -0.11706724 2.1972246 0.88888889   1   Recent\n136  2.0000000  -1.38629436 1.6094379 1.01111111   1   Recent\n137  2.5000000  -2.29072683 1.3862944 1.07777778   0    Never\n138  2.5000000  -2.29072683 1.3862944 0.56666667   0    Never\n139  2.5000000  -2.29072683 1.3862944 1.01111111   1   Recent\n140  2.0000000  -1.38629436 1.6094379 1.00000000   0    Never\n141  1.4285714  -0.50953563 1.9459101 0.51111111   1   Recent\n142  5.0000000  -8.04718956 0.6931472 0.84444444   0    Never\n143  3.3333333  -4.01324268 1.0986123 0.83333333   1   Recent\n144  2.0000000  -1.38629436 1.6094379 1.01111111   1   Recent\n145  1.6666667  -0.85137604 1.7917595 1.00000000   1   Recent\n146  1.4285714  -0.50953563 1.9459101 0.03333333   1   Recent\n147  2.5000000  -2.29072683 1.3862944 0.04444444   1   Recent\n148  5.0000000  -8.04718956 0.6931472 0.18333333   0    Never\n149  1.1111111  -0.11706724 2.1972246 0.17222222   1   Recent\n150  5.0000000  -8.04718956 0.6931472 0.96666667   0    Never\n151  3.3333333  -4.01324268 1.0986123 0.18888889   0    Never\n152  2.5000000  -2.29072683 1.3862944 0.33333333   0 Previous\n153  1.6666667  -0.85137604 1.7917595 0.43333333   0 Previous\n154  3.3333333  -4.01324268 1.0986123 1.01111111   0    Never\n155  5.0000000  -8.04718956 0.6931472 1.01111111   0    Never\n156 10.0000000 -23.02585093 0.0000000 0.43333333   0    Never\n157  5.0000000  -8.04718956 0.6931472 0.30555556   0    Never\n158  3.3333333  -4.01324268 1.0986123 1.23888889   0    Never\n159  0.4761905   0.35330350 3.0445224 0.13888889   0    Never\n160 10.0000000 -23.02585093 0.0000000 0.35000000   0    Never\n161  5.0000000  -8.04718956 0.6931472 0.73888889   0    Never\n162  1.6666667  -0.85137604 1.7917595 0.85555556   1   Recent\n163  3.3333333  -4.01324268 1.0986123 0.38888889   0    Never\n164  5.0000000  -8.04718956 0.6931472 0.36666667   0 Previous\n165  2.0000000  -1.38629436 1.6094379 0.22222222   1   Recent\n166  5.0000000  -8.04718956 0.6931472 0.41666667   0    Never\n167  3.3333333  -4.01324268 1.0986123 1.03888889   0 Previous\n168  3.3333333  -4.01324268 1.0986123 1.01666667   0    Never\n169  1.1111111  -0.11706724 2.1972246 1.01111111   0    Never\n170  3.3333333  -4.01324268 1.0986123 1.06666667   0 Previous\n171  2.0000000  -1.38629436 1.6094379 0.90000000   1   Recent\n172  3.3333333  -4.01324268 1.0986123 1.07222222   1   Recent\n173  3.3333333  -4.01324268 1.0986123 0.61666667   0    Never\n174  1.1111111  -0.11706724 2.1972246 1.01111111   1   Recent\n175  2.0000000  -1.38629436 1.6094379 1.00000000   0    Never\n176 10.0000000 -23.02585093 0.0000000 0.51666667   0    Never\n177  1.2500000  -0.27892944 2.0794415 0.92777778   0    Never\n178  3.3333333  -4.01324268 1.0986123 1.08888889   0    Never\n179  2.0000000  -1.38629436 1.6094379 0.58888889   0 Previous\n180  1.4285714  -0.50953563 1.9459101 0.87777778   1   Recent\n181  5.0000000  -8.04718956 0.6931472 1.01111111   0 Previous\n182  2.0000000  -1.38629436 1.6094379 0.98888889   0    Never\n183  3.3333333  -4.01324268 1.0986123 0.98888889   0 Previous\n184  0.9090909   0.08664562 2.3978953 0.97777778   1   Recent\n185 10.0000000 -23.02585093 0.0000000 1.05555556   0    Never\n186 10.0000000 -23.02585093 0.0000000 0.05555556   1   Recent\n187  1.1111111  -0.11706724 2.1972246 0.35555556   1   Recent\n188  3.3333333  -4.01324268 1.0986123 1.02222222   0 Previous\n189  2.5000000  -2.29072683 1.3862944 0.73333333   0    Never\n190  1.1111111  -0.11706724 2.1972246 1.00000000   0    Never\n191  2.5000000  -2.29072683 1.3862944 1.03333333   0 Previous\n192  3.3333333  -4.01324268 1.0986123 0.98888889   0    Never\n193 10.0000000 -23.02585093 0.0000000 1.01111111   0 Previous\n194  1.1111111  -0.11706724 2.1972246 0.62222222   1   Recent\n195  1.2500000  -0.27892944 2.0794415 1.00000000   1   Recent\n196  2.5000000  -2.29072683 1.3862944 0.81111111   0    Never\n197  1.6666667  -0.85137604 1.7917595 0.94444444   0    Never\n198  2.5000000  -2.29072683 1.3862944 0.25555556   1   Recent\n199  2.5000000  -2.29072683 1.3862944 0.94444444   1   Recent\n200  1.6666667  -0.85137604 1.7917595 1.00000000   1   Recent\n201  2.0000000  -1.38629436 1.6094379 0.58888889   0    Never\n202  3.3333333  -4.01324268 1.0986123 1.06666667   1   Recent\n203  3.3333333  -4.01324268 1.0986123 0.92222222   0 Previous\n204  1.1111111  -0.11706724 2.1972246 0.60000000   1   Recent\n205  1.0000000   0.00000000 2.3025851 0.87777778   0 Previous\n206  2.5000000  -2.29072683 1.3862944 0.90000000   1   Recent\n207  2.0000000  -1.38629436 1.6094379 0.20000000   0    Never\n208  2.0000000  -1.38629436 1.6094379 1.02222222   1   Recent\n209  0.9090909   0.08664562 2.3978953 0.21666667   0 Previous\n210 10.0000000 -23.02585093 0.0000000 0.98333333   0    Never\n211  3.3333333  -4.01324268 1.0986123 0.67777778   1   Recent\n212  2.0000000  -1.38629436 1.6094379 0.98888889   0    Never\n213  1.2500000  -0.27892944 2.0794415 0.96111111   0    Never\n214  0.3125000   0.36348463 3.4657359 0.29444444   1   Recent\n215  1.6666667  -0.85137604 1.7917595 0.52222222   1   Recent\n216  3.3333333  -4.01324268 1.0986123 0.90555556   0    Never\n217  1.4285714  -0.50953563 1.9459101 0.88888889   0 Previous\n218  1.4285714  -0.50953563 1.9459101 0.33888889   0 Previous\n219  0.7692308   0.20181866 2.5649494 0.22777778   0    Never\n220  1.6666667  -0.85137604 1.7917595 0.29444444   1   Recent\n221  2.0000000  -1.38629436 1.6094379 0.29444444   1   Recent\n222  0.8333333   0.15193463 2.4849066 0.07222222   0 Previous\n223  2.5000000  -2.29072683 1.3862944 1.01666667   0    Never\n224  2.0000000  -1.38629436 1.6094379 1.01111111   1   Recent\n225  2.5000000  -2.29072683 1.3862944 1.01666667   1   Recent\n226  1.0000000   0.00000000 2.3025851 0.35000000   1   Recent\n227  1.2500000  -0.27892944 2.0794415 0.61666667   1   Recent\n228  3.3333333  -4.01324268 1.0986123 0.96666667   0 Previous\n229  2.5000000  -2.29072683 1.3862944 0.96111111   1   Recent\n230  3.3333333  -4.01324268 1.0986123 0.66111111   0    Never\n231  2.5000000  -2.29072683 1.3862944 1.00000000   0    Never\n232  3.3333333  -4.01324268 1.0986123 0.54444444   0    Never\n233  2.0000000  -1.38629436 1.6094379 0.27777778   0    Never\n234  5.0000000  -8.04718956 0.6931472 0.98888889   0 Previous\n235  0.5882353   0.31213427 2.8332133 0.55555556   1   Recent\n236  2.0000000  -1.38629436 1.6094379 0.51666667   0 Previous\n237  2.5000000  -2.29072683 1.3862944 0.91666667   1   Recent\n238 10.0000000 -23.02585093 0.0000000 0.51666667   0    Never\n239  1.6666667  -0.85137604 1.7917595 0.48888889   1   Recent\n240  1.6666667  -0.85137604 1.7917595 0.85555556   0    Never\n241 10.0000000 -23.02585093 0.0000000 1.01111111   0    Never\n242  3.3333333  -4.01324268 1.0986123 1.05555556   0    Never\n243  5.0000000  -8.04718956 0.6931472 0.91111111   1   Recent\n244  1.6666667  -0.85137604 1.7917595 0.84444444   0 Previous\n245  2.5000000  -2.29072683 1.3862944 0.05555556   0 Previous\n246  1.2500000  -0.27892944 2.0794415 0.76666667   1   Recent\n247  0.7692308   0.20181866 2.5649494 1.00000000   0    Never\n248  1.1111111  -0.11706724 2.1972246 0.21111111   0    Never\n249  2.0000000  -1.38629436 1.6094379 0.66666667   0    Never\n250 10.0000000 -23.02585093 0.0000000 0.76666667   0    Never\n251  1.1111111  -0.11706724 2.1972246 0.94444444   0 Previous\n252  2.5000000  -2.29072683 1.3862944 1.02222222   1   Recent\n253  2.5000000  -2.29072683 1.3862944 0.61111111   0    Never\n254  1.0000000   0.00000000 2.3025851 0.22222222   1   Recent\n255  3.3333333  -4.01324268 1.0986123 0.96666667   1   Recent\n256 10.0000000 -23.02585093 0.0000000 1.01111111   0    Never\n257 10.0000000 -23.02585093 0.0000000 0.10000000   0    Never\n258  1.4285714  -0.50953563 1.9459101 0.24444444   1   Recent\n259  5.0000000  -8.04718956 0.6931472 0.96666667   0    Never\n260  0.9090909   0.08664562 2.3978953 0.95555556   1   Recent\n261  1.2500000  -0.27892944 2.0794415 0.94444444   1   Recent\n262 10.0000000 -23.02585093 0.0000000 0.92222222   0    Never\n263  1.4285714  -0.50953563 1.9459101 0.92222222   0    Never\n264  1.4285714  -0.50953563 1.9459101 1.02222222   0 Previous\n265  2.5000000  -2.29072683 1.3862944 0.94444444   0 Previous\n266  1.4285714  -0.50953563 1.9459101 0.40000000   0    Never\n267  3.3333333  -4.01324268 1.0986123 0.96666667   1   Recent\n268 10.0000000 -23.02585093 0.0000000 0.31111111   0    Never\n269  1.6666667  -0.85137604 1.7917595 0.52222222   1   Recent\n270 10.0000000 -23.02585093 0.0000000 0.41111111   1   Recent\n271  5.0000000  -8.04718956 0.6931472 1.03333333   1   Recent\n272  3.3333333  -4.01324268 1.0986123 0.98888889   0 Previous\n273  2.5000000  -2.29072683 1.3862944 0.46666667   1   Recent\n274 10.0000000 -23.02585093 0.0000000 0.07222222   0 Previous\n275  0.7692308   0.20181866 2.5649494 0.47222222   1   Recent\n276  2.5000000  -2.29072683 1.3862944 0.05000000   0 Previous\n277  2.0000000  -1.38629436 1.6094379 0.90000000   0 Previous\n278  0.4761905   0.35330350 3.0445224 0.25555556   1   Recent\n279  0.4761905   0.35330350 3.0445224 0.28888889   1   Recent\n280  1.0000000   0.00000000 2.3025851 0.93333333   0 Previous\n281  1.6666667  -0.85137604 1.7917595 0.25555556   1   Recent\n282  3.3333333  -4.01324268 1.0986123 0.95555556   0    Never\n283  2.0000000  -1.38629436 1.6094379 1.00000000   1   Recent\n284  1.1111111  -0.11706724 2.1972246 0.81111111   1   Recent\n285  1.2500000  -0.27892944 2.0794415 0.84444444   1   Recent\n286  5.0000000  -8.04718956 0.6931472 0.10000000   0    Never\n287  2.5000000  -2.29072683 1.3862944 0.52222222   0    Never\n288  3.3333333  -4.01324268 1.0986123 0.42222222   0    Never\n289  2.5000000  -2.29072683 1.3862944 0.22222222   0    Never\n290  2.5000000  -2.29072683 1.3862944 0.97777778   0    Never\n291 10.0000000 -23.02585093 0.0000000 0.57777778   0 Previous\n292  0.6250000   0.29375227 2.7725887 0.02777778   1   Recent\n293  1.4285714  -0.50953563 1.9459101 0.99444444   0    Never\n294  1.1111111  -0.11706724 2.1972246 0.19444444   1   Recent\n295  5.0000000  -8.04718956 0.6931472 0.13333333   0 Previous\n296  0.2777778   0.35581496 3.5835189 0.45555556   1   Recent\n297  2.5000000  -2.29072683 1.3862944 0.15555556   0    Never\n298 10.0000000 -23.02585093 0.0000000 0.45000000   0    Never\n299  3.3333333  -4.01324268 1.0986123 0.02222222   0    Never\n300  2.0000000  -1.38629436 1.6094379 0.53888889   1   Recent\n301 10.0000000 -23.02585093 0.0000000 0.43333333   0    Never\n302  2.5000000  -2.29072683 1.3862944 1.00555556   0    Never\n303  5.0000000  -8.04718956 0.6931472 0.16111111   0 Previous\n304  5.0000000  -8.04718956 0.6931472 0.77222222   0    Never\n305  3.3333333  -4.01324268 1.0986123 0.84444444   1   Recent\n306  1.6666667  -0.85137604 1.7917595 0.50000000   0 Previous\n307  2.0000000  -1.38629436 1.6094379 0.34444444   1   Recent\n308  0.7142857   0.24033731 2.6390573 0.61111111   0 Previous\n309  0.6666667   0.27031007 2.7080502 0.08333333   0    Never\n310  1.6666667  -0.85137604 1.7917595 0.37777778   0    Never\n311 10.0000000 -23.02585093 0.0000000 0.10555556   0    Never\n312  1.4285714  -0.50953563 1.9459101 0.25555556   1   Recent\n313  0.3225806   0.36496842 3.4339872 1.02222222   1   Recent\n314  1.1111111  -0.11706724 2.1972246 1.04444444   1   Recent\n315  1.4285714  -0.50953563 1.9459101 0.34444444   1   Recent\n316  2.5000000  -2.29072683 1.3862944 0.31111111   0 Previous\n317  0.9090909   0.08664562 2.3978953 0.64444444   1   Recent\n318  1.4285714  -0.50953563 1.9459101 1.25555556   1   Recent\n319  2.5000000  -2.29072683 1.3862944 0.77777778   0    Never\n320  2.0000000  -1.38629436 1.6094379 1.00000000   1   Recent\n321  1.4285714  -0.50953563 1.9459101 0.61111111   1   Recent\n322  1.6666667  -0.85137604 1.7917595 0.98888889   1   Recent\n323  1.2500000  -0.27892944 2.0794415 0.78888889   1   Recent\n324  1.6666667  -0.85137604 1.7917595 0.93333333   0    Never\n325  2.0000000  -1.38629436 1.6094379 0.86666667   0    Never\n326  2.0000000  -1.38629436 1.6094379 0.66666667   0    Never\n327  1.4285714  -0.50953563 1.9459101 0.91111111   1   Recent\n328  1.6666667  -0.85137604 1.7917595 0.90000000   0 Previous\n329  2.5000000  -2.29072683 1.3862944 0.19444444   1   Recent\n330  0.4347826   0.36213440 3.1354942 0.08888889   1   Recent\n331  5.0000000  -8.04718956 0.6931472 0.03888889   0 Previous\n332  2.0000000  -1.38629436 1.6094379 0.16666667   0    Never\n333  2.5000000  -2.29072683 1.3862944 0.58888889   0    Never\n334  0.9090909   0.08664562 2.3978953 0.96666667   0    Never\n335  5.0000000  -8.04718956 0.6931472 0.80000000   0    Never\n336  5.0000000  -8.04718956 0.6931472 0.13333333   0 Previous\n337  2.5000000  -2.29072683 1.3862944 0.09444444   0 Previous\n338  2.0000000  -1.38629436 1.6094379 0.53888889   0    Never\n339 10.0000000 -23.02585093 0.0000000 0.14444444   1   Recent\n340  3.3333333  -4.01324268 1.0986123 0.17222222   0    Never\n341  2.5000000  -2.29072683 1.3862944 0.15555556   1   Recent\n342  3.3333333  -4.01324268 1.0986123 0.83333333   1   Recent\n343  1.4285714  -0.50953563 1.9459101 0.22222222   1   Recent\n344  2.0000000  -1.38629436 1.6094379 1.15555556   0    Never\n345  0.7692308   0.20181866 2.5649494 0.94444444   1   Recent\n346 10.0000000 -23.02585093 0.0000000 1.22222222   0    Never\n347  0.5882353   0.31213427 2.8332133 1.11111111   1   Recent\n348  5.0000000  -8.04718956 0.6931472 0.81111111   1   Recent\n349  1.4285714  -0.50953563 1.9459101 0.72222222   1   Recent\n350  0.4761905   0.35330350 3.0445224 0.83333333   1   Recent\n351  5.0000000  -8.04718956 0.6931472 0.92222222   0    Never\n352  0.5555556   0.32654815 2.8903718 0.08333333   0 Previous\n353  0.3225806   0.36496842 3.4339872 0.24444444   1   Recent\n354  2.5000000  -2.29072683 1.3862944 0.03888889   0    Never\n355  2.0000000  -1.38629436 1.6094379 0.11111111   1   Recent\n356  0.7692308   0.20181866 2.5649494 0.97222222   1   Recent\n357  2.5000000  -2.29072683 1.3862944 0.39444444   0    Never\n358  3.3333333  -4.01324268 1.0986123 0.14444444   0    Never\n359  2.5000000  -2.29072683 1.3862944 0.89444444   1   Recent\n360  1.0000000   0.00000000 2.3025851 0.20000000   0    Never\n361  1.1111111  -0.11706724 2.1972246 0.16666667   1   Recent\n362  2.5000000  -2.29072683 1.3862944 0.99444444   0 Previous\n363  3.3333333  -4.01324268 1.0986123 1.10555556   1   Recent\n364  2.5000000  -2.29072683 1.3862944 1.01111111   1   Recent\n365  2.5000000  -2.29072683 1.3862944 1.24444444   0    Never\n366  1.4285714  -0.50953563 1.9459101 0.08888889   1   Recent\n367  3.3333333  -4.01324268 1.0986123 0.20000000   0    Never\n368  1.4285714  -0.50953563 1.9459101 0.22222222   0    Never\n369  2.5000000  -2.29072683 1.3862944 0.97777778   1   Recent\n370  1.4285714  -0.50953563 1.9459101 0.97777778   0    Never\n371  0.4761905   0.35330350 3.0445224 0.84444444   1   Recent\n372  5.0000000  -8.04718956 0.6931472 0.24444444   0    Never\n373  2.0000000  -1.38629436 1.6094379 1.22222222   1   Recent\n374  2.5000000  -2.29072683 1.3862944 0.94444444   1   Recent\n375  2.5000000  -2.29072683 1.3862944 0.11111111   1   Recent\n376  0.5555556   0.32654815 2.8903718 0.87222222   1   Recent\n377  0.3703704   0.36787103 3.2958369 0.73888889   1   Recent\n378  2.5000000  -2.29072683 1.3862944 0.46111111   0    Never\n379  2.0000000  -1.38629436 1.6094379 0.84444444   0    Never\n380  2.0000000  -1.38629436 1.6094379 0.93888889   0    Never\n381  2.5000000  -2.29072683 1.3862944 0.49444444   0 Previous\n382  1.6666667  -0.85137604 1.7917595 0.51111111   1   Recent\n383  2.0000000  -1.38629436 1.6094379 0.11666667   0 Previous\n384  0.3225806   0.36496842 3.4339872 0.17222222   1   Recent\n385  5.0000000  -8.04718956 0.6931472 0.17222222   0    Never\n386  0.5882353   0.31213427 2.8332133 0.73888889   1   Recent\n387  2.0000000  -1.38629436 1.6094379 0.85000000   1   Recent\n388 10.0000000 -23.02585093 0.0000000 1.00000000   0    Never\n389  0.4761905   0.35330350 3.0445224 1.13333333   1   Recent\n390  1.2500000  -0.27892944 2.0794415 0.94444444   1   Recent\n391  5.0000000  -8.04718956 0.6931472 0.98888889   0    Never\n392  0.7142857   0.24033731 2.6390573 0.31111111   1   Recent\n393  5.0000000  -8.04718956 0.6931472 1.00000000   1   Recent\n394  2.5000000  -2.29072683 1.3862944 0.93333333   0 Previous\n395  2.5000000  -2.29072683 1.3862944 0.94444444   1   Recent\n396  1.2500000  -0.27892944 2.0794415 0.40000000   1   Recent\n397  0.9090909   0.08664562 2.3978953 0.82222222   1   Recent\n398 10.0000000 -23.02585093 0.0000000 0.46666667   1   Recent\n399  5.0000000  -8.04718956 0.6931472 1.00000000   1   Recent\n400 10.0000000 -23.02585093 0.0000000 1.20000000   1   Recent\n401  2.5000000  -2.29072683 1.3862944 0.54444444   1   Recent\n402  5.0000000  -8.04718956 0.6931472 2.43333333   1   Recent\n403  1.4285714  -0.50953563 1.9459101 1.20000000   1   Recent\n404 10.0000000 -23.02585093 0.0000000 1.97777778   0    Never\n405  3.3333333  -4.01324268 1.0986123 0.46666667   0 Previous\n406  1.2500000  -0.27892944 2.0794415 2.02222222   1   Recent\n407  1.6666667  -0.85137604 1.7917595 0.06666667   0    Never\n408  2.5000000  -2.29072683 1.3862944 1.95000000   0 Previous\n409  5.0000000  -8.04718956 0.6931472 0.06666667   0    Never\n410 10.0000000 -23.02585093 0.0000000 0.03333333   1   Recent\n411  5.0000000  -8.04718956 0.6931472 0.50555556   0    Never\n412  5.0000000  -8.04718956 0.6931472 1.36111111   0    Never\n413  5.0000000  -8.04718956 0.6931472 2.06666667   0 Previous\n414 10.0000000 -23.02585093 0.0000000 1.21111111   0    Never\n415 10.0000000 -23.02585093 0.0000000 0.25555556   0 Previous\n416  1.4285714  -0.50953563 1.9459101 2.01666667   1   Recent\n417 10.0000000 -23.02585093 0.0000000 0.73888889   0    Never\n418  5.0000000  -8.04718956 0.6931472 0.03888889   1   Recent\n419 10.0000000 -23.02585093 0.0000000 0.62222222   0 Previous\n420  3.3333333  -4.01324268 1.0986123 0.23333333   0 Previous\n421  5.0000000  -8.04718956 0.6931472 1.87777778   1   Recent\n422  1.4285714  -0.50953563 1.9459101 0.31111111   1   Recent\n423  0.4761905   0.35330350 3.0445224 0.52222222   1   Recent\n424  1.6666667  -0.85137604 1.7917595 0.22222222   1   Recent\n425  2.0000000  -1.38629436 1.6094379 1.95555556   1   Recent\n426 10.0000000 -23.02585093 0.0000000 0.36666667   0 Previous\n427  1.4285714  -0.50953563 1.9459101 0.30555556   0    Never\n428  1.2500000  -0.27892944 2.0794415 1.91111111   0 Previous\n429 10.0000000 -23.02585093 0.0000000 0.85000000   0    Never\n430  1.1111111  -0.11706724 2.1972246 2.04444444   0 Previous\n431 10.0000000 -23.02585093 0.0000000 2.03333333   0    Never\n432  2.5000000  -2.29072683 1.3862944 0.24444444   0 Previous\n433 10.0000000 -23.02585093 0.0000000 2.03333333   0    Never\n434 10.0000000 -23.02585093 0.0000000 1.55555556   0    Never\n435  3.3333333  -4.01324268 1.0986123 0.21111111   0    Never\n436  0.7692308   0.20181866 2.5649494 2.04444444   1   Recent\n437 10.0000000 -23.02585093 0.0000000 0.55555556   0    Never\n438  3.3333333  -4.01324268 1.0986123 1.46666667   0    Never\n439  5.0000000  -8.04718956 0.6931472 1.42222222   1   Recent\n440  5.0000000  -8.04718956 0.6931472 0.59444444   0 Previous\n441 10.0000000 -23.02585093 0.0000000 2.04444444   0    Never\n442  2.0000000  -1.38629436 1.6094379 1.21666667   1   Recent\n443  1.6666667  -0.85137604 1.7917595 2.07777778   0 Previous\n444  5.0000000  -8.04718956 0.6931472 0.51111111   0    Never\n445 10.0000000 -23.02585093 0.0000000 0.25000000   0    Never\n446 10.0000000 -23.02585093 0.0000000 2.03333333   0    Never\n447  3.3333333  -4.01324268 1.0986123 2.04444444   1   Recent\n448  5.0000000  -8.04718956 0.6931472 0.86666667   0    Never\n449 10.0000000 -23.02585093 0.0000000 2.04444444   0    Never\n450  3.3333333  -4.01324268 1.0986123 2.07777778   0 Previous\n451  3.3333333  -4.01324268 1.0986123 1.12222222   1   Recent\n452 10.0000000 -23.02585093 0.0000000 1.56666667   0    Never\n453  5.0000000  -8.04718956 0.6931472 0.26666667   0    Never\n454  0.2439024   0.34414316 3.7135721 0.40000000   0 Previous\n455 10.0000000 -23.02585093 0.0000000 0.31111111   0    Never\n456  1.1111111  -0.11706724 2.1972246 2.03888889   0 Previous\n457  3.3333333  -4.01324268 1.0986123 0.38888889   0    Never\n458  1.4285714  -0.50953563 1.9459101 0.32222222   0 Previous\n459  2.0000000  -1.38629436 1.6094379 2.03333333   0    Never\n460  1.4285714  -0.50953563 1.9459101 0.05555556   1   Recent\n461  3.3333333  -4.01324268 1.0986123 2.37777778   1   Recent\n462  1.1111111  -0.11706724 2.1972246 2.18888889   0 Previous\n463 10.0000000 -23.02585093 0.0000000 0.98888889   0    Never\n464  5.0000000  -8.04718956 0.6931472 0.62222222   0    Never\n465  5.0000000  -8.04718956 0.6931472 0.10000000   0    Never\n466 10.0000000 -23.02585093 0.0000000 2.06666667   0    Never\n467  5.0000000  -8.04718956 0.6931472 1.68333333   0 Previous\n468  2.5000000  -2.29072683 1.3862944 0.17777778   0    Never\n469  2.0000000  -1.38629436 1.6094379 0.04444444   0 Previous\n470  5.0000000  -8.04718956 0.6931472 0.35000000   0    Never\n471  0.4761905   0.35330350 3.0445224 1.20000000   0 Previous\n472  5.0000000  -8.04718956 0.6931472 2.03333333   0 Previous\n473  5.0000000  -8.04718956 0.6931472 0.83888889   0    Never\n474 10.0000000 -23.02585093 0.0000000 0.07777778   0    Never\n475  3.3333333  -4.01324268 1.0986123 0.42222222   0    Never\n476  3.3333333  -4.01324268 1.0986123 0.97777778   0    Never\n477  1.2500000  -0.27892944 2.0794415 0.51666667   1   Recent\n478  2.5000000  -2.29072683 1.3862944 2.22222222   1   Recent\n479 10.0000000 -23.02585093 0.0000000 1.97777778   0 Previous\n480 10.0000000 -23.02585093 0.0000000 0.43333333   0    Never\n481  0.9090909   0.08664562 2.3978953 0.66111111   0 Previous\n482  3.3333333  -4.01324268 1.0986123 1.71111111   0 Previous\n483  3.3333333  -4.01324268 1.0986123 0.90555556   1   Recent\n484  1.2500000  -0.27892944 2.0794415 0.65555556   0 Previous\n485 10.0000000 -23.02585093 0.0000000 0.42222222   0    Never\n486 10.0000000 -23.02585093 0.0000000 0.64444444   0    Never\n487  2.5000000  -2.29072683 1.3862944 0.97777778   1   Recent\n488  5.0000000  -8.04718956 0.6931472 0.36666667   0    Never\n489  3.3333333  -4.01324268 1.0986123 0.38888889   1   Recent\n490  3.3333333  -4.01324268 1.0986123 0.37777778   0    Never\n491  0.3846154   0.36750440 3.2580965 2.12222222   0 Previous\n492  1.6666667  -0.85137604 1.7917595 0.38888889   0    Never\n493  5.0000000  -8.04718956 0.6931472 0.17777778   0    Never\n494 10.0000000 -23.02585093 0.0000000 0.31111111   0    Never\n495  2.5000000  -2.29072683 1.3862944 0.16666667   0    Never\n496  1.2500000  -0.27892944 2.0794415 0.07777778   1   Recent\n497 10.0000000 -23.02585093 0.0000000 0.47777778   0    Never\n498  1.6666667  -0.85137604 1.7917595 0.98888889   0 Previous\n499  5.0000000  -8.04718956 0.6931472 0.42222222   0    Never\n500  2.5000000  -2.29072683 1.3862944 2.26666667   1   Recent\n501  3.3333333  -4.01324268 1.0986123 0.84444444   0    Never\n502  2.5000000  -2.29072683 1.3862944 2.16666667   0    Never\n503  5.0000000  -8.04718956 0.6931472 2.04444444   0    Never\n504  5.0000000  -8.04718956 0.6931472 1.41111111   0 Previous\n505 10.0000000 -23.02585093 0.0000000 2.06111111   0    Never\n506  5.0000000  -8.04718956 0.6931472 2.17777778   0    Never\n507  3.3333333  -4.01324268 1.0986123 2.20000000   0    Never\n508  3.3333333  -4.01324268 1.0986123 1.88888889   1   Recent\n509  2.0000000  -1.38629436 1.6094379 0.27777778   1   Recent\n510 10.0000000 -23.02585093 0.0000000 0.90555556   0    Never\n511  0.8333333   0.15193463 2.4849066 2.02222222   0    Never\n512  2.5000000  -2.29072683 1.3862944 1.66666667   0    Never\n513  1.2500000  -0.27892944 2.0794415 0.18888889   1   Recent\n514  2.0000000  -1.38629436 1.6094379 0.18888889   1   Recent\n515  2.5000000  -2.29072683 1.3862944 2.03333333   0    Never\n516  1.2500000  -0.27892944 2.0794415 1.47777778   0 Previous\n517  2.5000000  -2.29072683 1.3862944 0.76666667   0 Previous\n518  1.4285714  -0.50953563 1.9459101 2.03333333   1   Recent\n519  1.4285714  -0.50953563 1.9459101 0.07777778   1   Recent\n520  3.3333333  -4.01324268 1.0986123 2.04444444   0 Previous\n521  0.6250000   0.29375227 2.7725887 0.49444444   1   Recent\n522  1.6666667  -0.85137604 1.7917595 2.03333333   0    Never\n523  0.7142857   0.24033731 2.6390573 1.96666667   1   Recent\n524  0.9090909   0.08664562 2.3978953 0.85555556   1   Recent\n525  0.4761905   0.35330350 3.0445224 1.36666667   0 Previous\n526  5.0000000  -8.04718956 0.6931472 1.62222222   0 Previous\n527  0.9090909   0.08664562 2.3978953 1.12777778   0    Never\n528  1.4285714  -0.50953563 1.9459101 2.00000000   1   Recent\n529  3.3333333  -4.01324268 1.0986123 0.87777778   0    Never\n530  1.6666667  -0.85137604 1.7917595 1.11666667   0 Previous\n531 10.0000000 -23.02585093 0.0000000 0.71666667   0 Previous\n532  5.0000000  -8.04718956 0.6931472 2.02777778   0    Never\n533  2.0000000  -1.38629436 1.6094379 0.88333333   0    Never\n534  3.3333333  -4.01324268 1.0986123 1.96666667   0 Previous\n535  5.0000000  -8.04718956 0.6931472 0.39444444   1   Recent\n536  3.3333333  -4.01324268 1.0986123 0.60000000   1   Recent\n537  2.0000000  -1.38629436 1.6094379 1.10000000   0 Previous\n538  5.0000000  -8.04718956 0.6931472 2.06666667   0    Never\n539  1.4285714  -0.50953563 1.9459101 0.27777778   0 Previous\n540 10.0000000 -23.02585093 0.0000000 0.26666667   0    Never\n541  2.5000000  -2.29072683 1.3862944 2.12222222   0    Never\n542  1.6666667  -0.85137604 1.7917595 0.95000000   0 Previous\n543  5.0000000  -8.04718956 0.6931472 0.80555556   0    Never\n544  0.4761905   0.35330350 3.0445224 2.03333333   0    Never\n545  2.5000000  -2.29072683 1.3862944 0.80000000   0    Never\n546  5.0000000  -8.04718956 0.6931472 0.24444444   0 Previous\n547  3.3333333  -4.01324268 1.0986123 0.77777778   0    Never\n548 10.0000000 -23.02585093 0.0000000 2.04444444   0    Never\n549  3.3333333  -4.01324268 1.0986123 1.04444444   0    Never\n550  1.1111111  -0.11706724 2.1972246 1.64444444   0 Previous\n551 10.0000000 -23.02585093 0.0000000 0.25555556   0    Never\n552  5.0000000  -8.04718956 0.6931472 1.42222222   0    Never\n553 10.0000000 -23.02585093 0.0000000 1.17777778   0    Never\n554 10.0000000 -23.02585093 0.0000000 0.51111111   0 Previous\n555 10.0000000 -23.02585093 0.0000000 0.83333333   0    Never\n556  3.3333333  -4.01324268 1.0986123 0.26666667   0    Never\n557  2.5000000  -2.29072683 1.3862944 0.32222222   1   Recent\n558  3.3333333  -4.01324268 1.0986123 1.98888889   0 Previous\n559  2.0000000  -1.38629436 1.6094379 1.88888889   1   Recent\n560  5.0000000  -8.04718956 0.6931472 2.02777778   0    Never\n561  2.0000000  -1.38629436 1.6094379 2.22222222   0 Previous\n562 10.0000000 -23.02585093 0.0000000 0.62222222   0    Never\n563  1.6666667  -0.85137604 1.7917595 0.26666667   0 Previous\n564  1.4285714  -0.50953563 1.9459101 0.11111111   0 Previous\n565  0.4545455   0.35838971 3.0910425 1.96666667   1   Recent\n566 10.0000000 -23.02585093 0.0000000 1.28888889   0    Never\n567  0.8333333   0.15193463 2.4849066 0.30000000   0 Previous\n568 10.0000000 -23.02585093 0.0000000 0.26666667   0    Never\n569  2.0000000  -1.38629436 1.6094379 0.63333333   0 Previous\n570 10.0000000 -23.02585093 0.0000000 0.51111111   0    Never\n571  1.4285714  -0.50953563 1.9459101 0.43333333   0 Previous\n572  2.0000000  -1.38629436 1.6094379 0.18888889   1   Recent\n573  2.5000000  -2.29072683 1.3862944 0.11666667   0 Previous\n574  3.3333333  -4.01324268 1.0986123 2.04444444   1   Recent\n575  0.6250000   0.29375227 2.7725887 0.05000000   1   Recent\n\n\n\nglimpse(uis2)\n\nRows: 575\nColumns: 19\n$ ID     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, …\n$ AGE    &lt;dbl&gt; 39, 33, 33, 32, 24, 30, 39, 27, 40, 36, 38, 29, 32, 41, 31, 27,…\n$ BECK   &lt;dbl&gt; 9.000, 34.000, 10.000, 20.000, 5.000, 32.550, 19.000, 10.000, 2…\n$ HC     &lt;dbl&gt; 4, 4, 2, 4, 2, 3, 4, 4, 2, 2, 2, 3, 3, 1, 1, 2, 1, 4, 3, 2, 3, …\n$ IV     &lt;dbl&gt; 3, 2, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 2, 1, 3, 1, …\n$ NDT    &lt;dbl&gt; 1, 8, 3, 1, 5, 1, 34, 2, 3, 7, 8, 1, 2, 8, 1, 3, 6, 1, 15, 5, 1…\n$ RACE   &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ TREAT  &lt;dbl&gt; 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, …\n$ SITE   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ LEN.T  &lt;dbl&gt; 123, 25, 7, 66, 173, 16, 179, 21, 176, 124, 176, 79, 182, 174, …\n$ TIME   &lt;dbl&gt; 188, 26, 207, 144, 551, 32, 459, 22, 210, 184, 212, 87, 598, 26…\n$ CENSOR &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, …\n$ Y      &lt;dbl&gt; 5.236442, 3.258097, 5.332719, 4.969813, 6.311735, 3.465736, 6.1…\n$ ND1    &lt;dbl&gt; 5.0000000, 1.1111111, 2.5000000, 5.0000000, 1.6666667, 5.000000…\n$ ND2    &lt;dbl&gt; -8.0471896, -0.1170672, -2.2907268, -8.0471896, -0.8513760, -8.…\n$ LNDT   &lt;dbl&gt; 0.6931472, 2.1972246, 1.3862944, 0.6931472, 1.7917595, 0.693147…\n$ FRAC   &lt;dbl&gt; 0.68333333, 0.13888889, 0.03888889, 0.73333333, 0.96111111, 0.0…\n$ IV3    &lt;dbl&gt; 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, …\n$ IV_fct &lt;fct&gt; Recent, Previous, Recent, Recent, Never, Recent, Recent, Recent…\n\n\nLet’s look at the three groups in our data defined by the IV variable. These are people who have never used IV drugs, those who have previously used IV drugs, and those who have recently used IV drugs. The following table shows how many people are in each group.\n\ntabyl(uis2, IV_fct) %&gt;%\n  adorn_totals()\n\n   IV_fct   n   percent\n    Never 223 0.3878261\n Previous 109 0.1895652\n   Recent 243 0.4226087\n    Total 575 1.0000000\n\n\nWe’re interested in depression as measured by the Beck Depression Inventory.\n\nExercise 2\nSearch the internet for the Beck Depression Inventory. (This search is much easier than for Exercise 1.) Write a short paragraph about it and how it purports to measure depression.\n\nPlease write up your answer here.\n\n\nA useful graph is a side-by-side boxplot.\n\nggplot(uis2, aes(y = BECK, x = IV_fct)) +\n    geom_boxplot()\n\n\n\n\nThis boxplot shows that the distribution of depression scores is similar across the groups. There are some small differences, but it’s not clear if these differences are statistically significant.\nWe can get the overall mean of all Beck scores, sometimes called the “grand mean”.\n\nuis2 %&gt;%\n  summarize(mean(BECK))\n\n  mean(BECK)\n1   17.36743\n\n\nIf we use group_by, we can separate this out by IV group:\n\nuis2 %&gt;%\n    group_by(IV_fct) %&gt;%\n    summarize(mean(BECK))\n\n# A tibble: 3 × 2\n  IV_fct   `mean(BECK)`\n  &lt;fct&gt;           &lt;dbl&gt;\n1 Never            15.9\n2 Previous         16.6\n3 Recent           19.0\n\n\n\n\nExericse 3\nWe have to be careful about the term “grand mean”. In some contexts, the term “grand mean” refers to the mean of all scores in the response variable (17.36743 above). In other cases, the term refers to the mean of the three group means (the mean of 15.94996, 16.64201, and 18.99363).\nFirst calculate the mean of the three group means above. (You can use R to do this if you want, or you can just use a calculator.) Explain mathematically why the overall mean 17.36743 is not the same as the mean of the three group means. What would have to be true of the sample for the overall mean to agree with the mean of the three group means? (Hint: think about the size of each of the three groups.)\n\nPlease write up your answer here."
  },
  {
    "objectID": "22-anova-web.html#the-f-distribution",
    "href": "22-anova-web.html#the-f-distribution",
    "title": "22  ANOVA",
    "section": "22.5 The F distribution",
    "text": "22.5 The F distribution\nTo keep the exposition simple here, we’ll assume that the term “grand mean” refers to the overall mean of the response variable, 17.36743.\nWhen assessing the differences among groups, there are two numbers that are important.\nThe first is called the “mean square between groups” (MSG). It measures how far away each group mean is away from the overall grand mean for the whole sample. For example, for those who never used IV drugs, their mean Beck score was 15.95. This is 1.42 points below the grand mean of 17.37. On the other hand, recent IV drug users had a mean Beck score of nearly 19. This is 1.63 points above the grand mean. MSG is calculated by taking these differences for each group, squaring them to make them positive, weighting them by the sizes of each group (larger groups should obviously count for more), and dividing by the “group degrees of freedom” \\(df_{G} = k - 1\\) where \\(k\\) is the number of groups. The idea is that MSG is a kind of “average variability” among the groups. In other words, how far away are the groups from the grand mean (and therefore, from each other)?\nThe second number of interest is the “mean square error” (MSE). It is a measure of variability within groups. In other words, it measures how far away data points are from their own group means. Even under the assumption of a null hypothesis that says all the groups should be the same, we still expect some variability. Its calculation also involves dividing by some degrees of freedom, but now it is \\(df_{E} = n - k\\).\nAll that is somewhat technical and complicated. We’ll leave it to the computer. The key insight comes from considering the ratio of \\(MSG\\) and \\(MSE\\). We will call this quantity F:\n\\[\nF = \\frac{MSG}{MSE}.\n\\]\nWhat can be said about this magical F? Under the assumption of the null hypothesis, we expect some variability among the groups, and we expect some variability within each group as well, but these two sources of variability should be about the same. In other words, \\(MSG\\) should be roughly equal to \\(MSE\\). Therefore, F ought to be close to 1.\nWe can simulate this using the infer package. Suppose that there were no difference in the mean BECK scores among the three groups. We can accomplish this by shuffling the IV labels, an idea we’ve seen several times before in this book. Permuting the IV values breaks any association that might have existed in the original data.\n\nset.seed(420)\nBECK_IV_test_sim &lt;- uis2 %&gt;%\n  specify(response = BECK, explanatory = IV_fct) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  generate(reps = 1000, type = \"permute\") %&gt;%\n  calculate(stat = \"F\")\nBECK_IV_test_sim\n\nResponse: BECK (numeric)\nExplanatory: IV_fct (factor)\nNull Hypothesis: independence\n# A tibble: 1,000 × 2\n   replicate  stat\n       &lt;int&gt; &lt;dbl&gt;\n 1         1 0.616\n 2         2 2.36 \n 3         3 1.38 \n 4         4 2.64 \n 5         5 0.333\n 6         6 0.732\n 7         7 1.33 \n 8         8 0.261\n 9         9 1.31 \n10        10 0.616\n# ℹ 990 more rows\n\n\n\nBECK_IV_test_sim %&gt;%\n  visualize()\n\n\n\n\nAs explained earlier, the F scores are clustered around 1. They can never be smaller than zero. (The bar at zero is centered on zero, but no F score can be less than zero.) There are occasional F scores much larger than 1, but just by chance.\nIt’s not particularly interesting if F is less than one. That just means that the variability between groups is small and the variability of the data within each group is large. That doesn’t allow us to conclude that there is a difference among groups. However, if F is really large, that means that there is much more variability between the groups than there is within each group. Therefore, the groups are far apart and there is evidence of a difference among groups.\n\\(MSG\\) and \\(MSE\\) are measures of variability, and that’s why this is called “Analysis of Variance”.\nThe F distribution is the correct sampling distribution model. Like a t model, there are infinitely many different F models because degrees of freedom are involved. But unlike a t model, the F model has two numbers called degrees of freedom, \\(df_{G}\\) and \\(df_{E}\\). Both of these numbers affect the precise shape of the F distribution.\nFor example, here is picture of a few different F models.\n\n# Don't worry about the syntax here.\n# You won't need to know how to do this on your own.\nggplot(data.frame(x = c(0, 5)), aes(x)) +\n    stat_function(fun = df, args = list(df1 = 2, df2 = 5),\n                  aes(color = \"2, 5\")) +\n    stat_function(fun = df, args = list(df1 = 2, df2 = 50),\n                  aes(color = \"2, 50\" )) +\n    stat_function(fun = df, args = list(df1 = 10, df2 = 50),\n                  aes(color = \"10, 50\")) +\n    scale_color_manual(name = expression(paste(df[G], \", \", df[E])),\n                       values = c(\"2, 5\" = \"red\",\n                                  \"2, 50\" = \"blue\",\n                                  \"10, 50\" = \"green\"),\n                       breaks =  c(\"2, 5\", \"2, 50\", \"10, 50\"))\n\n\n\n\nHere is the theoretical F distribution for our data:\n\nBECK_IV_test &lt;- uis2 %&gt;%\n  specify(response = BECK, explanatory = IV_fct) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  assume(distribution = \"F\")\nBECK_IV_test\n\nAn F distribution with 2 and 572 degrees of freedom.\n\n\n\nExercise 4\nExplain why there are 2 and 572 degrees of freedom. Which one is \\(df_{G}\\) and which one is \\(df_{E}\\)?\n\nPlease write up your answer here.\n\n\nHere are the simulated values again, but with the theoretical F distribution superimposed for comparison.\n\nBECK_IV_test_sim %&gt;%\n  visualize(method = \"both\")\n\nWarning: Check to make sure the conditions have been met for the theoretical\nmethod. {infer} currently does not check these for you.\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\nℹ The deprecated feature was likely used in the infer package.\n  Please report the issue at &lt;https://github.com/tidymodels/infer/issues&gt;.\n\n\n\n\n\nOther than the very left edge, the theoretical curve is a good fit to the simulated F scores."
  },
  {
    "objectID": "22-anova-web.html#assumptions",
    "href": "22-anova-web.html#assumptions",
    "title": "22  ANOVA",
    "section": "22.6 Assumptions",
    "text": "22.6 Assumptions\nWhat conditions can we check to justify the use of an F model for our sampling distribution? In addition to the typical “Random” and “10%” conditions that ensure independence, we also need to check the “Nearly normal” condition for each group, just like for the t tests. A new assumption is the “Constant variance” assumption, which says that each group should have the same variance in the population. This is impossible to check, although we can use our sample as a rough guide. If each group has about the same spread, that is some evidence that such an assumption might hold in the population as well. Also, ANOVA is pretty robust to this assumption, especially when the groups are close to the same size. Even when the group sizes are unequal (sometimes called “unbalanced”), some say the variances can be off by up to a factor of 3 and ANOVA will still work pretty well. So what we’re looking for here are gross violations, not minor ones.\nLet’s go through the rubric with commentary."
  },
  {
    "objectID": "22-anova-web.html#exploratory-data-analysis",
    "href": "22-anova-web.html#exploratory-data-analysis",
    "title": "22  ANOVA",
    "section": "22.7 Exploratory data analysis",
    "text": "22.7 Exploratory data analysis\n\n22.7.1 Use data documentation (help files, code books, Google, etc.) to determine as much as possible about the data provenance and structure.\nYou should have researched this extensively in a previous exercise.\n\nuis\n\n     ID AGE   BECK HC IV NDT RACE TREAT SITE LEN.T TIME CENSOR        Y\n1     1  39  9.000  4  3   1    0     1    0   123  188      1 5.236442\n2     2  33 34.000  4  2   8    0     1    0    25   26      1 3.258097\n3     3  33 10.000  2  3   3    0     1    0     7  207      1 5.332719\n4     4  32 20.000  4  3   1    0     0    0    66  144      1 4.969813\n5     5  24  5.000  2  1   5    1     1    0   173  551      0 6.311735\n6     6  30 32.550  3  3   1    0     1    0    16   32      1 3.465736\n7     7  39 19.000  4  3  34    0     1    0   179  459      1 6.129050\n8     8  27 10.000  4  3   2    0     1    0    21   22      1 3.091042\n9     9  40 29.000  2  3   3    0     1    0   176  210      1 5.347108\n10   10  36 25.000  2  3   7    0     1    0   124  184      1 5.214936\n11   12  38 18.900  2  3   8    0     1    0   176  212      1 5.356586\n12   13  29 16.000  3  1   1    0     1    0    79   87      1 4.465908\n13   14  32 36.000  3  3   2    1     1    0   182  598      0 6.393591\n14   15  41 19.000  1  3   8    0     1    0   174  260      1 5.560682\n15   16  31 18.000  1  3   1    0     1    0   181  210      1 5.347108\n16   17  27 12.000  2  3   3    0     1    0    61   84      1 4.430817\n17   18  28 34.000  1  3   6    0     1    0   177  196      1 5.278115\n18   19  28 23.000  4  2   1    0     1    0    19   19      1 2.944439\n19   20  36 26.000  3  1  15    1     1    0    27  441      1 6.089045\n20   21  32 18.900  2  3   5    0     1    0   175  449      1 6.107023\n21   22  33 15.000  3  1   1    0     0    0    12  659      0 6.490724\n22   23  28 25.200  1  3   8    0     0    0    21   21      1 3.044522\n23   24  29  6.632  4  2   0    0     0    0    48   53      1 3.970292\n24   25  35  2.100  2  3   9    0     0    0    90  225      1 5.416100\n25   26  45 26.000  1  3   6    0     0    0    91  161      1 5.081404\n26   27  35 39.789  4  3   5    0     0    0    87   87      1 4.465908\n27   28  24 20.000  3  1   3    0     0    0    88   89      1 4.488636\n28   29  36 16.000  1  3   7    0     0    0     9   44      1 3.784190\n29   31  39 22.000  1  3   9    0     0    0    94  523      0 6.259581\n30   32  36  9.947  4  2  10    0     0    0    91  226      1 5.420535\n31   33  37  9.450  4  3   1    0     0    0    90  259      1 5.556828\n32   34  30 39.000  2  3   1    0     0    0    89  289      1 5.666427\n33   35  44 41.000  1  3   5    0     0    0    89  103      1 4.634729\n34   36  28 31.000  3  1   6    1     0    0   100  624      0 6.436150\n35   37  25 20.000  3  1   3    1     0    0    67   68      1 4.219508\n36   38  30  8.000  2  3   7    0     1    0    25   57      1 4.043051\n37   39  24  9.000  4  1   1    0     0    0    12   65      1 4.174387\n38   40  27 20.000  3  1   1    0     0    0    79   79      1 4.369448\n39   41  30  8.000  3  1   2    1     0    0    79  559      0 6.326149\n40   42  34  8.000  2  3   0    0     1    0    78   79      1 4.369448\n41   43  33 23.000  4  2   2    0     1    0    84   87      1 4.465908\n42   44  34 18.000  3  3   6    0     1    0    91   91      1 4.510860\n43   45  36 13.000  2  3   1    0     1    0   162  297      1 5.693732\n44   46  27 23.000  1  3   0    0     1    0    45   45      1 3.806662\n45   47  35  9.000  4  3   1    1     1    0    61  246      1 5.505332\n46   48  24 14.000  1  3   0    0     1    0    19   37      1 3.610918\n47   49  28 23.000  4  1   2    1     1    0    37   37      1 3.610918\n48   50  46 10.000  1  3   8    0     1    0    51  538      0 6.287859\n49   51  26 11.000  3  3   1    0     1    0    60  541      0 6.293419\n50   52  42 16.000  1  3  25    0     1    0   177  184      1 5.214936\n51   53  30  0.000  3  1   0    0     1    0    43  122      1 4.804021\n52   55  30 12.000  4  1   3    1     1    0    21  156      1 5.049856\n53   56  27 21.000  2  3   2    0     0    0    88  121      1 4.795791\n54   57  38  0.000  1  3   6    0     0    0    96  231      1 5.442418\n55   58  48  8.000  4  3  10    0     0    0   111  111      1 4.709530\n56   59  36 25.000  1  3  10    0     0    0    38   38      1 3.637586\n57   60  28  6.300  3  1   7    0     0    0    15   15      1 2.708050\n58   61  31 20.000  4  2   5    0     0    0    50   54      1 3.988984\n59   62  28  4.000  2  3   5    0     0    0    61  127      1 4.844187\n60   63  28 20.000  3  1   1    0     0    0    31  105      1 4.653960\n61   64  26 17.000  2  1   2    1     0    0    11   11      1 2.397895\n62   65  34  3.000  4  3   6    0     0    0    90  153      1 5.030438\n63   66  26 29.000  2  3   5    0     0    0    11   11      1 2.397895\n64   68  31 26.000  1  3   5    0     0    0    46   46      1 3.828641\n65   69  41 12.000  1  3   0    1     0    0    38  655      0 6.484635\n66   70  30 24.000  4  3   0    0     0    0    90  166      1 5.111988\n67   72  39 15.750  4  3   5    0     0    0    88   95      1 4.553877\n68   74  33  9.000  2  3  12    0     0    0    91  151      1 5.017280\n69   75  33 18.000  4  2   6    0     0    0    85  220      1 5.393628\n70   76  29 20.000  4  1   0    1     0    0    90  227      1 5.424950\n71   77  36 17.000  1  3   5    0     0    0    52  343      1 5.837730\n72   78  26  3.000  4  3   3    0     0    0    88  119      1 4.779123\n73   79  37 27.000  1  3  13    0     0    0    43   43      1 3.761200\n74   81  29 31.500  1  3   8    0     0    0    37   47      1 3.850148\n75   83  30 19.000  3  1   0    1     0    0    87  805      0 6.690842\n76   84  35 15.000  3  2   2    0     0    0    20  321      1 5.771441\n77   85  33 22.000  3  1   1    0     0    0     9  167      1 5.117994\n78   87  36 16.000  2  3   1    0     0    0    85  491      1 6.196444\n79   88  28 17.000  1  3   2    0     0    0    18   35      1 3.555348\n80   89  31 32.550  1  3  12    1     0    0    71  123      1 4.812184\n81   90  23 24.000  1  3   2    0     0    0    88  597      0 6.391917\n82   91  33 22.000  3  2   1    0     0    0    67  762      0 6.635947\n83   93  37 18.000  2  3   4    0     0    0    30   31      1 3.433987\n84   94  25 17.850  3  1   1    0     1    0    68  228      1 5.429346\n85   95  56  5.000  2  2   9    1     1    0   182  553      0 6.315358\n86   96  23 39.000  1  3   1    0     1    0   182  190      1 5.247024\n87   97  26 21.000  3  1   1    0     1    0   146  307      1 5.726848\n88   98  26 11.000  1  3   1    0     1    0    40   73      1 4.290459\n89   99  23 14.000  3  1   1    0     1    0   177  208      1 5.337538\n90  100  28 31.000  4  2   2    1     1    0   181  267      1 5.587249\n91  102  30 14.000  1  3  15    0     1    0   168  169      1 5.129899\n92  104  25  6.000  2  3   5    0     1    0    90  655      0 6.484635\n93  105  33 16.000  1  3   5    0     1    0    61   70      1 4.248495\n94  106  22  6.000  3  1   3    1     1    0    63  398      1 5.986452\n95  108  25 20.000  4  2   8    1     1    0   121  122      1 4.804021\n96  111  38  9.000  3  1   1    1     0    0    89   96      1 4.564348\n97  112  35 11.000  2  1   3    0     1    0    51 1172      0 7.066467\n98  113  35 15.000  3  1   1    0     0    0    88  734      0 6.598509\n99  114  25 13.000  3  3   1    0     0    0    25   26      1 3.258097\n100 115  33 31.000  3  1   3    1     0    0    83   84      1 4.430817\n101 116  30  5.000  3  1   2    1     0    0    89  171      1 5.141664\n102 117  45 10.000  2  3   1    0     0    0    24  159      1 5.068904\n103 119  42 23.000  2  3  20    0     0    0     7    7      1 1.945910\n104 120  29 16.000  4  1   1    1     0    0    85  763      0 6.637258\n105 121  24 37.800  3  1   0    0     0    0    89  104      1 4.644391\n106 122  33 10.000  2  3   4    0     0    0    91  162      1 5.087596\n107 123  32  9.000  3  1   0    0     0    0    89   90      1 4.499810\n108 124  26 15.000  3  1   0    0     0    0    82  373      1 5.921578\n109 125  28  2.000  1  3   3    0     0    0    84  115      1 4.744932\n110 127  37 34.000  2  3   1    0     0    0    30   30      1 3.401197\n111 128  23 11.000  4  1   6    0     0    0     7    8      1 2.079442\n112 129  40 31.000  2  3   3    1     0    0    84  168      1 5.123964\n113 130  36 36.750  3  3   0    0     0    0    70   70      1 4.248495\n114 131  23 26.000  3  2   2    0     0    0    76  130      1 4.867534\n115 132  35  5.000  4  1   1    1     0    0    89  285      1 5.652489\n116 133  25 19.000  2  3   1    0     1    0   178  569      0 6.343880\n117 134  35 21.000  2  3   6    0     1    0    87   87      1 4.465908\n118 135  46  1.000  4  2   0    0     1    0   175  310      1 5.736572\n119 136  32  6.000  4  1   3    0     1    0    87   87      1 4.465908\n120 137  35 23.000  3  1  16    1     1    0   110  544      0 6.298949\n121 138  34 38.000  3  3   1    0     1    0    21  156      1 5.049856\n122 139  43 24.000  3  1   3    0     1    0   139  658      0 6.489205\n123 140  39  3.000  4  3  15    0     1    0   181  273      1 5.609472\n124 141  27 16.800  4  3   2    1     1    0    33  168      1 5.123964\n125 142  38 35.000  1  3   1    0     1    0    39   83      1 4.418841\n126 143  37 11.000  2  3   7    0     1    0     4    4      1 1.386294\n127 144  44  2.000  1  3   4    1     1    0   184  708      0 6.562444\n128 145  25 16.000  4  1   1    1     1    0   123  137      1 4.919981\n129 146  34 15.000  3  1   1    0     1    0   176  259      1 5.556828\n130 147  34 11.000  3  3   2    1     1    0   174  560      0 6.327937\n131 148  38 11.000  1  3   1    1     1    0   181  586      0 6.373320\n132 149  24 22.000  2  3   2    1     1    0   113  190      1 5.247024\n133 151  42 18.000  2  3   3    0     1    0   164  544      0 6.298949\n134 153  34 29.000  4  3   1    1     0    0    84  494      1 6.202536\n135 154  45 27.000  1  3   8    0     0    0    80  541      0 6.293419\n136 155  40 16.000  2  3   4    0     0    0    91   94      1 4.543295\n137 156  27  9.000  4  1   3    1     0    0    97  567      0 6.340359\n138 157  24  0.000  4  1   3    0     0    0    51   55      1 4.007333\n139 158  27 15.000  1  3   3    0     0    0    91   93      1 4.532599\n140 159  34 24.000  3  1   4    0     0    0    90  276      1 5.620401\n141 160  36  3.000  2  3   6    0     0    0    46   46      1 3.828641\n142 162  31  9.000  3  1   1    0     0    0    76  250      1 5.521461\n143 163  40  5.000  2  3   2    0     0    0    75  106      1 4.663439\n144 164  40 13.000  1  3   4    1     0    0    91  552      0 6.313548\n145 165  37 29.000  2  3   5    0     0    0    90   90      1 4.499810\n146 166  25 11.000  4  3   6    0     0    0     3  203      1 5.313206\n147 167  41 22.000  2  3   3    1     1    0     8   67      1 4.204693\n148 168  22  9.000  4  1   1    0     1    0    33  559      1 6.326149\n149 169  31 18.000  2  3   8    1     1    0    31  106      1 4.663439\n150 170  29 40.000  1  1   1    1     1    0   174  374      1 5.924256\n151 171  27 25.000  3  1   2    0     1    0    34  630      0 6.445720\n152 172  22 26.000  4  2   3    0     1    0    60   61      1 4.110874\n153 174  37 11.000  1  2   5    1     1    0    78  547      0 6.304449\n154 175  36  6.000  3  1   2    1     1    0   182  568      0 6.342121\n155 176  24 20.000  3  1   1    0     1    0   182  490      1 6.194405\n156 177  28  9.000  4  1   0    1     1    0    78  222      1 5.402677\n157 178  24  6.000  4  1   1    0     1    0    55   56      1 4.025352\n158 179  28  0.000  3  1   2    0     1    0   223  282      1 5.641907\n159 180  24  5.000  3  1  20    1     1    0    25   35      1 3.555348\n160 181  24 15.000  4  1   0    0     1    0    63  603      0 6.401917\n161 183  29 14.700  3  1   1    0     1    0   133  148      1 4.997212\n162 184  37  3.000  1  3   5    1     1    0   154  354      1 5.869297\n163 185  26 31.000  1  1   2    0     1    0    70  164      1 5.099866\n164 186  29 14.000  3  2   1    0     1    0    66   94      1 4.543295\n165 187  29 28.000  2  3   4    0     1    0    40   65      1 4.174387\n166 188  33 18.000  4  1   1    0     1    0    75  567      0 6.340359\n167 189  29 12.000  4  2   2    0     1    0   187  634      0 6.452049\n168 190  32  5.000  1  1   2    1     1    0   183  633      0 6.450470\n169 192  33 11.000  4  1   8    1     1    0   182  477      1 6.167516\n170 193  26 21.000  4  2   2    0     1    0   192  436      1 6.077642\n171 195  24 23.000  2  3   4    1     1    0   162  362      1 5.891644\n172 196  46 32.000  2  3   2    0     1    0   193  552      0 6.313548\n173 197  23 26.000  4  1   2    0     1    0   111  144      1 4.969813\n174 198  40 19.950  4  3   8    0     1    0   182  242      1 5.488938\n175 199  48 17.000  3  1   4    0     1    0   180  564      0 6.335054\n176 200  33 16.000  3  1   0    0     1    0    93  299      1 5.700444\n177 201  21 26.250  4  1   7    0     1    0   167  167      1 5.117994\n178 202  38 29.000  3  1   2    0     1    0   196  380      1 5.940171\n179 203  28 23.000  4  2   4    0     1    0   106  120      1 4.787492\n180 205  39  9.000  1  3   6    0     1    0   158  218      1 5.384495\n181 206  37 26.000  1  2   1    1     0    0    91  115      1 4.744932\n182 207  32 22.000  3  1   4    1     0    0    89  224      1 5.411646\n183 208  39 23.000  3  2   2    1     0    0    89  132      1 4.882802\n184 209  28  0.000  1  3  10    0     0    0    88  148      1 4.997212\n185 210  26 30.000  3  1   0    1     0    0    95  593      0 6.385194\n186 211  31 21.000  1  3   0    0     0    0     5   26      1 3.258097\n187 213  34 19.000  4  3   8    0     0    0    32   32      1 3.465736\n188 214  26 28.000  4  2   2    1     0    0    92  292      1 5.676754\n189 215  29  8.000  4  1   3    0     0    0    66   89      1 4.488636\n190 217  25 11.000  3  1   8    0     0    0    90  364      1 5.897154\n191 218  34 15.000  3  2   3    1     0    0    93  142      1 4.955827\n192 219  32  8.000  3  1   2    0     0    0    89  188      1 5.236442\n193 221  38 14.000  4  2   0    0     0    0    91   92      1 4.521789\n194 222  32  7.000  1  3   8    0     0    0    56   56      1 4.025352\n195 223  31 13.000  2  3   7    0     0    0    90  110      1 4.700480\n196 224  40 10.000  3  1   3    0     0    0    73  555      0 6.318968\n197 225  28 17.000  4  1   5    1     0    0    85  220      1 5.393628\n198 226  40 18.000  1  3   3    0     0    0    23   23      1 3.135494\n199 227  32  5.000  2  3   3    0     0    0    85  285      1 5.652489\n200 228  29 20.000  3  3   5    0     0    0    90   90      1 4.499810\n201 229  25 31.000  3  1   4    0     0    0    53   59      1 4.077537\n202 230  32 15.000  2  3   2    0     0    0    96  156      1 5.049856\n203 232  37  4.000  2  2   2    0     0    0    83  142      1 4.955827\n204 233  38 15.000  3  3   8    0     0    0    54   57      1 4.043051\n205 234  31 14.000  3  2   9    0     0    0    79  279      1 5.631212\n206 235  30 27.000  1  3   3    1     0    0    81  118      1 4.770685\n207 236  34 30.000  4  1   4    1     0    0    18  567      0 6.340359\n208 237  33 23.000  1  3   4    0     1    0   184  562      0 6.331502\n209 238  36 13.000  3  2  10    1     1    0    39  239      1 5.476464\n210 239  32 26.000  4  1   0    0     1    0   177  578      0 6.359574\n211 240  29 10.000  2  3   2    1     1    0   122  551      0 6.311735\n212 241  32  4.000  1  1   4    1     1    0   178  313      1 5.746203\n213 242  34  0.000  3  1   7    0     1    0   173  560      0 6.327937\n214 243  26 35.000  1  3  31    0     1    0    53   54      1 3.988984\n215 244  25 32.000  1  3   5    1     1    0    94  198      1 5.288267\n216 245  30  2.000  4  1   2    1     1    0   163  164      1 5.099866\n217 246  33 15.000  3  2   6    0     1    0   160  325      1 5.783825\n218 247  40 23.000  4  2   6    0     1    0    61   62      1 4.127134\n219 248  26 13.000  3  1  12    0     1    0    41   45      1 3.806662\n220 249  26 29.000  1  3   5    1     1    0    53   53      1 3.970292\n221 250  35 22.105  4  3   4    0     1    0    53  253      1 5.533389\n222 251  26 15.000  2  2  11    0     1    0    13   51      1 3.931826\n223 252  33  7.000  4  1   3    1     1    0   183  540      0 6.291569\n224 253  27  7.000  1  3   4    0     1    0   182  317      1 5.758902\n225 254  29 33.000  3  3   3    0     1    0   183  437      1 6.079933\n226 255  29 23.000  3  3   9    0     1    0    63  136      1 4.912655\n227 256  39 21.000  2  3   7    0     1    0   111  115      1 4.744932\n228 257  43 19.000  3  2   2    1     1    0   174  175      1 5.164786\n229 258  35  8.000  3  3   3    0     1    0   173  442      1 6.091310\n230 259  26 24.000  4  1   2    1     1    0   119  122      1 4.804021\n231 260  27 28.737  4  1   3    0     1    0   180  181      1 5.198497\n232 261  28 20.000  4  1   2    1     1    0    98  180      1 5.192957\n233 262  30 14.000  3  1   4    0     1    0    50   51      1 3.931826\n234 263  31 17.000  4  2   1    1     1    0   178  541      0 6.293419\n235 264  26 19.000  2  3  16    0     1    0   100  121      1 4.795791\n236 265  36  5.000  4  2   4    0     1    0    93  328      1 5.793014\n237 267  25  8.000  2  3   3    0     1    0   165  166      1 5.111988\n238 268  26 22.000  3  1   0    1     1    0    93  556      0 6.320768\n239 269  30 11.000  2  3   5    0     0    0    44  104      1 4.644391\n240 270  28 13.000  3  1   5    0     0    0    77  102      1 4.624973\n241 272  34 11.053  3  1   0    1     0    0    91  144      1 4.969813\n242 273  31 24.000  3  1   2    0     0    0    95  545      0 6.300786\n243 274  30 19.000  4  3   1    0     0    0    82  537      0 6.285998\n244 275  35 27.000  3  2   5    1     0    0    76  625      0 6.437752\n245 276  30  4.000  4  2   3    1     0    0     5    6      1 1.791759\n246 277  37 38.000  1  3   7    0     0    0    69  307      1 5.726848\n247 278  29 11.000  4  1  12    1     0    0    90  290      1 5.669881\n248 279  23 21.000  4  1   8    0     0    0    19   20      1 2.995732\n249 280  23  1.000  1  1   4    0     0    0    60   74      1 4.304065\n250 281  44  4.000  4  1   0    0     0    0    69  100      1 4.605170\n251 282  43  7.000  4  2   8    1     0    0    85  555      0 6.318968\n252 283  38 20.000  2  3   3    0     0    0    92  152      1 5.023881\n253 284  33 17.000  3  1   3    1     0    0    55  115      1 4.744932\n254 285  36  6.300  1  3   9    0     0    0    20   92      1 4.521789\n255 286  26 12.000  1  3   2    0     0    0    87  554      0 6.317165\n256 287  30 16.000  4  1   0    0     0    0    91   92      1 4.521789\n257 288  34 31.500  4  1   0    0     0    0     9   69      1 4.234107\n258 289  32 30.000  2  3   6    0     0    0    22   25      1 3.218876\n259 290  30  1.000  3  1   1    0     0    0    87  501      0 6.216606\n260 291  37 32.000  2  3  10    1     0    0    86   86      1 4.454347\n261 292  35 29.000  2  3   7    0     0    0    85   99      1 4.595120\n262 293  30  6.000  3  1   0    0     0    0    83   87      1 4.465908\n263 294  34 17.000  4  1   6    1     0    0    83  136      1 4.912655\n264 295  40 13.000  1  2   6    0     0    0    92  106      1 4.663439\n265 296  28 15.000  4  2   3    1     0    0    85  220      1 5.393628\n266 297  32 11.000  3  1   6    0     0    0    36   36      1 3.583519\n267 298  45 17.000  1  3   2    1     0    0    87  162      1 5.087596\n268 299  24 23.000  2  1   0    0     1    0    56  116      1 4.753590\n269 300  43 23.000  1  3   5    1     1    0    94  175      1 5.164786\n270 301  38 15.000  1  3   0    1     1    0    74  209      1 5.342334\n271 302  33 19.000  2  3   1    0     1    0   186  545      0 6.300786\n272 303  26 21.000  4  2   2    1     1    0   178  245      1 5.501258\n273 304  40  8.000  4  3   3    0     1    0    84  176      1 5.170484\n274 305  27 34.000  4  2   0    0     1    0    13   14      1 2.639057\n275 306  39 21.000  2  3  12    0     1    0    85  113      1 4.727388\n276 308  29 27.000  4  2   3    1     1    0     9  354      1 5.869297\n277 309  28 32.000  4  2   4    0     1    0   162  174      1 5.159055\n278 310  37 29.000  1  3  20    0     0    0    23   23      1 3.135494\n279 311  37 22.000  2  3  20    0     0    0    26   26      1 3.258097\n280 312  40 12.000  4  2   9    0     0    0    84   98      1 4.584967\n281 313  25 36.000  1  3   5    0     0    0    23   23      1 3.135494\n282 314  40 15.000  1  1   2    0     0    0    86  555      0 6.318968\n283 315  40  3.000  1  3   4    1     0    0    90  290      1 5.669881\n284 316  34 24.000  2  3   8    0     0    0    73  543      0 6.297109\n285 317  41 18.000  2  3   7    0     0    0    76  274      1 5.613128\n286 321  23  2.000  4  1   1    0     1    0    18  119      1 4.779123\n287 322  36 14.000  3  1   3    0     1    0    94  164      1 5.099866\n288 323  28 19.000  4  1   2    1     1    0    76  548      0 6.306275\n289 324  23  7.000  3  1   3    0     1    0    40  175      1 5.164786\n290 325  27  8.000  3  1   3    0     1    0   176  539      0 6.289716\n291 326  32 27.000  4  2   0    0     1    0   104  155      1 5.043425\n292 327  38 25.000  4  3  15    0     1    0     5   14      1 2.639057\n293 328  38 28.000  4  1   6    1     1    0   179  187      1 5.231109\n294 329  45 39.000  1  3   8    0     1    0    35   65      1 4.174387\n295 330  26 18.000  2  2   1    0     1    0    24  159      1 5.068904\n296 331  29  8.000  1  3  35    0     1    0    82   96      1 4.564348\n297 332  33 31.000  4  1   3    0     1    0    28  243      1 5.493061\n298 333  25  6.000  3  1   0    1     1    0    81   85      1 4.442651\n299 334  36 19.000  4  1   2    0     1    0     4    4      1 1.386294\n300 335  37 19.000  2  3   4    0     1    0    97  121      1 4.795791\n301 336  29 16.000  4  1   0    1     1    0    78  659      1 6.490724\n302 337  29 15.000  4  1   3    1     1    0   181  260      1 5.560682\n303 338  35 54.000  4  2   1    0     1    0    29  621      0 6.431331\n304 339  33 19.000  4  1   1    0     1    0   139  199      1 5.293305\n305 340  31 12.000  4  3   2    0     1    0   152  565      0 6.336826\n306 341  37 24.000  3  2   5    1     1    0    90  183      1 5.209486\n307 342  32 37.000  3  3   4    0     1    0    62  122      1 4.804021\n308 343  33  9.000  3  2  13    0     1    0   110  170      1 5.135798\n309 344  36 18.000  3  1  14    1     1    0    15   15      1 2.708050\n310 345  26  4.000  1  1   5    0     1    0    68  268      1 5.590987\n311 346  35 15.000  3  1   0    1     1    0    19   79      1 4.369448\n312 347  25 19.000  1  3   6    1     0    0    23   23      1 3.135494\n313 348  33 26.000  1  3  30    0     0    0    92  100      1 4.605170\n314 349  36 28.000  2  3   8    0     0    0    94   98      1 4.584967\n315 350  38 14.000  3  3   6    0     0    0    31   81      1 4.394449\n316 351  36 15.000  3  2   3    1     0    0    28  546      0 6.302619\n317 352  36 18.000  2  3  10    0     0    0    58   58      1 4.060443\n318 353  35 29.000  3  3   6    0     0    0   113  569      0 6.343880\n319 354  35 10.000  3  1   3    1     0    0    70  575      0 6.354370\n320 356  39 16.000  2  3   4    0     0    0    90   91      1 4.510860\n321 357  37  0.000  4  3   6    0     0    0    55   57      1 4.043051\n322 358  30 31.000  2  3   5    0     0    0    89  499      1 6.212606\n323 359  26 33.000  1  3   7    1     0    0    71  123      1 4.812184\n324 360  39 21.000  4  1   5    0     0    0    84  143      1 4.962845\n325 362  32 18.000  3  1   4    0     0    0    78  471      1 6.154858\n326 363  26 37.800  3  1   4    1     0    0    60   74      1 4.304065\n327 364  33 20.000  2  3   6    0     0    0    82   85      1 4.442651\n328 365  36 11.000  4  2   5    0     0    0    81   95      1 4.553877\n329 366  42 26.000  2  3   3    0     1    0    35   36      1 3.583519\n330 367  37 43.000  1  3  22    0     1    0    16   19      1 2.944439\n331 368  37 12.000  2  2   1    1     1    0     7   38      1 3.637586\n332 369  32 22.000  3  1   4    1     1    0    30  539      0 6.289716\n333 370  23 36.000  4  1   3    1     1    0   106  567      0 6.340359\n334 371  21 16.000  4  1  10    0     1    0   174  186      1 5.225747\n335 372  23 41.000  3  1   1    0     1    0   144  546      0 6.302619\n336 373  34 16.000  4  2   1    0     1    0    24   24      1 3.178054\n337 374  33  8.000  4  2   3    0     1    0    17  540      0 6.291569\n338 375  33 10.000  3  1   4    1     1    0    97  157      1 5.056246\n339 376  26 18.000  3  3   0    0     1    0    26   86      1 4.454347\n340 377  28 27.000  4  1   2    1     1    0    31  231      1 5.442418\n341 379  27 28.000  1  3   3    0     0    0    14   14      1 2.639057\n342 380  22 23.000  1  3   2    0     0    0    75   75      1 4.317488\n343 381  31 32.000  3  3   6    1     0    0    20  147      1 4.990433\n344 382  29 23.100  3  1   4    0     0    0   104  105      1 4.653960\n345 383  44 11.000  4  3  12    0     0    0    85  324      1 5.780744\n346 384  26  7.000  3  1   0    1     0    0   110  538      0 6.287859\n347 385  44 24.000  2  3  16    0     0    0   100  300      1 5.703782\n348 386  34 12.000  1  3   1    0     0    0    73   73      1 4.290459\n349 387  36 25.000  2  3   6    0     0    0    65   65      1 4.174387\n350 388  43  4.000  2  3  20    0     0    0    75  568      1 6.342121\n351 389  37  5.000  3  1   1    0     0    0    83   84      1 4.430817\n352 390  44 13.000  4  2  17    0     1    0    15   22      1 3.091042\n353 391  31 17.000  1  3  30    1     1    0    44   44      1 3.784190\n354 392  24 24.000  2  1   3    0     1    0     7    7      1 1.945910\n355 394  37 32.000  3  3   4    0     1    0    20   21      1 3.044522\n356 395  41 19.000  1  3  12    1     1    0   175  537      0 6.285998\n357 396  32  9.000  3  1   3    1     1    0    71  186      1 5.225747\n358 397  23  6.000  3  1   2    0     1    0    26   40      1 3.688879\n359 398  33 10.000  2  3   3    0     1    0   161  287      1 5.659482\n360 399  43 11.000  4  1   9    0     1    0    36  538      0 6.287859\n361 400  33 16.000  4  3   8    0     1    0    30   30      1 3.401197\n362 401  41 25.000  4  2   3    0     1    0   179  516      1 6.246107\n363 402  41 17.000  2  3   2    0     1    0   199  268      1 5.590987\n364 403  37 24.000  2  3   3    0     1    0   182  568      0 6.342121\n365 404  26 27.000  1  1   3    0     0    0   112  131      1 4.875197\n366 405  33 24.000  1  3   6    0     0    0     8  399      1 5.988961\n367 406  30 26.000  3  1   2    0     0    0    18   78      1 4.356709\n368 407  33 17.000  4  1   6    1     0    0    20   80      1 4.382027\n369 408  33 26.000  2  3   3    0     0    0    88  102      1 4.624973\n370 410  37 13.000  3  1   6    0     0    0    88  124      1 4.820282\n371 411  44 11.000  2  3  20    0     0    0    76   80      1 4.382027\n372 412  20  8.000  4  1   1    0     0    0    22   23      1 3.135494\n373 413  33 12.000  1  3   4    0     0    0   110  274      1 5.613128\n374 415  36 31.000  2  3   3    0     0    0    85  459      1 6.129050\n375 416  34  8.400  2  3   3    0     0    0    10   10      1 2.302585\n376 417  35 10.000  1  3  17    0     1    0   157  176      1 5.170484\n377 418  38 16.000  2  3  26    0     1    0   133  332      1 5.805135\n378 419  24 13.000  3  1   3    0     1    0    83  119      1 4.779123\n379 420  24 18.000  3  1   4    0     1    0   152  217      1 5.379897\n380 421  32 13.000  3  1   4    0     1    0   169  285      1 5.652489\n381 422  35 11.000  4  2   3    0     1    0    89  576      0 6.356108\n382 423  33 21.000  1  3   5    0     1    0    92  106      1 4.663439\n383 424  29 37.000  2  2   4    1     1    0    21   81      1 4.394449\n384 425  42 32.000  2  3  30    0     1    0    31   47      1 3.850148\n385 426  23 33.000  4  1   1    0     1    0    31   76      1 4.330733\n386 427  28 11.000  4  3  16    0     1    0   133  348      1 5.852202\n387 429  43 29.000  2  3   4    0     1    0   153  306      1 5.723585\n388 430  33 23.000  2  1   0    0     0    0    90  192      1 5.257495\n389 431  37 15.000  1  3  20    0     0    0   102  216      1 5.375278\n390 432  49 22.000  2  3   7    0     0    0    85  189      1 5.241747\n391 434  36 25.000  3  1   1    1     0    0    89  193      1 5.262690\n392 435  27 30.000  1  3  13    0     0    0    28   28      1 3.332205\n393 436  35 23.000  1  3   1    0     0    0    90  150      1 5.010635\n394 437  25 10.000  3  2   3    0     0    0    84   99      1 4.595120\n395 438  33  8.000  1  3   3    0     0    0    85  510      0 6.234411\n396 439  34 16.000  1  3   7    0     0    0    36  306      1 5.723585\n397 440  38  9.000  1  3  10    1     0    0    74  101      1 4.615121\n398 441  36 12.158  2  3   0    1     0    0    42  102      1 4.624973\n399 442  27  5.000  1  3   1    0     0    0    90  510      0 6.234411\n400 444  40 19.000  1  3   0    1     0    0   108  503      0 6.220590\n401 445  32 23.000  3  3   3    0     0    1    49   52      1 3.951244\n402 446  38 28.000  3  3   1    1     0    1   219  547      0 6.304449\n403 447  38 16.000  1  3   6    0     0    1   108  168      1 5.123964\n404 448  23 25.000  4  1   0    0     0    1   178  461      1 6.133398\n405 449  26 22.000  4  2   2    0     0    1    42  538      0 6.287859\n406 450  36 28.000  2  3   7    0     0    1   182  349      1 5.855072\n407 451  30 28.000  4  1   5    0     0    1     6   44      1 3.784190\n408 452  31 18.000  4  2   3    0     1    1   351  548      0 6.306275\n409 453  23 15.000  3  1   1    0     1    1    12   12      1 2.484907\n410 454  43  9.000  1  3   0    1     1    1     6    6      1 1.791759\n411 455  24 26.000  4  1   1    0     1    1    91  575      0 6.354370\n412 456  42 19.000  4  1   1    0     1    1   245  589      0 6.378426\n413 457  35 26.000  4  2   1    0     1    1   372  408      1 6.011267\n414 458  21 10.000  4  1   0    0     1    1   218  232      1 5.446737\n415 459  45  1.000  4  2   0    1     1    1    46  143      1 4.962845\n416 460  43 30.000  2  3   6    0     1    1   363  582      0 6.366470\n417 461  24  7.000  4  1   0    1     1    1   133  134      1 4.897840\n418 462  37 11.000  3  3   1    0     1    1     7    7      1 1.945910\n419 463  40 10.000  4  2   0    0     1    1   112  548      0 6.306275\n420 464  27 11.000  3  2   2    0     0    1    21   81      1 4.394449\n421 465  29 11.000  2  3   1    0     0    1   169  170      1 5.135798\n422 466  34 12.000  4  3   6    0     0    1    28   29      1 3.367296\n423 467  29 29.000  3  3  20    0     0    1    47   78      1 4.356709\n424 468  35 27.000  1  3   5    0     0    1    20   81      1 4.394449\n425 469  39 20.000  1  3   4    0     1    1   352  369      1 5.910797\n426 470  41  9.000  4  2   0    0     1    1    66   69      1 4.234107\n427 471  37 18.000  4  1   6    1     1    1    55  115      1 4.744932\n428 472  30 10.000  3  2   7    0     1    1   344  361      1 5.888878\n429 473  31  1.000  4  1   0    0     1    1   153  245      1 5.501258\n430 474  40  5.000  4  2   8    0     0    1   184  233      1 5.451038\n431 475  32 20.000  4  1   0    0     0    1   183  227      1 5.424950\n432 476  32  7.000  4  2   3    1     0    1    22   97      1 4.574711\n433 477  27  7.000  4  1   0    0     0    1   183  547      0 6.304449\n434 478  23 26.000  3  1   0    0     0    1   140  224      1 5.411646\n435 479  23  4.000  4  1   2    0     0    1    19  211      1 5.351858\n436 480  43 11.000  2  3  12    0     0    1   184  220      1 5.393628\n437 481  24 20.000  4  1   0    0     0    1    50   54      1 3.988984\n438 482  36 11.000  4  1   2    1     0    1   132  192      1 5.257495\n439 483  29 31.000  1  3   1    0     0    1   128  138      1 4.927254\n440 484  39 13.000  4  2   1    0     1    1   107  107      1 4.672829\n441 485  23  6.000  4  1   0    0     1    1   368  597      0 6.391917\n442 486  27 17.000  3  3   4    0     1    1   219  226      1 5.420535\n443 487  26  5.000  4  2   5    0     1    1   374  434      1 6.073045\n444 488  26 27.000  3  1   1    1     1    1    92  106      1 4.663439\n445 489  25  9.000  4  1   0    0     1    1    45  180      1 5.192957\n446 490  34 10.000  3  1   0    0     1    1   366  557      0 6.322565\n447 491  45  5.000  4  3   2    0     1    1   368  556      0 6.320768\n448 492  23 17.000  4  1   1    0     0    1    78  619      0 6.428105\n449 493  26  7.000  4  1   0    0     0    1   184  546      0 6.302619\n450 495  24 27.000  1  2   2    0     0    1   187  233      1 5.451038\n451 496  30 23.000  2  3   2    1     0    1   101  102      1 4.624973\n452 497  22 26.000  3  1   0    0     0    1   141  548      0 6.306275\n453 498  25 10.000  3  1   1    0     0    1    24   99      1 4.595120\n454 499  30  8.400  3  2  40    0     0    1    36   36      1 3.583519\n455 501  33 23.000  4  1   0    1     1    1    56   78      1 4.356709\n456 502  34 15.000  3  2   8    0     1    1   367  502      1 6.218600\n457 503  29 24.000  3  1   2    0     1    1    70   71      1 4.262680\n458 504  39 33.000  4  2   6    0     1    1    58   59      1 4.077537\n459 506  26 21.000  3  1   4    0     1    1   366  533      0 6.278521\n460 507  32 23.000  2  3   6    0     1    1    10   10      1 2.302585\n461 508  42 23.100  1  3   2    0     0    1   214  274      1 5.613128\n462 509  39 25.000  1  2   8    0     0    1   197  255      1 5.541264\n463 510  36  2.000  4  1   0    1     0    1    89  503      0 6.220590\n464 511  22 20.000  3  1   1    0     0    1    56  256      1 5.545177\n465 512  27 23.000  4  1   1    0     0    1     9    9      1 2.197225\n466 514  28  9.000  4  1   0    0     0    1   186  386      1 5.955837\n467 515  36 28.000  3  2   1    0     1    1   303  547      0 6.304449\n468 516  31 13.000  3  1   3    0     1    1    32   45      1 3.806662\n469 517  27 22.000  3  2   4    0     1    1     8   58      1 4.060443\n470 518  23 17.000  3  1   1    0     1    1    63  124      1 4.820282\n471 519  24 20.000  3  2  20    0     0    1   108  540      0 6.291569\n472 520  38  5.000  3  2   1    0     0    1   183  243      1 5.493061\n473 521  25  8.000  4  1   1    0     1    1   151  549      0 6.308098\n474 522  26 20.000  3  1   0    0     0    1     7   12      1 2.484907\n475 523  22 34.000  3  1   2    0     0    1    38   51      1 3.931826\n476 524  33 13.000  4  1   2    0     1    1   176  562      0 6.331502\n477 525  30 23.000  1  3   7    0     1    1    93   94      1 4.543295\n478 526  45  8.000  4  3   3    0     0    1   200  204      1 5.318120\n479 527  24 15.000  3  2   0    0     0    1   178  238      1 5.472271\n480 528  27 22.000  4  1   0    0     1    1    78  140      1 4.941642\n481 529  36 19.000  4  2  10    0     1    1   119  120      1 4.787492\n482 530  38 23.000  4  2   2    1     0    1   154  154      1 5.036953\n483 531  31 17.000  2  3   2    0     1    1   163  177      1 5.176150\n484 532  40 22.000  4  2   7    0     1    1   118  119      1 4.779123\n485 533  22 12.000  3  1   0    1     1    1    76   83      1 4.418841\n486 534  31 13.000  4  1   0    1     1    1   116  130      1 4.867534\n487 536  39  7.000  3  3   3    1     0    1    88  159      1 5.068904\n488 538  33 14.000  3  1   1    0     0    1    33   33      1 3.496508\n489 539  27 10.000  3  3   2    0     1    1    70   72      1 4.276666\n490 540  37  7.000  4  1   2    1     1    1    68  161      1 5.081404\n491 541  35 16.000  4  2  25    0     0    1   191  191      1 5.252273\n492 542  25 11.000  3  1   5    0     0    1    35  181      1 5.198497\n493 543  27 11.000  3  1   1    1     1    1    32  546      0 6.302619\n494 544  34 15.000  4  1   0    0     0    1    28  540      0 6.291569\n495 545  30 15.000  3  1   3    0     0    1    15   76      1 4.330733\n496 546  35 17.000  1  3   7    0     0    1     7    7      1 1.945910\n497 547  34 23.000  4  1   0    0     0    1    43   44      1 3.784190\n498 548  25 23.000  3  2   5    0     0    1    89  103      1 4.634729\n499 549  34 18.000  3  1   1    0     0    1    38   79      1 4.369448\n500 550  24 23.000  4  3   3    0     0    1   204  339      1 5.826000\n501 551  24 20.000  4  1   2    0     0    1    76   90      1 4.499810\n502 552  40 36.000  4  1   3    0     0    1   195  542      0 6.295266\n503 553  33  9.000  3  1   1    1     0    1   184  384      1 5.950643\n504 554  38 14.000  4  2   1    1     1    1   254  255      1 5.541264\n505 555  32  1.000  3  1   0    0     1    1   371  431      1 6.066108\n506 556  33  3.000  4  1   1    0     0    1   196  587      0 6.375025\n507 557  28 40.000  3  1   2    1     0    1   198  198      1 5.288267\n508 558  31 13.000  3  3   2    0     0    1   170  551      0 6.311735\n509 559  31 39.000  2  3   4    0     1    1    50  110      1 4.700480\n510 560  33 24.000  4  1   0    0     1    1   163  541      0 6.293419\n511 561  24 26.000  3  1  11    0     0    1   182  242      1 5.488938\n512 562  26 18.000  3  1   3    0     0    1   150  537      0 6.285998\n513 563  31 19.000  2  3   7    0     1    1    34   56      1 4.025352\n514 564  40 14.700  2  3   4    0     1    1    34   34      1 3.526361\n515 566  34  2.000  3  1   3    0     1    1   366  549      0 6.308098\n516 567  30 11.000  3  2   7    0     0    1   133  133      1 4.890349\n517 568  36  0.000  3  2   3    0     0    1    69  226      1 5.420535\n518 569  38 17.000  2  3   6    0     1    1   366  401      1 5.993961\n519 570  31 20.000  1  3   6    1     1    1    14   14      1 2.639057\n520 571  27 22.000  2  2   2    0     0    1   184  548      0 6.306275\n521 572  32 21.000  1  3  15    0     1    1    89  224      1 5.411646\n522 573  35 23.000  3  1   5    1     0    1   183  540      0 6.291569\n523 574  44 29.000  2  3  13    0     0    1   177  237      1 5.468060\n524 575  31  5.000  2  3  10    0     1    1   154  354      1 5.869297\n525 576  28 23.000  3  2  20    0     0    1   123  123      1 4.812184\n526 577  40  8.000  4  2   1    0     0    1   146  170      1 5.135798\n527 578  25 12.000  3  1  10    1     1    1   203  203      1 5.313206\n528 579  32 10.000  1  3   6    0     1    1   360  360      1 5.886104\n529 580  29 15.750  4  1   2    0     0    1    79  139      1 4.934474\n530 581  40  2.000  2  2   5    0     1    1   201  215      1 5.370638\n531 582  27  9.000  4  2   0    0     1    1   129  129      1 4.859812\n532 583  26  2.000  3  1   1    0     1    1   365  396      1 5.981414\n533 584  34 15.000  3  1   4    1     1    1   159  547      0 6.304449\n534 585  49  4.000  4  2   2    0     0    1   177  547      0 6.304449\n535 586  21 25.000  1  3   1    0     1    1    71   71      1 4.262680\n536 587  39 23.000  3  3   2    0     1    1   108  168      1 5.123964\n537 588  33 15.000  4  2   4    0     1    1   198  228      1 5.429346\n538 589  32  3.000  3  1   1    0     1    1   372  551      0 6.311735\n539 590  35  9.000  4  2   6    0     0    1    25  654      0 6.483107\n540 591  31 20.000  4  1   0    1     1    1    48   51      1 3.931826\n541 592  28  5.000  4  1   3    0     0    1   191  548      0 6.306275\n542 593  27 29.000  3  2   5    0     1    1   171  231      1 5.442418\n543 594  29 21.000  2  1   1    1     1    1   145  280      1 5.634790\n544 595  30  1.000  2  1  20    0     0    1   183  184      1 5.214936\n545 596  27 18.000  4  1   3    1     0    1    72   86      1 4.454347\n546 598  40 15.000  4  2   1    0     1    1    44   46      1 3.828641\n547 599  37 20.000  3  1   2    1     1    1   140  200      1 5.298317\n548 600  33 10.000  4  1   0    0     0    1   184  244      1 5.497168\n549 601  28 20.000  4  1   2    0     0    1    94  182      1 5.204007\n550 602  40 15.000  4  2   8    0     1    1   296  296      1 5.690359\n551 603  48 20.000  4  1   0    1     0    1    23   24      1 3.178054\n552 604  38 25.000  3  1   1    0     0    1   128  142      1 4.955827\n553 605  35 13.000  4  1   0    0     0    1   106  120      1 4.787492\n554 606  37 13.000  4  2   0    0     0    1    46   47      1 3.850148\n555 607  25 15.000  3  1   0    1     1    1   150  519      1 6.251904\n556 608  26  8.000  4  1   2    0     1    1    48  248      1 5.513429\n557 609  30  9.000  3  3   3    0     0    1    29   31      1 3.433987\n558 610  28 16.000  4  2   2    0     0    1   179  567      0 6.340359\n559 611  23 11.000  2  3   4    0     0    1   170  353      1 5.866468\n560 612  36 31.000  4  1   1    0     1    1   365  458      1 6.126869\n561 613  36 13.000  4  2   4    0     1    1   400  554      0 6.317165\n562 614  24  5.000  4  1   0    1     0    1    56  116      1 4.753590\n563 615  33  9.000  3  2   5    0     0    1    24   74      1 4.304065\n564 616  38 15.000  4  2   6    0     0    1    10   10      1 2.302585\n565 617  41 20.000  3  3  21    0     1    1   354  355      1 5.872118\n566 618  31 21.000  3  1   0    1     1    1   232  232      1 5.446737\n567 619  31 23.000  4  2  11    0     1    1    54   68      1 4.219508\n568 620  37  5.000  4  1   0    1     1    1    48   48      1 3.871201\n569 621  37 17.000  4  2   4    1     0    1    57   60      1 4.094345\n570 622  33 13.000  4  1   0    0     0    1    46   50      1 3.912023\n571 624  53  9.000  4  2   6    0     0    1    39  126      1 4.836282\n572 625  37 20.000  2  3   4    0     0    1    17   18      1 2.890372\n573 626  28 10.000  4  2   3    0     1    1    21   35      1 3.555348\n574 627  35 17.000  1  3   2    0     0    1   184  379      1 5.937536\n575 628  46 31.500  1  3  15    1     1    1     9  377      1 5.932245\n           ND1          ND2      LNDT       FRAC IV3\n1    5.0000000  -8.04718956 0.6931472 0.68333333   1\n2    1.1111111  -0.11706724 2.1972246 0.13888889   0\n3    2.5000000  -2.29072683 1.3862944 0.03888889   1\n4    5.0000000  -8.04718956 0.6931472 0.73333333   1\n5    1.6666667  -0.85137604 1.7917595 0.96111111   0\n6    5.0000000  -8.04718956 0.6931472 0.08888889   1\n7    0.2857143   0.35793228 3.5553481 0.99444444   1\n8    3.3333333  -4.01324268 1.0986123 0.11666667   1\n9    2.5000000  -2.29072683 1.3862944 0.97777778   1\n10   1.2500000  -0.27892944 2.0794415 0.68888889   1\n11   1.1111111  -0.11706724 2.1972246 0.97777778   1\n12   5.0000000  -8.04718956 0.6931472 0.43888889   0\n13   3.3333333  -4.01324268 1.0986123 1.01111111   1\n14   1.1111111  -0.11706724 2.1972246 0.96666667   1\n15   5.0000000  -8.04718956 0.6931472 1.00555556   1\n16   2.5000000  -2.29072683 1.3862944 0.33888889   1\n17   1.4285714  -0.50953563 1.9459101 0.98333333   1\n18   5.0000000  -8.04718956 0.6931472 0.10555556   0\n19   0.6250000   0.29375227 2.7725887 0.15000000   0\n20   1.6666667  -0.85137604 1.7917595 0.97222222   1\n21   5.0000000  -8.04718956 0.6931472 0.13333333   0\n22   1.1111111  -0.11706724 2.1972246 0.23333333   1\n23  10.0000000 -23.02585093 0.0000000 0.53333333   0\n24   1.0000000   0.00000000 2.3025851 1.00000000   1\n25   1.4285714  -0.50953563 1.9459101 1.01111111   1\n26   1.6666667  -0.85137604 1.7917595 0.96666667   1\n27   2.5000000  -2.29072683 1.3862944 0.97777778   0\n28   1.2500000  -0.27892944 2.0794415 0.10000000   1\n29   1.0000000   0.00000000 2.3025851 1.04444444   1\n30   0.9090909   0.08664562 2.3978953 1.01111111   0\n31   5.0000000  -8.04718956 0.6931472 1.00000000   1\n32   5.0000000  -8.04718956 0.6931472 0.98888889   1\n33   1.6666667  -0.85137604 1.7917595 0.98888889   1\n34   1.4285714  -0.50953563 1.9459101 1.11111111   0\n35   2.5000000  -2.29072683 1.3862944 0.74444444   0\n36   1.2500000  -0.27892944 2.0794415 0.13888889   1\n37   5.0000000  -8.04718956 0.6931472 0.13333333   0\n38   5.0000000  -8.04718956 0.6931472 0.87777778   0\n39   3.3333333  -4.01324268 1.0986123 0.87777778   0\n40  10.0000000 -23.02585093 0.0000000 0.43333333   1\n41   3.3333333  -4.01324268 1.0986123 0.46666667   0\n42   1.4285714  -0.50953563 1.9459101 0.50555556   1\n43   5.0000000  -8.04718956 0.6931472 0.90000000   1\n44  10.0000000 -23.02585093 0.0000000 0.25000000   1\n45   5.0000000  -8.04718956 0.6931472 0.33888889   1\n46  10.0000000 -23.02585093 0.0000000 0.10555556   1\n47   3.3333333  -4.01324268 1.0986123 0.20555556   0\n48   1.1111111  -0.11706724 2.1972246 0.28333333   1\n49   5.0000000  -8.04718956 0.6931472 0.33333333   1\n50   0.3846154   0.36750440 3.2580965 0.98333333   1\n51  10.0000000 -23.02585093 0.0000000 0.23888889   0\n52   2.5000000  -2.29072683 1.3862944 0.11666667   0\n53   3.3333333  -4.01324268 1.0986123 0.97777778   1\n54   1.4285714  -0.50953563 1.9459101 1.06666667   1\n55   0.9090909   0.08664562 2.3978953 1.23333333   1\n56   0.9090909   0.08664562 2.3978953 0.42222222   1\n57   1.2500000  -0.27892944 2.0794415 0.16666667   0\n58   1.6666667  -0.85137604 1.7917595 0.55555556   0\n59   1.6666667  -0.85137604 1.7917595 0.67777778   1\n60   5.0000000  -8.04718956 0.6931472 0.34444444   0\n61   3.3333333  -4.01324268 1.0986123 0.12222222   0\n62   1.4285714  -0.50953563 1.9459101 1.00000000   1\n63   1.6666667  -0.85137604 1.7917595 0.12222222   1\n64   1.6666667  -0.85137604 1.7917595 0.51111111   1\n65  10.0000000 -23.02585093 0.0000000 0.42222222   1\n66  10.0000000 -23.02585093 0.0000000 1.00000000   1\n67   1.6666667  -0.85137604 1.7917595 0.97777778   1\n68   0.7692308   0.20181866 2.5649494 1.01111111   1\n69   1.4285714  -0.50953563 1.9459101 0.94444444   0\n70  10.0000000 -23.02585093 0.0000000 1.00000000   0\n71   1.6666667  -0.85137604 1.7917595 0.57777778   1\n72   2.5000000  -2.29072683 1.3862944 0.97777778   1\n73   0.7142857   0.24033731 2.6390573 0.47777778   1\n74   1.1111111  -0.11706724 2.1972246 0.41111111   1\n75  10.0000000 -23.02585093 0.0000000 0.96666667   0\n76   3.3333333  -4.01324268 1.0986123 0.22222222   0\n77   5.0000000  -8.04718956 0.6931472 0.10000000   0\n78   5.0000000  -8.04718956 0.6931472 0.94444444   1\n79   3.3333333  -4.01324268 1.0986123 0.20000000   1\n80   0.7692308   0.20181866 2.5649494 0.78888889   1\n81   3.3333333  -4.01324268 1.0986123 0.97777778   1\n82   5.0000000  -8.04718956 0.6931472 0.74444444   0\n83   2.0000000  -1.38629436 1.6094379 0.33333333   1\n84   5.0000000  -8.04718956 0.6931472 0.37777778   0\n85   1.0000000   0.00000000 2.3025851 1.01111111   0\n86   5.0000000  -8.04718956 0.6931472 1.01111111   1\n87   5.0000000  -8.04718956 0.6931472 0.81111111   0\n88   5.0000000  -8.04718956 0.6931472 0.22222222   1\n89   5.0000000  -8.04718956 0.6931472 0.98333333   0\n90   3.3333333  -4.01324268 1.0986123 1.00555556   0\n91   0.6250000   0.29375227 2.7725887 0.93333333   1\n92   1.6666667  -0.85137604 1.7917595 0.50000000   1\n93   1.6666667  -0.85137604 1.7917595 0.33888889   1\n94   2.5000000  -2.29072683 1.3862944 0.35000000   0\n95   1.1111111  -0.11706724 2.1972246 0.67222222   0\n96   5.0000000  -8.04718956 0.6931472 0.98888889   0\n97   2.5000000  -2.29072683 1.3862944 0.28333333   0\n98   5.0000000  -8.04718956 0.6931472 0.97777778   0\n99   5.0000000  -8.04718956 0.6931472 0.27777778   1\n100  2.5000000  -2.29072683 1.3862944 0.92222222   0\n101  3.3333333  -4.01324268 1.0986123 0.98888889   0\n102  5.0000000  -8.04718956 0.6931472 0.26666667   1\n103  0.4761905   0.35330350 3.0445224 0.07777778   1\n104  5.0000000  -8.04718956 0.6931472 0.94444444   0\n105 10.0000000 -23.02585093 0.0000000 0.98888889   0\n106  2.0000000  -1.38629436 1.6094379 1.01111111   1\n107 10.0000000 -23.02585093 0.0000000 0.98888889   0\n108 10.0000000 -23.02585093 0.0000000 0.91111111   0\n109  2.5000000  -2.29072683 1.3862944 0.93333333   1\n110  5.0000000  -8.04718956 0.6931472 0.33333333   1\n111  1.4285714  -0.50953563 1.9459101 0.07777778   0\n112  2.5000000  -2.29072683 1.3862944 0.93333333   1\n113 10.0000000 -23.02585093 0.0000000 0.77777778   1\n114  3.3333333  -4.01324268 1.0986123 0.84444444   0\n115  5.0000000  -8.04718956 0.6931472 0.98888889   0\n116  5.0000000  -8.04718956 0.6931472 0.98888889   1\n117  1.4285714  -0.50953563 1.9459101 0.48333333   1\n118 10.0000000 -23.02585093 0.0000000 0.97222222   0\n119  2.5000000  -2.29072683 1.3862944 0.48333333   0\n120  0.5882353   0.31213427 2.8332133 0.61111111   0\n121  5.0000000  -8.04718956 0.6931472 0.11666667   1\n122  2.5000000  -2.29072683 1.3862944 0.77222222   0\n123  0.6250000   0.29375227 2.7725887 1.00555556   1\n124  3.3333333  -4.01324268 1.0986123 0.18333333   1\n125  5.0000000  -8.04718956 0.6931472 0.21666667   1\n126  1.2500000  -0.27892944 2.0794415 0.02222222   1\n127  2.0000000  -1.38629436 1.6094379 1.02222222   1\n128  5.0000000  -8.04718956 0.6931472 0.68333333   0\n129  5.0000000  -8.04718956 0.6931472 0.97777778   0\n130  3.3333333  -4.01324268 1.0986123 0.96666667   1\n131  5.0000000  -8.04718956 0.6931472 1.00555556   1\n132  3.3333333  -4.01324268 1.0986123 0.62777778   1\n133  2.5000000  -2.29072683 1.3862944 0.91111111   1\n134  5.0000000  -8.04718956 0.6931472 0.93333333   1\n135  1.1111111  -0.11706724 2.1972246 0.88888889   1\n136  2.0000000  -1.38629436 1.6094379 1.01111111   1\n137  2.5000000  -2.29072683 1.3862944 1.07777778   0\n138  2.5000000  -2.29072683 1.3862944 0.56666667   0\n139  2.5000000  -2.29072683 1.3862944 1.01111111   1\n140  2.0000000  -1.38629436 1.6094379 1.00000000   0\n141  1.4285714  -0.50953563 1.9459101 0.51111111   1\n142  5.0000000  -8.04718956 0.6931472 0.84444444   0\n143  3.3333333  -4.01324268 1.0986123 0.83333333   1\n144  2.0000000  -1.38629436 1.6094379 1.01111111   1\n145  1.6666667  -0.85137604 1.7917595 1.00000000   1\n146  1.4285714  -0.50953563 1.9459101 0.03333333   1\n147  2.5000000  -2.29072683 1.3862944 0.04444444   1\n148  5.0000000  -8.04718956 0.6931472 0.18333333   0\n149  1.1111111  -0.11706724 2.1972246 0.17222222   1\n150  5.0000000  -8.04718956 0.6931472 0.96666667   0\n151  3.3333333  -4.01324268 1.0986123 0.18888889   0\n152  2.5000000  -2.29072683 1.3862944 0.33333333   0\n153  1.6666667  -0.85137604 1.7917595 0.43333333   0\n154  3.3333333  -4.01324268 1.0986123 1.01111111   0\n155  5.0000000  -8.04718956 0.6931472 1.01111111   0\n156 10.0000000 -23.02585093 0.0000000 0.43333333   0\n157  5.0000000  -8.04718956 0.6931472 0.30555556   0\n158  3.3333333  -4.01324268 1.0986123 1.23888889   0\n159  0.4761905   0.35330350 3.0445224 0.13888889   0\n160 10.0000000 -23.02585093 0.0000000 0.35000000   0\n161  5.0000000  -8.04718956 0.6931472 0.73888889   0\n162  1.6666667  -0.85137604 1.7917595 0.85555556   1\n163  3.3333333  -4.01324268 1.0986123 0.38888889   0\n164  5.0000000  -8.04718956 0.6931472 0.36666667   0\n165  2.0000000  -1.38629436 1.6094379 0.22222222   1\n166  5.0000000  -8.04718956 0.6931472 0.41666667   0\n167  3.3333333  -4.01324268 1.0986123 1.03888889   0\n168  3.3333333  -4.01324268 1.0986123 1.01666667   0\n169  1.1111111  -0.11706724 2.1972246 1.01111111   0\n170  3.3333333  -4.01324268 1.0986123 1.06666667   0\n171  2.0000000  -1.38629436 1.6094379 0.90000000   1\n172  3.3333333  -4.01324268 1.0986123 1.07222222   1\n173  3.3333333  -4.01324268 1.0986123 0.61666667   0\n174  1.1111111  -0.11706724 2.1972246 1.01111111   1\n175  2.0000000  -1.38629436 1.6094379 1.00000000   0\n176 10.0000000 -23.02585093 0.0000000 0.51666667   0\n177  1.2500000  -0.27892944 2.0794415 0.92777778   0\n178  3.3333333  -4.01324268 1.0986123 1.08888889   0\n179  2.0000000  -1.38629436 1.6094379 0.58888889   0\n180  1.4285714  -0.50953563 1.9459101 0.87777778   1\n181  5.0000000  -8.04718956 0.6931472 1.01111111   0\n182  2.0000000  -1.38629436 1.6094379 0.98888889   0\n183  3.3333333  -4.01324268 1.0986123 0.98888889   0\n184  0.9090909   0.08664562 2.3978953 0.97777778   1\n185 10.0000000 -23.02585093 0.0000000 1.05555556   0\n186 10.0000000 -23.02585093 0.0000000 0.05555556   1\n187  1.1111111  -0.11706724 2.1972246 0.35555556   1\n188  3.3333333  -4.01324268 1.0986123 1.02222222   0\n189  2.5000000  -2.29072683 1.3862944 0.73333333   0\n190  1.1111111  -0.11706724 2.1972246 1.00000000   0\n191  2.5000000  -2.29072683 1.3862944 1.03333333   0\n192  3.3333333  -4.01324268 1.0986123 0.98888889   0\n193 10.0000000 -23.02585093 0.0000000 1.01111111   0\n194  1.1111111  -0.11706724 2.1972246 0.62222222   1\n195  1.2500000  -0.27892944 2.0794415 1.00000000   1\n196  2.5000000  -2.29072683 1.3862944 0.81111111   0\n197  1.6666667  -0.85137604 1.7917595 0.94444444   0\n198  2.5000000  -2.29072683 1.3862944 0.25555556   1\n199  2.5000000  -2.29072683 1.3862944 0.94444444   1\n200  1.6666667  -0.85137604 1.7917595 1.00000000   1\n201  2.0000000  -1.38629436 1.6094379 0.58888889   0\n202  3.3333333  -4.01324268 1.0986123 1.06666667   1\n203  3.3333333  -4.01324268 1.0986123 0.92222222   0\n204  1.1111111  -0.11706724 2.1972246 0.60000000   1\n205  1.0000000   0.00000000 2.3025851 0.87777778   0\n206  2.5000000  -2.29072683 1.3862944 0.90000000   1\n207  2.0000000  -1.38629436 1.6094379 0.20000000   0\n208  2.0000000  -1.38629436 1.6094379 1.02222222   1\n209  0.9090909   0.08664562 2.3978953 0.21666667   0\n210 10.0000000 -23.02585093 0.0000000 0.98333333   0\n211  3.3333333  -4.01324268 1.0986123 0.67777778   1\n212  2.0000000  -1.38629436 1.6094379 0.98888889   0\n213  1.2500000  -0.27892944 2.0794415 0.96111111   0\n214  0.3125000   0.36348463 3.4657359 0.29444444   1\n215  1.6666667  -0.85137604 1.7917595 0.52222222   1\n216  3.3333333  -4.01324268 1.0986123 0.90555556   0\n217  1.4285714  -0.50953563 1.9459101 0.88888889   0\n218  1.4285714  -0.50953563 1.9459101 0.33888889   0\n219  0.7692308   0.20181866 2.5649494 0.22777778   0\n220  1.6666667  -0.85137604 1.7917595 0.29444444   1\n221  2.0000000  -1.38629436 1.6094379 0.29444444   1\n222  0.8333333   0.15193463 2.4849066 0.07222222   0\n223  2.5000000  -2.29072683 1.3862944 1.01666667   0\n224  2.0000000  -1.38629436 1.6094379 1.01111111   1\n225  2.5000000  -2.29072683 1.3862944 1.01666667   1\n226  1.0000000   0.00000000 2.3025851 0.35000000   1\n227  1.2500000  -0.27892944 2.0794415 0.61666667   1\n228  3.3333333  -4.01324268 1.0986123 0.96666667   0\n229  2.5000000  -2.29072683 1.3862944 0.96111111   1\n230  3.3333333  -4.01324268 1.0986123 0.66111111   0\n231  2.5000000  -2.29072683 1.3862944 1.00000000   0\n232  3.3333333  -4.01324268 1.0986123 0.54444444   0\n233  2.0000000  -1.38629436 1.6094379 0.27777778   0\n234  5.0000000  -8.04718956 0.6931472 0.98888889   0\n235  0.5882353   0.31213427 2.8332133 0.55555556   1\n236  2.0000000  -1.38629436 1.6094379 0.51666667   0\n237  2.5000000  -2.29072683 1.3862944 0.91666667   1\n238 10.0000000 -23.02585093 0.0000000 0.51666667   0\n239  1.6666667  -0.85137604 1.7917595 0.48888889   1\n240  1.6666667  -0.85137604 1.7917595 0.85555556   0\n241 10.0000000 -23.02585093 0.0000000 1.01111111   0\n242  3.3333333  -4.01324268 1.0986123 1.05555556   0\n243  5.0000000  -8.04718956 0.6931472 0.91111111   1\n244  1.6666667  -0.85137604 1.7917595 0.84444444   0\n245  2.5000000  -2.29072683 1.3862944 0.05555556   0\n246  1.2500000  -0.27892944 2.0794415 0.76666667   1\n247  0.7692308   0.20181866 2.5649494 1.00000000   0\n248  1.1111111  -0.11706724 2.1972246 0.21111111   0\n249  2.0000000  -1.38629436 1.6094379 0.66666667   0\n250 10.0000000 -23.02585093 0.0000000 0.76666667   0\n251  1.1111111  -0.11706724 2.1972246 0.94444444   0\n252  2.5000000  -2.29072683 1.3862944 1.02222222   1\n253  2.5000000  -2.29072683 1.3862944 0.61111111   0\n254  1.0000000   0.00000000 2.3025851 0.22222222   1\n255  3.3333333  -4.01324268 1.0986123 0.96666667   1\n256 10.0000000 -23.02585093 0.0000000 1.01111111   0\n257 10.0000000 -23.02585093 0.0000000 0.10000000   0\n258  1.4285714  -0.50953563 1.9459101 0.24444444   1\n259  5.0000000  -8.04718956 0.6931472 0.96666667   0\n260  0.9090909   0.08664562 2.3978953 0.95555556   1\n261  1.2500000  -0.27892944 2.0794415 0.94444444   1\n262 10.0000000 -23.02585093 0.0000000 0.92222222   0\n263  1.4285714  -0.50953563 1.9459101 0.92222222   0\n264  1.4285714  -0.50953563 1.9459101 1.02222222   0\n265  2.5000000  -2.29072683 1.3862944 0.94444444   0\n266  1.4285714  -0.50953563 1.9459101 0.40000000   0\n267  3.3333333  -4.01324268 1.0986123 0.96666667   1\n268 10.0000000 -23.02585093 0.0000000 0.31111111   0\n269  1.6666667  -0.85137604 1.7917595 0.52222222   1\n270 10.0000000 -23.02585093 0.0000000 0.41111111   1\n271  5.0000000  -8.04718956 0.6931472 1.03333333   1\n272  3.3333333  -4.01324268 1.0986123 0.98888889   0\n273  2.5000000  -2.29072683 1.3862944 0.46666667   1\n274 10.0000000 -23.02585093 0.0000000 0.07222222   0\n275  0.7692308   0.20181866 2.5649494 0.47222222   1\n276  2.5000000  -2.29072683 1.3862944 0.05000000   0\n277  2.0000000  -1.38629436 1.6094379 0.90000000   0\n278  0.4761905   0.35330350 3.0445224 0.25555556   1\n279  0.4761905   0.35330350 3.0445224 0.28888889   1\n280  1.0000000   0.00000000 2.3025851 0.93333333   0\n281  1.6666667  -0.85137604 1.7917595 0.25555556   1\n282  3.3333333  -4.01324268 1.0986123 0.95555556   0\n283  2.0000000  -1.38629436 1.6094379 1.00000000   1\n284  1.1111111  -0.11706724 2.1972246 0.81111111   1\n285  1.2500000  -0.27892944 2.0794415 0.84444444   1\n286  5.0000000  -8.04718956 0.6931472 0.10000000   0\n287  2.5000000  -2.29072683 1.3862944 0.52222222   0\n288  3.3333333  -4.01324268 1.0986123 0.42222222   0\n289  2.5000000  -2.29072683 1.3862944 0.22222222   0\n290  2.5000000  -2.29072683 1.3862944 0.97777778   0\n291 10.0000000 -23.02585093 0.0000000 0.57777778   0\n292  0.6250000   0.29375227 2.7725887 0.02777778   1\n293  1.4285714  -0.50953563 1.9459101 0.99444444   0\n294  1.1111111  -0.11706724 2.1972246 0.19444444   1\n295  5.0000000  -8.04718956 0.6931472 0.13333333   0\n296  0.2777778   0.35581496 3.5835189 0.45555556   1\n297  2.5000000  -2.29072683 1.3862944 0.15555556   0\n298 10.0000000 -23.02585093 0.0000000 0.45000000   0\n299  3.3333333  -4.01324268 1.0986123 0.02222222   0\n300  2.0000000  -1.38629436 1.6094379 0.53888889   1\n301 10.0000000 -23.02585093 0.0000000 0.43333333   0\n302  2.5000000  -2.29072683 1.3862944 1.00555556   0\n303  5.0000000  -8.04718956 0.6931472 0.16111111   0\n304  5.0000000  -8.04718956 0.6931472 0.77222222   0\n305  3.3333333  -4.01324268 1.0986123 0.84444444   1\n306  1.6666667  -0.85137604 1.7917595 0.50000000   0\n307  2.0000000  -1.38629436 1.6094379 0.34444444   1\n308  0.7142857   0.24033731 2.6390573 0.61111111   0\n309  0.6666667   0.27031007 2.7080502 0.08333333   0\n310  1.6666667  -0.85137604 1.7917595 0.37777778   0\n311 10.0000000 -23.02585093 0.0000000 0.10555556   0\n312  1.4285714  -0.50953563 1.9459101 0.25555556   1\n313  0.3225806   0.36496842 3.4339872 1.02222222   1\n314  1.1111111  -0.11706724 2.1972246 1.04444444   1\n315  1.4285714  -0.50953563 1.9459101 0.34444444   1\n316  2.5000000  -2.29072683 1.3862944 0.31111111   0\n317  0.9090909   0.08664562 2.3978953 0.64444444   1\n318  1.4285714  -0.50953563 1.9459101 1.25555556   1\n319  2.5000000  -2.29072683 1.3862944 0.77777778   0\n320  2.0000000  -1.38629436 1.6094379 1.00000000   1\n321  1.4285714  -0.50953563 1.9459101 0.61111111   1\n322  1.6666667  -0.85137604 1.7917595 0.98888889   1\n323  1.2500000  -0.27892944 2.0794415 0.78888889   1\n324  1.6666667  -0.85137604 1.7917595 0.93333333   0\n325  2.0000000  -1.38629436 1.6094379 0.86666667   0\n326  2.0000000  -1.38629436 1.6094379 0.66666667   0\n327  1.4285714  -0.50953563 1.9459101 0.91111111   1\n328  1.6666667  -0.85137604 1.7917595 0.90000000   0\n329  2.5000000  -2.29072683 1.3862944 0.19444444   1\n330  0.4347826   0.36213440 3.1354942 0.08888889   1\n331  5.0000000  -8.04718956 0.6931472 0.03888889   0\n332  2.0000000  -1.38629436 1.6094379 0.16666667   0\n333  2.5000000  -2.29072683 1.3862944 0.58888889   0\n334  0.9090909   0.08664562 2.3978953 0.96666667   0\n335  5.0000000  -8.04718956 0.6931472 0.80000000   0\n336  5.0000000  -8.04718956 0.6931472 0.13333333   0\n337  2.5000000  -2.29072683 1.3862944 0.09444444   0\n338  2.0000000  -1.38629436 1.6094379 0.53888889   0\n339 10.0000000 -23.02585093 0.0000000 0.14444444   1\n340  3.3333333  -4.01324268 1.0986123 0.17222222   0\n341  2.5000000  -2.29072683 1.3862944 0.15555556   1\n342  3.3333333  -4.01324268 1.0986123 0.83333333   1\n343  1.4285714  -0.50953563 1.9459101 0.22222222   1\n344  2.0000000  -1.38629436 1.6094379 1.15555556   0\n345  0.7692308   0.20181866 2.5649494 0.94444444   1\n346 10.0000000 -23.02585093 0.0000000 1.22222222   0\n347  0.5882353   0.31213427 2.8332133 1.11111111   1\n348  5.0000000  -8.04718956 0.6931472 0.81111111   1\n349  1.4285714  -0.50953563 1.9459101 0.72222222   1\n350  0.4761905   0.35330350 3.0445224 0.83333333   1\n351  5.0000000  -8.04718956 0.6931472 0.92222222   0\n352  0.5555556   0.32654815 2.8903718 0.08333333   0\n353  0.3225806   0.36496842 3.4339872 0.24444444   1\n354  2.5000000  -2.29072683 1.3862944 0.03888889   0\n355  2.0000000  -1.38629436 1.6094379 0.11111111   1\n356  0.7692308   0.20181866 2.5649494 0.97222222   1\n357  2.5000000  -2.29072683 1.3862944 0.39444444   0\n358  3.3333333  -4.01324268 1.0986123 0.14444444   0\n359  2.5000000  -2.29072683 1.3862944 0.89444444   1\n360  1.0000000   0.00000000 2.3025851 0.20000000   0\n361  1.1111111  -0.11706724 2.1972246 0.16666667   1\n362  2.5000000  -2.29072683 1.3862944 0.99444444   0\n363  3.3333333  -4.01324268 1.0986123 1.10555556   1\n364  2.5000000  -2.29072683 1.3862944 1.01111111   1\n365  2.5000000  -2.29072683 1.3862944 1.24444444   0\n366  1.4285714  -0.50953563 1.9459101 0.08888889   1\n367  3.3333333  -4.01324268 1.0986123 0.20000000   0\n368  1.4285714  -0.50953563 1.9459101 0.22222222   0\n369  2.5000000  -2.29072683 1.3862944 0.97777778   1\n370  1.4285714  -0.50953563 1.9459101 0.97777778   0\n371  0.4761905   0.35330350 3.0445224 0.84444444   1\n372  5.0000000  -8.04718956 0.6931472 0.24444444   0\n373  2.0000000  -1.38629436 1.6094379 1.22222222   1\n374  2.5000000  -2.29072683 1.3862944 0.94444444   1\n375  2.5000000  -2.29072683 1.3862944 0.11111111   1\n376  0.5555556   0.32654815 2.8903718 0.87222222   1\n377  0.3703704   0.36787103 3.2958369 0.73888889   1\n378  2.5000000  -2.29072683 1.3862944 0.46111111   0\n379  2.0000000  -1.38629436 1.6094379 0.84444444   0\n380  2.0000000  -1.38629436 1.6094379 0.93888889   0\n381  2.5000000  -2.29072683 1.3862944 0.49444444   0\n382  1.6666667  -0.85137604 1.7917595 0.51111111   1\n383  2.0000000  -1.38629436 1.6094379 0.11666667   0\n384  0.3225806   0.36496842 3.4339872 0.17222222   1\n385  5.0000000  -8.04718956 0.6931472 0.17222222   0\n386  0.5882353   0.31213427 2.8332133 0.73888889   1\n387  2.0000000  -1.38629436 1.6094379 0.85000000   1\n388 10.0000000 -23.02585093 0.0000000 1.00000000   0\n389  0.4761905   0.35330350 3.0445224 1.13333333   1\n390  1.2500000  -0.27892944 2.0794415 0.94444444   1\n391  5.0000000  -8.04718956 0.6931472 0.98888889   0\n392  0.7142857   0.24033731 2.6390573 0.31111111   1\n393  5.0000000  -8.04718956 0.6931472 1.00000000   1\n394  2.5000000  -2.29072683 1.3862944 0.93333333   0\n395  2.5000000  -2.29072683 1.3862944 0.94444444   1\n396  1.2500000  -0.27892944 2.0794415 0.40000000   1\n397  0.9090909   0.08664562 2.3978953 0.82222222   1\n398 10.0000000 -23.02585093 0.0000000 0.46666667   1\n399  5.0000000  -8.04718956 0.6931472 1.00000000   1\n400 10.0000000 -23.02585093 0.0000000 1.20000000   1\n401  2.5000000  -2.29072683 1.3862944 0.54444444   1\n402  5.0000000  -8.04718956 0.6931472 2.43333333   1\n403  1.4285714  -0.50953563 1.9459101 1.20000000   1\n404 10.0000000 -23.02585093 0.0000000 1.97777778   0\n405  3.3333333  -4.01324268 1.0986123 0.46666667   0\n406  1.2500000  -0.27892944 2.0794415 2.02222222   1\n407  1.6666667  -0.85137604 1.7917595 0.06666667   0\n408  2.5000000  -2.29072683 1.3862944 1.95000000   0\n409  5.0000000  -8.04718956 0.6931472 0.06666667   0\n410 10.0000000 -23.02585093 0.0000000 0.03333333   1\n411  5.0000000  -8.04718956 0.6931472 0.50555556   0\n412  5.0000000  -8.04718956 0.6931472 1.36111111   0\n413  5.0000000  -8.04718956 0.6931472 2.06666667   0\n414 10.0000000 -23.02585093 0.0000000 1.21111111   0\n415 10.0000000 -23.02585093 0.0000000 0.25555556   0\n416  1.4285714  -0.50953563 1.9459101 2.01666667   1\n417 10.0000000 -23.02585093 0.0000000 0.73888889   0\n418  5.0000000  -8.04718956 0.6931472 0.03888889   1\n419 10.0000000 -23.02585093 0.0000000 0.62222222   0\n420  3.3333333  -4.01324268 1.0986123 0.23333333   0\n421  5.0000000  -8.04718956 0.6931472 1.87777778   1\n422  1.4285714  -0.50953563 1.9459101 0.31111111   1\n423  0.4761905   0.35330350 3.0445224 0.52222222   1\n424  1.6666667  -0.85137604 1.7917595 0.22222222   1\n425  2.0000000  -1.38629436 1.6094379 1.95555556   1\n426 10.0000000 -23.02585093 0.0000000 0.36666667   0\n427  1.4285714  -0.50953563 1.9459101 0.30555556   0\n428  1.2500000  -0.27892944 2.0794415 1.91111111   0\n429 10.0000000 -23.02585093 0.0000000 0.85000000   0\n430  1.1111111  -0.11706724 2.1972246 2.04444444   0\n431 10.0000000 -23.02585093 0.0000000 2.03333333   0\n432  2.5000000  -2.29072683 1.3862944 0.24444444   0\n433 10.0000000 -23.02585093 0.0000000 2.03333333   0\n434 10.0000000 -23.02585093 0.0000000 1.55555556   0\n435  3.3333333  -4.01324268 1.0986123 0.21111111   0\n436  0.7692308   0.20181866 2.5649494 2.04444444   1\n437 10.0000000 -23.02585093 0.0000000 0.55555556   0\n438  3.3333333  -4.01324268 1.0986123 1.46666667   0\n439  5.0000000  -8.04718956 0.6931472 1.42222222   1\n440  5.0000000  -8.04718956 0.6931472 0.59444444   0\n441 10.0000000 -23.02585093 0.0000000 2.04444444   0\n442  2.0000000  -1.38629436 1.6094379 1.21666667   1\n443  1.6666667  -0.85137604 1.7917595 2.07777778   0\n444  5.0000000  -8.04718956 0.6931472 0.51111111   0\n445 10.0000000 -23.02585093 0.0000000 0.25000000   0\n446 10.0000000 -23.02585093 0.0000000 2.03333333   0\n447  3.3333333  -4.01324268 1.0986123 2.04444444   1\n448  5.0000000  -8.04718956 0.6931472 0.86666667   0\n449 10.0000000 -23.02585093 0.0000000 2.04444444   0\n450  3.3333333  -4.01324268 1.0986123 2.07777778   0\n451  3.3333333  -4.01324268 1.0986123 1.12222222   1\n452 10.0000000 -23.02585093 0.0000000 1.56666667   0\n453  5.0000000  -8.04718956 0.6931472 0.26666667   0\n454  0.2439024   0.34414316 3.7135721 0.40000000   0\n455 10.0000000 -23.02585093 0.0000000 0.31111111   0\n456  1.1111111  -0.11706724 2.1972246 2.03888889   0\n457  3.3333333  -4.01324268 1.0986123 0.38888889   0\n458  1.4285714  -0.50953563 1.9459101 0.32222222   0\n459  2.0000000  -1.38629436 1.6094379 2.03333333   0\n460  1.4285714  -0.50953563 1.9459101 0.05555556   1\n461  3.3333333  -4.01324268 1.0986123 2.37777778   1\n462  1.1111111  -0.11706724 2.1972246 2.18888889   0\n463 10.0000000 -23.02585093 0.0000000 0.98888889   0\n464  5.0000000  -8.04718956 0.6931472 0.62222222   0\n465  5.0000000  -8.04718956 0.6931472 0.10000000   0\n466 10.0000000 -23.02585093 0.0000000 2.06666667   0\n467  5.0000000  -8.04718956 0.6931472 1.68333333   0\n468  2.5000000  -2.29072683 1.3862944 0.17777778   0\n469  2.0000000  -1.38629436 1.6094379 0.04444444   0\n470  5.0000000  -8.04718956 0.6931472 0.35000000   0\n471  0.4761905   0.35330350 3.0445224 1.20000000   0\n472  5.0000000  -8.04718956 0.6931472 2.03333333   0\n473  5.0000000  -8.04718956 0.6931472 0.83888889   0\n474 10.0000000 -23.02585093 0.0000000 0.07777778   0\n475  3.3333333  -4.01324268 1.0986123 0.42222222   0\n476  3.3333333  -4.01324268 1.0986123 0.97777778   0\n477  1.2500000  -0.27892944 2.0794415 0.51666667   1\n478  2.5000000  -2.29072683 1.3862944 2.22222222   1\n479 10.0000000 -23.02585093 0.0000000 1.97777778   0\n480 10.0000000 -23.02585093 0.0000000 0.43333333   0\n481  0.9090909   0.08664562 2.3978953 0.66111111   0\n482  3.3333333  -4.01324268 1.0986123 1.71111111   0\n483  3.3333333  -4.01324268 1.0986123 0.90555556   1\n484  1.2500000  -0.27892944 2.0794415 0.65555556   0\n485 10.0000000 -23.02585093 0.0000000 0.42222222   0\n486 10.0000000 -23.02585093 0.0000000 0.64444444   0\n487  2.5000000  -2.29072683 1.3862944 0.97777778   1\n488  5.0000000  -8.04718956 0.6931472 0.36666667   0\n489  3.3333333  -4.01324268 1.0986123 0.38888889   1\n490  3.3333333  -4.01324268 1.0986123 0.37777778   0\n491  0.3846154   0.36750440 3.2580965 2.12222222   0\n492  1.6666667  -0.85137604 1.7917595 0.38888889   0\n493  5.0000000  -8.04718956 0.6931472 0.17777778   0\n494 10.0000000 -23.02585093 0.0000000 0.31111111   0\n495  2.5000000  -2.29072683 1.3862944 0.16666667   0\n496  1.2500000  -0.27892944 2.0794415 0.07777778   1\n497 10.0000000 -23.02585093 0.0000000 0.47777778   0\n498  1.6666667  -0.85137604 1.7917595 0.98888889   0\n499  5.0000000  -8.04718956 0.6931472 0.42222222   0\n500  2.5000000  -2.29072683 1.3862944 2.26666667   1\n501  3.3333333  -4.01324268 1.0986123 0.84444444   0\n502  2.5000000  -2.29072683 1.3862944 2.16666667   0\n503  5.0000000  -8.04718956 0.6931472 2.04444444   0\n504  5.0000000  -8.04718956 0.6931472 1.41111111   0\n505 10.0000000 -23.02585093 0.0000000 2.06111111   0\n506  5.0000000  -8.04718956 0.6931472 2.17777778   0\n507  3.3333333  -4.01324268 1.0986123 2.20000000   0\n508  3.3333333  -4.01324268 1.0986123 1.88888889   1\n509  2.0000000  -1.38629436 1.6094379 0.27777778   1\n510 10.0000000 -23.02585093 0.0000000 0.90555556   0\n511  0.8333333   0.15193463 2.4849066 2.02222222   0\n512  2.5000000  -2.29072683 1.3862944 1.66666667   0\n513  1.2500000  -0.27892944 2.0794415 0.18888889   1\n514  2.0000000  -1.38629436 1.6094379 0.18888889   1\n515  2.5000000  -2.29072683 1.3862944 2.03333333   0\n516  1.2500000  -0.27892944 2.0794415 1.47777778   0\n517  2.5000000  -2.29072683 1.3862944 0.76666667   0\n518  1.4285714  -0.50953563 1.9459101 2.03333333   1\n519  1.4285714  -0.50953563 1.9459101 0.07777778   1\n520  3.3333333  -4.01324268 1.0986123 2.04444444   0\n521  0.6250000   0.29375227 2.7725887 0.49444444   1\n522  1.6666667  -0.85137604 1.7917595 2.03333333   0\n523  0.7142857   0.24033731 2.6390573 1.96666667   1\n524  0.9090909   0.08664562 2.3978953 0.85555556   1\n525  0.4761905   0.35330350 3.0445224 1.36666667   0\n526  5.0000000  -8.04718956 0.6931472 1.62222222   0\n527  0.9090909   0.08664562 2.3978953 1.12777778   0\n528  1.4285714  -0.50953563 1.9459101 2.00000000   1\n529  3.3333333  -4.01324268 1.0986123 0.87777778   0\n530  1.6666667  -0.85137604 1.7917595 1.11666667   0\n531 10.0000000 -23.02585093 0.0000000 0.71666667   0\n532  5.0000000  -8.04718956 0.6931472 2.02777778   0\n533  2.0000000  -1.38629436 1.6094379 0.88333333   0\n534  3.3333333  -4.01324268 1.0986123 1.96666667   0\n535  5.0000000  -8.04718956 0.6931472 0.39444444   1\n536  3.3333333  -4.01324268 1.0986123 0.60000000   1\n537  2.0000000  -1.38629436 1.6094379 1.10000000   0\n538  5.0000000  -8.04718956 0.6931472 2.06666667   0\n539  1.4285714  -0.50953563 1.9459101 0.27777778   0\n540 10.0000000 -23.02585093 0.0000000 0.26666667   0\n541  2.5000000  -2.29072683 1.3862944 2.12222222   0\n542  1.6666667  -0.85137604 1.7917595 0.95000000   0\n543  5.0000000  -8.04718956 0.6931472 0.80555556   0\n544  0.4761905   0.35330350 3.0445224 2.03333333   0\n545  2.5000000  -2.29072683 1.3862944 0.80000000   0\n546  5.0000000  -8.04718956 0.6931472 0.24444444   0\n547  3.3333333  -4.01324268 1.0986123 0.77777778   0\n548 10.0000000 -23.02585093 0.0000000 2.04444444   0\n549  3.3333333  -4.01324268 1.0986123 1.04444444   0\n550  1.1111111  -0.11706724 2.1972246 1.64444444   0\n551 10.0000000 -23.02585093 0.0000000 0.25555556   0\n552  5.0000000  -8.04718956 0.6931472 1.42222222   0\n553 10.0000000 -23.02585093 0.0000000 1.17777778   0\n554 10.0000000 -23.02585093 0.0000000 0.51111111   0\n555 10.0000000 -23.02585093 0.0000000 0.83333333   0\n556  3.3333333  -4.01324268 1.0986123 0.26666667   0\n557  2.5000000  -2.29072683 1.3862944 0.32222222   1\n558  3.3333333  -4.01324268 1.0986123 1.98888889   0\n559  2.0000000  -1.38629436 1.6094379 1.88888889   1\n560  5.0000000  -8.04718956 0.6931472 2.02777778   0\n561  2.0000000  -1.38629436 1.6094379 2.22222222   0\n562 10.0000000 -23.02585093 0.0000000 0.62222222   0\n563  1.6666667  -0.85137604 1.7917595 0.26666667   0\n564  1.4285714  -0.50953563 1.9459101 0.11111111   0\n565  0.4545455   0.35838971 3.0910425 1.96666667   1\n566 10.0000000 -23.02585093 0.0000000 1.28888889   0\n567  0.8333333   0.15193463 2.4849066 0.30000000   0\n568 10.0000000 -23.02585093 0.0000000 0.26666667   0\n569  2.0000000  -1.38629436 1.6094379 0.63333333   0\n570 10.0000000 -23.02585093 0.0000000 0.51111111   0\n571  1.4285714  -0.50953563 1.9459101 0.43333333   0\n572  2.0000000  -1.38629436 1.6094379 0.18888889   1\n573  2.5000000  -2.29072683 1.3862944 0.11666667   0\n574  3.3333333  -4.01324268 1.0986123 2.04444444   1\n575  0.6250000   0.29375227 2.7725887 0.05000000   1\n\n\n\nglimpse(uis)\n\nRows: 575\nColumns: 18\n$ ID     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, …\n$ AGE    &lt;dbl&gt; 39, 33, 33, 32, 24, 30, 39, 27, 40, 36, 38, 29, 32, 41, 31, 27,…\n$ BECK   &lt;dbl&gt; 9.000, 34.000, 10.000, 20.000, 5.000, 32.550, 19.000, 10.000, 2…\n$ HC     &lt;dbl&gt; 4, 4, 2, 4, 2, 3, 4, 4, 2, 2, 2, 3, 3, 1, 1, 2, 1, 4, 3, 2, 3, …\n$ IV     &lt;dbl&gt; 3, 2, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 2, 1, 3, 1, …\n$ NDT    &lt;dbl&gt; 1, 8, 3, 1, 5, 1, 34, 2, 3, 7, 8, 1, 2, 8, 1, 3, 6, 1, 15, 5, 1…\n$ RACE   &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ TREAT  &lt;dbl&gt; 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, …\n$ SITE   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ LEN.T  &lt;dbl&gt; 123, 25, 7, 66, 173, 16, 179, 21, 176, 124, 176, 79, 182, 174, …\n$ TIME   &lt;dbl&gt; 188, 26, 207, 144, 551, 32, 459, 22, 210, 184, 212, 87, 598, 26…\n$ CENSOR &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, …\n$ Y      &lt;dbl&gt; 5.236442, 3.258097, 5.332719, 4.969813, 6.311735, 3.465736, 6.1…\n$ ND1    &lt;dbl&gt; 5.0000000, 1.1111111, 2.5000000, 5.0000000, 1.6666667, 5.000000…\n$ ND2    &lt;dbl&gt; -8.0471896, -0.1170672, -2.2907268, -8.0471896, -0.8513760, -8.…\n$ LNDT   &lt;dbl&gt; 0.6931472, 2.1972246, 1.3862944, 0.6931472, 1.7917595, 0.693147…\n$ FRAC   &lt;dbl&gt; 0.68333333, 0.13888889, 0.03888889, 0.73333333, 0.96111111, 0.0…\n$ IV3    &lt;dbl&gt; 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, …\n\n\n\n\n22.7.2 Prepare the data for analysis. [Not always necessary.]\nWe need IV to be a factor variable.\n\n# Although we've already done this above, \n# we include it here again for completeness.\nuis2 &lt;- uis %&gt;%\n  mutate(IV_fct = factor(IV, levels = c(1, 2, 3),\n                         labels = c(\"Never\", \"Previous\", \"Recent\")))\nuis2\n\n     ID AGE   BECK HC IV NDT RACE TREAT SITE LEN.T TIME CENSOR        Y\n1     1  39  9.000  4  3   1    0     1    0   123  188      1 5.236442\n2     2  33 34.000  4  2   8    0     1    0    25   26      1 3.258097\n3     3  33 10.000  2  3   3    0     1    0     7  207      1 5.332719\n4     4  32 20.000  4  3   1    0     0    0    66  144      1 4.969813\n5     5  24  5.000  2  1   5    1     1    0   173  551      0 6.311735\n6     6  30 32.550  3  3   1    0     1    0    16   32      1 3.465736\n7     7  39 19.000  4  3  34    0     1    0   179  459      1 6.129050\n8     8  27 10.000  4  3   2    0     1    0    21   22      1 3.091042\n9     9  40 29.000  2  3   3    0     1    0   176  210      1 5.347108\n10   10  36 25.000  2  3   7    0     1    0   124  184      1 5.214936\n11   12  38 18.900  2  3   8    0     1    0   176  212      1 5.356586\n12   13  29 16.000  3  1   1    0     1    0    79   87      1 4.465908\n13   14  32 36.000  3  3   2    1     1    0   182  598      0 6.393591\n14   15  41 19.000  1  3   8    0     1    0   174  260      1 5.560682\n15   16  31 18.000  1  3   1    0     1    0   181  210      1 5.347108\n16   17  27 12.000  2  3   3    0     1    0    61   84      1 4.430817\n17   18  28 34.000  1  3   6    0     1    0   177  196      1 5.278115\n18   19  28 23.000  4  2   1    0     1    0    19   19      1 2.944439\n19   20  36 26.000  3  1  15    1     1    0    27  441      1 6.089045\n20   21  32 18.900  2  3   5    0     1    0   175  449      1 6.107023\n21   22  33 15.000  3  1   1    0     0    0    12  659      0 6.490724\n22   23  28 25.200  1  3   8    0     0    0    21   21      1 3.044522\n23   24  29  6.632  4  2   0    0     0    0    48   53      1 3.970292\n24   25  35  2.100  2  3   9    0     0    0    90  225      1 5.416100\n25   26  45 26.000  1  3   6    0     0    0    91  161      1 5.081404\n26   27  35 39.789  4  3   5    0     0    0    87   87      1 4.465908\n27   28  24 20.000  3  1   3    0     0    0    88   89      1 4.488636\n28   29  36 16.000  1  3   7    0     0    0     9   44      1 3.784190\n29   31  39 22.000  1  3   9    0     0    0    94  523      0 6.259581\n30   32  36  9.947  4  2  10    0     0    0    91  226      1 5.420535\n31   33  37  9.450  4  3   1    0     0    0    90  259      1 5.556828\n32   34  30 39.000  2  3   1    0     0    0    89  289      1 5.666427\n33   35  44 41.000  1  3   5    0     0    0    89  103      1 4.634729\n34   36  28 31.000  3  1   6    1     0    0   100  624      0 6.436150\n35   37  25 20.000  3  1   3    1     0    0    67   68      1 4.219508\n36   38  30  8.000  2  3   7    0     1    0    25   57      1 4.043051\n37   39  24  9.000  4  1   1    0     0    0    12   65      1 4.174387\n38   40  27 20.000  3  1   1    0     0    0    79   79      1 4.369448\n39   41  30  8.000  3  1   2    1     0    0    79  559      0 6.326149\n40   42  34  8.000  2  3   0    0     1    0    78   79      1 4.369448\n41   43  33 23.000  4  2   2    0     1    0    84   87      1 4.465908\n42   44  34 18.000  3  3   6    0     1    0    91   91      1 4.510860\n43   45  36 13.000  2  3   1    0     1    0   162  297      1 5.693732\n44   46  27 23.000  1  3   0    0     1    0    45   45      1 3.806662\n45   47  35  9.000  4  3   1    1     1    0    61  246      1 5.505332\n46   48  24 14.000  1  3   0    0     1    0    19   37      1 3.610918\n47   49  28 23.000  4  1   2    1     1    0    37   37      1 3.610918\n48   50  46 10.000  1  3   8    0     1    0    51  538      0 6.287859\n49   51  26 11.000  3  3   1    0     1    0    60  541      0 6.293419\n50   52  42 16.000  1  3  25    0     1    0   177  184      1 5.214936\n51   53  30  0.000  3  1   0    0     1    0    43  122      1 4.804021\n52   55  30 12.000  4  1   3    1     1    0    21  156      1 5.049856\n53   56  27 21.000  2  3   2    0     0    0    88  121      1 4.795791\n54   57  38  0.000  1  3   6    0     0    0    96  231      1 5.442418\n55   58  48  8.000  4  3  10    0     0    0   111  111      1 4.709530\n56   59  36 25.000  1  3  10    0     0    0    38   38      1 3.637586\n57   60  28  6.300  3  1   7    0     0    0    15   15      1 2.708050\n58   61  31 20.000  4  2   5    0     0    0    50   54      1 3.988984\n59   62  28  4.000  2  3   5    0     0    0    61  127      1 4.844187\n60   63  28 20.000  3  1   1    0     0    0    31  105      1 4.653960\n61   64  26 17.000  2  1   2    1     0    0    11   11      1 2.397895\n62   65  34  3.000  4  3   6    0     0    0    90  153      1 5.030438\n63   66  26 29.000  2  3   5    0     0    0    11   11      1 2.397895\n64   68  31 26.000  1  3   5    0     0    0    46   46      1 3.828641\n65   69  41 12.000  1  3   0    1     0    0    38  655      0 6.484635\n66   70  30 24.000  4  3   0    0     0    0    90  166      1 5.111988\n67   72  39 15.750  4  3   5    0     0    0    88   95      1 4.553877\n68   74  33  9.000  2  3  12    0     0    0    91  151      1 5.017280\n69   75  33 18.000  4  2   6    0     0    0    85  220      1 5.393628\n70   76  29 20.000  4  1   0    1     0    0    90  227      1 5.424950\n71   77  36 17.000  1  3   5    0     0    0    52  343      1 5.837730\n72   78  26  3.000  4  3   3    0     0    0    88  119      1 4.779123\n73   79  37 27.000  1  3  13    0     0    0    43   43      1 3.761200\n74   81  29 31.500  1  3   8    0     0    0    37   47      1 3.850148\n75   83  30 19.000  3  1   0    1     0    0    87  805      0 6.690842\n76   84  35 15.000  3  2   2    0     0    0    20  321      1 5.771441\n77   85  33 22.000  3  1   1    0     0    0     9  167      1 5.117994\n78   87  36 16.000  2  3   1    0     0    0    85  491      1 6.196444\n79   88  28 17.000  1  3   2    0     0    0    18   35      1 3.555348\n80   89  31 32.550  1  3  12    1     0    0    71  123      1 4.812184\n81   90  23 24.000  1  3   2    0     0    0    88  597      0 6.391917\n82   91  33 22.000  3  2   1    0     0    0    67  762      0 6.635947\n83   93  37 18.000  2  3   4    0     0    0    30   31      1 3.433987\n84   94  25 17.850  3  1   1    0     1    0    68  228      1 5.429346\n85   95  56  5.000  2  2   9    1     1    0   182  553      0 6.315358\n86   96  23 39.000  1  3   1    0     1    0   182  190      1 5.247024\n87   97  26 21.000  3  1   1    0     1    0   146  307      1 5.726848\n88   98  26 11.000  1  3   1    0     1    0    40   73      1 4.290459\n89   99  23 14.000  3  1   1    0     1    0   177  208      1 5.337538\n90  100  28 31.000  4  2   2    1     1    0   181  267      1 5.587249\n91  102  30 14.000  1  3  15    0     1    0   168  169      1 5.129899\n92  104  25  6.000  2  3   5    0     1    0    90  655      0 6.484635\n93  105  33 16.000  1  3   5    0     1    0    61   70      1 4.248495\n94  106  22  6.000  3  1   3    1     1    0    63  398      1 5.986452\n95  108  25 20.000  4  2   8    1     1    0   121  122      1 4.804021\n96  111  38  9.000  3  1   1    1     0    0    89   96      1 4.564348\n97  112  35 11.000  2  1   3    0     1    0    51 1172      0 7.066467\n98  113  35 15.000  3  1   1    0     0    0    88  734      0 6.598509\n99  114  25 13.000  3  3   1    0     0    0    25   26      1 3.258097\n100 115  33 31.000  3  1   3    1     0    0    83   84      1 4.430817\n101 116  30  5.000  3  1   2    1     0    0    89  171      1 5.141664\n102 117  45 10.000  2  3   1    0     0    0    24  159      1 5.068904\n103 119  42 23.000  2  3  20    0     0    0     7    7      1 1.945910\n104 120  29 16.000  4  1   1    1     0    0    85  763      0 6.637258\n105 121  24 37.800  3  1   0    0     0    0    89  104      1 4.644391\n106 122  33 10.000  2  3   4    0     0    0    91  162      1 5.087596\n107 123  32  9.000  3  1   0    0     0    0    89   90      1 4.499810\n108 124  26 15.000  3  1   0    0     0    0    82  373      1 5.921578\n109 125  28  2.000  1  3   3    0     0    0    84  115      1 4.744932\n110 127  37 34.000  2  3   1    0     0    0    30   30      1 3.401197\n111 128  23 11.000  4  1   6    0     0    0     7    8      1 2.079442\n112 129  40 31.000  2  3   3    1     0    0    84  168      1 5.123964\n113 130  36 36.750  3  3   0    0     0    0    70   70      1 4.248495\n114 131  23 26.000  3  2   2    0     0    0    76  130      1 4.867534\n115 132  35  5.000  4  1   1    1     0    0    89  285      1 5.652489\n116 133  25 19.000  2  3   1    0     1    0   178  569      0 6.343880\n117 134  35 21.000  2  3   6    0     1    0    87   87      1 4.465908\n118 135  46  1.000  4  2   0    0     1    0   175  310      1 5.736572\n119 136  32  6.000  4  1   3    0     1    0    87   87      1 4.465908\n120 137  35 23.000  3  1  16    1     1    0   110  544      0 6.298949\n121 138  34 38.000  3  3   1    0     1    0    21  156      1 5.049856\n122 139  43 24.000  3  1   3    0     1    0   139  658      0 6.489205\n123 140  39  3.000  4  3  15    0     1    0   181  273      1 5.609472\n124 141  27 16.800  4  3   2    1     1    0    33  168      1 5.123964\n125 142  38 35.000  1  3   1    0     1    0    39   83      1 4.418841\n126 143  37 11.000  2  3   7    0     1    0     4    4      1 1.386294\n127 144  44  2.000  1  3   4    1     1    0   184  708      0 6.562444\n128 145  25 16.000  4  1   1    1     1    0   123  137      1 4.919981\n129 146  34 15.000  3  1   1    0     1    0   176  259      1 5.556828\n130 147  34 11.000  3  3   2    1     1    0   174  560      0 6.327937\n131 148  38 11.000  1  3   1    1     1    0   181  586      0 6.373320\n132 149  24 22.000  2  3   2    1     1    0   113  190      1 5.247024\n133 151  42 18.000  2  3   3    0     1    0   164  544      0 6.298949\n134 153  34 29.000  4  3   1    1     0    0    84  494      1 6.202536\n135 154  45 27.000  1  3   8    0     0    0    80  541      0 6.293419\n136 155  40 16.000  2  3   4    0     0    0    91   94      1 4.543295\n137 156  27  9.000  4  1   3    1     0    0    97  567      0 6.340359\n138 157  24  0.000  4  1   3    0     0    0    51   55      1 4.007333\n139 158  27 15.000  1  3   3    0     0    0    91   93      1 4.532599\n140 159  34 24.000  3  1   4    0     0    0    90  276      1 5.620401\n141 160  36  3.000  2  3   6    0     0    0    46   46      1 3.828641\n142 162  31  9.000  3  1   1    0     0    0    76  250      1 5.521461\n143 163  40  5.000  2  3   2    0     0    0    75  106      1 4.663439\n144 164  40 13.000  1  3   4    1     0    0    91  552      0 6.313548\n145 165  37 29.000  2  3   5    0     0    0    90   90      1 4.499810\n146 166  25 11.000  4  3   6    0     0    0     3  203      1 5.313206\n147 167  41 22.000  2  3   3    1     1    0     8   67      1 4.204693\n148 168  22  9.000  4  1   1    0     1    0    33  559      1 6.326149\n149 169  31 18.000  2  3   8    1     1    0    31  106      1 4.663439\n150 170  29 40.000  1  1   1    1     1    0   174  374      1 5.924256\n151 171  27 25.000  3  1   2    0     1    0    34  630      0 6.445720\n152 172  22 26.000  4  2   3    0     1    0    60   61      1 4.110874\n153 174  37 11.000  1  2   5    1     1    0    78  547      0 6.304449\n154 175  36  6.000  3  1   2    1     1    0   182  568      0 6.342121\n155 176  24 20.000  3  1   1    0     1    0   182  490      1 6.194405\n156 177  28  9.000  4  1   0    1     1    0    78  222      1 5.402677\n157 178  24  6.000  4  1   1    0     1    0    55   56      1 4.025352\n158 179  28  0.000  3  1   2    0     1    0   223  282      1 5.641907\n159 180  24  5.000  3  1  20    1     1    0    25   35      1 3.555348\n160 181  24 15.000  4  1   0    0     1    0    63  603      0 6.401917\n161 183  29 14.700  3  1   1    0     1    0   133  148      1 4.997212\n162 184  37  3.000  1  3   5    1     1    0   154  354      1 5.869297\n163 185  26 31.000  1  1   2    0     1    0    70  164      1 5.099866\n164 186  29 14.000  3  2   1    0     1    0    66   94      1 4.543295\n165 187  29 28.000  2  3   4    0     1    0    40   65      1 4.174387\n166 188  33 18.000  4  1   1    0     1    0    75  567      0 6.340359\n167 189  29 12.000  4  2   2    0     1    0   187  634      0 6.452049\n168 190  32  5.000  1  1   2    1     1    0   183  633      0 6.450470\n169 192  33 11.000  4  1   8    1     1    0   182  477      1 6.167516\n170 193  26 21.000  4  2   2    0     1    0   192  436      1 6.077642\n171 195  24 23.000  2  3   4    1     1    0   162  362      1 5.891644\n172 196  46 32.000  2  3   2    0     1    0   193  552      0 6.313548\n173 197  23 26.000  4  1   2    0     1    0   111  144      1 4.969813\n174 198  40 19.950  4  3   8    0     1    0   182  242      1 5.488938\n175 199  48 17.000  3  1   4    0     1    0   180  564      0 6.335054\n176 200  33 16.000  3  1   0    0     1    0    93  299      1 5.700444\n177 201  21 26.250  4  1   7    0     1    0   167  167      1 5.117994\n178 202  38 29.000  3  1   2    0     1    0   196  380      1 5.940171\n179 203  28 23.000  4  2   4    0     1    0   106  120      1 4.787492\n180 205  39  9.000  1  3   6    0     1    0   158  218      1 5.384495\n181 206  37 26.000  1  2   1    1     0    0    91  115      1 4.744932\n182 207  32 22.000  3  1   4    1     0    0    89  224      1 5.411646\n183 208  39 23.000  3  2   2    1     0    0    89  132      1 4.882802\n184 209  28  0.000  1  3  10    0     0    0    88  148      1 4.997212\n185 210  26 30.000  3  1   0    1     0    0    95  593      0 6.385194\n186 211  31 21.000  1  3   0    0     0    0     5   26      1 3.258097\n187 213  34 19.000  4  3   8    0     0    0    32   32      1 3.465736\n188 214  26 28.000  4  2   2    1     0    0    92  292      1 5.676754\n189 215  29  8.000  4  1   3    0     0    0    66   89      1 4.488636\n190 217  25 11.000  3  1   8    0     0    0    90  364      1 5.897154\n191 218  34 15.000  3  2   3    1     0    0    93  142      1 4.955827\n192 219  32  8.000  3  1   2    0     0    0    89  188      1 5.236442\n193 221  38 14.000  4  2   0    0     0    0    91   92      1 4.521789\n194 222  32  7.000  1  3   8    0     0    0    56   56      1 4.025352\n195 223  31 13.000  2  3   7    0     0    0    90  110      1 4.700480\n196 224  40 10.000  3  1   3    0     0    0    73  555      0 6.318968\n197 225  28 17.000  4  1   5    1     0    0    85  220      1 5.393628\n198 226  40 18.000  1  3   3    0     0    0    23   23      1 3.135494\n199 227  32  5.000  2  3   3    0     0    0    85  285      1 5.652489\n200 228  29 20.000  3  3   5    0     0    0    90   90      1 4.499810\n201 229  25 31.000  3  1   4    0     0    0    53   59      1 4.077537\n202 230  32 15.000  2  3   2    0     0    0    96  156      1 5.049856\n203 232  37  4.000  2  2   2    0     0    0    83  142      1 4.955827\n204 233  38 15.000  3  3   8    0     0    0    54   57      1 4.043051\n205 234  31 14.000  3  2   9    0     0    0    79  279      1 5.631212\n206 235  30 27.000  1  3   3    1     0    0    81  118      1 4.770685\n207 236  34 30.000  4  1   4    1     0    0    18  567      0 6.340359\n208 237  33 23.000  1  3   4    0     1    0   184  562      0 6.331502\n209 238  36 13.000  3  2  10    1     1    0    39  239      1 5.476464\n210 239  32 26.000  4  1   0    0     1    0   177  578      0 6.359574\n211 240  29 10.000  2  3   2    1     1    0   122  551      0 6.311735\n212 241  32  4.000  1  1   4    1     1    0   178  313      1 5.746203\n213 242  34  0.000  3  1   7    0     1    0   173  560      0 6.327937\n214 243  26 35.000  1  3  31    0     1    0    53   54      1 3.988984\n215 244  25 32.000  1  3   5    1     1    0    94  198      1 5.288267\n216 245  30  2.000  4  1   2    1     1    0   163  164      1 5.099866\n217 246  33 15.000  3  2   6    0     1    0   160  325      1 5.783825\n218 247  40 23.000  4  2   6    0     1    0    61   62      1 4.127134\n219 248  26 13.000  3  1  12    0     1    0    41   45      1 3.806662\n220 249  26 29.000  1  3   5    1     1    0    53   53      1 3.970292\n221 250  35 22.105  4  3   4    0     1    0    53  253      1 5.533389\n222 251  26 15.000  2  2  11    0     1    0    13   51      1 3.931826\n223 252  33  7.000  4  1   3    1     1    0   183  540      0 6.291569\n224 253  27  7.000  1  3   4    0     1    0   182  317      1 5.758902\n225 254  29 33.000  3  3   3    0     1    0   183  437      1 6.079933\n226 255  29 23.000  3  3   9    0     1    0    63  136      1 4.912655\n227 256  39 21.000  2  3   7    0     1    0   111  115      1 4.744932\n228 257  43 19.000  3  2   2    1     1    0   174  175      1 5.164786\n229 258  35  8.000  3  3   3    0     1    0   173  442      1 6.091310\n230 259  26 24.000  4  1   2    1     1    0   119  122      1 4.804021\n231 260  27 28.737  4  1   3    0     1    0   180  181      1 5.198497\n232 261  28 20.000  4  1   2    1     1    0    98  180      1 5.192957\n233 262  30 14.000  3  1   4    0     1    0    50   51      1 3.931826\n234 263  31 17.000  4  2   1    1     1    0   178  541      0 6.293419\n235 264  26 19.000  2  3  16    0     1    0   100  121      1 4.795791\n236 265  36  5.000  4  2   4    0     1    0    93  328      1 5.793014\n237 267  25  8.000  2  3   3    0     1    0   165  166      1 5.111988\n238 268  26 22.000  3  1   0    1     1    0    93  556      0 6.320768\n239 269  30 11.000  2  3   5    0     0    0    44  104      1 4.644391\n240 270  28 13.000  3  1   5    0     0    0    77  102      1 4.624973\n241 272  34 11.053  3  1   0    1     0    0    91  144      1 4.969813\n242 273  31 24.000  3  1   2    0     0    0    95  545      0 6.300786\n243 274  30 19.000  4  3   1    0     0    0    82  537      0 6.285998\n244 275  35 27.000  3  2   5    1     0    0    76  625      0 6.437752\n245 276  30  4.000  4  2   3    1     0    0     5    6      1 1.791759\n246 277  37 38.000  1  3   7    0     0    0    69  307      1 5.726848\n247 278  29 11.000  4  1  12    1     0    0    90  290      1 5.669881\n248 279  23 21.000  4  1   8    0     0    0    19   20      1 2.995732\n249 280  23  1.000  1  1   4    0     0    0    60   74      1 4.304065\n250 281  44  4.000  4  1   0    0     0    0    69  100      1 4.605170\n251 282  43  7.000  4  2   8    1     0    0    85  555      0 6.318968\n252 283  38 20.000  2  3   3    0     0    0    92  152      1 5.023881\n253 284  33 17.000  3  1   3    1     0    0    55  115      1 4.744932\n254 285  36  6.300  1  3   9    0     0    0    20   92      1 4.521789\n255 286  26 12.000  1  3   2    0     0    0    87  554      0 6.317165\n256 287  30 16.000  4  1   0    0     0    0    91   92      1 4.521789\n257 288  34 31.500  4  1   0    0     0    0     9   69      1 4.234107\n258 289  32 30.000  2  3   6    0     0    0    22   25      1 3.218876\n259 290  30  1.000  3  1   1    0     0    0    87  501      0 6.216606\n260 291  37 32.000  2  3  10    1     0    0    86   86      1 4.454347\n261 292  35 29.000  2  3   7    0     0    0    85   99      1 4.595120\n262 293  30  6.000  3  1   0    0     0    0    83   87      1 4.465908\n263 294  34 17.000  4  1   6    1     0    0    83  136      1 4.912655\n264 295  40 13.000  1  2   6    0     0    0    92  106      1 4.663439\n265 296  28 15.000  4  2   3    1     0    0    85  220      1 5.393628\n266 297  32 11.000  3  1   6    0     0    0    36   36      1 3.583519\n267 298  45 17.000  1  3   2    1     0    0    87  162      1 5.087596\n268 299  24 23.000  2  1   0    0     1    0    56  116      1 4.753590\n269 300  43 23.000  1  3   5    1     1    0    94  175      1 5.164786\n270 301  38 15.000  1  3   0    1     1    0    74  209      1 5.342334\n271 302  33 19.000  2  3   1    0     1    0   186  545      0 6.300786\n272 303  26 21.000  4  2   2    1     1    0   178  245      1 5.501258\n273 304  40  8.000  4  3   3    0     1    0    84  176      1 5.170484\n274 305  27 34.000  4  2   0    0     1    0    13   14      1 2.639057\n275 306  39 21.000  2  3  12    0     1    0    85  113      1 4.727388\n276 308  29 27.000  4  2   3    1     1    0     9  354      1 5.869297\n277 309  28 32.000  4  2   4    0     1    0   162  174      1 5.159055\n278 310  37 29.000  1  3  20    0     0    0    23   23      1 3.135494\n279 311  37 22.000  2  3  20    0     0    0    26   26      1 3.258097\n280 312  40 12.000  4  2   9    0     0    0    84   98      1 4.584967\n281 313  25 36.000  1  3   5    0     0    0    23   23      1 3.135494\n282 314  40 15.000  1  1   2    0     0    0    86  555      0 6.318968\n283 315  40  3.000  1  3   4    1     0    0    90  290      1 5.669881\n284 316  34 24.000  2  3   8    0     0    0    73  543      0 6.297109\n285 317  41 18.000  2  3   7    0     0    0    76  274      1 5.613128\n286 321  23  2.000  4  1   1    0     1    0    18  119      1 4.779123\n287 322  36 14.000  3  1   3    0     1    0    94  164      1 5.099866\n288 323  28 19.000  4  1   2    1     1    0    76  548      0 6.306275\n289 324  23  7.000  3  1   3    0     1    0    40  175      1 5.164786\n290 325  27  8.000  3  1   3    0     1    0   176  539      0 6.289716\n291 326  32 27.000  4  2   0    0     1    0   104  155      1 5.043425\n292 327  38 25.000  4  3  15    0     1    0     5   14      1 2.639057\n293 328  38 28.000  4  1   6    1     1    0   179  187      1 5.231109\n294 329  45 39.000  1  3   8    0     1    0    35   65      1 4.174387\n295 330  26 18.000  2  2   1    0     1    0    24  159      1 5.068904\n296 331  29  8.000  1  3  35    0     1    0    82   96      1 4.564348\n297 332  33 31.000  4  1   3    0     1    0    28  243      1 5.493061\n298 333  25  6.000  3  1   0    1     1    0    81   85      1 4.442651\n299 334  36 19.000  4  1   2    0     1    0     4    4      1 1.386294\n300 335  37 19.000  2  3   4    0     1    0    97  121      1 4.795791\n301 336  29 16.000  4  1   0    1     1    0    78  659      1 6.490724\n302 337  29 15.000  4  1   3    1     1    0   181  260      1 5.560682\n303 338  35 54.000  4  2   1    0     1    0    29  621      0 6.431331\n304 339  33 19.000  4  1   1    0     1    0   139  199      1 5.293305\n305 340  31 12.000  4  3   2    0     1    0   152  565      0 6.336826\n306 341  37 24.000  3  2   5    1     1    0    90  183      1 5.209486\n307 342  32 37.000  3  3   4    0     1    0    62  122      1 4.804021\n308 343  33  9.000  3  2  13    0     1    0   110  170      1 5.135798\n309 344  36 18.000  3  1  14    1     1    0    15   15      1 2.708050\n310 345  26  4.000  1  1   5    0     1    0    68  268      1 5.590987\n311 346  35 15.000  3  1   0    1     1    0    19   79      1 4.369448\n312 347  25 19.000  1  3   6    1     0    0    23   23      1 3.135494\n313 348  33 26.000  1  3  30    0     0    0    92  100      1 4.605170\n314 349  36 28.000  2  3   8    0     0    0    94   98      1 4.584967\n315 350  38 14.000  3  3   6    0     0    0    31   81      1 4.394449\n316 351  36 15.000  3  2   3    1     0    0    28  546      0 6.302619\n317 352  36 18.000  2  3  10    0     0    0    58   58      1 4.060443\n318 353  35 29.000  3  3   6    0     0    0   113  569      0 6.343880\n319 354  35 10.000  3  1   3    1     0    0    70  575      0 6.354370\n320 356  39 16.000  2  3   4    0     0    0    90   91      1 4.510860\n321 357  37  0.000  4  3   6    0     0    0    55   57      1 4.043051\n322 358  30 31.000  2  3   5    0     0    0    89  499      1 6.212606\n323 359  26 33.000  1  3   7    1     0    0    71  123      1 4.812184\n324 360  39 21.000  4  1   5    0     0    0    84  143      1 4.962845\n325 362  32 18.000  3  1   4    0     0    0    78  471      1 6.154858\n326 363  26 37.800  3  1   4    1     0    0    60   74      1 4.304065\n327 364  33 20.000  2  3   6    0     0    0    82   85      1 4.442651\n328 365  36 11.000  4  2   5    0     0    0    81   95      1 4.553877\n329 366  42 26.000  2  3   3    0     1    0    35   36      1 3.583519\n330 367  37 43.000  1  3  22    0     1    0    16   19      1 2.944439\n331 368  37 12.000  2  2   1    1     1    0     7   38      1 3.637586\n332 369  32 22.000  3  1   4    1     1    0    30  539      0 6.289716\n333 370  23 36.000  4  1   3    1     1    0   106  567      0 6.340359\n334 371  21 16.000  4  1  10    0     1    0   174  186      1 5.225747\n335 372  23 41.000  3  1   1    0     1    0   144  546      0 6.302619\n336 373  34 16.000  4  2   1    0     1    0    24   24      1 3.178054\n337 374  33  8.000  4  2   3    0     1    0    17  540      0 6.291569\n338 375  33 10.000  3  1   4    1     1    0    97  157      1 5.056246\n339 376  26 18.000  3  3   0    0     1    0    26   86      1 4.454347\n340 377  28 27.000  4  1   2    1     1    0    31  231      1 5.442418\n341 379  27 28.000  1  3   3    0     0    0    14   14      1 2.639057\n342 380  22 23.000  1  3   2    0     0    0    75   75      1 4.317488\n343 381  31 32.000  3  3   6    1     0    0    20  147      1 4.990433\n344 382  29 23.100  3  1   4    0     0    0   104  105      1 4.653960\n345 383  44 11.000  4  3  12    0     0    0    85  324      1 5.780744\n346 384  26  7.000  3  1   0    1     0    0   110  538      0 6.287859\n347 385  44 24.000  2  3  16    0     0    0   100  300      1 5.703782\n348 386  34 12.000  1  3   1    0     0    0    73   73      1 4.290459\n349 387  36 25.000  2  3   6    0     0    0    65   65      1 4.174387\n350 388  43  4.000  2  3  20    0     0    0    75  568      1 6.342121\n351 389  37  5.000  3  1   1    0     0    0    83   84      1 4.430817\n352 390  44 13.000  4  2  17    0     1    0    15   22      1 3.091042\n353 391  31 17.000  1  3  30    1     1    0    44   44      1 3.784190\n354 392  24 24.000  2  1   3    0     1    0     7    7      1 1.945910\n355 394  37 32.000  3  3   4    0     1    0    20   21      1 3.044522\n356 395  41 19.000  1  3  12    1     1    0   175  537      0 6.285998\n357 396  32  9.000  3  1   3    1     1    0    71  186      1 5.225747\n358 397  23  6.000  3  1   2    0     1    0    26   40      1 3.688879\n359 398  33 10.000  2  3   3    0     1    0   161  287      1 5.659482\n360 399  43 11.000  4  1   9    0     1    0    36  538      0 6.287859\n361 400  33 16.000  4  3   8    0     1    0    30   30      1 3.401197\n362 401  41 25.000  4  2   3    0     1    0   179  516      1 6.246107\n363 402  41 17.000  2  3   2    0     1    0   199  268      1 5.590987\n364 403  37 24.000  2  3   3    0     1    0   182  568      0 6.342121\n365 404  26 27.000  1  1   3    0     0    0   112  131      1 4.875197\n366 405  33 24.000  1  3   6    0     0    0     8  399      1 5.988961\n367 406  30 26.000  3  1   2    0     0    0    18   78      1 4.356709\n368 407  33 17.000  4  1   6    1     0    0    20   80      1 4.382027\n369 408  33 26.000  2  3   3    0     0    0    88  102      1 4.624973\n370 410  37 13.000  3  1   6    0     0    0    88  124      1 4.820282\n371 411  44 11.000  2  3  20    0     0    0    76   80      1 4.382027\n372 412  20  8.000  4  1   1    0     0    0    22   23      1 3.135494\n373 413  33 12.000  1  3   4    0     0    0   110  274      1 5.613128\n374 415  36 31.000  2  3   3    0     0    0    85  459      1 6.129050\n375 416  34  8.400  2  3   3    0     0    0    10   10      1 2.302585\n376 417  35 10.000  1  3  17    0     1    0   157  176      1 5.170484\n377 418  38 16.000  2  3  26    0     1    0   133  332      1 5.805135\n378 419  24 13.000  3  1   3    0     1    0    83  119      1 4.779123\n379 420  24 18.000  3  1   4    0     1    0   152  217      1 5.379897\n380 421  32 13.000  3  1   4    0     1    0   169  285      1 5.652489\n381 422  35 11.000  4  2   3    0     1    0    89  576      0 6.356108\n382 423  33 21.000  1  3   5    0     1    0    92  106      1 4.663439\n383 424  29 37.000  2  2   4    1     1    0    21   81      1 4.394449\n384 425  42 32.000  2  3  30    0     1    0    31   47      1 3.850148\n385 426  23 33.000  4  1   1    0     1    0    31   76      1 4.330733\n386 427  28 11.000  4  3  16    0     1    0   133  348      1 5.852202\n387 429  43 29.000  2  3   4    0     1    0   153  306      1 5.723585\n388 430  33 23.000  2  1   0    0     0    0    90  192      1 5.257495\n389 431  37 15.000  1  3  20    0     0    0   102  216      1 5.375278\n390 432  49 22.000  2  3   7    0     0    0    85  189      1 5.241747\n391 434  36 25.000  3  1   1    1     0    0    89  193      1 5.262690\n392 435  27 30.000  1  3  13    0     0    0    28   28      1 3.332205\n393 436  35 23.000  1  3   1    0     0    0    90  150      1 5.010635\n394 437  25 10.000  3  2   3    0     0    0    84   99      1 4.595120\n395 438  33  8.000  1  3   3    0     0    0    85  510      0 6.234411\n396 439  34 16.000  1  3   7    0     0    0    36  306      1 5.723585\n397 440  38  9.000  1  3  10    1     0    0    74  101      1 4.615121\n398 441  36 12.158  2  3   0    1     0    0    42  102      1 4.624973\n399 442  27  5.000  1  3   1    0     0    0    90  510      0 6.234411\n400 444  40 19.000  1  3   0    1     0    0   108  503      0 6.220590\n401 445  32 23.000  3  3   3    0     0    1    49   52      1 3.951244\n402 446  38 28.000  3  3   1    1     0    1   219  547      0 6.304449\n403 447  38 16.000  1  3   6    0     0    1   108  168      1 5.123964\n404 448  23 25.000  4  1   0    0     0    1   178  461      1 6.133398\n405 449  26 22.000  4  2   2    0     0    1    42  538      0 6.287859\n406 450  36 28.000  2  3   7    0     0    1   182  349      1 5.855072\n407 451  30 28.000  4  1   5    0     0    1     6   44      1 3.784190\n408 452  31 18.000  4  2   3    0     1    1   351  548      0 6.306275\n409 453  23 15.000  3  1   1    0     1    1    12   12      1 2.484907\n410 454  43  9.000  1  3   0    1     1    1     6    6      1 1.791759\n411 455  24 26.000  4  1   1    0     1    1    91  575      0 6.354370\n412 456  42 19.000  4  1   1    0     1    1   245  589      0 6.378426\n413 457  35 26.000  4  2   1    0     1    1   372  408      1 6.011267\n414 458  21 10.000  4  1   0    0     1    1   218  232      1 5.446737\n415 459  45  1.000  4  2   0    1     1    1    46  143      1 4.962845\n416 460  43 30.000  2  3   6    0     1    1   363  582      0 6.366470\n417 461  24  7.000  4  1   0    1     1    1   133  134      1 4.897840\n418 462  37 11.000  3  3   1    0     1    1     7    7      1 1.945910\n419 463  40 10.000  4  2   0    0     1    1   112  548      0 6.306275\n420 464  27 11.000  3  2   2    0     0    1    21   81      1 4.394449\n421 465  29 11.000  2  3   1    0     0    1   169  170      1 5.135798\n422 466  34 12.000  4  3   6    0     0    1    28   29      1 3.367296\n423 467  29 29.000  3  3  20    0     0    1    47   78      1 4.356709\n424 468  35 27.000  1  3   5    0     0    1    20   81      1 4.394449\n425 469  39 20.000  1  3   4    0     1    1   352  369      1 5.910797\n426 470  41  9.000  4  2   0    0     1    1    66   69      1 4.234107\n427 471  37 18.000  4  1   6    1     1    1    55  115      1 4.744932\n428 472  30 10.000  3  2   7    0     1    1   344  361      1 5.888878\n429 473  31  1.000  4  1   0    0     1    1   153  245      1 5.501258\n430 474  40  5.000  4  2   8    0     0    1   184  233      1 5.451038\n431 475  32 20.000  4  1   0    0     0    1   183  227      1 5.424950\n432 476  32  7.000  4  2   3    1     0    1    22   97      1 4.574711\n433 477  27  7.000  4  1   0    0     0    1   183  547      0 6.304449\n434 478  23 26.000  3  1   0    0     0    1   140  224      1 5.411646\n435 479  23  4.000  4  1   2    0     0    1    19  211      1 5.351858\n436 480  43 11.000  2  3  12    0     0    1   184  220      1 5.393628\n437 481  24 20.000  4  1   0    0     0    1    50   54      1 3.988984\n438 482  36 11.000  4  1   2    1     0    1   132  192      1 5.257495\n439 483  29 31.000  1  3   1    0     0    1   128  138      1 4.927254\n440 484  39 13.000  4  2   1    0     1    1   107  107      1 4.672829\n441 485  23  6.000  4  1   0    0     1    1   368  597      0 6.391917\n442 486  27 17.000  3  3   4    0     1    1   219  226      1 5.420535\n443 487  26  5.000  4  2   5    0     1    1   374  434      1 6.073045\n444 488  26 27.000  3  1   1    1     1    1    92  106      1 4.663439\n445 489  25  9.000  4  1   0    0     1    1    45  180      1 5.192957\n446 490  34 10.000  3  1   0    0     1    1   366  557      0 6.322565\n447 491  45  5.000  4  3   2    0     1    1   368  556      0 6.320768\n448 492  23 17.000  4  1   1    0     0    1    78  619      0 6.428105\n449 493  26  7.000  4  1   0    0     0    1   184  546      0 6.302619\n450 495  24 27.000  1  2   2    0     0    1   187  233      1 5.451038\n451 496  30 23.000  2  3   2    1     0    1   101  102      1 4.624973\n452 497  22 26.000  3  1   0    0     0    1   141  548      0 6.306275\n453 498  25 10.000  3  1   1    0     0    1    24   99      1 4.595120\n454 499  30  8.400  3  2  40    0     0    1    36   36      1 3.583519\n455 501  33 23.000  4  1   0    1     1    1    56   78      1 4.356709\n456 502  34 15.000  3  2   8    0     1    1   367  502      1 6.218600\n457 503  29 24.000  3  1   2    0     1    1    70   71      1 4.262680\n458 504  39 33.000  4  2   6    0     1    1    58   59      1 4.077537\n459 506  26 21.000  3  1   4    0     1    1   366  533      0 6.278521\n460 507  32 23.000  2  3   6    0     1    1    10   10      1 2.302585\n461 508  42 23.100  1  3   2    0     0    1   214  274      1 5.613128\n462 509  39 25.000  1  2   8    0     0    1   197  255      1 5.541264\n463 510  36  2.000  4  1   0    1     0    1    89  503      0 6.220590\n464 511  22 20.000  3  1   1    0     0    1    56  256      1 5.545177\n465 512  27 23.000  4  1   1    0     0    1     9    9      1 2.197225\n466 514  28  9.000  4  1   0    0     0    1   186  386      1 5.955837\n467 515  36 28.000  3  2   1    0     1    1   303  547      0 6.304449\n468 516  31 13.000  3  1   3    0     1    1    32   45      1 3.806662\n469 517  27 22.000  3  2   4    0     1    1     8   58      1 4.060443\n470 518  23 17.000  3  1   1    0     1    1    63  124      1 4.820282\n471 519  24 20.000  3  2  20    0     0    1   108  540      0 6.291569\n472 520  38  5.000  3  2   1    0     0    1   183  243      1 5.493061\n473 521  25  8.000  4  1   1    0     1    1   151  549      0 6.308098\n474 522  26 20.000  3  1   0    0     0    1     7   12      1 2.484907\n475 523  22 34.000  3  1   2    0     0    1    38   51      1 3.931826\n476 524  33 13.000  4  1   2    0     1    1   176  562      0 6.331502\n477 525  30 23.000  1  3   7    0     1    1    93   94      1 4.543295\n478 526  45  8.000  4  3   3    0     0    1   200  204      1 5.318120\n479 527  24 15.000  3  2   0    0     0    1   178  238      1 5.472271\n480 528  27 22.000  4  1   0    0     1    1    78  140      1 4.941642\n481 529  36 19.000  4  2  10    0     1    1   119  120      1 4.787492\n482 530  38 23.000  4  2   2    1     0    1   154  154      1 5.036953\n483 531  31 17.000  2  3   2    0     1    1   163  177      1 5.176150\n484 532  40 22.000  4  2   7    0     1    1   118  119      1 4.779123\n485 533  22 12.000  3  1   0    1     1    1    76   83      1 4.418841\n486 534  31 13.000  4  1   0    1     1    1   116  130      1 4.867534\n487 536  39  7.000  3  3   3    1     0    1    88  159      1 5.068904\n488 538  33 14.000  3  1   1    0     0    1    33   33      1 3.496508\n489 539  27 10.000  3  3   2    0     1    1    70   72      1 4.276666\n490 540  37  7.000  4  1   2    1     1    1    68  161      1 5.081404\n491 541  35 16.000  4  2  25    0     0    1   191  191      1 5.252273\n492 542  25 11.000  3  1   5    0     0    1    35  181      1 5.198497\n493 543  27 11.000  3  1   1    1     1    1    32  546      0 6.302619\n494 544  34 15.000  4  1   0    0     0    1    28  540      0 6.291569\n495 545  30 15.000  3  1   3    0     0    1    15   76      1 4.330733\n496 546  35 17.000  1  3   7    0     0    1     7    7      1 1.945910\n497 547  34 23.000  4  1   0    0     0    1    43   44      1 3.784190\n498 548  25 23.000  3  2   5    0     0    1    89  103      1 4.634729\n499 549  34 18.000  3  1   1    0     0    1    38   79      1 4.369448\n500 550  24 23.000  4  3   3    0     0    1   204  339      1 5.826000\n501 551  24 20.000  4  1   2    0     0    1    76   90      1 4.499810\n502 552  40 36.000  4  1   3    0     0    1   195  542      0 6.295266\n503 553  33  9.000  3  1   1    1     0    1   184  384      1 5.950643\n504 554  38 14.000  4  2   1    1     1    1   254  255      1 5.541264\n505 555  32  1.000  3  1   0    0     1    1   371  431      1 6.066108\n506 556  33  3.000  4  1   1    0     0    1   196  587      0 6.375025\n507 557  28 40.000  3  1   2    1     0    1   198  198      1 5.288267\n508 558  31 13.000  3  3   2    0     0    1   170  551      0 6.311735\n509 559  31 39.000  2  3   4    0     1    1    50  110      1 4.700480\n510 560  33 24.000  4  1   0    0     1    1   163  541      0 6.293419\n511 561  24 26.000  3  1  11    0     0    1   182  242      1 5.488938\n512 562  26 18.000  3  1   3    0     0    1   150  537      0 6.285998\n513 563  31 19.000  2  3   7    0     1    1    34   56      1 4.025352\n514 564  40 14.700  2  3   4    0     1    1    34   34      1 3.526361\n515 566  34  2.000  3  1   3    0     1    1   366  549      0 6.308098\n516 567  30 11.000  3  2   7    0     0    1   133  133      1 4.890349\n517 568  36  0.000  3  2   3    0     0    1    69  226      1 5.420535\n518 569  38 17.000  2  3   6    0     1    1   366  401      1 5.993961\n519 570  31 20.000  1  3   6    1     1    1    14   14      1 2.639057\n520 571  27 22.000  2  2   2    0     0    1   184  548      0 6.306275\n521 572  32 21.000  1  3  15    0     1    1    89  224      1 5.411646\n522 573  35 23.000  3  1   5    1     0    1   183  540      0 6.291569\n523 574  44 29.000  2  3  13    0     0    1   177  237      1 5.468060\n524 575  31  5.000  2  3  10    0     1    1   154  354      1 5.869297\n525 576  28 23.000  3  2  20    0     0    1   123  123      1 4.812184\n526 577  40  8.000  4  2   1    0     0    1   146  170      1 5.135798\n527 578  25 12.000  3  1  10    1     1    1   203  203      1 5.313206\n528 579  32 10.000  1  3   6    0     1    1   360  360      1 5.886104\n529 580  29 15.750  4  1   2    0     0    1    79  139      1 4.934474\n530 581  40  2.000  2  2   5    0     1    1   201  215      1 5.370638\n531 582  27  9.000  4  2   0    0     1    1   129  129      1 4.859812\n532 583  26  2.000  3  1   1    0     1    1   365  396      1 5.981414\n533 584  34 15.000  3  1   4    1     1    1   159  547      0 6.304449\n534 585  49  4.000  4  2   2    0     0    1   177  547      0 6.304449\n535 586  21 25.000  1  3   1    0     1    1    71   71      1 4.262680\n536 587  39 23.000  3  3   2    0     1    1   108  168      1 5.123964\n537 588  33 15.000  4  2   4    0     1    1   198  228      1 5.429346\n538 589  32  3.000  3  1   1    0     1    1   372  551      0 6.311735\n539 590  35  9.000  4  2   6    0     0    1    25  654      0 6.483107\n540 591  31 20.000  4  1   0    1     1    1    48   51      1 3.931826\n541 592  28  5.000  4  1   3    0     0    1   191  548      0 6.306275\n542 593  27 29.000  3  2   5    0     1    1   171  231      1 5.442418\n543 594  29 21.000  2  1   1    1     1    1   145  280      1 5.634790\n544 595  30  1.000  2  1  20    0     0    1   183  184      1 5.214936\n545 596  27 18.000  4  1   3    1     0    1    72   86      1 4.454347\n546 598  40 15.000  4  2   1    0     1    1    44   46      1 3.828641\n547 599  37 20.000  3  1   2    1     1    1   140  200      1 5.298317\n548 600  33 10.000  4  1   0    0     0    1   184  244      1 5.497168\n549 601  28 20.000  4  1   2    0     0    1    94  182      1 5.204007\n550 602  40 15.000  4  2   8    0     1    1   296  296      1 5.690359\n551 603  48 20.000  4  1   0    1     0    1    23   24      1 3.178054\n552 604  38 25.000  3  1   1    0     0    1   128  142      1 4.955827\n553 605  35 13.000  4  1   0    0     0    1   106  120      1 4.787492\n554 606  37 13.000  4  2   0    0     0    1    46   47      1 3.850148\n555 607  25 15.000  3  1   0    1     1    1   150  519      1 6.251904\n556 608  26  8.000  4  1   2    0     1    1    48  248      1 5.513429\n557 609  30  9.000  3  3   3    0     0    1    29   31      1 3.433987\n558 610  28 16.000  4  2   2    0     0    1   179  567      0 6.340359\n559 611  23 11.000  2  3   4    0     0    1   170  353      1 5.866468\n560 612  36 31.000  4  1   1    0     1    1   365  458      1 6.126869\n561 613  36 13.000  4  2   4    0     1    1   400  554      0 6.317165\n562 614  24  5.000  4  1   0    1     0    1    56  116      1 4.753590\n563 615  33  9.000  3  2   5    0     0    1    24   74      1 4.304065\n564 616  38 15.000  4  2   6    0     0    1    10   10      1 2.302585\n565 617  41 20.000  3  3  21    0     1    1   354  355      1 5.872118\n566 618  31 21.000  3  1   0    1     1    1   232  232      1 5.446737\n567 619  31 23.000  4  2  11    0     1    1    54   68      1 4.219508\n568 620  37  5.000  4  1   0    1     1    1    48   48      1 3.871201\n569 621  37 17.000  4  2   4    1     0    1    57   60      1 4.094345\n570 622  33 13.000  4  1   0    0     0    1    46   50      1 3.912023\n571 624  53  9.000  4  2   6    0     0    1    39  126      1 4.836282\n572 625  37 20.000  2  3   4    0     0    1    17   18      1 2.890372\n573 626  28 10.000  4  2   3    0     1    1    21   35      1 3.555348\n574 627  35 17.000  1  3   2    0     0    1   184  379      1 5.937536\n575 628  46 31.500  1  3  15    1     1    1     9  377      1 5.932245\n           ND1          ND2      LNDT       FRAC IV3   IV_fct\n1    5.0000000  -8.04718956 0.6931472 0.68333333   1   Recent\n2    1.1111111  -0.11706724 2.1972246 0.13888889   0 Previous\n3    2.5000000  -2.29072683 1.3862944 0.03888889   1   Recent\n4    5.0000000  -8.04718956 0.6931472 0.73333333   1   Recent\n5    1.6666667  -0.85137604 1.7917595 0.96111111   0    Never\n6    5.0000000  -8.04718956 0.6931472 0.08888889   1   Recent\n7    0.2857143   0.35793228 3.5553481 0.99444444   1   Recent\n8    3.3333333  -4.01324268 1.0986123 0.11666667   1   Recent\n9    2.5000000  -2.29072683 1.3862944 0.97777778   1   Recent\n10   1.2500000  -0.27892944 2.0794415 0.68888889   1   Recent\n11   1.1111111  -0.11706724 2.1972246 0.97777778   1   Recent\n12   5.0000000  -8.04718956 0.6931472 0.43888889   0    Never\n13   3.3333333  -4.01324268 1.0986123 1.01111111   1   Recent\n14   1.1111111  -0.11706724 2.1972246 0.96666667   1   Recent\n15   5.0000000  -8.04718956 0.6931472 1.00555556   1   Recent\n16   2.5000000  -2.29072683 1.3862944 0.33888889   1   Recent\n17   1.4285714  -0.50953563 1.9459101 0.98333333   1   Recent\n18   5.0000000  -8.04718956 0.6931472 0.10555556   0 Previous\n19   0.6250000   0.29375227 2.7725887 0.15000000   0    Never\n20   1.6666667  -0.85137604 1.7917595 0.97222222   1   Recent\n21   5.0000000  -8.04718956 0.6931472 0.13333333   0    Never\n22   1.1111111  -0.11706724 2.1972246 0.23333333   1   Recent\n23  10.0000000 -23.02585093 0.0000000 0.53333333   0 Previous\n24   1.0000000   0.00000000 2.3025851 1.00000000   1   Recent\n25   1.4285714  -0.50953563 1.9459101 1.01111111   1   Recent\n26   1.6666667  -0.85137604 1.7917595 0.96666667   1   Recent\n27   2.5000000  -2.29072683 1.3862944 0.97777778   0    Never\n28   1.2500000  -0.27892944 2.0794415 0.10000000   1   Recent\n29   1.0000000   0.00000000 2.3025851 1.04444444   1   Recent\n30   0.9090909   0.08664562 2.3978953 1.01111111   0 Previous\n31   5.0000000  -8.04718956 0.6931472 1.00000000   1   Recent\n32   5.0000000  -8.04718956 0.6931472 0.98888889   1   Recent\n33   1.6666667  -0.85137604 1.7917595 0.98888889   1   Recent\n34   1.4285714  -0.50953563 1.9459101 1.11111111   0    Never\n35   2.5000000  -2.29072683 1.3862944 0.74444444   0    Never\n36   1.2500000  -0.27892944 2.0794415 0.13888889   1   Recent\n37   5.0000000  -8.04718956 0.6931472 0.13333333   0    Never\n38   5.0000000  -8.04718956 0.6931472 0.87777778   0    Never\n39   3.3333333  -4.01324268 1.0986123 0.87777778   0    Never\n40  10.0000000 -23.02585093 0.0000000 0.43333333   1   Recent\n41   3.3333333  -4.01324268 1.0986123 0.46666667   0 Previous\n42   1.4285714  -0.50953563 1.9459101 0.50555556   1   Recent\n43   5.0000000  -8.04718956 0.6931472 0.90000000   1   Recent\n44  10.0000000 -23.02585093 0.0000000 0.25000000   1   Recent\n45   5.0000000  -8.04718956 0.6931472 0.33888889   1   Recent\n46  10.0000000 -23.02585093 0.0000000 0.10555556   1   Recent\n47   3.3333333  -4.01324268 1.0986123 0.20555556   0    Never\n48   1.1111111  -0.11706724 2.1972246 0.28333333   1   Recent\n49   5.0000000  -8.04718956 0.6931472 0.33333333   1   Recent\n50   0.3846154   0.36750440 3.2580965 0.98333333   1   Recent\n51  10.0000000 -23.02585093 0.0000000 0.23888889   0    Never\n52   2.5000000  -2.29072683 1.3862944 0.11666667   0    Never\n53   3.3333333  -4.01324268 1.0986123 0.97777778   1   Recent\n54   1.4285714  -0.50953563 1.9459101 1.06666667   1   Recent\n55   0.9090909   0.08664562 2.3978953 1.23333333   1   Recent\n56   0.9090909   0.08664562 2.3978953 0.42222222   1   Recent\n57   1.2500000  -0.27892944 2.0794415 0.16666667   0    Never\n58   1.6666667  -0.85137604 1.7917595 0.55555556   0 Previous\n59   1.6666667  -0.85137604 1.7917595 0.67777778   1   Recent\n60   5.0000000  -8.04718956 0.6931472 0.34444444   0    Never\n61   3.3333333  -4.01324268 1.0986123 0.12222222   0    Never\n62   1.4285714  -0.50953563 1.9459101 1.00000000   1   Recent\n63   1.6666667  -0.85137604 1.7917595 0.12222222   1   Recent\n64   1.6666667  -0.85137604 1.7917595 0.51111111   1   Recent\n65  10.0000000 -23.02585093 0.0000000 0.42222222   1   Recent\n66  10.0000000 -23.02585093 0.0000000 1.00000000   1   Recent\n67   1.6666667  -0.85137604 1.7917595 0.97777778   1   Recent\n68   0.7692308   0.20181866 2.5649494 1.01111111   1   Recent\n69   1.4285714  -0.50953563 1.9459101 0.94444444   0 Previous\n70  10.0000000 -23.02585093 0.0000000 1.00000000   0    Never\n71   1.6666667  -0.85137604 1.7917595 0.57777778   1   Recent\n72   2.5000000  -2.29072683 1.3862944 0.97777778   1   Recent\n73   0.7142857   0.24033731 2.6390573 0.47777778   1   Recent\n74   1.1111111  -0.11706724 2.1972246 0.41111111   1   Recent\n75  10.0000000 -23.02585093 0.0000000 0.96666667   0    Never\n76   3.3333333  -4.01324268 1.0986123 0.22222222   0 Previous\n77   5.0000000  -8.04718956 0.6931472 0.10000000   0    Never\n78   5.0000000  -8.04718956 0.6931472 0.94444444   1   Recent\n79   3.3333333  -4.01324268 1.0986123 0.20000000   1   Recent\n80   0.7692308   0.20181866 2.5649494 0.78888889   1   Recent\n81   3.3333333  -4.01324268 1.0986123 0.97777778   1   Recent\n82   5.0000000  -8.04718956 0.6931472 0.74444444   0 Previous\n83   2.0000000  -1.38629436 1.6094379 0.33333333   1   Recent\n84   5.0000000  -8.04718956 0.6931472 0.37777778   0    Never\n85   1.0000000   0.00000000 2.3025851 1.01111111   0 Previous\n86   5.0000000  -8.04718956 0.6931472 1.01111111   1   Recent\n87   5.0000000  -8.04718956 0.6931472 0.81111111   0    Never\n88   5.0000000  -8.04718956 0.6931472 0.22222222   1   Recent\n89   5.0000000  -8.04718956 0.6931472 0.98333333   0    Never\n90   3.3333333  -4.01324268 1.0986123 1.00555556   0 Previous\n91   0.6250000   0.29375227 2.7725887 0.93333333   1   Recent\n92   1.6666667  -0.85137604 1.7917595 0.50000000   1   Recent\n93   1.6666667  -0.85137604 1.7917595 0.33888889   1   Recent\n94   2.5000000  -2.29072683 1.3862944 0.35000000   0    Never\n95   1.1111111  -0.11706724 2.1972246 0.67222222   0 Previous\n96   5.0000000  -8.04718956 0.6931472 0.98888889   0    Never\n97   2.5000000  -2.29072683 1.3862944 0.28333333   0    Never\n98   5.0000000  -8.04718956 0.6931472 0.97777778   0    Never\n99   5.0000000  -8.04718956 0.6931472 0.27777778   1   Recent\n100  2.5000000  -2.29072683 1.3862944 0.92222222   0    Never\n101  3.3333333  -4.01324268 1.0986123 0.98888889   0    Never\n102  5.0000000  -8.04718956 0.6931472 0.26666667   1   Recent\n103  0.4761905   0.35330350 3.0445224 0.07777778   1   Recent\n104  5.0000000  -8.04718956 0.6931472 0.94444444   0    Never\n105 10.0000000 -23.02585093 0.0000000 0.98888889   0    Never\n106  2.0000000  -1.38629436 1.6094379 1.01111111   1   Recent\n107 10.0000000 -23.02585093 0.0000000 0.98888889   0    Never\n108 10.0000000 -23.02585093 0.0000000 0.91111111   0    Never\n109  2.5000000  -2.29072683 1.3862944 0.93333333   1   Recent\n110  5.0000000  -8.04718956 0.6931472 0.33333333   1   Recent\n111  1.4285714  -0.50953563 1.9459101 0.07777778   0    Never\n112  2.5000000  -2.29072683 1.3862944 0.93333333   1   Recent\n113 10.0000000 -23.02585093 0.0000000 0.77777778   1   Recent\n114  3.3333333  -4.01324268 1.0986123 0.84444444   0 Previous\n115  5.0000000  -8.04718956 0.6931472 0.98888889   0    Never\n116  5.0000000  -8.04718956 0.6931472 0.98888889   1   Recent\n117  1.4285714  -0.50953563 1.9459101 0.48333333   1   Recent\n118 10.0000000 -23.02585093 0.0000000 0.97222222   0 Previous\n119  2.5000000  -2.29072683 1.3862944 0.48333333   0    Never\n120  0.5882353   0.31213427 2.8332133 0.61111111   0    Never\n121  5.0000000  -8.04718956 0.6931472 0.11666667   1   Recent\n122  2.5000000  -2.29072683 1.3862944 0.77222222   0    Never\n123  0.6250000   0.29375227 2.7725887 1.00555556   1   Recent\n124  3.3333333  -4.01324268 1.0986123 0.18333333   1   Recent\n125  5.0000000  -8.04718956 0.6931472 0.21666667   1   Recent\n126  1.2500000  -0.27892944 2.0794415 0.02222222   1   Recent\n127  2.0000000  -1.38629436 1.6094379 1.02222222   1   Recent\n128  5.0000000  -8.04718956 0.6931472 0.68333333   0    Never\n129  5.0000000  -8.04718956 0.6931472 0.97777778   0    Never\n130  3.3333333  -4.01324268 1.0986123 0.96666667   1   Recent\n131  5.0000000  -8.04718956 0.6931472 1.00555556   1   Recent\n132  3.3333333  -4.01324268 1.0986123 0.62777778   1   Recent\n133  2.5000000  -2.29072683 1.3862944 0.91111111   1   Recent\n134  5.0000000  -8.04718956 0.6931472 0.93333333   1   Recent\n135  1.1111111  -0.11706724 2.1972246 0.88888889   1   Recent\n136  2.0000000  -1.38629436 1.6094379 1.01111111   1   Recent\n137  2.5000000  -2.29072683 1.3862944 1.07777778   0    Never\n138  2.5000000  -2.29072683 1.3862944 0.56666667   0    Never\n139  2.5000000  -2.29072683 1.3862944 1.01111111   1   Recent\n140  2.0000000  -1.38629436 1.6094379 1.00000000   0    Never\n141  1.4285714  -0.50953563 1.9459101 0.51111111   1   Recent\n142  5.0000000  -8.04718956 0.6931472 0.84444444   0    Never\n143  3.3333333  -4.01324268 1.0986123 0.83333333   1   Recent\n144  2.0000000  -1.38629436 1.6094379 1.01111111   1   Recent\n145  1.6666667  -0.85137604 1.7917595 1.00000000   1   Recent\n146  1.4285714  -0.50953563 1.9459101 0.03333333   1   Recent\n147  2.5000000  -2.29072683 1.3862944 0.04444444   1   Recent\n148  5.0000000  -8.04718956 0.6931472 0.18333333   0    Never\n149  1.1111111  -0.11706724 2.1972246 0.17222222   1   Recent\n150  5.0000000  -8.04718956 0.6931472 0.96666667   0    Never\n151  3.3333333  -4.01324268 1.0986123 0.18888889   0    Never\n152  2.5000000  -2.29072683 1.3862944 0.33333333   0 Previous\n153  1.6666667  -0.85137604 1.7917595 0.43333333   0 Previous\n154  3.3333333  -4.01324268 1.0986123 1.01111111   0    Never\n155  5.0000000  -8.04718956 0.6931472 1.01111111   0    Never\n156 10.0000000 -23.02585093 0.0000000 0.43333333   0    Never\n157  5.0000000  -8.04718956 0.6931472 0.30555556   0    Never\n158  3.3333333  -4.01324268 1.0986123 1.23888889   0    Never\n159  0.4761905   0.35330350 3.0445224 0.13888889   0    Never\n160 10.0000000 -23.02585093 0.0000000 0.35000000   0    Never\n161  5.0000000  -8.04718956 0.6931472 0.73888889   0    Never\n162  1.6666667  -0.85137604 1.7917595 0.85555556   1   Recent\n163  3.3333333  -4.01324268 1.0986123 0.38888889   0    Never\n164  5.0000000  -8.04718956 0.6931472 0.36666667   0 Previous\n165  2.0000000  -1.38629436 1.6094379 0.22222222   1   Recent\n166  5.0000000  -8.04718956 0.6931472 0.41666667   0    Never\n167  3.3333333  -4.01324268 1.0986123 1.03888889   0 Previous\n168  3.3333333  -4.01324268 1.0986123 1.01666667   0    Never\n169  1.1111111  -0.11706724 2.1972246 1.01111111   0    Never\n170  3.3333333  -4.01324268 1.0986123 1.06666667   0 Previous\n171  2.0000000  -1.38629436 1.6094379 0.90000000   1   Recent\n172  3.3333333  -4.01324268 1.0986123 1.07222222   1   Recent\n173  3.3333333  -4.01324268 1.0986123 0.61666667   0    Never\n174  1.1111111  -0.11706724 2.1972246 1.01111111   1   Recent\n175  2.0000000  -1.38629436 1.6094379 1.00000000   0    Never\n176 10.0000000 -23.02585093 0.0000000 0.51666667   0    Never\n177  1.2500000  -0.27892944 2.0794415 0.92777778   0    Never\n178  3.3333333  -4.01324268 1.0986123 1.08888889   0    Never\n179  2.0000000  -1.38629436 1.6094379 0.58888889   0 Previous\n180  1.4285714  -0.50953563 1.9459101 0.87777778   1   Recent\n181  5.0000000  -8.04718956 0.6931472 1.01111111   0 Previous\n182  2.0000000  -1.38629436 1.6094379 0.98888889   0    Never\n183  3.3333333  -4.01324268 1.0986123 0.98888889   0 Previous\n184  0.9090909   0.08664562 2.3978953 0.97777778   1   Recent\n185 10.0000000 -23.02585093 0.0000000 1.05555556   0    Never\n186 10.0000000 -23.02585093 0.0000000 0.05555556   1   Recent\n187  1.1111111  -0.11706724 2.1972246 0.35555556   1   Recent\n188  3.3333333  -4.01324268 1.0986123 1.02222222   0 Previous\n189  2.5000000  -2.29072683 1.3862944 0.73333333   0    Never\n190  1.1111111  -0.11706724 2.1972246 1.00000000   0    Never\n191  2.5000000  -2.29072683 1.3862944 1.03333333   0 Previous\n192  3.3333333  -4.01324268 1.0986123 0.98888889   0    Never\n193 10.0000000 -23.02585093 0.0000000 1.01111111   0 Previous\n194  1.1111111  -0.11706724 2.1972246 0.62222222   1   Recent\n195  1.2500000  -0.27892944 2.0794415 1.00000000   1   Recent\n196  2.5000000  -2.29072683 1.3862944 0.81111111   0    Never\n197  1.6666667  -0.85137604 1.7917595 0.94444444   0    Never\n198  2.5000000  -2.29072683 1.3862944 0.25555556   1   Recent\n199  2.5000000  -2.29072683 1.3862944 0.94444444   1   Recent\n200  1.6666667  -0.85137604 1.7917595 1.00000000   1   Recent\n201  2.0000000  -1.38629436 1.6094379 0.58888889   0    Never\n202  3.3333333  -4.01324268 1.0986123 1.06666667   1   Recent\n203  3.3333333  -4.01324268 1.0986123 0.92222222   0 Previous\n204  1.1111111  -0.11706724 2.1972246 0.60000000   1   Recent\n205  1.0000000   0.00000000 2.3025851 0.87777778   0 Previous\n206  2.5000000  -2.29072683 1.3862944 0.90000000   1   Recent\n207  2.0000000  -1.38629436 1.6094379 0.20000000   0    Never\n208  2.0000000  -1.38629436 1.6094379 1.02222222   1   Recent\n209  0.9090909   0.08664562 2.3978953 0.21666667   0 Previous\n210 10.0000000 -23.02585093 0.0000000 0.98333333   0    Never\n211  3.3333333  -4.01324268 1.0986123 0.67777778   1   Recent\n212  2.0000000  -1.38629436 1.6094379 0.98888889   0    Never\n213  1.2500000  -0.27892944 2.0794415 0.96111111   0    Never\n214  0.3125000   0.36348463 3.4657359 0.29444444   1   Recent\n215  1.6666667  -0.85137604 1.7917595 0.52222222   1   Recent\n216  3.3333333  -4.01324268 1.0986123 0.90555556   0    Never\n217  1.4285714  -0.50953563 1.9459101 0.88888889   0 Previous\n218  1.4285714  -0.50953563 1.9459101 0.33888889   0 Previous\n219  0.7692308   0.20181866 2.5649494 0.22777778   0    Never\n220  1.6666667  -0.85137604 1.7917595 0.29444444   1   Recent\n221  2.0000000  -1.38629436 1.6094379 0.29444444   1   Recent\n222  0.8333333   0.15193463 2.4849066 0.07222222   0 Previous\n223  2.5000000  -2.29072683 1.3862944 1.01666667   0    Never\n224  2.0000000  -1.38629436 1.6094379 1.01111111   1   Recent\n225  2.5000000  -2.29072683 1.3862944 1.01666667   1   Recent\n226  1.0000000   0.00000000 2.3025851 0.35000000   1   Recent\n227  1.2500000  -0.27892944 2.0794415 0.61666667   1   Recent\n228  3.3333333  -4.01324268 1.0986123 0.96666667   0 Previous\n229  2.5000000  -2.29072683 1.3862944 0.96111111   1   Recent\n230  3.3333333  -4.01324268 1.0986123 0.66111111   0    Never\n231  2.5000000  -2.29072683 1.3862944 1.00000000   0    Never\n232  3.3333333  -4.01324268 1.0986123 0.54444444   0    Never\n233  2.0000000  -1.38629436 1.6094379 0.27777778   0    Never\n234  5.0000000  -8.04718956 0.6931472 0.98888889   0 Previous\n235  0.5882353   0.31213427 2.8332133 0.55555556   1   Recent\n236  2.0000000  -1.38629436 1.6094379 0.51666667   0 Previous\n237  2.5000000  -2.29072683 1.3862944 0.91666667   1   Recent\n238 10.0000000 -23.02585093 0.0000000 0.51666667   0    Never\n239  1.6666667  -0.85137604 1.7917595 0.48888889   1   Recent\n240  1.6666667  -0.85137604 1.7917595 0.85555556   0    Never\n241 10.0000000 -23.02585093 0.0000000 1.01111111   0    Never\n242  3.3333333  -4.01324268 1.0986123 1.05555556   0    Never\n243  5.0000000  -8.04718956 0.6931472 0.91111111   1   Recent\n244  1.6666667  -0.85137604 1.7917595 0.84444444   0 Previous\n245  2.5000000  -2.29072683 1.3862944 0.05555556   0 Previous\n246  1.2500000  -0.27892944 2.0794415 0.76666667   1   Recent\n247  0.7692308   0.20181866 2.5649494 1.00000000   0    Never\n248  1.1111111  -0.11706724 2.1972246 0.21111111   0    Never\n249  2.0000000  -1.38629436 1.6094379 0.66666667   0    Never\n250 10.0000000 -23.02585093 0.0000000 0.76666667   0    Never\n251  1.1111111  -0.11706724 2.1972246 0.94444444   0 Previous\n252  2.5000000  -2.29072683 1.3862944 1.02222222   1   Recent\n253  2.5000000  -2.29072683 1.3862944 0.61111111   0    Never\n254  1.0000000   0.00000000 2.3025851 0.22222222   1   Recent\n255  3.3333333  -4.01324268 1.0986123 0.96666667   1   Recent\n256 10.0000000 -23.02585093 0.0000000 1.01111111   0    Never\n257 10.0000000 -23.02585093 0.0000000 0.10000000   0    Never\n258  1.4285714  -0.50953563 1.9459101 0.24444444   1   Recent\n259  5.0000000  -8.04718956 0.6931472 0.96666667   0    Never\n260  0.9090909   0.08664562 2.3978953 0.95555556   1   Recent\n261  1.2500000  -0.27892944 2.0794415 0.94444444   1   Recent\n262 10.0000000 -23.02585093 0.0000000 0.92222222   0    Never\n263  1.4285714  -0.50953563 1.9459101 0.92222222   0    Never\n264  1.4285714  -0.50953563 1.9459101 1.02222222   0 Previous\n265  2.5000000  -2.29072683 1.3862944 0.94444444   0 Previous\n266  1.4285714  -0.50953563 1.9459101 0.40000000   0    Never\n267  3.3333333  -4.01324268 1.0986123 0.96666667   1   Recent\n268 10.0000000 -23.02585093 0.0000000 0.31111111   0    Never\n269  1.6666667  -0.85137604 1.7917595 0.52222222   1   Recent\n270 10.0000000 -23.02585093 0.0000000 0.41111111   1   Recent\n271  5.0000000  -8.04718956 0.6931472 1.03333333   1   Recent\n272  3.3333333  -4.01324268 1.0986123 0.98888889   0 Previous\n273  2.5000000  -2.29072683 1.3862944 0.46666667   1   Recent\n274 10.0000000 -23.02585093 0.0000000 0.07222222   0 Previous\n275  0.7692308   0.20181866 2.5649494 0.47222222   1   Recent\n276  2.5000000  -2.29072683 1.3862944 0.05000000   0 Previous\n277  2.0000000  -1.38629436 1.6094379 0.90000000   0 Previous\n278  0.4761905   0.35330350 3.0445224 0.25555556   1   Recent\n279  0.4761905   0.35330350 3.0445224 0.28888889   1   Recent\n280  1.0000000   0.00000000 2.3025851 0.93333333   0 Previous\n281  1.6666667  -0.85137604 1.7917595 0.25555556   1   Recent\n282  3.3333333  -4.01324268 1.0986123 0.95555556   0    Never\n283  2.0000000  -1.38629436 1.6094379 1.00000000   1   Recent\n284  1.1111111  -0.11706724 2.1972246 0.81111111   1   Recent\n285  1.2500000  -0.27892944 2.0794415 0.84444444   1   Recent\n286  5.0000000  -8.04718956 0.6931472 0.10000000   0    Never\n287  2.5000000  -2.29072683 1.3862944 0.52222222   0    Never\n288  3.3333333  -4.01324268 1.0986123 0.42222222   0    Never\n289  2.5000000  -2.29072683 1.3862944 0.22222222   0    Never\n290  2.5000000  -2.29072683 1.3862944 0.97777778   0    Never\n291 10.0000000 -23.02585093 0.0000000 0.57777778   0 Previous\n292  0.6250000   0.29375227 2.7725887 0.02777778   1   Recent\n293  1.4285714  -0.50953563 1.9459101 0.99444444   0    Never\n294  1.1111111  -0.11706724 2.1972246 0.19444444   1   Recent\n295  5.0000000  -8.04718956 0.6931472 0.13333333   0 Previous\n296  0.2777778   0.35581496 3.5835189 0.45555556   1   Recent\n297  2.5000000  -2.29072683 1.3862944 0.15555556   0    Never\n298 10.0000000 -23.02585093 0.0000000 0.45000000   0    Never\n299  3.3333333  -4.01324268 1.0986123 0.02222222   0    Never\n300  2.0000000  -1.38629436 1.6094379 0.53888889   1   Recent\n301 10.0000000 -23.02585093 0.0000000 0.43333333   0    Never\n302  2.5000000  -2.29072683 1.3862944 1.00555556   0    Never\n303  5.0000000  -8.04718956 0.6931472 0.16111111   0 Previous\n304  5.0000000  -8.04718956 0.6931472 0.77222222   0    Never\n305  3.3333333  -4.01324268 1.0986123 0.84444444   1   Recent\n306  1.6666667  -0.85137604 1.7917595 0.50000000   0 Previous\n307  2.0000000  -1.38629436 1.6094379 0.34444444   1   Recent\n308  0.7142857   0.24033731 2.6390573 0.61111111   0 Previous\n309  0.6666667   0.27031007 2.7080502 0.08333333   0    Never\n310  1.6666667  -0.85137604 1.7917595 0.37777778   0    Never\n311 10.0000000 -23.02585093 0.0000000 0.10555556   0    Never\n312  1.4285714  -0.50953563 1.9459101 0.25555556   1   Recent\n313  0.3225806   0.36496842 3.4339872 1.02222222   1   Recent\n314  1.1111111  -0.11706724 2.1972246 1.04444444   1   Recent\n315  1.4285714  -0.50953563 1.9459101 0.34444444   1   Recent\n316  2.5000000  -2.29072683 1.3862944 0.31111111   0 Previous\n317  0.9090909   0.08664562 2.3978953 0.64444444   1   Recent\n318  1.4285714  -0.50953563 1.9459101 1.25555556   1   Recent\n319  2.5000000  -2.29072683 1.3862944 0.77777778   0    Never\n320  2.0000000  -1.38629436 1.6094379 1.00000000   1   Recent\n321  1.4285714  -0.50953563 1.9459101 0.61111111   1   Recent\n322  1.6666667  -0.85137604 1.7917595 0.98888889   1   Recent\n323  1.2500000  -0.27892944 2.0794415 0.78888889   1   Recent\n324  1.6666667  -0.85137604 1.7917595 0.93333333   0    Never\n325  2.0000000  -1.38629436 1.6094379 0.86666667   0    Never\n326  2.0000000  -1.38629436 1.6094379 0.66666667   0    Never\n327  1.4285714  -0.50953563 1.9459101 0.91111111   1   Recent\n328  1.6666667  -0.85137604 1.7917595 0.90000000   0 Previous\n329  2.5000000  -2.29072683 1.3862944 0.19444444   1   Recent\n330  0.4347826   0.36213440 3.1354942 0.08888889   1   Recent\n331  5.0000000  -8.04718956 0.6931472 0.03888889   0 Previous\n332  2.0000000  -1.38629436 1.6094379 0.16666667   0    Never\n333  2.5000000  -2.29072683 1.3862944 0.58888889   0    Never\n334  0.9090909   0.08664562 2.3978953 0.96666667   0    Never\n335  5.0000000  -8.04718956 0.6931472 0.80000000   0    Never\n336  5.0000000  -8.04718956 0.6931472 0.13333333   0 Previous\n337  2.5000000  -2.29072683 1.3862944 0.09444444   0 Previous\n338  2.0000000  -1.38629436 1.6094379 0.53888889   0    Never\n339 10.0000000 -23.02585093 0.0000000 0.14444444   1   Recent\n340  3.3333333  -4.01324268 1.0986123 0.17222222   0    Never\n341  2.5000000  -2.29072683 1.3862944 0.15555556   1   Recent\n342  3.3333333  -4.01324268 1.0986123 0.83333333   1   Recent\n343  1.4285714  -0.50953563 1.9459101 0.22222222   1   Recent\n344  2.0000000  -1.38629436 1.6094379 1.15555556   0    Never\n345  0.7692308   0.20181866 2.5649494 0.94444444   1   Recent\n346 10.0000000 -23.02585093 0.0000000 1.22222222   0    Never\n347  0.5882353   0.31213427 2.8332133 1.11111111   1   Recent\n348  5.0000000  -8.04718956 0.6931472 0.81111111   1   Recent\n349  1.4285714  -0.50953563 1.9459101 0.72222222   1   Recent\n350  0.4761905   0.35330350 3.0445224 0.83333333   1   Recent\n351  5.0000000  -8.04718956 0.6931472 0.92222222   0    Never\n352  0.5555556   0.32654815 2.8903718 0.08333333   0 Previous\n353  0.3225806   0.36496842 3.4339872 0.24444444   1   Recent\n354  2.5000000  -2.29072683 1.3862944 0.03888889   0    Never\n355  2.0000000  -1.38629436 1.6094379 0.11111111   1   Recent\n356  0.7692308   0.20181866 2.5649494 0.97222222   1   Recent\n357  2.5000000  -2.29072683 1.3862944 0.39444444   0    Never\n358  3.3333333  -4.01324268 1.0986123 0.14444444   0    Never\n359  2.5000000  -2.29072683 1.3862944 0.89444444   1   Recent\n360  1.0000000   0.00000000 2.3025851 0.20000000   0    Never\n361  1.1111111  -0.11706724 2.1972246 0.16666667   1   Recent\n362  2.5000000  -2.29072683 1.3862944 0.99444444   0 Previous\n363  3.3333333  -4.01324268 1.0986123 1.10555556   1   Recent\n364  2.5000000  -2.29072683 1.3862944 1.01111111   1   Recent\n365  2.5000000  -2.29072683 1.3862944 1.24444444   0    Never\n366  1.4285714  -0.50953563 1.9459101 0.08888889   1   Recent\n367  3.3333333  -4.01324268 1.0986123 0.20000000   0    Never\n368  1.4285714  -0.50953563 1.9459101 0.22222222   0    Never\n369  2.5000000  -2.29072683 1.3862944 0.97777778   1   Recent\n370  1.4285714  -0.50953563 1.9459101 0.97777778   0    Never\n371  0.4761905   0.35330350 3.0445224 0.84444444   1   Recent\n372  5.0000000  -8.04718956 0.6931472 0.24444444   0    Never\n373  2.0000000  -1.38629436 1.6094379 1.22222222   1   Recent\n374  2.5000000  -2.29072683 1.3862944 0.94444444   1   Recent\n375  2.5000000  -2.29072683 1.3862944 0.11111111   1   Recent\n376  0.5555556   0.32654815 2.8903718 0.87222222   1   Recent\n377  0.3703704   0.36787103 3.2958369 0.73888889   1   Recent\n378  2.5000000  -2.29072683 1.3862944 0.46111111   0    Never\n379  2.0000000  -1.38629436 1.6094379 0.84444444   0    Never\n380  2.0000000  -1.38629436 1.6094379 0.93888889   0    Never\n381  2.5000000  -2.29072683 1.3862944 0.49444444   0 Previous\n382  1.6666667  -0.85137604 1.7917595 0.51111111   1   Recent\n383  2.0000000  -1.38629436 1.6094379 0.11666667   0 Previous\n384  0.3225806   0.36496842 3.4339872 0.17222222   1   Recent\n385  5.0000000  -8.04718956 0.6931472 0.17222222   0    Never\n386  0.5882353   0.31213427 2.8332133 0.73888889   1   Recent\n387  2.0000000  -1.38629436 1.6094379 0.85000000   1   Recent\n388 10.0000000 -23.02585093 0.0000000 1.00000000   0    Never\n389  0.4761905   0.35330350 3.0445224 1.13333333   1   Recent\n390  1.2500000  -0.27892944 2.0794415 0.94444444   1   Recent\n391  5.0000000  -8.04718956 0.6931472 0.98888889   0    Never\n392  0.7142857   0.24033731 2.6390573 0.31111111   1   Recent\n393  5.0000000  -8.04718956 0.6931472 1.00000000   1   Recent\n394  2.5000000  -2.29072683 1.3862944 0.93333333   0 Previous\n395  2.5000000  -2.29072683 1.3862944 0.94444444   1   Recent\n396  1.2500000  -0.27892944 2.0794415 0.40000000   1   Recent\n397  0.9090909   0.08664562 2.3978953 0.82222222   1   Recent\n398 10.0000000 -23.02585093 0.0000000 0.46666667   1   Recent\n399  5.0000000  -8.04718956 0.6931472 1.00000000   1   Recent\n400 10.0000000 -23.02585093 0.0000000 1.20000000   1   Recent\n401  2.5000000  -2.29072683 1.3862944 0.54444444   1   Recent\n402  5.0000000  -8.04718956 0.6931472 2.43333333   1   Recent\n403  1.4285714  -0.50953563 1.9459101 1.20000000   1   Recent\n404 10.0000000 -23.02585093 0.0000000 1.97777778   0    Never\n405  3.3333333  -4.01324268 1.0986123 0.46666667   0 Previous\n406  1.2500000  -0.27892944 2.0794415 2.02222222   1   Recent\n407  1.6666667  -0.85137604 1.7917595 0.06666667   0    Never\n408  2.5000000  -2.29072683 1.3862944 1.95000000   0 Previous\n409  5.0000000  -8.04718956 0.6931472 0.06666667   0    Never\n410 10.0000000 -23.02585093 0.0000000 0.03333333   1   Recent\n411  5.0000000  -8.04718956 0.6931472 0.50555556   0    Never\n412  5.0000000  -8.04718956 0.6931472 1.36111111   0    Never\n413  5.0000000  -8.04718956 0.6931472 2.06666667   0 Previous\n414 10.0000000 -23.02585093 0.0000000 1.21111111   0    Never\n415 10.0000000 -23.02585093 0.0000000 0.25555556   0 Previous\n416  1.4285714  -0.50953563 1.9459101 2.01666667   1   Recent\n417 10.0000000 -23.02585093 0.0000000 0.73888889   0    Never\n418  5.0000000  -8.04718956 0.6931472 0.03888889   1   Recent\n419 10.0000000 -23.02585093 0.0000000 0.62222222   0 Previous\n420  3.3333333  -4.01324268 1.0986123 0.23333333   0 Previous\n421  5.0000000  -8.04718956 0.6931472 1.87777778   1   Recent\n422  1.4285714  -0.50953563 1.9459101 0.31111111   1   Recent\n423  0.4761905   0.35330350 3.0445224 0.52222222   1   Recent\n424  1.6666667  -0.85137604 1.7917595 0.22222222   1   Recent\n425  2.0000000  -1.38629436 1.6094379 1.95555556   1   Recent\n426 10.0000000 -23.02585093 0.0000000 0.36666667   0 Previous\n427  1.4285714  -0.50953563 1.9459101 0.30555556   0    Never\n428  1.2500000  -0.27892944 2.0794415 1.91111111   0 Previous\n429 10.0000000 -23.02585093 0.0000000 0.85000000   0    Never\n430  1.1111111  -0.11706724 2.1972246 2.04444444   0 Previous\n431 10.0000000 -23.02585093 0.0000000 2.03333333   0    Never\n432  2.5000000  -2.29072683 1.3862944 0.24444444   0 Previous\n433 10.0000000 -23.02585093 0.0000000 2.03333333   0    Never\n434 10.0000000 -23.02585093 0.0000000 1.55555556   0    Never\n435  3.3333333  -4.01324268 1.0986123 0.21111111   0    Never\n436  0.7692308   0.20181866 2.5649494 2.04444444   1   Recent\n437 10.0000000 -23.02585093 0.0000000 0.55555556   0    Never\n438  3.3333333  -4.01324268 1.0986123 1.46666667   0    Never\n439  5.0000000  -8.04718956 0.6931472 1.42222222   1   Recent\n440  5.0000000  -8.04718956 0.6931472 0.59444444   0 Previous\n441 10.0000000 -23.02585093 0.0000000 2.04444444   0    Never\n442  2.0000000  -1.38629436 1.6094379 1.21666667   1   Recent\n443  1.6666667  -0.85137604 1.7917595 2.07777778   0 Previous\n444  5.0000000  -8.04718956 0.6931472 0.51111111   0    Never\n445 10.0000000 -23.02585093 0.0000000 0.25000000   0    Never\n446 10.0000000 -23.02585093 0.0000000 2.03333333   0    Never\n447  3.3333333  -4.01324268 1.0986123 2.04444444   1   Recent\n448  5.0000000  -8.04718956 0.6931472 0.86666667   0    Never\n449 10.0000000 -23.02585093 0.0000000 2.04444444   0    Never\n450  3.3333333  -4.01324268 1.0986123 2.07777778   0 Previous\n451  3.3333333  -4.01324268 1.0986123 1.12222222   1   Recent\n452 10.0000000 -23.02585093 0.0000000 1.56666667   0    Never\n453  5.0000000  -8.04718956 0.6931472 0.26666667   0    Never\n454  0.2439024   0.34414316 3.7135721 0.40000000   0 Previous\n455 10.0000000 -23.02585093 0.0000000 0.31111111   0    Never\n456  1.1111111  -0.11706724 2.1972246 2.03888889   0 Previous\n457  3.3333333  -4.01324268 1.0986123 0.38888889   0    Never\n458  1.4285714  -0.50953563 1.9459101 0.32222222   0 Previous\n459  2.0000000  -1.38629436 1.6094379 2.03333333   0    Never\n460  1.4285714  -0.50953563 1.9459101 0.05555556   1   Recent\n461  3.3333333  -4.01324268 1.0986123 2.37777778   1   Recent\n462  1.1111111  -0.11706724 2.1972246 2.18888889   0 Previous\n463 10.0000000 -23.02585093 0.0000000 0.98888889   0    Never\n464  5.0000000  -8.04718956 0.6931472 0.62222222   0    Never\n465  5.0000000  -8.04718956 0.6931472 0.10000000   0    Never\n466 10.0000000 -23.02585093 0.0000000 2.06666667   0    Never\n467  5.0000000  -8.04718956 0.6931472 1.68333333   0 Previous\n468  2.5000000  -2.29072683 1.3862944 0.17777778   0    Never\n469  2.0000000  -1.38629436 1.6094379 0.04444444   0 Previous\n470  5.0000000  -8.04718956 0.6931472 0.35000000   0    Never\n471  0.4761905   0.35330350 3.0445224 1.20000000   0 Previous\n472  5.0000000  -8.04718956 0.6931472 2.03333333   0 Previous\n473  5.0000000  -8.04718956 0.6931472 0.83888889   0    Never\n474 10.0000000 -23.02585093 0.0000000 0.07777778   0    Never\n475  3.3333333  -4.01324268 1.0986123 0.42222222   0    Never\n476  3.3333333  -4.01324268 1.0986123 0.97777778   0    Never\n477  1.2500000  -0.27892944 2.0794415 0.51666667   1   Recent\n478  2.5000000  -2.29072683 1.3862944 2.22222222   1   Recent\n479 10.0000000 -23.02585093 0.0000000 1.97777778   0 Previous\n480 10.0000000 -23.02585093 0.0000000 0.43333333   0    Never\n481  0.9090909   0.08664562 2.3978953 0.66111111   0 Previous\n482  3.3333333  -4.01324268 1.0986123 1.71111111   0 Previous\n483  3.3333333  -4.01324268 1.0986123 0.90555556   1   Recent\n484  1.2500000  -0.27892944 2.0794415 0.65555556   0 Previous\n485 10.0000000 -23.02585093 0.0000000 0.42222222   0    Never\n486 10.0000000 -23.02585093 0.0000000 0.64444444   0    Never\n487  2.5000000  -2.29072683 1.3862944 0.97777778   1   Recent\n488  5.0000000  -8.04718956 0.6931472 0.36666667   0    Never\n489  3.3333333  -4.01324268 1.0986123 0.38888889   1   Recent\n490  3.3333333  -4.01324268 1.0986123 0.37777778   0    Never\n491  0.3846154   0.36750440 3.2580965 2.12222222   0 Previous\n492  1.6666667  -0.85137604 1.7917595 0.38888889   0    Never\n493  5.0000000  -8.04718956 0.6931472 0.17777778   0    Never\n494 10.0000000 -23.02585093 0.0000000 0.31111111   0    Never\n495  2.5000000  -2.29072683 1.3862944 0.16666667   0    Never\n496  1.2500000  -0.27892944 2.0794415 0.07777778   1   Recent\n497 10.0000000 -23.02585093 0.0000000 0.47777778   0    Never\n498  1.6666667  -0.85137604 1.7917595 0.98888889   0 Previous\n499  5.0000000  -8.04718956 0.6931472 0.42222222   0    Never\n500  2.5000000  -2.29072683 1.3862944 2.26666667   1   Recent\n501  3.3333333  -4.01324268 1.0986123 0.84444444   0    Never\n502  2.5000000  -2.29072683 1.3862944 2.16666667   0    Never\n503  5.0000000  -8.04718956 0.6931472 2.04444444   0    Never\n504  5.0000000  -8.04718956 0.6931472 1.41111111   0 Previous\n505 10.0000000 -23.02585093 0.0000000 2.06111111   0    Never\n506  5.0000000  -8.04718956 0.6931472 2.17777778   0    Never\n507  3.3333333  -4.01324268 1.0986123 2.20000000   0    Never\n508  3.3333333  -4.01324268 1.0986123 1.88888889   1   Recent\n509  2.0000000  -1.38629436 1.6094379 0.27777778   1   Recent\n510 10.0000000 -23.02585093 0.0000000 0.90555556   0    Never\n511  0.8333333   0.15193463 2.4849066 2.02222222   0    Never\n512  2.5000000  -2.29072683 1.3862944 1.66666667   0    Never\n513  1.2500000  -0.27892944 2.0794415 0.18888889   1   Recent\n514  2.0000000  -1.38629436 1.6094379 0.18888889   1   Recent\n515  2.5000000  -2.29072683 1.3862944 2.03333333   0    Never\n516  1.2500000  -0.27892944 2.0794415 1.47777778   0 Previous\n517  2.5000000  -2.29072683 1.3862944 0.76666667   0 Previous\n518  1.4285714  -0.50953563 1.9459101 2.03333333   1   Recent\n519  1.4285714  -0.50953563 1.9459101 0.07777778   1   Recent\n520  3.3333333  -4.01324268 1.0986123 2.04444444   0 Previous\n521  0.6250000   0.29375227 2.7725887 0.49444444   1   Recent\n522  1.6666667  -0.85137604 1.7917595 2.03333333   0    Never\n523  0.7142857   0.24033731 2.6390573 1.96666667   1   Recent\n524  0.9090909   0.08664562 2.3978953 0.85555556   1   Recent\n525  0.4761905   0.35330350 3.0445224 1.36666667   0 Previous\n526  5.0000000  -8.04718956 0.6931472 1.62222222   0 Previous\n527  0.9090909   0.08664562 2.3978953 1.12777778   0    Never\n528  1.4285714  -0.50953563 1.9459101 2.00000000   1   Recent\n529  3.3333333  -4.01324268 1.0986123 0.87777778   0    Never\n530  1.6666667  -0.85137604 1.7917595 1.11666667   0 Previous\n531 10.0000000 -23.02585093 0.0000000 0.71666667   0 Previous\n532  5.0000000  -8.04718956 0.6931472 2.02777778   0    Never\n533  2.0000000  -1.38629436 1.6094379 0.88333333   0    Never\n534  3.3333333  -4.01324268 1.0986123 1.96666667   0 Previous\n535  5.0000000  -8.04718956 0.6931472 0.39444444   1   Recent\n536  3.3333333  -4.01324268 1.0986123 0.60000000   1   Recent\n537  2.0000000  -1.38629436 1.6094379 1.10000000   0 Previous\n538  5.0000000  -8.04718956 0.6931472 2.06666667   0    Never\n539  1.4285714  -0.50953563 1.9459101 0.27777778   0 Previous\n540 10.0000000 -23.02585093 0.0000000 0.26666667   0    Never\n541  2.5000000  -2.29072683 1.3862944 2.12222222   0    Never\n542  1.6666667  -0.85137604 1.7917595 0.95000000   0 Previous\n543  5.0000000  -8.04718956 0.6931472 0.80555556   0    Never\n544  0.4761905   0.35330350 3.0445224 2.03333333   0    Never\n545  2.5000000  -2.29072683 1.3862944 0.80000000   0    Never\n546  5.0000000  -8.04718956 0.6931472 0.24444444   0 Previous\n547  3.3333333  -4.01324268 1.0986123 0.77777778   0    Never\n548 10.0000000 -23.02585093 0.0000000 2.04444444   0    Never\n549  3.3333333  -4.01324268 1.0986123 1.04444444   0    Never\n550  1.1111111  -0.11706724 2.1972246 1.64444444   0 Previous\n551 10.0000000 -23.02585093 0.0000000 0.25555556   0    Never\n552  5.0000000  -8.04718956 0.6931472 1.42222222   0    Never\n553 10.0000000 -23.02585093 0.0000000 1.17777778   0    Never\n554 10.0000000 -23.02585093 0.0000000 0.51111111   0 Previous\n555 10.0000000 -23.02585093 0.0000000 0.83333333   0    Never\n556  3.3333333  -4.01324268 1.0986123 0.26666667   0    Never\n557  2.5000000  -2.29072683 1.3862944 0.32222222   1   Recent\n558  3.3333333  -4.01324268 1.0986123 1.98888889   0 Previous\n559  2.0000000  -1.38629436 1.6094379 1.88888889   1   Recent\n560  5.0000000  -8.04718956 0.6931472 2.02777778   0    Never\n561  2.0000000  -1.38629436 1.6094379 2.22222222   0 Previous\n562 10.0000000 -23.02585093 0.0000000 0.62222222   0    Never\n563  1.6666667  -0.85137604 1.7917595 0.26666667   0 Previous\n564  1.4285714  -0.50953563 1.9459101 0.11111111   0 Previous\n565  0.4545455   0.35838971 3.0910425 1.96666667   1   Recent\n566 10.0000000 -23.02585093 0.0000000 1.28888889   0    Never\n567  0.8333333   0.15193463 2.4849066 0.30000000   0 Previous\n568 10.0000000 -23.02585093 0.0000000 0.26666667   0    Never\n569  2.0000000  -1.38629436 1.6094379 0.63333333   0 Previous\n570 10.0000000 -23.02585093 0.0000000 0.51111111   0    Never\n571  1.4285714  -0.50953563 1.9459101 0.43333333   0 Previous\n572  2.0000000  -1.38629436 1.6094379 0.18888889   1   Recent\n573  2.5000000  -2.29072683 1.3862944 0.11666667   0 Previous\n574  3.3333333  -4.01324268 1.0986123 2.04444444   1   Recent\n575  0.6250000   0.29375227 2.7725887 0.05000000   1   Recent\n\n\n\nglimpse(uis2)\n\nRows: 575\nColumns: 19\n$ ID     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, …\n$ AGE    &lt;dbl&gt; 39, 33, 33, 32, 24, 30, 39, 27, 40, 36, 38, 29, 32, 41, 31, 27,…\n$ BECK   &lt;dbl&gt; 9.000, 34.000, 10.000, 20.000, 5.000, 32.550, 19.000, 10.000, 2…\n$ HC     &lt;dbl&gt; 4, 4, 2, 4, 2, 3, 4, 4, 2, 2, 2, 3, 3, 1, 1, 2, 1, 4, 3, 2, 3, …\n$ IV     &lt;dbl&gt; 3, 2, 3, 3, 1, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3, 2, 1, 3, 1, …\n$ NDT    &lt;dbl&gt; 1, 8, 3, 1, 5, 1, 34, 2, 3, 7, 8, 1, 2, 8, 1, 3, 6, 1, 15, 5, 1…\n$ RACE   &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ TREAT  &lt;dbl&gt; 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, …\n$ SITE   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ LEN.T  &lt;dbl&gt; 123, 25, 7, 66, 173, 16, 179, 21, 176, 124, 176, 79, 182, 174, …\n$ TIME   &lt;dbl&gt; 188, 26, 207, 144, 551, 32, 459, 22, 210, 184, 212, 87, 598, 26…\n$ CENSOR &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, …\n$ Y      &lt;dbl&gt; 5.236442, 3.258097, 5.332719, 4.969813, 6.311735, 3.465736, 6.1…\n$ ND1    &lt;dbl&gt; 5.0000000, 1.1111111, 2.5000000, 5.0000000, 1.6666667, 5.000000…\n$ ND2    &lt;dbl&gt; -8.0471896, -0.1170672, -2.2907268, -8.0471896, -0.8513760, -8.…\n$ LNDT   &lt;dbl&gt; 0.6931472, 2.1972246, 1.3862944, 0.6931472, 1.7917595, 0.693147…\n$ FRAC   &lt;dbl&gt; 0.68333333, 0.13888889, 0.03888889, 0.73333333, 0.96111111, 0.0…\n$ IV3    &lt;dbl&gt; 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, …\n$ IV_fct &lt;fct&gt; Recent, Previous, Recent, Recent, Never, Recent, Recent, Recent…\n\n\n\n\n22.7.3 Make tables or plots to explore the data visually.\nWe should calculate group statistics:\n\ntabyl(uis2, IV_fct) %&gt;%\n  adorn_totals()\n\n   IV_fct   n   percent\n    Never 223 0.3878261\n Previous 109 0.1895652\n   Recent 243 0.4226087\n    Total 575 1.0000000\n\n\n\nuis2 %&gt;%\n  summarise(mean(BECK))\n\n  mean(BECK)\n1   17.36743\n\n\n\nuis2 %&gt;%\n  group_by(IV_fct) %&gt;%\n  summarise(mean(BECK))\n\n# A tibble: 3 × 2\n  IV_fct   `mean(BECK)`\n  &lt;fct&gt;           &lt;dbl&gt;\n1 Never            15.9\n2 Previous         16.6\n3 Recent           19.0\n\n\nHere are two graphs that are appropriate for one categorical and one numerical variable: a side-by-side boxplot and a stacked histogram.\n\nggplot(uis2, aes(y = BECK, x = IV_fct)) +\n    geom_boxplot()\n\n\n\n\n\nggplot(uis2, aes(x = BECK)) +\n    geom_histogram(binwidth = 5, boundary = 0) +\n    facet_grid(IV_fct ~ .)\n\n\n\n\nBoth graphs show that the distribution of depression scores in each group is similar.\nThe distributions look reasonably normal, or perhaps a bit right skewed, but we can also check the QQ plots:\n\nggplot(uis2, aes(sample = BECK)) +\n    geom_qq()  +\n    geom_qq_line() +\n    facet_grid(IV_fct ~ .)\n\n\n\n\nThere is one mild outlier in the “Previous” group, but with sample sizes as large as we have in each group, it’s unlikely that this outlier will be influential. So we’ll just leave it in the data and not worry about it."
  },
  {
    "objectID": "22-anova-web.html#hypotheses",
    "href": "22-anova-web.html#hypotheses",
    "title": "22  ANOVA",
    "section": "22.8 Hypotheses",
    "text": "22.8 Hypotheses\n\n22.8.1 Identify the sample (or samples) and a reasonable population (or populations) of interest.\nThe sample consists of people who participated in the UIS drug treatment study. Because the UIS studied the effects of residential treatment for drug abuse, the population is, presumably, all drug addicts.\n\n\n22.8.2 Express the null and alternative hypotheses as contextually meaningful full sentences.\n\\(H_{0}:\\) There is no difference in depression levels among those who have no history of IV drug use, those who have some previous IV drug use, and those who have recent IV drug use.\n\\(H_{A}:\\) There is a difference in depression levels among those who have no history of IV drug use, those who have some previous IV drug use, and those who have recent IV drug use.\n\n\n22.8.3 Express the null and alternative hypotheses in symbols (when possible).\n\\(H_{0}: \\mu_{never} = \\mu_{previous} = \\mu_{recent}\\)\nThere is no easy way to express the alternate hypothesis in symbols because any deviation in any of the categories can lead to rejection of the null. You can’t just say \\(\\mu_{never} \\neq \\mu_{previous} \\neq \\mu_{recent}\\) because two of these categories might be the same and the third different and that would still be consistent with the alternative hypothesis.\nSo the only requirement here is to express the null in symbols."
  },
  {
    "objectID": "22-anova-web.html#model",
    "href": "22-anova-web.html#model",
    "title": "22  ANOVA",
    "section": "22.9 Model",
    "text": "22.9 Model\n\n22.9.1 Identify the sampling distribution model.\nWe will use an F model with \\(df_{G} = 2\\) and \\(df_{E} = 572\\).\nCommentary: Remember that\n\\[\ndf_{G} = k - 1 = 3 - 1 = 2,\n\\]\n(\\(k\\) is the number of groups, in this case, 3), and\n\\[\ndf_{E} = n - k = 575 - 3 = 572.\n\\]\n\n\n22.9.2 Check the relevant conditions to ensure that model assumptions are met.\n\nRandom\n\nWe have little information about how this sample was collected, so we have to hope it’s representative.\n\n10%\n\n575 is definitely less than 10% of all drug addicts.\n\nNearly normal\n\nThe earlier stacked histograms and QQ plots showed that each group is nearly normal. (There was one outlier in one group, but our sample sizes are quite large.)\n\nConstant variance\n\nThe spread of data looks pretty consistent from group to group in the stacked histogram and side-by-side boxplot."
  },
  {
    "objectID": "22-anova-web.html#mechanics",
    "href": "22-anova-web.html#mechanics",
    "title": "22  ANOVA",
    "section": "22.10 Mechanics",
    "text": "22.10 Mechanics\n\n22.10.1 Compute the test statistic.\n\nBECK_IV_F &lt;- uis2 %&gt;% \n  specify(response = BECK, explanatory = IV_fct) %&gt;%\n  calculate(stat = \"F\")\nBECK_IV_F\n\nResponse: BECK (numeric)\nExplanatory: IV_fct (factor)\n# A tibble: 1 × 1\n   stat\n  &lt;dbl&gt;\n1  6.72\n\n\n\n\n22.10.2 Report the test statistic in context (when possible).\nThe F score is 6.721405.\nCommentary: F scores (much like chi-square values earlier in the course) are not particularly interpretable on their own, so there isn’t really any context we can provide. It’s only required that you report the F score in a full sentence.\n\n\n22.10.3 Plot the null distribution.\n\nBECK_IV_test &lt;- uis2 %&gt;%\n  specify(response = BECK, explanatory = IV_fct) %&gt;%\n  hypothesize(null = \"independence\") %&gt;%\n  assume(distribution = \"F\")\nBECK_IV_test\n\nAn F distribution with 2 and 572 degrees of freedom.\n\n\n\nBECK_IV_test %&gt;%\n  visualize() +\n  shade_p_value(obs_stat = BECK_IV_F, direction = \"greater\")\n\n\n\n\n\n\n22.10.4 Calculate the P-value.\n\nBECK_IV_P &lt;- BECK_IV_test %&gt;%\n  get_p_value(obs_stat = BECK_IV_F, direction = \"greater\")\nBECK_IV_P\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1 0.00130\n\n\nCommentary: Note that this is, by definition, a one-sided test. Extreme values of F are the ones that are far away from 1, and only those values in the right tail are far from 1.\n\n\n22.10.5 Interpret the P-value as a probability given the null.\nThe P-value is 0.0013023. If there were no differences in depression scores among the three IV groups, there would be a 0.1302279% chance of seeing data at least as extreme as the data we saw."
  },
  {
    "objectID": "22-anova-web.html#conclusion",
    "href": "22-anova-web.html#conclusion",
    "title": "22  ANOVA",
    "section": "22.11 Conclusion",
    "text": "22.11 Conclusion\n\n22.11.1 State the statistical conclusion.\nWe reject the null hypothesis.\n\n\n22.11.2 State (but do not overstate) a contextually meaningful conclusion.\nThere is sufficient evidence that there is a difference in depression levels among those who have no history of IV drug use, those who have some previous IV drug use, and those who have recent IV drug use.\n\n\n22.11.3 Express reservations or uncertainty about the generalizability of the conclusion.\nOur lack of uncertainty about the sample means we don’t know for sure if we can generalize to a larger population of drug users. We hope that the researchers would obtain a representative sample. Also, the study in question is from the 1990s, so we should not suppose that the conclusions are still true today.\n\n\n22.11.4 Identify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.\nIf we’ve made a Type I error, that means that there really isn’t a difference among the three groups, but our sample is an unusual one that did detect a difference.\n\nExercise 5(a)\nEverything we saw earlier in the exploratory data analysis pointed toward failing to reject the null. All three groups look very similar in all the plots, and the means are not all that far from each other. So why did we get such a tiny P-value and reject the null? In other words, what is it about our data that allows for small effects to be statistically significant?\n\nPlease write up your answer here.\n\n\n\nExercise 5(b)\nIf you were a psychologist working with drug addicts, would the statistical conclusion (rejecting the null and concluding that there was a difference among groups) be of clinical importance to you? In other words, if there is a difference, is it of practical significance and not just statistical significance?\n\nPlease write up your answer here.\n\n\nThere is no confidence interval for ANOVA. We are not hypothesizing about the value of any particular parameter, so there’s nothing to estimate with a confidence interval."
  },
  {
    "objectID": "22-anova-web.html#your-turn",
    "href": "22-anova-web.html#your-turn",
    "title": "22  ANOVA",
    "section": "22.12 Your turn",
    "text": "22.12 Your turn\nUsing the penguins data, determine if there is a difference in the average body masses among the three species represented in the data (Adelie, Chinstrap, and Gentoo).\nThere are two missing values of body mass, and as we saw earlier in the book, that does affect certain functions. To make it a little easier on you, here is some code to remove those missing values:\n\npenguins2 &lt;- penguins %&gt;%\n  drop_na(species, body_mass_g)\n\nFor this whole section, be sure to use penguins2.\nThe rubric outline is reproduced below. You may refer to the worked example above and modify it accordingly. Remember to strip out all the commentary. That is just exposition for your benefit in understanding the steps, but is not meant to form part of the formal inference process.\nAnother word of warning: the copy/paste process is not a substitute for your brain. You will often need to modify more than just the names of the data frames and variables to adapt the worked examples to your own work. Do not blindly copy and paste code without understanding what it does. And you should never copy and paste text. All the sentences and paragraphs you write are expressions of your own analysis. They must reflect your own understanding of the inferential process.\nAlso, so that your answers here don’t mess up the code chunks above, use new variable names everywhere.\n\nExploratory data analysis\n\nUse data documentation (help files, code books, Google, etc.) to determine as much as possible about the data provenance and structure.\n\nPlease write up your answer here\n\n# Add code here to print the data\n\n\n# Add code here to glimpse the variables\n\n\n\n\nPrepare the data for analysis. [Not always necessary.]\n\n\n# Add code here to prepare the data for analysis.\n\n\n\n\nMake tables or plots to explore the data visually.\n\n\n# Add code here to make tables or plots.\n\n\n\n\n\nHypotheses\n\nIdentify the sample (or samples) and a reasonable population (or populations) of interest.\n\nPlease write up your answer here.\n\n\n\nExpress the null and alternative hypotheses as contextually meaningful full sentences.\n\n\\(H_{0}:\\) Null hypothesis goes here.\n\\(H_{A}:\\) Alternative hypothesis goes here.\n\n\n\nExpress the null and alternative hypotheses in symbols (when possible).\n\n\\(H_{0}: math\\)\n\\(H_{A}: math\\)\n\n\n\n\nModel\n\nIdentify the sampling distribution model.\n\nPlease write up your answer here.\n\n\n\nCheck the relevant conditions to ensure that model assumptions are met.\n\nPlease write up your answer here. (Some conditions may require R code as well.)\n\n\n\n\nMechanics\n\nCompute the test statistic.\n\n\n# Add code here to compute the test statistic.\n\n\n\n\nReport the test statistic in context (when possible).\n\nPlease write up your answer here.\n\n\n\nPlot the null distribution.\n\n\n# IF CONDUCTING A SIMULATION...\nset.seed(1)\n# Add code here to simulate the null distribution.\n\n\n# Add code here to plot the null distribution.\n\n\n\n\nCalculate the P-value.\n\n\n# Add code here to calculate the P-value.\n\n\n\n\nInterpret the P-value as a probability given the null.\n\nPlease write up your answer here.\n\n\n\n\nConclusion\n\nState the statistical conclusion.\n\nPlease write up your answer here.\n\n\n\nState (but do not overstate) a contextually meaningful conclusion.\n\nPlease write up your answer here.\n\n\n\nExpress reservations or uncertainty about the generalizability of the conclusion.\n\nPlease write up your answer here.\n\n\n\nIdentify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.\n\nPlease write up your answer here."
  },
  {
    "objectID": "22-anova-web.html#bonus-section-post-hoc-analysis",
    "href": "22-anova-web.html#bonus-section-post-hoc-analysis",
    "title": "22  ANOVA",
    "section": "22.13 Bonus section: post-hoc analysis",
    "text": "22.13 Bonus section: post-hoc analysis\nSuppose our ANOVA test leads us to reject the null hypothesis. Then we have statistically significant evidence that there is some difference between the means of the various groups. However, ANOVA doesn’t tell us which groups are actually different – unsatisfying!\nWe could consider just doing a bunch of individual t-tests between each pair of groups. However, the problem with this approach is that it greatly increases the chances that we might commit a Type I error. (For an exploration of this problem, please see the following XKCD comic.)\nFortunately, there is a tool called post-hoc analysis that allows us to determine which groups differ from the others in a way that doesn’t inflate the Type I error rate.\nThere are several methods for conducting post-hoc analysis. You may have heard of the Bonferroni correction, in which the usual significance level is divided by the number of pairwise comparisons contemplated. Another method, and the one we’ll explore here, is called the Tukey Honestly-Significant-Difference test. The precise details of this test are a little outside the scope of this course, but here’s how it’s done in R.\nWe’ll start by using a different function, called aov, to conduct the ANOVA test. This function produces a slightly different format of outputs than we’re used to, but it produces all the same values as our other tools:\n\nBECK_IV_aov &lt;- aov(BECK ~ IV_fct, uis2)\nsummary(BECK_IV_aov)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)   \nIV_fct        2   1148   574.0   6.721 0.0013 **\nResiduals   572  48850    85.4                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNotice in particular that the F score and the P-value are the same as we obtained using infer tools above.\nNow that we have the result of the aov command stored in a new variable, we can feed it into the new command TukeyHSD:\n\nTukeyHSD(BECK_IV_aov)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = BECK ~ IV_fct, data = uis2)\n\n$IV_fct\n                    diff        lwr      upr     p adj\nPrevious-Never  0.692054 -1.8458349 3.229943 0.7976511\nRecent-Never    3.043674  1.0299195 5.057429 0.0012039\nRecent-Previous 2.351620 -0.1517446 4.854986 0.0707718\n\n\nHere’s how to read these results: Start by looking at the p adj column, which tells us adjusted p-values. Look for a p-value that is below the usual significance level (= 0.05). In our example, the second p-value is the only one that is small enough to reach significance.\nOnce you’ve located the significant p-values, read the row to determine which comparisons are significant. Here, the second row is the meaningful one: this is the comparison between the “Recent” group and the “Never” group.\nThe column labeled diff reports the difference between the means of the two groups; the order of subtraction is reported in the first column. Here, the difference in Beck depression scores is 3.043674, which is computed by subtracting the mean of the “Never” group from the mean of the “Recent” group.\nAs usual, we report our results in a contextually-meaningful sentence. Here’s our example:\n\nTukey’s HSD test reports that recent IV drug users have a Beck inventory score that is 3.043674 points higher than those who have never used IV drugs.\n\n\n22.13.1 Your turn\nConduct a post-hoc analysis to determine which penguin species is heavier or lighter than the others.\n\n# Add code here to produce the aov model\n\n# Add code here to run Tukey's HSD test on the aov model\n\nReport your results in a contextually-meaningful sentence:\n\nPlease write your answer here."
  },
  {
    "objectID": "22-anova-web.html#conclusion-2",
    "href": "22-anova-web.html#conclusion-2",
    "title": "22  ANOVA",
    "section": "22.14 Conclusion",
    "text": "22.14 Conclusion\nWhen analyzing a numerical response variable across three or more levels of a categorical predictor variable, ANOVA provides a way of comparing the variability of the response between the groups to the variability within the groups. When there is more variability between the groups than within the groups, this is evidence that the groups are truly different from one another (rather than simply arising from random sampling variability). The result of comparing the two sources of variability gives rise to the F distribution, which can be used to determine when the difference is more than one would expect from chance alone.\n\n22.14.1 Preparing and submitting your assignment\n\nFrom the “Run” menu, select “Restart R and Run All Chunks”.\nDeal with any code errors that crop up. Repeat steps 1–-2 until there are no more code errors.\nSpell check your document by clicking the icon with “ABC” and a check mark.\nHit the “Preview” button one last time to generate the final draft of the .nb.html file.\nProofread the HTML file carefully. If there are errors, go back and fix them, then repeat steps 1–5 again.\n\nIf you have completed this chapter as part of a statistics course, follow the directions you receive from your professor to submit your assignment."
  },
  {
    "objectID": "Rubric.html#exploratory-data-analysis",
    "href": "Rubric.html#exploratory-data-analysis",
    "title": "Appendix A — Rubric for inference",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nUse data documentation (help files, code books, Google, etc.) to determine as much as possible about the data provenance and structure.\n\nPlease write up your answer here\n\n# Add code here to print the data\n\n\n# Add code here to glimpse the variables\n\n\n\n\nPrepare the data for analysis. [Not always necessary.]\n\n\n# Add code here to prepare the data for analysis.\n\n\n\n\nMake tables or plots to explore the data visually.\n\n\n# Add code here to make tables or plots."
  },
  {
    "objectID": "Rubric.html#hypotheses",
    "href": "Rubric.html#hypotheses",
    "title": "Appendix A — Rubric for inference",
    "section": "Hypotheses",
    "text": "Hypotheses\n\nIdentify the sample (or samples) and a reasonable population (or populations) of interest.\n\nPlease write up your answer here.\n\n\n\nExpress the null and alternative hypotheses as contextually meaningful full sentences.\n\n\\(H_{0}:\\) Null hypothesis goes here.\n\\(H_{A}:\\) Alternative hypothesis goes here.\n\n\n\nExpress the null and alternative hypotheses in symbols (when possible).\n\n\\(H_{0}: math\\)\n\\(H_{A}: math\\)"
  },
  {
    "objectID": "Rubric.html#model",
    "href": "Rubric.html#model",
    "title": "Appendix A — Rubric for inference",
    "section": "Model",
    "text": "Model\n\nIdentify the sampling distribution model.\n\nPlease write up your answer here.\n\n\n\nCheck the relevant conditions to ensure that model assumptions are met.\n\nPlease write up your answer here. (Some conditions may require R code as well.)"
  },
  {
    "objectID": "Rubric.html#mechanics",
    "href": "Rubric.html#mechanics",
    "title": "Appendix A — Rubric for inference",
    "section": "Mechanics",
    "text": "Mechanics\n\nCompute the test statistic.\n\n\n# Add code here to compute the test statistic.\n\n\n\n\nReport the test statistic in context (when possible).\n\nPlease write up your answer here.\n\n\n\nPlot the null distribution.\n\n\n# IF CONDUCTING A SIMULATION...\nset.seed(1)\n# Add code here to simulate the null distribution.\n\n\n# Add code here to plot the null distribution.\n\n\n\n\nCalculate the P-value.\n\n\n# Add code here to calculate the P-value.\n\n\n\n\nInterpret the P-value as a probability given the null.\n\nPlease write up your answer here."
  },
  {
    "objectID": "Rubric.html#conclusion",
    "href": "Rubric.html#conclusion",
    "title": "Appendix A — Rubric for inference",
    "section": "Conclusion",
    "text": "Conclusion\n\nState the statistical conclusion.\n\nPlease write up your answer here.\n\n\n\nState (but do not overstate) a contextually meaningful conclusion.\n\nPlease write up your answer here.\n\n\n\nExpress reservations or uncertainty about the generalizability of the conclusion.\n\nPlease write up your answer here.\n\n\n\nIdentify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.\n\nPlease write up your answer here."
  },
  {
    "objectID": "Rubric.html#confidence-interval",
    "href": "Rubric.html#confidence-interval",
    "title": "Appendix A — Rubric for inference",
    "section": "Confidence interval",
    "text": "Confidence interval\n\nCheck the relevant conditions to ensure that model assumptions are met.\n\nPlease write up your answer here. (Some conditions may require R code as well.)\n\n\n\nCalculate and graph the confidence interval.\n\n\n# Add code here to calculate the confidence interval.\n\n\n# Add code here to graph the confidence interval.\n\n\n\n\nState (but do not overstate) a contextually meaningful interpretation.\n\nPlease write up your answer here.\n\n\n\nIf running a two-sided test, explain how the confidence interval reinforces the conclusion of the hypothesis test. [Not always applicable.]\n\nPlease write up your answer here.\n\n\n\nWhen comparing two groups, comment on the effect size and the practical significance of the result. [Not always applicable.]\n\nPlease write up your answer here."
  },
  {
    "objectID": "Concordance.html",
    "href": "Concordance.html",
    "title": "Appendix B — Concordance with Introduction to Modern Statistics (IMS)",
    "section": "",
    "text": "This book is meant to be somewhat aligned pedagogically with part of the book Introduction to Modern Statistics (IMS) by Mine Çetinkaya-Rundel and Johanna Hardin. But it’s not a perfect, one-to-one match. The table below shows the concordance between the two books with some notes that explain when one book does something different from the other.\n\n\n\n\n\n\n\n\nThis book\nIMS\nNotes\n\n\n\n\nCh. 1\n\nThis book contains a specific introduction to R and RStudio with some basic statistical vocabulary.\n\n\n\nCh. 1\nIMS introduces a lot of vocabulary. This book introduces most of that same vocabulary, but across multiple chapters.\n\n\nCh. 2\n\nThis book contains a specific introduction to R Markdown.\n\n\n\nCh. 2\nIMS discusses study design and sampling. Some of that information is scattered across multiple chapters of this book, but not all of it. (For example, this book doesn’t get into stratified or cluster sampling.)\n\n\n\nCh. 3\nIMS has “Applications” chapters at the end of each section. In this book, the applications are woven into each chapter.\n\n\nCh. 3\nCh. 4\nCategorical data.\n\n\nCh. 4\nCh. 5\nNumerical data.\n\n\nCh. 5\n\nThis book has a dedicated chapter on manipulating data using dplyr.\n\n\n\nCh. 6\nApplications.\n\n\nCh. 6\nCh. 7\nCorrelation.\n\n\nCh. 7\nCh. 7\nSimple linear regression.\n\n\n\nCh. 8\nMultiple regression—not covered in this book.\n\n\n\nCh. 9\nLogistic regression—not covered in this book.\n\n\n\nCh. 10\nApplications.\n\n\nCh. 8\nCh. 11\nIntroduction to randomization, Part 1—This book takes four chapters to cover the material that IMS covers in one chapter.\n\n\nCh. 9\nCh. 11\nIntroduction to randomization, Part 2.\n\n\nCh. 10\nCh. 11\nHypothesis testing with randomization, Part 1.\n\n\nCh. 11\nCh. 11\nHypothesis testing with randomization, Part 2.\n\n\nCh. 12\nCh. 12\nConfidence intervals.\n\n\nCh. 13\nCh. 13\nNormal models—This book takes two chapters to cover the material that IMS covers in one chapter.\n\n\nCh. 14\nCh. 13\nSampling distribution models.\n\n\n\nCh. 14\nIMS has a chapter on decision errors that was covered in this book back in Ch. 10. It also covers the concept of power, which is not covered in this book.\n\n\n\nCh. 15\nApplications.\n\n\nCh. 15\nCh. 16\nInference for one proportion.\n\n\nCh. 16\nCh. 17\nInference for two proportions.\n\n\nCh. 17\n\nChi-square goodness-of-fit test. (This is only covered in IMS in a standalone R tutorial appearing in Ch. 23.)\n\n\nCh. 18\nCh. 18\nChi-square test for independence.\n\n\nCh. 19\nCh. 19\nInference for one mean.\n\n\nCh. 20\nCh. 21\nInference for paired data.\n\n\nCh. 21\nCh. 20\nInference for two independent means.\n\n\nCh. 22\nCh. 22\nANOVA. This is the last chapter of this book.\n\n\n\nCh. 23\nApplications.\n\n\n\nCh. 24\nInference for linear regression with a single predictor.\n\n\n\nCh. 25\nInference for linear regression with multiple predictors.\n\n\n\nCh. 26\nInference for logistic regression.\n\n\n\nCh. 27\nApplications."
  }
]