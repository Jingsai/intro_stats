---
title: "7. Regression"
author: "Put your name here"
date: "Put the date here"
output:
    html_notebook:
        toc: yes
        toc_float: yes
---

<!-- Please don't mess with the next few lines! -->
<style>h5{font-size:2em;color:#0000FF}h6{font-size:1.5em;color:#0000FF}div.answer{margin-left:5%;border:1px solid #0000FF;border-left-width:10px;padding:25px} div.summary{background-color:rgba(30,144,255,0.1);border:3px double #0000FF;padding:25px}</style>`r options(scipen=999)`<p style="color:#ffffff">`r intToUtf8(c(50,46,48))`</p>
<!-- Please don't mess with the previous few lines! -->

::: {.summary}

### Functions introduced in this chapter

`geom_smooth`, `lm`, `tidy`, `augment`, `glance`

:::


## Introduction

In this chapter we will learn how to run a regression analysis. Regression provides a model for the linear relationship between two numerical variables.

### Install new packages

If you are using RStudio Workbench, you do not need to install any packages. (Any packages you need should already be installed by the server administrators.)

If you are using R and RStudio on your own machine instead of accessing RStudio Workbench through a browser, you'll need to type the following command at the Console:

```
install.packages("broom")
```

### Download the R notebook file

Check the upper-right corner in RStudio to make sure you're in your `intro_stats` project. Then click on the following link to download this chapter as an R notebook file (`.Rmd`).

<a href = "https://vectorposse.github.io/intro_stats/chapter_downloads/07-regression.Rmd" download>https://vectorposse.github.io/intro_stats/chapter_downloads/07-regression.Rmd</a>

Once the file is downloaded, move it to your project folder in RStudio and open it there.

### Restart R and run all chunks

In RStudio, select "Restart R and Run All Chunks" from the "Run" menu.

### Load packages

We load the `tidyverse` package. The `faraway` package will give access to the Chicago redlining data introduced in the previous chapter. Finally, the `broom` package will provide tools for cleaning up the output of the regression analysis we perform.

```{r}
library(tidyverse)
library(faraway)
library(broom)
```


## Regression

When we have a linear relationship between two numerical variables, we learned in the last chapter that we can compute the correlation coefficient. One serious limitation of the correlation coefficient is that it is only a single number, and therefore, it doesn't provide a whole lot of information about the nature of the linear relationship itself. It only gives clues as to the strength and direction of the association.

It will be helpful to model this linear relationship with an actual straight line. Such a line is called a *regression line*. It is also known as a *best-fit line* or *least-squares line* for reasons that we will get to later in the chapter.

The mathematics involved in figuring out what this line should be is more complicated than we cover in this book. Fortunately, R will do all the complicated calculations for us and we'll focus on understanding what they mean.

Recall the `chredlin` data set from the last chapter investigating the practice of redlining in Chicago in the 1970s. Let's review the scatterplot of `involact`, the number of FAIR policies per 100 housing units, against `race`, the racial composition of each ZIP code as a percentage of minority residents. (Recall that each row of the data represents an entire ZIP code.)

```{r}
ggplot(chredlin, aes(y = involact, x = race)) +
    geom_point()
```

##### Exercise 1

Does the Chicago redlining data come from an observational study or an experiment? How do you know?

::: {.answer}

Please write up your answer here.

:::

*****


If certain conditions are met, we can graph a regression line; just add a `geom_smooth` layer to the scatterplot:

```{r}
ggplot(chredlin, aes(y = involact, x = race)) +
    geom_point() +
    geom_smooth(method = lm, se = FALSE)
```

The `method = lm` argument is telling `ggplot` to use a "linear model". The `se = FALSE` argument tells `ggplot` to draw just the line and nothing else. (What else might it try to draw? You are encouraged to go back to the code above and take out `se = FALSE` to see for yourself. However, we are not yet in a position to be able to explain the gray band that appears. We will return to this mystery in a future chapter.)

Of all possible lines, the blue line comes the closest to each point in the scatterplot. If we wiggled the line a little bit, it might get closer to a few points, but the net effect would be to make it further from other points. This is the mathematically optimal line of best fit.


## Models

We used the word "model" when referring to the regression line above. What does that word mean in this context?

A model is something that represents something else, often on a smaller scale or in simplified form. A model is often an idealized form of something that may be quite messy or complex in reality. In statistics, a model is a representation of the way data is generated. For example, we may believe that as minority representation increases in a neighborhood, that neighborhood is more likely to be subject to racially discriminatory practices. We may even posit that the relationship is linear; i.e., for every percentage point increase in racial minorities, we expect some kind of proportional increase in racial discrimination, as measured in this case by FAIR policies. We say that this is our hypothesis about the *data-generating process*: we suspect that the data we see results from a sociological process that uses the minority representation of a neighborhood to generate data about FAIR policies.

The assumption of a linear relationship between these two quantities is just that---an assumption. It is not necessarily "true", whatever "true" might mean in this kind of question. It is a convenient device that makes a simplifying assumption in order to allow us to do something meaningful in a statistical analysis. If such a model---despite its simplifying caricature---helps us make meaningful predictions to study something important like racial discrimination, then the model is useful.

The first thing we acknowledge when working with a model is that the model does not generate the data in a rigid, deterministic way. If you look at the scatterplot above, even assuming the blue line represents a "correct" data-generating process, the data points don't fall on the blue line. The blue line gives us only a sense of where the data might be, but there is additional space between the line and the points. These spaces are often referred to as *errors*. In statistics, the word "error" does not mean the same thing as "mistake". Error is just the difference between an idealized model prediction and the real location of data. In the context of linear regression, we will use the term *residual* instead. After the model is done making a prediction, the residuals are "left over" to account for the different between the model and the actual data.

The most important thing to remember about models is that they aren't real. They are idealizations and simplifications. The degree to which we can trust models, then, comes down to certain assumptions we make about the data-generating process. These assumptions cannot be completely verified---after all, we will never know the exact data-generating process. But there are certain *conditions* we can check to know if the assumptions we make are reasonable.

##### Exercise 2

Do an internet search for the phrase "statistical model" and/or "statistical modeling". Read at least two or three sources. List below one important aspect of statistical modeling you find in your search that wasn't mentioned in the paragraphs above. (Some of the sources you find may be a little technical. You should, for now, skip over the technical explanations. Try to find several sources that address the issue in non-technical ways. The additional information you mention below should be something non-technical that you understand.)

::: {.answer}

Please write up your answer here.

:::

*****


## Checking conditions

We need to be careful here. Although we graphed the blue regression line above, we have not checked any conditions. Therefore, it is inappropriate to fit a regression line at this point. Once the line is seen, it cannot easily be "unseen", and it's crucial that you don't trick your eyes into believing there is a linear relationship before checking the conditions that justify that belief. 

The regression line we saw above makes no sense unless we know that regression is appropriate. The conditions for running a regression analysis include all the conditions you checked for a correlation analysis in the last chapter:

1. The two variables must be numerical.
2. There is a somewhat linear relationship between the variables, as shown in a scatterplot.
3. There are no serious outliers.


##### Exercise 3

Check these three conditions for the regression between `involact` and `race` (using the scatterplot above for conditions (2) and (3).)

::: {.answer}

1.
2.
3.

:::

*****


However, there is an additional condition to check to ensure that our regression model is appropriate. It concerns the residuals, but as we haven't computed anything yet, we have nothing to analyze. We'll return to this condition later.


## Calculating the regression line

What is the equation of the regression line? In your algebra class you learned that a line takes the form $y = mx + b$ where $m$ is the slope and $b$ is the y-intercept. Statisticians write the equation in a slightly different form:

$$
\hat{y} = b_{0} + b_{1} x
$$

The intercept is $b_{0}$ and the slope is $b_{1}$. We use $\hat{y}$ (pronounced "y hat") instead of $y$ because when we plug in values of $x$, we do not get back the exact values of $y$ from the data. The line, after all, does not actually pass through most (if any) actual data points. Instead, this equation gives us "predicted" values of $y$ that lie on the regression line. These predicted $y$ values are called $\hat{y}$.

To run a regression analysis and calculate the values of the intercept and slope, we use the `lm` command in R. (Again, `lm` stands for "linear model".) This command requires us to specify a "formula" that tells R the relationship we want to model. It uses special syntax in a very specific order:

- The response variable,
- a "tilde" ~ (this key is usually in the upper-left corner of your keyboard, above the backtick),
- the predictor variable.

After a comma, we then specify the data set in which those variables live using `data =`. Here's the whole command:

```{r}
lm(involact ~ race, data = chredlin)
```

**The response variable always goes before the tilde and the predictor variable always goes after.**

Let's store that result for future use. The convention we'll use in this book is to name things using the variables involved. For example,

```{r}
involact_race_lm <- lm(involact ~ race, data = chredlin)
involact_race_lm
```

The variable `involact_race_lm` now contains all the information we need about the linear regression model.


## Interpreting the coefficients

Look at the output of the `lm` command above.

The intercept is 0.12922 and the slope is 0.01388. The number 0.12922 is labeled with `(Intercept)`, so that's pretty obvious. But how do we know the number 0.01388 corresponds to the slope? Process of elimination, I suppose. But there's another good reason too. The equation of the regression line can be written

$$
\hat{y} = 0.12922 + 0.01388 x
$$

When we report the equation of the regression line, we typically use words instead of $\hat{y}$ and $x$ to make the equation more interpretable in the context of the problem. For example, for this data, we would write the equation as

$$
\widehat{involact} = 0.12922 + 0.01388 race
$$

The slope is the *coefficient* of `race`, or the number attached to `race`. (The intercept is not attached to anything; it's just a constant term out front there.)

The slope $b_{1}$ is always interpretable. This model predicts that one unit of increase in the x-direction corresponds to a change of 0.01388 units in the y-direction. Let's phrase it this way:

> The model predicts that an increase of one percentage point in the composition of racial minorities corresponds to an increase of 0.01388 new FAIR policies per 100 housing units.

The intercept $b_{0}$ is a different story. There is always a literal interpretation:

> The model predicts that a ZIP code with 0% racial minorites will generate 0.12922 new FAIR policies.

In some cases (rarely), that interpretation might make sense. In most cases, though, it is physically impossible for the predictor variable to take a value of 0, or the value 0 is way outside the range of the data. Whenever we use a model to make a prediction outside of reasonable values, we call that *extrapolation*.

For the Chicago data, we likely don't have a case of extrapolation. While it is not literally true that any ZIP code has 0% racial minorities, we can see in the scatterplot that there are values very close to zero.

##### Exercise 4

Use the `arrange` command from `dplyr` to sort the `chredlin` data frame by race (using the default ascending order). What is the value of `race` for the three ZIP codes with the smallest percentage of minority residents?

::: {.answer}

```{r}
# Add code here to sort by race
```

Please write up your answer here.

:::

*****

Again, even though there are no ZIP codes with 0% racial minorities, there are a bunch that are close to zero, so the literal interpretation of the intercept is also likely a sensible one in this case.

##### Exercise 5

Let's think through something else the intercept might be telling us in this case. The presumption is that FAIR policies are obtained mostly by folks who can't get insurance policies in other ways. Some of that is driven by racial discrimination, but maybe not all of it. What does the intercept have to say about the number of FAIR policies that are obtained *not* due to denial of coverage from racial discrimination?

::: {.answer}

Please write up your answer here.

:::


## Rescaling to make interpretations more meaningful

Let's revisit the interpretation of the slope:

> The model predicts that an increase of one percentage point in the composition of racial minorities corresponds to an increase of 0.01388 new FAIR policies per 100 housing units.

This is a perfectly correct statement, but one percentage point change is not very much. It's hard to think about comparing two neighborhoods that differ by only one percent. This scale also makes the predicted change in the response variable hard to interpret. How many policies is 0.01388 per 100 housing units?

One way to make these kinds of statements more interpretable is to change the scale. What if we increase 10 percentage points instead of only 1 percentage point? In other words, what if we move 10 times as far along the x-axis. The repsonse variable will also have to move 10 times as far. This is the new statement:

> The model predicts that an increase of 10 percentage points in the composition of racial minorities corresponds to an increase of 0.1388 new FAIR policies per 100 housing units.

In this case, the decimal 0.1388 is maybe still not completely clear, but at least an increase of 10 percentage points is a meaningful difference between neighborhoods.

##### Exercise 6

Since the last number is a *per capita* type measure, we can also rescale it. If the model predicts an increase in 0.1388 new FAIR policies per 100 households (corresponding to 10 percentage points increase in racial minorities), how many FAIR policies would that be in 1000 households?

::: {.answer}

Please write up your answer here.

:::


## The `tidy` command

Recall the output of the `lm` command:

```{r}
involact_race_lm
```

(We did not have to run `lm` again. We had this output stored in the variable `involact_race_lm`.)

That summary is fine, but what if we needed to reference the slope and intercept using inline code? Or what if we wanted to grab those numbers and use them in further calculations?

The problem is that the results of `lm` just print the output in an unstructured way. If we want structured input, we can use the `tidy` command from the `broom` package. This will take the results of `lm` and organize the output into a tibble.

```{r}
tidy(involact_race_lm)
```

Let's store that tibble so we can refer to it in the future.

```{r}
involact_race_tidy <- tidy(involact_race_lm)
involact_race_tidy
```

The intercept is stored in the `estimate` column, in the first row. The slope is stored in the same column, but in the second row.

We can grab the `estimate` column with the dollar sign as we've seen before:

```{r}
involact_race_tidy$estimate
```

This is a "vector" of two values, the intercept and the slope, respectively.

What if we want only one value at a time? We can grab individual elements of a vector using square brackets as follows:

```{r}
involact_race_tidy$estimate[1]
```

```{r}
involact_race_tidy$estimate[2]
```

Here is the interpretation of the slope again, but this time, we'll use inline code:

> The model predicts that an increase of 1 percentage points in the composition of racial minorities corresponds to an increase of `r involact_race_tidy$estimate[2] ` new FAIR policies per 100 housing units.

Click somewhere inside the backticks on the line above and hitCtrl-Enter or Cmd-Enter (PC or Mac respectively). You should see the number 0.01388235 pop up.

What if we want to apply re-scaling to make this number more interpretable? The stuff inside the inline code chunk is just R code, so we can do any kind of calculation with it we want.

> The model predicts that an increase of 10 percentage points in the composition of racial minorities corresponds to an increase of `r 10 * involact_race_tidy$estimate[2] ` new FAIR policies per 100 housing units.


## Residuals

Residuals are the vertical distances from each data point to the regression line. More formally, we're saying that the residual $e$ is given by the following formula:

$$e = y - \hat{y}.$$

We know that some of the points are going to lie above the line (positive residuals) and some of the points will lie below the line (negative residuals). What we need is for there not to be any pattern among these residuals.

To calculate the residuals, we introduce a new function from the `broom` package. Whereas `tidy` serves up information about the parameters of interest (in this case, the intercept and the slope of the regression line), `augment` gives us extra information for each data point.

```{r}

```

The first two columns are the actual data values we started with. But now we've "augmented" the original data with some new stuff too. The third column---here called `.fitted`---is $\hat{y}$, or the point on the line that corresponds to the given $x$ value. Let's check and make sure this is working as advertised.

The regression equation from above is

$$\widehat{weight} = -44.77 + 10.81 wrist.$$

The first subject listed in row 1 has wrist diameter 10.4 cm. Plug that value into the equation above:

$$\widehat{weight} = -44.77 + 10.81(10.4) = 67.6.$$

The model predicts that a person with a wrist diameter of 10.4 will weigh 67.6 kg. The first number in the `.fitted` column is 67.60664, so that's correct.

Now skip over to the fifth column of the `augment` output, the one that says `.resid`. If this is the residual $e$, then it should be $y - \hat{y}$. Since $y$ is the actual value of `wgt` and $\hat{y}$ is the value predicted by the model, we should get for the first row of output

$$e = y - \hat{y} = 65.6 - 67.60664 = -2.00664.$$

Yup, it works!

Skip to the last column of the output `.std.resid`. (You may have to click on the little black arrow to scroll all the way to the right.) These are similar to z-scores for the residuals, so they're easy to interpret.^[The real story is quite a bit more complicated than that, but there is little harm in thinking of these as if they were z-scores.] One can examine either the raw residuals (stored in `.resid`) or the standardized residuals (`.std.resid`), but we'll do the latter.

To check for patterns in the residuals, we'll use several different plots. First, we want our residuals to be normally distributed. We check this with a histogram and a QQ plot, as usual.

```{r}

```

```{r}

```

We see that the shape is mostly normal. There's a little bit of skew in the histogram, but nothing alarming.

We should also create a *residual plot*, which looks at the residuals above each value along the x-axis. (In the command below, we also add a horizontal reference line so that it is clear which points have positive or negative residuals.)

```{r}

```

This looks good. There are no systematic patterns in the residuals. A residual plot should look like the most boring plot you've ever seen.

Residual patterns that are problematic often involve curved data (where the dots follow a curve around the horizontal reference line instead of spreading evenly around it) and *heteroscedasticity*, which is a fanning out pattern.

As an example of the latter, let's look at the residual plot for the tipping example from the correlation chapter.

```{r}

```

The residuals are quite small toward the left end of the residual plot, and then spread out and get larger toward the right end. This is a violation of the "patterns in the residuals" condition. It would be problematic to pursue a regression analysis of the tip data even though correlation was okay to report.


## $R^2$

The correlation coefficient R is of limited utility. The number doesn't have any kind of intrinsic meaning; it can only be judged by how close it is to 0 or 1 (or -1) in conjunction with a scatterplot to give you a sense of the strength of the correlation. In particular, some people try to interpret R as some kind of percentage, but it's not.

On the other hand, $R^2$ can be interpreted as a percentage. It represents the percent of variation in the y variable that can be explained by variation in the x variable.

Here we introduce the last of the `broom` functions: `glance`. Whereas `tidy` reports summary statistics related to parameters of the model, and `augment` reports values associated to each data point separately, the `glance` function gathers up summaries for the entire model.

```{r}

```

A more advanced statistics course might discuss the other model summaries present in the `glance` output. The $R^{2}$ value is stored in `wgt_wrist_glance$r.squared`. Its value is 0.585. We will word it this way:

> of the variability in weight can be explained by variability in wrist diameter.

Thus, $R^2$ is a measure of the fit of the model. High values of $R^2$ mean that the line predicts the data values closely, whereas lower values of $R^2$ mean that there is still a lot of variability left in the residuals (presumably due to other factors that are not measured here). What we're saying here is that there are lots of factors that account for variability in weight, and not all that can be explained by the variability in wrist diameter.


## Inference for the regression slope

The sample gives us the regression equation

$$\hat{y} = b_{0} + b_{1} x.$$

The idea of inference is that this line is meant to be an estimate of a true regression line

$$\hat{y} = \beta_{0} + \beta_{1} x.$$

In other words, if we plotted weight against wrist diameters for everybody in the world, there is, in theory, some perfect regression line that goes through the middle of all those points. And that ideal intercept and slope are the true population parameters $\beta_{0}$ and $\beta_{1}$. ($\beta$ is the Greek letter "beta".)

We have already seen that the intercept is not particularly interesting, so we restrict attention to inference for the slope. As with correlation, the full inferential rubric for the regression slope is a little overkill. Nevertheless, the rubric forces us to be careful to identify our sample and population, establish hypotheses, check conditions, and state proper conclusions.

The sampling distribution for the slope parameter is somewhat complicated. There is a formula for the standard error of the sample slope estimates, and with that, one can compute t scores that are distributed as a t model with $n - 2$ degrees of freedom. We won't get into any of the mathematical details here.

Also note that a typical regression analysis will start by checking conditions so that the regression line can be calculated and graphed. Therefore, we will work through the rubric under the assumption that the conditions have already been checked.



## Your turn

##### Exercise 1

Run the `lm` command and then use `tidy`, `augment`, and `glance` respectively on the output. (It's technically incorrect to run regression before checking conditions, but we need the output of `lm` in order to check those conditions.)

::: {.answer}

```{r}
# Add code here to generate regression output with lm
```

```{r}
# Add code here to "tidy" the output from lm
```

```{r}
# Add code here to "augment" the output from lm
```

```{r}
# Add code here to "glance" at the output from lm
```

:::

##### Exercise 2

Check conditions for regression.

::: {.answer}

```{r}
# Add code here to check conditions for regression
```

Please write up your answer here.

:::

##### Exercise 3

If the conditions are met, plot the regression line on top of a scatterplot of the data.

::: {.answer}

```{r}
# Add code here to plot the regression line
```

:::

##### Exercise 4

Write the regression equation mathematically (enclosing your answer in double dollar signs as above), using contextually meaningful variable names.

::: {.answer}

Please write up your answer here.

:::

##### Exercise 5

Interpret the coefficients: interpret the slope, give a literal interpretation of the intercept, and then comment on the appropriateness of that interpretation.

::: {.answer}

Please write up your answer here.

:::

##### Exercise 6

Report and interpret $R^2$.

::: {.answer}

Please write up your answer here.

:::

*****

Now we will run the full rubric for inference on the slope parameter.


##### Exploratory data analysis

###### Use data documentation (help files, code books, Google, etc.), the `str` command, and other summary functions to understand the data.

::: {.answer}

```{r}
# Add code here to understand the data.
```

:::

###### Prepare the data for analysis. [Not always necessary.]

::: {.answer}

```{r}
# Add code here to prepare the data for analysis.
```

:::

###### Make tables or plots to explore the data visually.

::: {.answer}

```{r}
# Add code here to make tables or plots.
```

:::


##### Hypotheses

###### Identify the sample (or samples) and a reasonable population (or populations) of interest.

::: {.answer}

Please write up your answer here.

:::

###### Express the null and alternative hypotheses as contextually meaningful full sentences.

::: {.answer}

$H_{0}:$ Null hypothesis goes here.

$H_{A}:$ Alternative hypothesis goes here.

:::

###### Express the null and alternative hypotheses in symbols (when possible).

::: {.answer}

$H_{0}: math$

$H_{A}: math$

:::


##### Model

###### Identify the sampling distribution model.

::: {.answer}

Please write up your answer here.

:::

###### Check the relevant conditions to ensure that model assumptions are met.

::: {.answer}

Please write up your answer here. (Some conditions may require R code as well.)

:::


##### Mechanics

###### Compute the test statistic.

::: {.answer}

```{r}
# Add code here to compute the test statistic.
```

:::

###### Report the test statistic in context (when possible).

::: {.answer}

Please write up your answer here.

:::

###### Plot the null distribution.

::: {.answer}

```{r}
# Add code here to plot the null distribution.
```

:::

###### Calculate the P-value.

::: {.answer}

```{r}
# Add code here to calculate the P-value.
```

:::

###### Interpret the P-value as a probability given the null.

::: {.answer}

Please write up your answer here.

:::


##### Conclusion

###### State the statistical conclusion.

::: {.answer}

Please write up your answer here.

:::

###### State (but do not overstate) a contextually meaningful conclusion.

::: {.answer}

Please write up your answer here.

:::

###### Identify the possibility of either a Type I or Type II error and state what making such an error means in the context of the hypotheses.

::: {.answer}

Please write up your answer here.

:::


##### Confidence interval

###### Check the relevant conditions to ensure that model assumptions are met.

::: {.answer}

Please write up your answer here. (Some conditions may require R code as well.)

:::

###### Calculate the confidence interval.

::: {.answer}

```{r}
# Add code here to calculate the confidence interval.
```

:::

###### State (but do not overstate) a contextually meaningful interpretation.

::: {.answer}

Please write up your answer here.

:::

###### If running a two-sided test, explain how the confidence interval reinforces the conclusion of the hypothesis test.

::: {.answer}

Please write up your answer here.

:::
